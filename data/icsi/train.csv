Source,Summary
"Are we on? We're on. O_K. Is it on? Yeah. Yeah. O_K  so  uh  we haven't sent around the agenda. So  One  two - u- O_K . Why is it so cold in here? i- uh  any agenda items anybody has  wants to talk about  what's going on? I c- I could talk about the meeting. Does everyone - has everyone met Don? Yeah. Yeah? O_K. Yeah. Now  yeah. It's on? Hello. Yeah. Yeah. O_K  agenda item one  introduce Don. O_K  we did that. Uh - We went - Well  I had a - just a quick question but I know there was discussion of it at a previous meeting that I missed  but just about the - the wish list item of getting good quality close-talking mikes on every speaker. O_K  so let's - let's - So let's just do agenda building right now. O_K  so let's talk about that a bit. I mean  that was - Uh  @@ tuss- close talking mikes  better quality. O_K  uh  we can talk about that. You were gonna - starting to say something? Well  you - you  um  already know about the meeting that's coming up and I don't know if - if this is appropriate for this. I don't know. I mean  maybe - maybe it's something we should handle outside of the meeting. What meeting? No  no  that's O_K. We can - so - we can ta- so n- NIST is - NIST folks are coming by next week and so we can talk about that. O_K. Yeah. Who's coming? I think Uh  uh  John Fiscus and  uh  I think George Doddington will be Mm-hmm. around as well. Uh  O_K  so we can talk about that. Uh  I guess just hear about how things are going with  uh  uh  the transcriptions. That's right. That would sorta be an obvious thing to discuss. Sure. Mm-hmm. Um  An- anything else  uh  strike anybody? Uh  we started running recognition on one conversation but it's the r- isn't working yet. So  O_K. But if anyone has - Wha- uh  the main thing would be if anyone has  um  knowledge about ways to  uh  post-process the wave forms that would give us better recognition  that would be helpful to know about. Um  Dome yeah  it sounds like a topic of conversation. Yeah  so  uh - What about  uh  is there anything new with the speech  nonspeech Yeah   we're working more on it but  it's not finished. stuff? O_K. Alright  that seems like a - a good collection of things. And we'll undoubtedly think of other things. I had thought under my topic that I would mention the  uh  four items that I - I  uh  put out for being on the agenda f- on that meeting  which includes like the pre-segmentation and the - and the developments in multitrans. Oh  under the NIST meeting. Yeah  under the NIST thing. Yeah. O_K. Alright  why don't we start off with this  u- u- I guess the order we brought them up seems fine. Um  Yeah. so  better quality close talking mikes. So the one issue was that the - the  uh  lapel mike  uh  isn't as good as you would like. And so  uh  it - it'd be better if we had close talking mikes for everybody. Right? Is that - is that basically the point? Ri- um  yeah  the - And actually in addition to that  that the - the close talking mikes are worn in such a way as to best capture the signal. And the reason here is just that for the people doing work not on microphones but on sort of like dialogue and so forth  uh - or and even on prosody  which Don is gonna be working on soon  it adds this extra  you know  vari- variable for each speaker to - to deal with when the microphones aren't similar. Right. Mm-hmm. So - And I also talked to Mari this morning and she also had a strong preference for doing that. And in fact she said that that's useful for them to know in starting to collect their data too. Mm-hmm. Right  so one th- Well  so - uh  well one thing I was gonna say was that  um  i- we could get more  uh  of the head mounted microphones even beyond the number of radio channels we have because I think whether it's radio or wire is probably second-order. And the main thing is having the microphone close to you  u- although  not too close. Mm-hmm. Right  so  uh  actually the way Jose is wearing his is - is c- correct. The good way. So you want to - Yeah. Is - Yeah. I- it's not cor- it's correct? Is. Yeah  th- that's good. So it's towards the corner of your mouth so that breath sounds don't get on it. And then just sort of Yeah. Yes. Yeah. Yeah. Yeah. about  uh  a thumb or - a thumb and a half away from your - from your mouth. Yeah. Uh  yeah. Right. How am I d- But we have more than one type of - I mean  for instance  you're - Yeah. And this one isn't very adjustable  so this about as good as I can get cuz it's a fixed boom. Yeah. Right. Yeah. Is fixed. Yeah. Yeah. But if we could actually standardize  you know  the - the microphones  uh  as much as possible that would be really helpful. Mm-hmm. Yeah. Mm-hmm. Well  I mean it doesn't hurt to have a few extra microphones around  so why don't we just go out and - and get an order of - of if this microphone seems O_K to people  Yeah. uh  I'd just get a half dozen of these things. Well the onl- the only problem with that is right now  um  some of the Jimlets aren't working. The little - the boxes under the table. Yeah. And so  w- Uh  I've only been able to find three jacks that are working. Yeah. Can we get these  wireless? So - No  but my point is - But y- we could just record these signals separately and time align them with the start of the meeting. R- r- right - I - I'm not sure I'm follow. Say that again? Right now  we've got  uh  two microphones in the room  that are not quote-unquote standard. So why don't we replace those - O_K  just two. Well  however many we can plug in. You know  if we can plug in three  let's plug in three. Also what we've talked before about getting another  uh  radio  O_K. Mm-yeah. Right. and so then that would be  you know  three more. Right. O_K. Mm-hmm. So  uh - so we should go out to our full complement of whatever we can do  but have them all be the same mike. I think the original reason that it was done the other way was because  it w- it was sort of an experimental thing and I don't think anybody knew whether people would rather have more variety or - Right. or  uh  more uniformity  but - @@ but uh  sounds - sounds fine. Sounds like uniformity wins. Right . Yeah. Well  for short term research it's just - there's just so much effort that would have to be done up front n- uh  so - yeah  uniformity would be great. Well - Yeah. Is it because - You - you're saying the - for dialogue purposes  so that means that the transcribers are having trouble with those mikes? Is that what you mean? Or - ? Well Jane would know more about the transcribers. And that's true. I mean  I - we did discuss this. Uh  and - and - Yep. Couple times. a couple times  so  um  yeah  the transcribers notice - And in fact there're some where  um - ugh well  I mean there's - it's the double thing. It's the equipment and also how it's worn. And he's always - they always - they just rave about how wonderful Adam's - Adam's channel is. Right. What can I say. And then  So does the recognizer. Yeah. Oh  really? Yeah  I'm not surprised. I mean  ""Baaah!"" Yeah. Yeah  but I mean it's not just that  it's also you know you- Even if - if you're talking on someone else's mike it's still you w- Yeah. It's also like n- no breathing  no - You know  it's like it's - Yeah. it's um  it's really - it makes a big difference from the transcribers' point of view and also from the research s- point of view. Yeah. Yeah  it's an advantage when you don't breath. Yeah  I think that the point of doing the close talking mike is to get a good quality signal. We're not doing research on close talking mikes. Right. When we're doing - Yeah. So we might as well get it as uniform as we can. Yeah. Right. Now  this is locking the barn door after the horse was stolen. We do have thirty hours  of - of speech  which is done this way. But - Yeah. That's O_K. but  uh  yeah  for future ones we can get it a bit more uniform. Great  great. So I think just do a field trip at some point. Yeah  probably - yeah  to the store we talked about and that - Yep. And there was some talk about  uh  maybe the h- headphones that are uncomfortable for people  to - Yep. So  as - as I said  we'll do a field trip and see if we can get all of the same mike that's more comfortable than - than these things  which I think are horrible. O_K. So. Good. Great  thank you very much. It's makes our job a lot easier. Especially for people with big heads. O_K. And  you know  we're researchers  so we all have big heads. O_K. Yeah. O_K. Yeah. Uh  O_K  second item was the  uh  NIST visit  and what's going on there. Yeah. O_K  so  um  uh  Jonathan Fiscus is coming on the second of February and I've spoken with  uh  u- u- a lot of people here  not everyone. Um  and  um  he expressed an interest in seeing the room and in  um  seeing a demonstration of the modified multitrans  which I'll mention in a second  and also  um  he was interested in the pre-segmentation and then he's also interested in the transcription conventions. Mm-hmm. And  um - So  um  it seems to me in terms of like  um  i- i- it wou- You know  O_K. So the room  it's things like the audio and c- and audi- audio and acoustic - acoustic properties of the room and how it - how the recordings are done  and that kind of thing. And  um. O_K  in terms of the multi-trans  well that - that's being modified by Dave Gelbart to  uh  handle multi-channel recording. Oh  I should've - I was just thinking I should have invited him to this meeting. I forgot to do it. Yeah  O_K. So. Yeah. Sorry. Yeah. Well that's O_K  I mean we'll - Yeah  and it's t- and it looks really great. He - he has a prototype. I - I  uh  @@ didn't - didn't see it  uh  yesterday but I'm going to see it today. And  uh  that's - that will enable us to do nice um  tight time marking of the beginning and ending of overlapping segments. At present it's not possible with limitations of - of the  uh  original design of the software. And um. So  I don't know. In terms of  like  pre-segmentation  that - that continues to be  um  a terrific asset to the - to the transcribers. Do you - I know that you're al- also supplementing it further. Do you want to mention something about that c- Thilo  or - ? Um  yeah. What - what I'm doing right now is I'm trying to include some information about which channel  uh  there's some speech in. But that's not working at the moment. I'm just trying to do this by comparing energies  uh - O_K. normalizing energies and comparing energies of the different channels. And so to - to give the transcribers some information in which channel there's - there's speech in addition to - to the thing we - we did now which is just  uh  speech-nonspeech detection on the mixed file. This is good. Mm-hmm. So I'm - I'm relying on - on the segmentation of the mixed file but I'm - I'm trying to subdivide the speech portions into different portions if there is some activity in - in different channels. Excellent  so this'd be like w- e- providing also speaker I_D potentially. Wonderful. But - Yeah. Yeah. Wonderful. Um  something I guess I didn't put in the list but  uh  on that  uh  same day later on in - or maybe it's - No  actually it's this week  uh  Dave Gelbart and I will be  uh  visiting with John Canny who i- you know  is a C_S professor  Oh. H_C_C . who's interested in ar- in array microphones. Oh  he's doing array mikes. Yeah. And so we wanna see what commonality there is here. You know  maybe they'd wanna stick an array mike here when we're doing things or - or maybe That would be cool. Yeah  that would be neat. Yeah. That would be really neat. it's - it's not a specific array microphone they want but they might wanna just  - uh  you know  you could imagine them taking the four signals from these - these table mikes and trying to do something with them - Um  I also had a discussion - So  w- uh  we'll be over - over there talking with him  um  after class on Friday. Um  we'll let you know what - what goes with that. Also had a completely unrelated thing. I had a  uh  discussion today with  uh  Birger Kollmeier who's a  uh  a German  uh  scientist who's got a fair sized group doing a range of things. It's sort of auditory related  largely for hearing aids and so on. But - but  uh  he does stuff with auditory models and he's very interested in directionality  and location  and - and  uh  head models and microphone things. And so  uh  he's - he and possibly a student  there w- there's  uh  a student of his who gave a talk here last year  uh  may come here  uh  in the fall for  uh  sort of a five month  uh  sabbatical. So he might be around. Get him to give some talks and so on. But anyway  he might be interested in this stuff. Mm-hmm. That - that reminds me  I had a - a thought of an interesting project that somebody could try to do with the data from here  either using  you know  the - the mikes on the table or Mm-hmm. using signal energies from the head worn mikes  and that is to try to construct a map of where people were sitting  Right. Uh-huh. Well Dan - Dan had worked on that. Dan Ellis  yeah. uh  based on - Uh-huh. Oh  did he? Oh  that's interesting. So that - that's the cross-correlation stuff  was - Yeah. was doing b- beam-forming. And so you could plot out who was sitting next to who and - A little bit  I mean  he didn't do a very extreme thing but just - it was just sort of Yeah  yeah. No  he did start on it. e- e- given that  the - the - the block of wood with the - the - the two mikes on either side  Mm-hmm. if I'm speaking  or if you're speaking  or someone over there is speaking  it - if you look at cross-correlation functions  you end up with a - Yeah. if - if someone who was on the axis between the two is talking  then you - you get a big peak there. And if - if someone's talking on - on - on  uh  one side or the other  it goes the other way. And then  Mm-hmm. uh  it - it - it even looks different if th- t- if the two - two people on either side are talking than if one in the middle. It - it actually looks somewhat different  so. Hmm. Well I was just thinking  you know  as I was sitting here next to Thilo that Yeah. um  when he's talking  my mike probably picks it up better than your guys's mikes. So if you just looked at - Yeah. Oh  that's another cl- cue  that's true. Yeah. yeah  looked at the energy on my mike and you could get an idea about who's closest to who. Yeah. Mm-hmm. Yeah. Right. Yeah. And - Or who talks the loudest. Yeah. Yeah. Yeah  well you have to - the appropriate normalizations are tricky  and - and - and are probably the key. Yeah. Yeah. You just search for Adam's voice on each individual microphone  you pretty much know where everybody's sitting. Yeah. Yeah. Yeah. We've switched positions recently so you can't - Anyway. O_K. So those are just a little couple of news items. Can I ask one thing? Uh  so  um  Jonathan Fiscus expressed an interest in  uh  microphone arrays. Yes. Um  is there - I mean - b- And I also want to say  his - he can't stay all day. He needs to uh  leave for - uh  from here to make a two forty-five flight Oh  so just morning. from - from Oakland. Right. So it makes the scheduling a little bit tight but do you think that  um - that  uh  i- John Canny should be involved in this somehow or not. I have no idea. Probably not but I - I'll - I'll - I'll know better after I see him this Friday what - what kind of level he wants to get involved. It's premature. Fine. Good. Uh  he might be excited to and it might be very appropriate for him to  uh  or he might have no interest whatsoever. I - I just really don't know. O_K. Is he involved in - Ach! I'm blanking on the name of the project. NIST has - has done a big meeting room - instrumented meeting room with video and microphone arrays  and very elaborate software. Is - is he the one working on that? Well that's what they're starting up. O_K. Yeah. No  I mean  that's what all this is about. They - they haven't done it yet. O_K. I had read some papers that looked like they had already done some work. They wanted to do it - Uh  well I think they've instrumented a room but I don't think they - they haven't started recordings yet. They don't have the t- the transcription standards. They don't have the - Are they going to do video as well? Hmm. Yeah. Hmm. I think. I think they are. Oh  cuz what - what I had read was  uh  they had a uh very large amount of software infrastructure for coordinating all this  both in terms of recording and also live room where you're interacting - the participants are interacting with the computer  and with the video  and lots of other stuff. Well  I'm - I'm - I'm not sure. All - all I know is that they've been talking to me about a project that they're going to start up So. O_K. Well - recording people meet- in meetings. And  uh  it is related to ours. They were interested in ours. They wanted to get some uniformity with us  uh  about the transcriptions and so on. Alright. And one - one notable difference - u- u- actually I can't remember whether they were going to routinely collect video or not  but one - one  uh  difference from the audio side was that they are interested in using array mikes. So  um  I mean  I'll just tell you the party line on that. The reason I didn't go for that here was because  uh  the focus  uh  both of my interest and of Adam's interest was uh  in impromptu situations. And we're not recording a bunch of impromptu situations but that's because it's different to get data for research than to actually apply it. And so  uh  for scientific reasons we thought it was good to instrument this room as we wanted it. Hmm. But the thing we ultimately wanted to aim at was a situation where you were talking with  uh  one or more other people i- uh  in - in an p- impromptu way  where you didn't - didn't actually know what the situation was going to be. And therefore it would not - it'd be highly unlikely that room would be outfitted with - with some very carefully designed array of microphones. Um  so it was only for that reason. It was just  you know  yet another piece of research and it seemed like we had enough troubles just - So there's no like portable array of mikes? No. So there's - there's - Hmm. uh  there's a whole range of things - there's a whole array of things  that people do on this. So  um  the  uh - the big arrays  uh  places  uh  like uh  Rutgers  and Brown  and other - other places  uh  they have  uh  big arrays with  I don't know  a hundred - hundred mikes or something. And so there's a wall of mikes. And you get Xerox. Wow. really  really good beam-forming with that sort of thing. And it's - and  um  in fact at one point we had a - a proposal in with Rutgers where we were gonna do some of the sort of per channel signal-processing and they were gonna do the multi-channel stuff  but it d- it d- we ended up not doing it. But - I've seen demonstrations of the microphone arrays. It's amazing Yeah  it's r- how - how they can cut out noise. It's really neat stuff. And then they had the little ones  yeah. And then they have little ones too but I mean - but they don't have our block of wood  right? Yeah  our block of wood is unique. But the- But the- No  there are these commercial things now you can buy that have four mikes or something and - and  uh  Yeah. Mm-hmm. um - So  yeah  there's - there's - there's a range of things that people do. Huh. Um  so if we connected up with somebody who was interested in doing that sort of thing that's - that's a good thing to do. I mean  whenever I've described this to other people who are interested on the - with the acoustic side that's invariably the question they ask. Just like someone who is interested in the general dialogue thing will always ask ""um  are you recording video? "" Right  right. Um  right? And - and the acoustic people will always say  ""well are you doing  uh  uh  array microphones?"" So it's - it's a good thing to do  but it doesn't solve the problem of how do you solve things when there's one mike or at best two mikes in - in this imagined P_D_A that we have. So maybe - maybe we'll do some more of it. Well one thing I - I mean  I don't know. I mean  I know that having an array of - I mean  I would imagine it would be more expensive to have a - an array of microphones. But couldn't you kind of approximate the natural sis- situation by just shutting off uh  channels when you're - later on? I mean  it seems like if the microphones don't effect each other then couldn't you just  you know  record them with an array and then just not use all the data? It's - it's just a lot of infrastructure that I see. Fine. for our particular purpose we felt we didn't need to set up. Yeah. O_K. Yeah  if ninety-nine percent of what you're doing is c- is shutting off most of the mikes  then going through the - But if you get somebody who's - who - who has that as a primary interest then that put - then that drives it in that direction. That's right  I mean if someone - if someone came in and said we really want to do it  Right. I mean  we don't care. That would be fine  Buy more disk space. So to save that data you - You have to have one channel recording per mike in the array? Is that - Well  uh  at some level - at some level. But then  you know  there's - it - I usually do a mix. there's - What you save  I mean  if you're going to do research with it. yeah There's - I - I don't know what they're going to do and I don't know how big their array is. Obviously if you were gonna save all of those channels for later research you'd use up a lot of space. Yeah. Hmm. And  th- Well their software infrastructure had a very elaborate design for plugging in filters  and mixers  and all sorts of processing. So that they can do stuff in real time and not save out each channel individually. Yeah. Mmm. Yeah. So it was  uh - Yeah. But I mean  uh  for optimum flexibility later you'd want to save each channel. But I think in practical situations you would have some engine of some sort doing some processing to reduce this to some - to the equivalent of a single microphone that was very directional. Uh  oh  O_K  I see. Right? So - Sort of saving the result of the beam-forming. I mean  it seems - Yeah. it seems to me that there's - you know  there are good political reasons for - for doing this  just getting the data  because there's a number of sites - like right now S_R_I is probably gonna invest a lot of internal funding into recording meetings also  which is good  um  but they'll be recording with video and they'll be - You know  it'd be nice if we can have at least  uh  make use of the data that we're recording as we go since it's sort of - this is the first site that has really collected these really impromptu meetings  um  and just have this other information available. So  if we can get the investment in just for the infra- infrastructure and then  I don't know  save it out or have whoever's interested save that data out  transfer it there  it'd be g- it'd be good to have - have the recording. You mean to - to actually get a microphone array and I think. Well  if - do that? And video and - Even if we're not - I'm not sure about video. That's sort of an - video has a little different nature since right n- right now we're all being recorded but we're not being taped. Um  but it - definitely in the case of microphone arrays  since if there was a community interested in this  Well  but I think we need a researcher here who's interested in it. then - To push it along. See the problem is it - it took  uh  uh  it took at least six months for Dan to get together the hardware and the software  and debug stuff in - in the microphones  and in the boxes. And it was a really big deal. And so I think we could get a microphone array in here pretty easily and  uh  have it mixed to - to one channel of some sort. But  Mm-hmm. e- I think for @@ I mean  how we're gonna decide - For - for maximum flexibility later you really don't want to end up with just one channel that's pointed in the direction of the - the - the p- the person with the maximum energy or something like that. I mean  you - you want actually to - you want actually to have multiple channels being recorded so that you can - And to do that  it - we're going to end up greatly increasing the disk space that we use up  we also only have boards that will take up to sixteen channels and in this meeting  we've got eight people and - and six mikes. And there we're already using fourteen. And we actually only have fifteen. One of them's - E- Yeah. Details. But fifteen  not sixteen. Mm-hmm. Yeah. Yeah. Well if there's a way to say time - to sort of solve each of these f- those - So suppose you can get an array in because there's some person at Berkeley who's interested and has some equipment  uh  and suppose we can - as we save it we can  you know  transfer it off to some other place that - that holds this - this data  who's interested  and even if ICSI it- itself isn't. Um  and it - it seems like as long as we can time align the beginning  do we need to mix it with the rest? I don't know. You know? The- So - Yeah. So I think you'd need a separate - a separate set up and the assumption that you could time align the two. Yeah. And y- it'd certainly gets skew. I mean it's just - it's worth considering as sort of once you make the up front investment and can sort of save it out each time  and - and not have to worry about the disk space factor  then it mi- it might be worth having the data. I'm not so much worried about disk space actually. I mentioned that  b- as a practical matter  but the real issue is Just - that  uh  there is no way to do a recording extended to what we have now with low skew. So you would have a t- completely separate set up  which would mean that the sampling times and so forth would be all over the place compared to this. Right. So it would depend on the level of pr- processing you were doing later  but if you're d- i- the kind of person who's doing array processing you actually care about funny little times. And - and so you actually wou- would want to have a completely different set up than we have  one that would go up to thirty- two channels or something. I see. Mmm. So basically - or a hun- Yeah. So  Or a hundred thirty-two. I'm kinda skeptical  but um I think that - Mmm. So  uh  I don't think we can share the resource in that way. But what we could do is if there was someone else who's interested they could have a separate set up which they wouldn't be trying to synch with ours which might be useful for - for them. Right  I mean at least they'd have the data and the transcripts  and - And then we can offer up the room  Yeah  we can o- offer the meetings  and the physical space  and - and - yeah  the transcripts  and so on. Right. O_K. Right  I mean  just - it'd be nice if we have more information on the same data. You know  and - Yeah. But it's - if it's impossible or if it's a lot of effort then you have to just balance the two  Well I thi- yeah  the thing will be  u- u- in - in - again  in talking to these other people to see what - you know  what - what we can do. so - Right. Uh  we'll see. Is there an interest in getting video recordings for these meetings? Right  so we have - we - I mean t- Yes  absolutely. But it's exactly the same problem  that you have an infrastructure problem  you have a problem with people not wanting to be video taped  and you have the problem that no one who's currently involved in the project is really hot to do it. Hmm. So there's not enough interest to overcome all of - Mm-hmm. Right. Internally  but I know there is interest from other places that are interested in looking at meeting data and having the video. So it's just - Yeah  w- although I - m- I - I have to u- u- mention the human subjects problems  that i- increase with video. Right  that's true. Yeah  so it's  uh  people - people getting shy about it. There's this human subjects problem. Yeah. There's the fact that then um  if - i- I- I've heard comments about this before  ""why don't you just put on a video camera?"" But you know  it's sort of like saying  ""uh  well we're primarily interested in - in some dialogue things  uh  but  uh  why don't we just throw a microphone out there."" I mean  the thing is  once you actually have serious interest in any of these things then you actually have to put a lot of effort in. Mmm. And  uh  you really want to do it right. So I think I know . Yep. NIST or L_D_C  or somebody like that I think is much better shape to do all that. We - there will be other meeting recordings. We won't be the only place doing meeting recordings. We are doing what we're doing. Mm-hmm. And  uh  hopefully it'll be useful. I - it - it occurred to me  has Don signed a human subject's form? Oh! Probably not. Has Don - have you s- did you si- I thought you did actually. Didn't you read a digit string? A permission form? I was - Yeah  I was - I was here - I was here before once. You were here at a meeting before. You were here at a meeting before. Yeah. Yeah  and you - and you signed a form. So. Did you sign a form? Oh  I think so. Did I? I don't know. I'm pretty sure. Well I'll - I'll get another one before the end of the meeting. Thank you. O_K. Yeah. O_K. Yeah. Yeah. You don't - you don't have to leave for it. But I just - you know. Yeah  we - we - Well I can't  I'm wired in. Can I verbally consent? Yeah. We - we - we - we don't  uh - o- You're on recor- you're being recorded and - Yeah. we don't - we don't perform electro-shock during these meetings  and - I don't care. You can do whatever you want with it . That's fine. Usually. Yeah. O_K. Uh  transcriptions. Transcriptions  O_K. Um  I thought about - there are maybe three aspects of this. So first of all  um  I've got eight transcribers. Uh  seven of them are linguists. One of them is a graduate student in psychology. Um  Each - I gave each of them  uh  their own data set. Two of them have already finished the data sets. And the meetings run  you know  let's say an hour. Sometimes as man- much as an hour and a half. How big is the data set? Oh  it's - what I mean is one meeting. Each - each person got their own meeting. I didn't want to have any conflicts of  you know  of - of Ah  O_K. when to stop transcribing this one or - So I wanted to keep it clear whose data were whose  and - and - Uh-huh. and so - And  uh  meetings  you know  I think that they're - they go as long as a - almost two hours in some - in some cases. So  you know  that means - you know  if we've got two already finished and they're working on - Uh  right now all eight of them have differe- uh  uh  additional data sets. That means potentially as many as ten might be finished by the end of the month. Hope so. But the pre-segmentation really helps a huge amount. And  uh  also Dan Ellis's innovation of the  uh - the multi-channel to here really helped a r- a lot Wow. O_K. in terms of clearing - clearing up h- hearings that involve overlaps. But  um  just out of curiosity I asked one of them how long it was taking her  one of these two who has already finished her data set. She said it takes about  uh  sixty minutes transcription for every five minutes of real time. So it's about twelve to one  which is what we were thinking. or Yep. It's pretty good. It's well in the range. O_K. Uh  these still  when they're finished  um  that means that they're finished with their pass through. They still need to be edited and all but - But it's word level  speaker change  the things that were mentioned. O_K  now I wanted to mention the  um  teleconference I had with  uh  Jonathan Fiscus. We spoke for an hour and a half Hmm. and  um  had an awful lot of things in common. He  um  um  he in- indicated to me that they've - that he's been  uh  looking  uh  uh  spending a lot of time with - I'm not quite sure the connection  but spending a lot of time with the ATLAS system. And I guess that - I mean  I - I need to read up on that. And there's a web site that has lots of papers. But it looks to me like that's the name that has developed for the system that Bird and Liberman developed for the annotated graphs approach. Mm-hmm. So what he wants me to do and what we - what we will do and - uh  is to provide them with the u- already transcribed meeting for him to be able to experiment with in this ATLAS System. And they do have some sort of software  at least that's my impression  related to ATLAS and that he wants to experiment with taking our data and putting them in that format  and see how that works out. I - I - I explained to him in - in detail the  uh  conventions that we're using here in this - in this word level transcript. And  um  you know  I - I explained  you know  the reasons that - that we were not coding more elaborately and - and the focus on reliability. He expressed a lot of interest in reliability. It's like he's - he's really up on these things. He's - he's very - Um  independently he asked  ""well what about reliability?"" So  he's interested in the consistency of the encoding and that sort of thing. O_K  um - Sorry  can you explain what the ATLAS - I'm not familiar with this ATLAS system. Well  you know  at this point I think - Uh  well Adam's read more - in more detail than I have on this. I need to acquaint myself more with it. But  um  there - there is a way of viewing - Uh  whenever you have coding categories  um  and you're dealing with uh  a taxonomy  then you can have branches that - that have alternative  uh  choices that you could use for each - each of them. And it just ends up looking like a graphical representation. Is - is - Is ATLAS the - his annotated transcription graph stuff? I don't remember the acronym. The - the one - the - what I think you're referring to  they - they have this concept of an an- annotated transcription graph representation. Yeah. And that's basically what I based the format that I did - Oh. Oh. I based it on their work almost directly  in combination with the T_E_I stuff. And so it's very  very similar. And so it's - it's a data representation and a set of tools for manipulating transcription graphs of various types. Is this the project that's sort of  uh  between  uh  NIST and - and  uh  a couple of other places? The - the - Mm-hmm. Including L_D_C. I think so. Yep. Yeah  y- right  O_K. Mm-hmm. Then there's their web site that has lots of papers. And I looked through them and they mainly had to do with this  um  this  uh  tree structure  uh  annotated tree diagram thing. Mmm. So  um  um - and  you know  in terms of like the conventions that I'm a- that I've adopted  it - there - there's no conflict at all. Right. And he was  you know  very interested. And  ""oh  and how'd you handle this?"" And I said  ""well  you know  this way"" and - And - and we had a really nice conversation. Um  O_K  now I also wanted to say in a different - a different direction is  Brian Kingsbury. So  um  I corresponded briefly with him. I  uh  c- I - He still has an account here. I told him he could S_S_H on and use multi-trans  and have a look at the already done  uh  transcription. And he - and he did. And what he said was that  um  what they'll be providing is - will not be as fine grained in terms of the time information. And  um  that's  uh - You know  I need to get back to him and - and  uh  you know  explore that a little bit more and see what they'll be giving us in specific  but I just haven't had time yet. Hmm. The p- the people - Sorry  what? The - the folks that they're  uh  subcontracting out the transcription to  are they like court reporters or - Yes. Apparently - Well  I get the sense they're kind of like that. Like it's like a pool of - of somewhat uh  secretarial - I don't think that they're court reporters. I don't think they have the special keyboards and that - and that type of training. I - I get the sense they're more secretarial. And that  um  Mm-hmm. Hmm. uh  what they're doing is giving them - Like medical transcriptionist type people - Nu- it's mostly - it's for their speech recognition products  Yep. that they've hired these people to do. But aren't - they're - Oh  so they're hiring them  they're coming. It's not a service they send the tapes out to. Well they - they do send it out but my understanding is that that's all this company does is transcriptions for I_B_M for their speech product. Ah! Oh. O_K. I gotcha. So most of it's ViaVoice  people reading their training material for that. I see. Mm-hmm. I see. Up to now it's been monologues  uh  as far my understood. And - and what they're doing is Brian himself downloaded - So - Yep  exactly. Yep. Mm-hmm. So  um  Adam sent them a C_D and Brian himself downloaded - uh  cuz  you know  I mean  we wanted to have it so that they were in familiar f- terms with what they wanted to do. He downloaded from the C_D onto audio tapes. And apparently he did it one channel per audio tape. So each of these people is transcribing from one channel. Right. And then what he's going to do is check it  Oh. a- before they go be- beyond the first one. Check it and  you know  adjust it  and all that. So each person gets one of these channels - Right. O_K. So if they hear something off in the distance they don't - they just go - Well  but that's O_K  because  you know  you'll do all them and then combine them. I - I don't know. I have t- I  you know I - But there could be problems  right? with that. Yep. I think it would be difficult to do it that way. I really d- uh  in my case - Yeah. Well if you're tran- if you got that channel right there - No  no. We're talking about close talking  not the - not the desktop. Yeah. No  close talk. Are you? Yes. I sure hope so. It'd be really foolish to do otherwise. Well I th- I think so. Yeah  I - I would think that it would be kind of hard to come out with - Yeah. I - I think it's sort of hard just playing the - you know  just having played the individual files. And I - I mean  I know you. I know what your voice sounds like. I'm sort of familiar with - Yeah. Uh  it's pretty hard to follow  especially One side. I agree. there are a lot of words that are so reduced phonetically that make sense when you know what the person was saying before. Yeah  that's - Yeah. And especially since a lot of these - Uh  it sort of depends where you are in - Yeah. But I mean we had this - we've had this discussion many times. And the answer is we don't actually know the answer because we haven't tried both ways. Yeah  we have. Well  except I can say that my transcribers use the mixed signal mostly So. Mm-hmm. unless there's a huge disparity in terms of the volume on - on the mix. Mm-hmm. Right. In which case  you know  they - they wouldn't be able to catch anything except the prominent channel  then they'll switch between. But - but really - Right. Well I think that - that might change if you wanted really fine time markings. Yeah. Well  O_K. Yeah  well - So. But they're not giving f- really fine time markings. Right. Actually  are th- so are they giving any time markings? In other words  if - Well  I have to ask him. And that's - that's my email to him. That needs to be forthcoming. But - but the  uh - I did want to say that Yeah. Cuz - O_K. it's hard to follow one channel of a conversation even if you know the people  and if you're dealing furthermore with highly abstract network concepts you've never heard of - So  you know  one of these people was - was transcribing the  uh  networks group talk and she said  ""I don't really know what a lot of these abbreviations are "" ""but I just put them in parentheses cuz that's the - that's the convention and I just"" - Cuz you know  if you don't know - Oh  I'd be curious to - to look at that. Just out of curiosity  I mean - They also all have h- heavy accents. The networks group meetings are all - Yeah. Yeah. Yeah. Given all of the effort that is going on here in transcribing why do we have I_B_ M doing it? Why not just do it all ourselves? Um  it's historical. I mean  uh  some point ago we thought that No  just - Uh-huh. uh  it - ""boy  we'd really have to ramp up to do that""  you know  like we just did  Mm-hmm. and  um  here's  uh  a - a  uh  collaborating institution that's volunteered to do it. Mm-hmm. So  that was a contribution they could make. Uh Mm-hmm. in terms of time  money  you know? And it still might be a good thing but - I'm just wondering now - Well  I'm - I'm wondering now if it's - Actu- yeah  Mar- Mari asked me the same question as sort of - Well we can talk about more details later. um  you know  yeah  Yeah. whether to - Yeah. Yeah  so. Hmm. We'll see. I mean  I think  th- you know  they - they - they've proceeded along a bit. Let's see what comes out of it  and - and  uh  you know  have some more discussions with them. Mm-hmm. It's very - a real benefit having Brian involved because of his knowledge of what the - how the data need to be used and so what's useful to have in the format. Yeah. Mm-hmm. Yeah. So  um  Liz  with - with the S_R_I recognizer  can it make use of some time marks? O_K  so this is a  um  I - I guess I don't know what that means. and actually I should say this is what Don has b- uh  he's already been really helpful in  uh  chopping up these - So - so first of all you - um  I mean  for the S_R_I front-end  we really need to chop things up into pieces that are f- not too huge. Um  but second of all  uh - in general because some of these channels  I'd say  like  I don't know  at least half of them probably on average are g- are ha- are - have a lot of cross-ta- sorry  some of the segments have a lot of cross-talk. Um  it's good to get sort of short segments if you're gonna do recognition  especially forced alignment. So  uh  Don has been taking a first stab actually using Jane's first - the fir- the meeting that Jane transcribed which we did have some problems with  and Thilo  uh  I think told me why this was  but that people were switching microphones around in the very beginning  so - the S_R_I re- No  th- Yeah. No. They - they were not switching them but what they were - they were adjusting them  so. and they - They were not - Mmm. Adjusting. Oh. @@ Yeah. And aft- after a minute or so it's - it's way better. So - So we have to sort of normalize the front-end and so forth  and have these small segments. So Yep. we've taken that and chopped it into pieces based always on your - your  um  cuts that you made on the mixed signal. And so that every - every speaker has the same cuts. And if they have speech in it we run it through. And if they don't have speech in it we don't run it through. And we base that knowledge on the transcription. On - Just on the marks. Right? Um  the problem is if we have no time marks  then for forced alignment we actually don't know where - you know  in the signal the transcriber heard that word. And so - Oh  I see  it's for the length. I see. I mean  if - if it's a whole conversation and we get a long  uh  you know  par- paragraph of - of talk  uh  I don't know how they do this. Um  we actually don't know which piece goes I understand. where. And  um  I think with - Well you would need to - like a forced alignment before you did the chopping  right? No  we used the fact that - So when Jane transcribes them the way she has transcribers doing this  whether it's with the pre-segmentation or not  It's already chunked. they have a chunk and then they transcribes the words in the chunk. And maybe they choose the chunk or now they use a pre-segmentation and then correct it if necessary. But there's first a chunk and then a transcription. Then a chunk  then a transcription. That's great  Mm-hmm. cuz the recognizer can - Uh  it's all pretty good sized for the recognizer also. Right  and it - it helps that it's made based on Good. Oh good. sort of heuristics and human ear I think . Th- but there's going to be a real problem  uh  even if we chop up based on speech silence these  uh  the transcripts from I_B_ M  we don't actually know where the words were  which segment they belonged to. So that's sort of what I'm worried about Right. Why not do a - a - a forced alignment? right now. That's what she's saying  is that you can't. If you do a forced alignment on something really - well even if you do it on something really long you need to know - Got uh six- sixty minutes of - you can always chop it up but you need to have a reference of which words went with which  uh  chop. So - Now wasn't - I thought that one of the proposals was that I_B_M was going to do an initial forced alignment  after they - Yeah  but - I - I think that they are  um  We'll have to talk to Brian. yeah  I'm sure they will and so we - we have to have a dialogue with them about it. I mean  it sounds like Yeah. Maybe they have some - you know  maybe actually there is some  even if they're not fine grained  maybe the transcribers - O_K. Liz has some concerns and - uh  I don't know  maybe it's saved out in pieces or - or something. That would help. But  Yeah. uh  it's just an unknown right now. Yeah. I - I need to - to write to him. I just - you know  it's like I got over-taxed with the timing. So. Right. But the - it is true that the segments - I haven't tried the segments that Thilo gave you but the segments that in your first meeting are great. I mean  that's - that's a good length. Mm-hmm. A good size. Good. Well  I - I was thinking it would be fun to - to - uh  uh  if - if you - wouldn't mind  to give us a pre-segmentation. Uh  maybe you have one already of that first m- of the meeting that Right  cuz - y- yeah. Yeah. uh  the first transcribed meeting  the one that I transcribed. Do you have a - could you generate a pre-segmentation? Um  I'm sure I have some but - but that's the one where we're  um  trai- training on  so that's a little bit - February sixteenth I think. Oh. Oh  I see. Oh  darn. Of course  of course  of course. Yeah  O_K. It's a little bit at odd to - Yeah. And actually as you get transcripts just  um  for new meetings  Uh-huh. um  we can try - I mean  the - the more data we have to try the - the alignments on  um  the better. So it'd be good for - just to know as transcriptions are coming through the pipeline from the transcribers  just to sort of - we're playing around with sort of uh  parameters f- on the recognizer  cuz that would be helpful. Mm-hmm. Excellent  good. Especially as you get  en- more voices. The first meeting had I think just four people  yeah. Four speakers  yeah. Mm-hmm. Yeah  Liz and I spoke d- w- at some length on Tuesday and - and I - and I was planning to do just a - a preliminary look over of the two that are finished and then give them to you. Oh  great  great. Yeah. So. That's great. I guess the other thing  I - I can't remember if we discussed this in the meeting but  uh  I know you and I talked about this a little bit  there was an issue of  uh  suppose we get in the  uh  I guess it's enviable position although maybe it's just saying where the weak link is in the chain  uh  where we - we  uh - uh  we have all the data transcribed and we have these transcribers and we were - we're - the - we're still a bit slow on feeding - at that point we've caught up and the - the - the  uh  the weak link is - is recording meetings. O_K  um  two questions come  is you know what - how - how do we - uh  it's not really a problem at the moment cuz we haven't reached that point but how do we step out the recorded meetings? And the other one is  um  uh  is there some good use that we can make of the transcribers to do other things? So  um  I - I can't remember how much we talked about this in this meeting but there was - We had spoken with them about it. And there is one use that - that also we discussed which was when  uh  Dave finishes the - and maybe it's already finished - the - the modification to multi-trans which will allow fine grained encoding of overlaps. Uh  then it would be very - these people would be very good to shift over to finer grain encoding of overlaps. It's just a matter of  you know  providing - So if right now you have two overlapping segments in the same time bin  well with - with the improvement in the database - in - in the  uh  sorry  in the interface  it'd be possible to  um  you know  just do a click and drag thing  and get the - uh  the specific place of each of those  the time tag associated with the beginning and end of - of each segment. Right  so I think we talking about three level - three things. One - one was Mm-hmm. Yeah. uh  we had s- had some discussion in the past about some very high level The types of overlaps - labelings  types of overlaps  and so forth that - that someone could do. Mm-hmm. Second was  uh  somewhat lower level just doing these more precise timings. And the third one is - is  uh  just a completely wild hair brained idea that I have which is that  um  if  uh - if we have time and people are able to do it  to take some subset of the data and do some very fine grained analysis of the speech. For instance  uh  marking in some overlapping - potentially overlapping fashion  uh  the value of  uh  ar- articulatory features. Yeah. You know  just sort of say  O_K  it's voiced from here to here  there's - it's nasal from here to here  and so forth. Um  as opposed to doing Hmm! phonetic - uh  you know  phonemic and the phonetic analysis  and  uh  assuming  uh  articulatory feature values for those - those things. Um  obviously that's extremely time-consuming. Uh - That would be really valuable I think. but  uh  we could do it on some small subset. Also if you're dealing with consonants that would be easier than vowels  wouldn't it? I mean  I would think that - that  uh  being able to code that there's a - a fricative extending from here to here would be a lot easier than classifying precisely which vowel that was. Which one. Mmm. I think vowels - vowels are I think harder. Mm-hmm. Yeah . Well  yeah  but I think also it's just the issue that - that when you look at the - u- w- u- u- when you look at Switchboard for instance Mm-hmm. very close up there are places where whether it's a consonant or a vowel you still have trouble calling it a particular phone Mm-hmm  O_K. Yeah  I'm sure. Uh  yeah  I - I know. Yeah  but - but just saying what the - at that point because it's - you know  there's this movement from here to here and - and - and it's - Right. You're saying r- sort of remove the high level constraints and go bottom-up. so I- Yep  just features. Yeah  describe - describe it. Now I'm suggesting articulatory features. Maybe there's - there's even a better way to do it but it - but - Then just say - Mmm. but that's  you know  sort of a traditional way of describing these things  Mm-hmm. um  and - uh  I mean  actually this might be a g- neat thing to talk to - That's nice. Acoustic features versus psychological categories. Yeah. Sort of. I mean  it's still - Yeah. some sort of categories but - but something that allows for overlapping change of these things and then this would give some more ground work for people who were building statistical models that allowed for overlapping changes  different timing changes as opposed to just ""click  you're now in this state  which corresponds to this speech sound"" and so on. Mm-hmm. Mm-hmm. So this is like gestural - uh  these g- Right. Yeah  something like that. I mean  actually if we get into that it might be good to  uh  uh  haul O_K. John Ohala into this and ask his - his views on it I think. Yeah. Right. But is - is the goal there to have this on meeting data  like Excellent. so that you can do far field studies of those gestures or - um  or is it because you think there's a different kind of actual production in meetings that people use? Or - ? No  I think - I think it's - for - for - for that purpose I'm just viewing meetings as being a - a neat way to get people talking naturally. And then you have i- and then - and then it's natural in all senses  in the sense that you have microphones that are at a distance that Just a source of data? I see. you know  one might have  and you have the close mikes  and you have people talking naturally. And the overlap is just indicative of the fact that people are talking naturally  right? So - Uh-huh. Yeah. Right. Yeah. so I think that given that it's that kind of corpus  if it's gonna be a very useful corpus um  if you say w- O_K  we've limited the use by some of our  uh  uh  censored choices  we don't have the video  we don't - and so forth  but there's a lot of use that we could make of it by expanding the annotation choices. Mm-hmm. And  uh  most of the things we've talked about have been fairly high level  and being kind of a bottom-up person I thought maybe we'd  do some of the others. Yeah. Hmm. It's a nice balance. That would be really nice to offer those things with that wide range. Really nice. Right. Yeah  that would be good. Right. Yeah and hopefully someone would make use of it. I mean  people didn't - Yeah. uh  I mean  people have made a lot of use of - of TIMIT and  uh w- due to its markings  and then the Switchboard transcription thing  well I think has been very useful for a lot of people. So - Right. That's true. I guess I wanted to  um  Cool. sort of make a pitch for trying to collect more meetings. Um  Yeah. I- actually I talked to Chuck Fillmore and I think they've what  vehemently said no before but this time he wasn't vehement and he said Yeah. you know  ""well  Liz  come to the meeting tomorrow and try to convince people"". So I'm gonna try. Go to their meeting tomorrow and see Mm-hmm. if we can try  uh  to convince them because they have - Good. Cuz they have something like three or four different meetings  right? And they have very interesting meetings from the point of view of Mm-hmm. a very different type of - of talk than we have here and definitely than the front end meeting  probably. Talk - Um - You mean in terms of the topic - topics? Well  yes and in terms of the - the fact that they're describing abstract things and  uh  just dialogue-wise  right. Mm-hmm. Mm-hmm. Um  so I'll try. And then the other thing is  I don't know if this is at all useful  but I asked Lila if I can maybe go around and talk to the different departments in this building Yes. to see if there's any groups that  for a free lunch  if we can still offer that  might be willing - You mean non-ICSI? Great. non-ICSI  non-academic  you know  like government people  I don't know. So. Yeah  I guess you - you can try but - The problem is so much of their stuff is confidential. It would be very hard for them. Yeah. Yeah. Also it does seem like it takes us way out of the demographic. I mean  it seems like we - we had this idea before of having like linguistics students brought down for free lunches and that's a nice idea. Yeah. Is - is it in these departments? Well  tha- I think that's her point. Right  and then we could also - we might try advertising again because I think it'd be good if - if we can get a few different Yeah. sort of non-internal types of meetings and just also more data. Yeah. And I think  uh  if we could get - Mm-hmm. Does - does John Ohala have weekly phonetics lab meetings? So. So I actually wrote to him and he answered  ""great  that sounds really interesting"". But I never heard back because we didn't actually advertise openly. We a- I mean w- I told - I d- asked him privately. Um  and it is a little bit of a trek for campus folks. Yeah. Mm-hmm. You might give them a free lunch. But  um  Um  so it's still worthwhile. it would be nice if we got someone other than me who knew how to set it up and could do the recording so u- I didn't have to So - Exactly  and - and - do it each time. Yeah. That's right. and I was thinking - He- he's supposed - he's supposed to be trained to do it. Yeah. Plus we could also get O_K  next week you're going to do it all. Yeah. you know  a s- a student. And I'm willing to try to learn. I mean  I'm - I would do my best. Um  the other thing is that - It's not that hard. there was a number of things at the transcription side that  um  transcribers can do  like dialogue act tagging  disfluency tagging  um  things that are in the speech that are actually something we're y- working on for language modeling. And Mari's also interested in it  Andreas as well. So if you wanna process a utterance and the first thing they say is  ""well""  and that ""well"" is coded as some kind of interrupt u- tag. Uh  and things like that  um  Of course some of that can be li- done lexically. And I also - they are doing disfluency tagging th- A lot of it can be done - to some degree already. Yeah. Great. So a - a lot of this kind of - I think there's a second pass and I don't really know what would exist in it. But there's definitely a second pass worth Mm-hmm. doing to maybe encode some kinds of  you know  is it a question or not  or - um  that maybe these transcribers could do. So - They'd be really good. They're - they're very - they're very consistent. Uh  I wanted to - whi- while we're - Uh  so  to return just briefly to this question of more meeting data  um - Yeah. Mm-hmm. That'd be great. I have two questions. One of them is  um  Jerry Feldman's group  they - they  uh  are they - I know that they recorded one meeting. Are they willing? I think they're open to it. I think  you know  all these things are - I think there's - Oh  yeah. we should go beyond  uh  ICSI but  I mean  there's a lot of stuff happening at ICSI that we're not getting now that we could. Mm-hmm. So it's just - Yeah. So the - Oh  that we could. O_K. I thought that all these people had sort of said ""no"" twice already. If that's not the case then - No  no. No. So th- there was the thing in Fillmore's group but even there he hadn't - What he'd said "" no "" to was for the main meeting. But they have several smaller meetings a week  So. and  uh  the notion was raised before that that could happen. And it just  you know - it just didn't come together but - Just - O_K. Well  and - and the other thing too is when they originally said ""no"" they didn't know about this post-editing capability thing. Oh. Right. Yeah. Yeah. Right. That was a big That's important. So. fear. Yeah  so I mean there's possibilities there. I think Jerry's group  yes. Uh  there's - there's  uh  the networks group  uh  I don't - Do they still meeting regularly or - ? O_K. Well  I don't know if they meet regularly or not but they are no longer recording. But I mean  ha- ha- have they said they don't want to anymore or - ? Um  ugh  what was his name? Uh  i- i- Joe Sokol? Yeah. Yeah. When - with him gone  it sorta trickled off. They - and they stopped - O_K  so they're down to three or four people but the thing is three or four people is O_K. Yeah. Mm-hmm. Yep. We might be able to get the administration - Well he was sort of my contact  so I just need to find out who's running it now. So. O_K. I see that Lila has a luncheon meeting in here periodically. I don't know - Yeah  I mean  it - One thing that would be nice and this - it sounds bizarre but  I'd really like to look at - to get some meetings where there's a little bit of heated discussion  like ar- arguments and - or emotion  and things like that. And so I was thinking if there's any like Berkeley political groups or something. I mean  that'd be perfect. Some group  ""yes  we must -"" Who's willing to get recorded and distributed? Well  you know  something - Um - Yeah  I don't think the more political argumentative ones would be willing to - Yeah. Yeah  with - with - with potential use from the defense department. Yeah. Yeah. Well  O_K. No  but maybe stu- student  uh  groups or  um  film-makers  or som- Something a little bit Yeah. Yeah. Exactly. Yeah. Yeah. Yeah. Yeah  of course there is this problem though  that if we give them the chance to excise later we e- might end up with like five minutes out of a f- Yeah. Film-maker. colorful. Of beeps  yeah. of m- one hour of - Yeah. Yeah. Is - Yes. Really. And I don't mean that they're angry but just something with some more variation in prosodic contours and so forth would be neat. So if anyone has ideas  I'm willing to do the leg work to go try to talk to people but I don't really know which groups are worth Well there was this K_P_F_ A idea  but - pursuing. No that's - Yeah  th- there's a problem there in terms of  uh  the O_K. Legal. um commercial value of - of st- uh  it - it - it - it turned out to be a bit of a problem. O_K  O_K. And I had one other - one other aspect of this which is  um  uh  uh  Jonathan Fiscus expressed Or - primar- uh y- a major interest in having meetings which were all English speakers. Now he wasn't trying to shape us in terms of what we gather but Mm-hmm. that's what he wanted me to show him. So I'm giving him our  um - our initial meeting because he asked for all English. And I think we don't have a lot of all English meetings right now. Did he mean  uh - did he mean and non-British? Of all - all nat- all native speakers. Well - The all native. That's what I mean  yeah. Well if he meant and non- British I think we have zero. He doesn't care. No. Eh  well  British is O_K. But - but - He said British was O_K? Sure  sure  sure. Yeah. British is English? Why? Different varieties of English. Ooo  ooo. Well  I don't - I don't - I don't think - if he didn't say that - Native speaking. Native speaking English. I bet he meant native speaking American. Yes. I bet he did. American English? Oh  really. So  why would he care? Knowing the application - I remember wh- I- I remember a study - That's - English  I was thinking  knowing the  uh  n- National Institute of Standards  it is all - I remember a study that B_B_N did where they trained on - this was in Wall Street Journal days or something  they trained on American English and then they tested on  uh  different native speakers from different areas. And  uh  uh  the worst match was people whose native tongue was Mandarin Chinese. The second worst was British English. That's funny. So h- it's  you know  t- Alright. And so that would make sense. I didn't have the context of that. the - the - the - German was much better  it was Swiss w- Yeah  so it's - so I think  you know  if he's - if he's thinking in terms of recognition kind of technology I - I - I think he would probably want  uh Ooo  ooo. American English  yeah. It - it - yeah  unless we're gonna train with a whole bunch of - I wonder if we have any. All America  O_K. I think that the - Feldman's meetings tend to be more that way  aren't they? I mean  I sort of feel like they have - Maybe. Maybe. I think so  yeah. Yeah  mm-hmm. Yeah. Mmm. And maybe there are a few of - with us where it was - you know  Dan wasn't there and before Jose started coming  and - Yeah. Yeah. Yeah. It's pretty tough  uh  this group. Yeah. So  uh  what about - what about people who involved in some artistic endeavor? I mean  film-making or something like that. You'd think like they would be - God. Yeah. Mmm. Yeah. A film-maker. Exactly  that's what I was - something where there - there is actually discussion where there's no right or wrong answer but - but it's a matter of opinion kind of thing. It's be fun. Uh  anyway  if you - if you have ideas - RASTA. P_L_P. RASTA. P_L_P. We can just discu- we can just have a political discussion one day. Yes. A- any department that calls itself science ri- Yeah  we could - Department. Yeah. Uh  I could make that pretty - Well  like computer science. That - Computer sci- We could get Julia Child. I know. That's - I'm - I'm actually serious because  uh  you know  we have the set up here and - and that - that has Got a ticket. Yeah  I know you are. a chance to give us some very interesting fun data. So if anyone has ideas  Yeah. Yeah. if you know any groups that are m- you know  Well I had asked some - some of the students at the business school. I could - Yeah. I know - student groups c- like clubs  things like that. Not - not - Put a little ad up saying  ""come here and argue"". Yeah. ""If you're really angry at someone use our conference room."" The Business school. Uh  the business school might be good. I actually spoke with some students up there and they - they - they expressed willingness back when they thought they would be doing more stuff with speech. Oh  O_K. Really. But when they lost interest in speech they also stopped answering my email about other stuff  so. Hmm. I - Or people who are really h- They could have a discussion about te- We should probably bleep that out. about - about tax cuts or something. I heard that at Cal Tech they have a special room - someone said that they had a special room to get all your frustrations out that you can go to and like throw things and break things. So we can like post a - Yeah  now that is not actually what we - Th- that's not what we want. No  not to that extent but  um. Well  far field mikes can pick up where they threw stuff on the wall. Yeah. Yeah  but we don't want them to throw the far field mikes is the thing. That's right. Yeah. Oh. Yeah  right. The fa- "" Please throw everything in that direction."" b- b- Yeah. Anyway. Padded cell. It'd be fun to get like a - a p- visit from the - There was a dorm room at Tech that  uh  someone had coated the walls and the ceiling  and  uh  the floor with mattresses. Mmm. The entire room. I had as my fourth thing here processing of wave forms. What did we mean by that? Remember @@ ? Yeah. Uh  Liz wanted to talk about Pre-processing. methods of improving accuracy by doing pre-processing. Well I think that - that was just sort of - I- I already asked Thilo but Oh  you already did that. that  um  it would be helpful if I can stay in the loop somehow with  um  people who are doing any kind of post-processing  whether it's to separate speakers or to improve the signal-to-noise ratio  or both  um  that we can sort of try out as we're running recognition. Um  so  i- is that - Who else is work- I guess Dan Ellis and you Dan  yeah. Yep. Yeah  and Dave uh Gel- Gelbart again  he's - he's interested in - in fact we're look- starting to look at some echo cancellation kind of things. and Dave. Yep. Yeah. O_K. O_K. Right . Which uh - I am not sure how much that's an issue with the close talking mikes  but who knows? Hmm? Well  let's - w- i- isn't that what - what you want - t- I don't know. I'm bad - It's like - like - No  so - No  i- w- wha- what you - what you want - when you're saying improving the wave form you want the close talking microphone to be better. Right? Right. And the question is to w- to what extent is it getting hurt by  uh - by any room acoustics or is it just - uh  given that it's close it's not a problem? Uh - It doesn't seem like big room acoustics problems to my ear but I'm not an expert. It seems like a problem with cross-talk. O_K  so it's - e- I bet with the lapel mike there's plenty  uh  room acoustic but I- Yeah . That - that may be true. But I don't know how good it can get either by those - the - those methods - That's true. I think the rest is cross-talk. Yeah. O_K. So I - I think it's just  Oh  I don't know. yeah  what you said  cross-talk. All I meant is just that as sort of - as this pipeline of research is going on we're also experimenting with different A_S_R  uh  techniques. And so it'd be w- good to know about it. Mm-hmm. So the problem is like  uh  on the microphone of somebody who's not talking they're picking up signals from other people and that's R- right  although if they're not talking  using the - causing problems? the inhouse transcriptions  were sort of O_ K because the t- no one transcribed any words there and we throw it out. Mm-hmm. But if they're talking at all and they're not talking the whole time  so you get some speech and then a ""mm-hmm""  and some more speech  so that whole thing is one chunk. And the person in the middle who said only a little bit is picking up the speech around it  that's where it's a big problem. You know  this does like seem like it would relate to some of what Jose's been working on as well  the encoding of the - And - and he also  he was - Yeah. The energy  right. Exactly. Yeah  energy. I was t- I was trying to remember  you have this interface where you - i- you ha- you showed us one time on your laptop that you - you had different visual displays as speech and nonspeech events. Yeah  c- Yeah. May - I - I only display the different colors for the different situation. But  eh  for me and for my problems  is uh - is enough. Because  eh  it's possible  eh  eh  in a simp- sample view  uh  to  nnn  to compare with c- with the segment  the - the kind of assessment what happened with the - the different parameters. And only with a different bands of color for the  uh  few situation  eh  I consider for acoustic event is enough to @@ . I - Mm-hmm. I - I see that  eh  you are considering now  eh  a very sophisticated  eh  ehm  eh  @@ set of  eh  graphic s- eh  eh  ehm  si- symbols to - to transcribe. No? Oh  I w- Uh-huh. Because  uh  before  you - you are talking about the - the possibility to include in the Transcriber program eh  um  a set of symbols  of graphic symbol to - t- to mark the different situations during the transcription - during the transcription . No? Well  you're saying - So  uh  symbols for differences between laugh  and sigh  and - and - and slam the door and stuff? Or some other kind of thing? Yeah. Yeah  yeah. The s- the symbols  you - you talk of before. No? To - to mark - Well  I wouldn't say symbols so much. The - the main change that I - that I see in the interface is - is just that we'll be able to more finely c- uh  Yeah. time things. But I - I also st- there was another aspect of your work that I was thinking about when I was talking to you which is that it sounded Yeah. Hmm. to me  Liz  as though you - and  uh  maybe I didn't q- understand this  but it sounded to me as though part of the analysis that you're doing involves taking segments which are of a particular type and putting them together. And th- so if you have like a p- a s- you know  speech from one speaker  then you cut out the part that's not that speaker  and you Yeah. Mm-hmm. combine segments from that same speaker to - and run them through the recognizer. Is that right? Well we try to find as close of start and end time of - as we can to the speech from an individual speaker  Mm-hmm. because then we - we're more guaranteed that the recognizer will - for the forced alignment which is just to give us the time boundaries  because from those time boundaries then the plan is to compute prosodic features. Mm-hmm. And the sort of more space you have that isn't the thing you're trying to align the more errors we have. Mm-hmm. Um  so  you know  that - that - it would help to have Cuz i- O_K. either pre-processing of a signal that creates very good signal-to-noise ratio  which I don't know how possible this is for the lapel  um  or to have very - to have closer  um  time - you know  synch times  basically  around the speech that gets transcribed in it  or both. And it's just sort of a open world right now of exploring that. So I just wanted to see  you know  on the transcribing end from here things look good. Uh  the I_B_M one is more - it's an open question right now. And then the issue of like global processing of some signal and then  you know  before we chop it up is - is yet another way we can improve things in that. What about increasing the flexibility of the alignment? O_K. Do you remember that thing that Michael Finka did? that experiment he did a while back? Mm-hmm. Right. You can  um - Hhh. The problem is just that the acoustic - when the signal-to-noise ratio is too low  um  you - you'll get  a- uh - an alignment with the wrong duration pattern or it - Oh  so that's the problem  is the - the signal-to-noise ratio. Yeah. It's not the fact that you have like - I mean  what he did is allow you to have  uh  words that were in another segment move over to the - at the edges of - of segmentations. Mm-hmm. Or even words inserted that weren't - weren't there. Right  things - things near the boundaries where if you got your alignment wrong - cuz what they had done there is align and then chop. Mm-hmm. Mm-hmm. Um  and this problem is a little bit j- more global. It's that there are problems even in- inside the alignments  uh  because of the fact that there's enough acoustic signal there t- for the recognizer to - to eat  as part of a word. And it tends to do that. S- Yeah. So  uh  but we probably will have to do something like that in addition. Anyway. So  yeah  bottom - bottom line is just I wanted to make sure I can be aware of whoever's working on these signal-processing techniques for  Yeah. uh  detecting energies  because that - that'll really help us. O_ K  uh tea has started out there I suggest we c- run through our digits and  O_K. Uh  So  Transcript three zero three one dash three zero five zero. O_ three six eight zero four nine six zero one seven zero five zero two nine three four six two seven two O_ four six four O_ eight four one five two eight one nine O_ nine three seven seven six two zero one O_ nine three seven six O_ O_ two eight four two five four nine five six seven seven eight nine zero. Transcript three zero seven one three zero nine zero. zero zero four two one seven three three seven four three two five five six three nine eight two six six seven O_ seven four O_ eight O_ one O_ zero five one four zero six two three nine six four five zero seven nine seven one seven six one seven one eight five O_ three eight seven two nine seven zero four two nine nine O_ nine nine zero nine three. Transcript three one one one dash three one three zero. two eight seven eight zero two four three O_ four O_ eight O_ six three one six eight eight four seven six eight five nine eight nine O_ zero one zero six six five three three four nine six four O_ O_ eight five nine zero six eight nine seven nine eight eight O_ four nine O_ zero three one six zero two eight one. Transcript three one five one dash three one seven zero. four four nine nine five five zero six zero six seven eight O_ O_ seven seven O_ three nine four zero four seven zero six one six eight two four two nine  three nine nine four five O_ seven two six zero eight four nine five six seven seven O_ one O_ eight zero one two O_ one eight four two six four zero one nine. Transcript three three - three three one one three three three zero. zero six seven one three zero seven four two seven three eight six four five zero seven two eight four nine six O_ four five O_ five O_ zero eight eight one one two four seven three nine seven five four eight two six six four five seven seven nine eight eight nine O_ O_ seven one two three one. Transcript two eight nine one  two nine one O_. four one eight five two seven seven two four zero six five three four seven seven nine one six eight six three nine O_ one five O_ one two five two four three four seven five eight four O_ four five seven five six O_ nine four seven zero five six nine three two nine five zero nine O_ six zero three one nine one O_ five two three. Transcript two nine three one dash two nine five zero. five zero three six eight one six nine zero zero eight nine one nine eight seven O_ six O_ five zero four eight zero two eight four one two three O_ five three one eight five two one six five six seven four zero eight nine three zero nine nine nine six four four O_ zero zero three seven five two one one O_ eight eight nine. three one four five zero seven four five six six three. Transcript two nine nine one dash three O_ one O_. eight five nine O_ zero one three one five O_ five four four O_ O_ nine four two five six seven four six eight three six seven seven eight four eight nine O_ O_ one zero one six one three five zero one five seven two eight nine three four five zero eight seven four seven one one eight four zero three zero five one. O_K  we're done. ",The Berkeley Meeting Recorder group discussed digits data  recent ASR results  the status of transcriptions  and disk space and storage format issues. Approximately two hours of digits have been recorded  half of which have been extracted. Researchers doing ASR are looking into methods for generating a better channel-based segmentation to improve recognition results for close-talking microphone data. Transcription checking procedures were reviewed  and efforts to coordinate the channelization and presegmention of data with the tightening of time bins were discussed. The group also talked about downsampling and strategies for coping with low disk space. Digits forms will instruct speakers to read digits separately and not as connected numbers. A tentative decision was made to collect overlapping digits from speakers. Transcribers will be given channelized data that has been segmented for speech/non-speech boundaries to determine whether such pre-processing facilitates the transcription process. Participants have been slow in returning the relevant forms necessary for matching digits data with speaker IDs. Forms containing digits that were written out in English were unsuccessful for obtaining desired prosodic groupings. Acoustic adaptation is problematic for speakers who seldom talk during meetings. Speaker overlap causes recognition errors. The lapel microphone is problematic as it picks up overlapping background speech. More disk space is needed. Loading long waveforms in X Waves is very time consuming. The dictionary must be updated with forms introduced as part of speaker fe008's set of transcription conventions. The ongoing tightening of boundaries on time bins causes segment boundaries to change  indicating potential problems for other ongoing processing tasks. A test set of digits data totalling two hours is nearly complete. Digit extraction has been performed on roughly half of this data. Future work may involve experimenting with the reading of digits in different prosodic groupings. Preliminary ASR results were discussed. Subsequent efforts will involve generating a better channel-based segmentation  and possibly doing echo cancellation  to improve recognition on the close-talking microphone data. Filename conventions are being standardized. Transcription checking procedures have been formalized  including a spell check  producing an exhaustive list of forms identified in the data  attributing every utterance to the appropriate speaker ID  glossing spoken forms with their full orthographic counterparts  e.g. 'cuz' and 'because'  transcribing acronyms  and encoding comments  i.e. glosses  vocalic and non-vocalic non-speech events  pragmatic cues  and the standardization of spoken forms  e.g. 'mm-hmm'. Scripts will be used to convert a channelized master copy into forms that are appropriate for doing both recognition and linguistic analysis. Coordinated efforts are in progress between the transcriber pool and speaker mn014 to fine-tune presegmented output to handle data from a variety of speakers and regions of speaker overlap. 
"Nice. O_K. to - to handle. Is that good? Right. Yeah  I've have never handled them. Goats eat cans  to my understanding. Did we need to do these things? Tin cans. You don't need to read the digits if we think that that's torture but the reason these are out there is do put your name on it because it's the only record I have of actually who was sitting there. Wow . O_K. Um  your name and some but it . O_K. You should be in play . And we can - you can flip this and uh get rid of the screen but @@ Could I hit - hit F_seven to do that? on the - The uh - the - the remote @@ Robert? @@ Oh  the remote will do it O_K. Cuz I'm already up there? O_K. @@ I'm in control here. O_K. Have a great meeting. I'll - I'll come back up in about an hour and check and see if you're still meeting . You are in control. All ready? Wow  we're all so high tech here. Yet another p- PowerPoint presentation. I - Well it makes it easier to do PowerPoint. Certainly does. So  we were - Ah! Johno  where are you? O_K. So  Let's see. Which one of these buttons will do this for me? Aha! O_K. Should you go back to the first one? Well - Do I wanna go back to the first one? O_K. I'm sorry I - Well  I mean  just to - O_K. Introduce. O_K. Yeah  um- Well  ""the search for the middle layer"". It's basically uh I can read! I'm kidding. talks about uh - It just refers to the fact that uh one of main things we had to do was to decide what the intermediate sort of nodes were  you know  because - Mm-hmm. But if you really want to find out what it's about you have to click on the little light bulb. Although I've - I've never - I don't know what the light bulb is for. I didn't i- install that into my PowerPoint presentation. It opens the Assistant that tells you that the font type is too small. Ah. Do you wanna try? Ach u- I'd prefer not to. O_K. Continue. It's a needless good idea. Is that the idea? O_K. Why are you doing this in this mode and not in the presentation mode? Because I'm gonna switch to the JavaBayes Oh! O_K. Of course. Mm-hmm. program and then if I do that it'll mess everything up. I was wondering. Is that O_K? Yeah  it's O_K. Sure. Can you maximize the window? Proceed. You want me to - Wait  what do you want me to do? Can you maximize the window so all that stuff on the side isn't - doesn't appear? No  It's O_K. It's - It'll work . Well I can do that  but then I have to end the presentation in the middle so I can go back to open up JavaBayes. O_K  fine. Here  let's see if I can - Alright. Very nice. Is that better? Yeah. O_K. Uh - I'll also get rid of this ""Click to add notes"". O_K. Perfect. So then the features we decided - or we decided we were - talked about  right? Uh the - the prosody  the discourse  verb choice. You know. We had a list of things like ""to go"" and ""to visit"" and what not. The ""landmark-iness"" of uh - I knew you'd like that. Nice coinage. Thank you. uh  of a - of a building. Whether the- and this i- we actually have a separate feature but I decided to put it on the same line for space. ""Nice walls"" which we can look up because I mean if you're gonna get real close to a building in the Tango mode  right  there's gotta be a reason for it. And it's either because you're in route to something else or you wanna look at the walls. The context  which in this case we've limited to ""business person""  ""tourist""  or ""unknown""  the time of day  and ""open to suggestions""  isn't actually a feature. It's ""We are open to suggestions."" Right. can I just ask the nice walls part of it is that uh  in this particular domain - you said - be - i- it could be on two different lines but are you saying that in this particular domain it happens the - that landmark-iness cor- is correlated with No. We have a separate Oh - They're separate things. their being nice w- O_K. feature. Yeah. I either could put ""nice walls"" on its own line or ""open to suggestions"" off the slide. And - and - Like you could have a p- By ""nice"" you mean - You - Like you could have a post office with uh - you know  nice murals or something. Right. O_K. So ""nice walls"" is a stand in for like architecturally it  uh - significant or something like that. Or one time I was at this - Architecturally appealing from the outside. But see the thing is  if it's - O_K. Yeah but if it's architecturally significant you might be able to see it from - Like you m- might be able to ""Vista"" it  right? And be able to - Mm-hmm. Mm-hmm. Appreciate it. Yeah  versus  like  I was at this place in Europe where they had little carvings Mm-hmm. of  like  dead people on the walls or something. I don't remember w- Uh-huh. It was a long time ago. There's a lot of those. But if you looked at it real close  you could see the - the in- intricacy of the - of the walls. O_K. So that count as - counts as a nice wall. Right. The - O_K. Mm-hmm. Right. Something you want to inspect at close range because it's interesting. O_K. The - Exactly. Hmm. Robert? Well there - there is a term that's often used. That's ""saliency""  or the ""salience"" of an object. And I was just wondering whether that's the same as what you describe as ""landmark-iness"". But it's really not. I mean an object can be very salient but Hmm. Not a landmark at all. not a landmark at all. There's landmark for um  touristic reasons and landmark for I don't know navigational reasons or something. Right. Yep. Yeah  we meant  uh  touristic reasons. O_K. Yeah. Hmm. Right. O_K. but you can imagine maybe wanting the oth- both kinds of things there for different um  goals. Hmm. Yeah. Right. Right? But - Yeah. Tourist-y landmarks also happen to be - Wouldn't - couldn't they also be - They're not exclusive groups  are they? Like Or it can be als- non-tourist-y landmarks and They're not mutually exclusive? direct navigational - Yeah. Right. Right. Definitely. O_K. O_K  So our initial idea was not very satisfying  because - uh our initial idea was basically all the features pointing to the output node. Uh. So  a big flat structure. Right? Right. Yep. And uh  so we - Reasons being  you know  it'd be a pain to set up all the probabilities for that. If we moved onto the next step and did learning of some sort  uh according Bhaskara we'd be handicapped. I don't know Well usually  I mean  you know  N_ - belief-nets very well. If you have N_ features  then it's two to the N_ - or exponential in N_. And they wouldn't look pretty. So. Yeah  they'd all be like pointing to the one node. Mm-hmm. Uh. So then our next idea was to add a middle layer  right? So the thinking behind that was we have the features that we've drawn from the communication of some - Like  the someone s- The person at the screen is trying to communicate some abstract idea  like ""I'm -"" the - the abstract idea being ""I am a tourist I want to go to this place."" Right? So we're gonna set up features along the lines of where they want to go and what they've said previously and whatnot. And then we have the means that they should use. Right? but the middle thing  we were thinking along the lines of maybe trying to figure out  like  the concept of whether they're a tourist or whether they're running an errand or something like that along those lines. Or - Yes  we could- things we couldn't extract the - from the data  the hidden variables. Yes  good. So then the hidden variables - hair- variables we came up with were whether someone was on a tour  running an errand  or whether they were in a hurry  because we were thinking uh  if they were in a hurry there'd be less likely to - like - Want to do Vista  right? Because if you want to view things you wouldn't be in a hurry. or th- Right. Or they might be more likely to be using the place that they want to go to as a - like a navigational point to go to another place. Mm-hmm. Whether the destination was their final destination  whether the destination was closed. Those are all - And then ""Let's look at the belief-net"" O_K. So that means that I should switch to the other program. Um right now it's still kind of in a toy version of it  because we didn't know the probabilities of - or - Well I'll talk about it when I get the picture up. No one knows it. O_K. So this right - what we - Let's see. What happens if I maximize this? There we go. But uh - So. The mode basically has three different outputs. The probability - whether the probability of a Vista  Tango  or Enter. Um - The ""context""  we simplified. Basically it's just the businessman  the tourist  unknown. ""Verb used"" is actually personally amusing mainly because it's - it's just whether the verb is a Tango verb  an Enter verb  or a Vista verb. Yeah  that one needs a lot of - And are those mutually exclusive sets? Right. Got it. Uh-huh. Not at all. That's - that - that needs a lot of work. But uh that would've made the probably significantly be more complicated to enter  so we decided that for the purposes of this it'd be simpler to just have three verbs. No. Yeah. Simple. Yeah. Stab at it. Yep. Right. Um - Why don't you mention things about this  Bhaskara  that I am - not - that are not coming to my mind right now. O_K  so - Yeah  so note the four nodes down there  the - sort of  the things that are not directly extracted. Actually  the five things. The ""closed"" is also not directly extracted I guess  from the uh - From the utterance? Well i- it's - it's so- Hmm. Actually  no  wait. It is. O_K  ""closed"" sort of is. it sort of is because it's - because have the - the time of day and the close- it just had the - Right  so f- er and what time it closed. Right  but the other ones  the final destination  the whether they're doing business  whether they're in a hurry  and whether they're tourists  that kind of thing is all uh sort of - you know probabilistically depends on the other things. Yeah. Inferred from the other ones? O_K. And the mode  you know  depends on all those things only. Yeah the - the actual parse is somewhere Yeah. So we haven't uh  managed - Like we don't have nodes for ""discourse"" and ""parse""  although like in some sense they are parts of this belief-net. But uh - up around in here. Mm-hmm. The idea is that we just extract those features from them  so we don't actually have a node for the entire parse  because we'd never do inference on it anyway  so. Mm-hmm. Right. So some of the - the top row of things - What's - what's ""Disc admission fee""? whether they discuss the admission fees. So we looked at the data and in a lot of data people were saying things like ""Can I get to this place?"" ""What is the admission fee?"". So that's like a huge uh clue that they're trying to Enter the place rather than uh to Tango or Vista  so. Oh. Uh-huh. Right. O_K. I see. There were - there'd be other things besides just the admission fee  Mm-hmm. but you know  we didn't have - That was like our example. That was the initial one that we found. Mm-hmm. O_K. So there are certain cues that are very strong either lexical or topic-based um  concept cues From the discourse that - Yeah. for one of those. And then in that second row or whatever that row of Time of Day through that - So all of those - Some of them come from the utterance and some of them are sort of either world knowledge or situational things. Right? So that you have no distinction between those and @@ O_K. Right. One  uh - Uh. Um  anything else you want to say Bhaskara? Um. ""Unmark @@ Time of Day"" Yeah  I m- I mean - One thing - uh - Yeah. They're - they're are a couple of more things. I mean Uh. I would actually suggest we go through this one more time so we - we all uh  agree on what - what the meaning of these things is at the moment and maybe what changes Yeah  th- we - O_K. so one thing I - I'm you know unsure about  is how we have the discus- uh - the ""admission fee"" thing set up. So one thing that we were thinking was by doing the layers like this  Uh - we kept um - things from directly affecting the mode beyond the concept  but you could see perhaps discus- the ""admission fee"" going directly to the mode pointing at ""Enter""  Mm-hmm. right? Versus pointing to- just at ""tourist""  Mm-hmm. Mm-hmm. O_K? But we just decided to keep all the things we extracted to point at the middle and then down. Mm-hmm. Why is the landmark - O_K. The landmark is facing to the tourists. That's because we're talking about landmarks as touristic landmarks not Right. Yeah. as possible um Navigational landmarks  yeah. Navigational navigational landmarks so cue. Mm-hmm. Then - Yeah  that would be whatever building they referred to. Prosody. Right. So let's see. The variables. Disc- ""admission fee"" is a binary thing  ""time of day"" is like morning  afternoon  night. Is that the deal? Mm-hmm. Yeah. That's how we have it currently set up  but it could be  you know  Yep. Yeah. based upon hour or dis- we could discrete it - des- descret-ize it. Yeah. Whatever granularity. Uh-huh. Yeah. Mm-hmm. Yeah. Yeah. Normally context will include a huge amount of information  but um  we are just using the particular part of the context which consists of the switch that they flick to indicate whether they're a tourist or not  I guess. Yep. O_K. So that's given in their input. Right? Right. So - Right  so it's not really all of context. Similarly prosody is not all of prosody but simply for our purposes whether or not they appear tense or relaxed. Mm-hmm. that's very nice  huh? O_K. The - the - So the context is a switch between tourist or non-tourist? Or also unknown? and Or un- unknown  yeah. Yeah. Unknown  right? O_K. So final dest- So it seems like that would really help you for doing business versus tourist  Which is th- Which one? but O_K. so the the context being um  e- I don't know if that question's sort of in general  ""are you -"" I mean the - ar- ar- are do they allow business people to be doing non-business things at the moment? Yeah  it does. O_K. So then you just have some probabilities over - O_K. - over which which of those it is. Everything is probablistic  and - There's always - Yeah. Um  right. So then landmark is - Oh  sorry. ""Verb used"" is like  right now we only have three values  but in general they would be a probability distribution over all verbs. Mm-hmm. Rather  let me rephrase that. It - it can take values in the set of all verbs  that they could possibly use. Mm-hmm. Um ""nice walls"" is binary  ""closed"" is binary ""final destination""  again - Yeah  all those are binary I guess. And ""mode"" is one of three things. So  the - the middle layer is also binary? No. Yeah  anything with a question mark after it in that picture is a binary node. Uh. It - Yeah. But all those things without question marks are also binary. Right? Which things? Mm-hmm. Nice walls? Wi- Oh. ""Nice walls"" is uh - something that we extract from our world knowledge. Mm-hmm. Yeah  a- Oh yeah. Sorry. It is binary. It is binary but it doesn't have question mark because it's extracted. Yeah. That's true. Yeah. O_K  I see your point. Yeah. O_K. I - I gotcha. Uh-huh. Yeah  similarly ""closed""  I guess. So we can either be in a hurry or not  but we cannot be in a medium hurry at the moment? Well  we- To do that we would add another uh - value for that. Mm-hmm. O_K. And that would require s- updating the probability distribution for ""mode"" as well. Mm-hmm. Because it would now have to like uh - take that possibility into account. Mm-hmm. Take a conti- Mm-hmm. So um  of course this will happen when we think more about the kinds of verbs that are used in each cases but you can imagine that it's verb plus various other things Yeah  yeah. Yeah. that are also not in the bottom layer that would - that would help you - Like it's a conjunction of  I don't know  you know  the verb used and some other stuff that - that would determine - Yeah. Exactly. Right. Other syntactic information you mean? Yeah. Um. well the - the - sort of the landmark is - is sort of the object right? the argument in a sense? Usually. I - I don't know if that's always the case I - I guess haven't looked at the data as much as you guys have. So. Um. that's always warping on something - some entity  and um - Mm-hmm. Mm-hmm. Uh maybe at this stage we will - we do want to - uh sort of get - uh modifiers Hmm. in there because they may also tell us whether the person is in a hurry or not Yeah. Yeah. I want to get to the church quickly  and uh - Mm-hmm. That would be a cue. Yeah  right. what's the fastest way Yeah  correct. Mm-hmm. Um. Right. Excellent. Do we have anything else to say about this? We can do a little demo. Oh the- Yeah  we could. But the demo doesn't work very well. O_K. No  then it wouldn't be a demo I mean - We can do a demo in the sense that we can um  I was just gonna s- Observe nodes. just ob- observe the fact that this will  in fact do inference. So we can  you know  set some of the uh nodes and then Yeah. Go ahead. try to find the probability of other nodes. O_K. Dat-dat-dah. What should I observe? Just se- set a few of them. You don't have to do the whole thing that we did last time. Just like uh  O_K. maybe the fact that they use a certain verb - Actually forget the verb. O_K. just uh - I don't know  say they discussed the admission fee - O_K. and uh - the place has nice walls I love nice walls  O_K? I'm a big fan. it's starting to grow on me and it's night. And the time of day is night? Yeah  no wait. That - that doesn't uh - it's not really consistent. They don't discuss the admission fee. Make that false. Alright. And it's night. Oh  they - O_K. Oh whoops. I forgot to uh - That didn't work. Ach! I'd like to do that again. One thing that bugs me about JavaBayes is you have to click that and do this. Yeah. That seems kind of redundant but. O_K. That all you want? Yes. O_K. So let's see. I want ""Go"" and  right  ""query"". to query  right? the mode. O_K  and then on here - So let's see. So that is the probability that they're Entering  Vista-ing or Tango-ing. And uh - Mm-hmm. Yeah. So slightly biased toward ""Tango""ing O_K. Yeah. If it's night time  they have not discussed admission fee  and the n- walls are nice. So  yeah. I guess that sort of makes sense. The reason I say the demo doesn't work very well is yesterday we uh - observed everything in favor of taking a tour  and it came up as ""Tango""  right? Over and over again. So. We couldn't - Uh-huh. we couldn't figure out how to turn it off of ""Tango"". Huh! It loves the Tango. Um. Well  that's obviously just to do with our probabilities. Like  Yeah  yeah. Yeah. we totally hand-tuned the probabilities  right. We were like O_K. ""hmm  well if the person does this and this and this  let's say forty percent for this  fifty per-"" Like  you know. So obviously that's gonna happen. Right. Yeah. Yeah but it - it Maybe the bias toward ""Tango""ing was yours  then? Yeah  that's - that's at - Yeah. It's - So we have to like fit the probabilities. Spent my youth practicing the tango de la muerte. So  the real case? However you know  it - The purpose was not really  at this stage  to come up with meaningful probabilities but to get thinking about that hidden middle layer. Mm-hmm. And so th- And - We would actually - I guess once we look at the data more we'll get more hidden nodes  Mm-hmm. Yeah. but I'd like to see more. Not because it would expedite the probabilities  cuz it wouldn't. It would actually slow that down tremendously. Um. Well  yeah  I guess. Not that much though. Only a little early . But. No  I think we should have uh - exponentially more middle nodes than features we've extracted. O_K. I'm ju- I'm just jo- So. Are ""doing business"" versus ""tourist"" - They refer to your current task. Like - like current thing you want to do at this moment. Um. Yeah  well - That's - that's an interesting point. Whether you're - It's whether - And are th- It's not - I think it's more like ""Are you are tourist? are you in Ham- like Heidelberg for a -"" Oh  so  I thought that was directly given by the context switch. That's a different thing. What if the context  which is not set  but still they say things like  ""I want to go uh  see the uh - Is it - the - the castle and uh  Well the- I kind of thought of ""doing business"" as more of running an errand type thing. et cetera."" Yeah. Business on the other hand is  uh  definitely what you're doing. So if you run out of cash as a tourist  and - and - and you need to go to the A_T- So i- wi- th- O_K. Oh  I see  you may have a task. wh- you have to go get money and so you are doing business at that stage. Mmm. Yeah. Right. ""How do I get to the bank?"" I see. Hmm. And that'll affect whether you want to enter or you if you - kinda thing. O_K. So the ""tourists"" node should be um  very consistent with the context node. Right? If you say that's more their - Yeah  I think this context node is a bit of a - in general what their background is. I don't know  like in d- Uh - Do we wanna have - Like it's - Are you assuming that or not? Like is that to be - I mean if that's accurate then that would determine tourist node. If the context were to set one way or another  that like strongly uh Mm-hmm. um  says something about whether - whether or not they're tourists. So what's interesting is when it's not - when it's set to ""unknown"". Mm-hmm. Mm-hmm. We- what set the - they set the context to ""unknown""? O_K. O_K. Right now we haven't observed it  so I guess it's sort of averaging over all those three possibilities. Mm-hmm. Mm-hmm. Right. But yes  you can set it to un- ""unknown"". And if we now do - leave everything else Oops. as is the results should be the same  right? No. Well no  because we - Th- the way we set the probabilities might not have - Yeah  it's - it's an - it's an issue  right? Like - Pretty much the same? Yeah  it is. So the issue is that um in belief-nets  it's not common to do what we did of like having  you know  a d- bunch of values and then ""unknown"" as an actual value. Yeah. What's common is you just like don't observe the variable  right  and then just marginalizes - Yeah. Yep. But uh - We didn't do this because we felt that there'd - I guess we were thinking in terms of a switch that actually - We were thi- Yeah  Mm-hmm. We were th- But uh - I don't know y- what the right thing is to do for that. I'm not - I don't know if I totally am happy with the way it is. Why don't we - Can we  um - How long would it take to - to add another node on the observatory and  um  play around with it? Another node on what? Uh  well it depends on how many things it's linked to. Let's just say make it really simple. If we create something that for example would be um - So th- some things can be landmarks in your sense but they can never be entered? So for example s- a statue. Good point. Yeah? Right. Mm-hmm. So maybe we wanna have ""landmark"" meaning now "" enterable landmark"" Yeah  that's true. versus  um something that's simply just a vista point  for example. Yeah? uh  a statue or um - So basically it's addressing a variable that's ""enterable or not"". So like an ""enterable  question mark"". Also - you know  didn't we have a size as one? What? The size of the landmark. Um. Not when we were doing this  but I guess at some point we did. Cuz if it's - Yeah. For some reason I had that - O_K  that was a thought that I had at one point but then went away. So you want to have a - a node for like whether or not it can be entered? Well  for example  if we include that  yeah? Yeah. um  accessibility or something  yeah? Hmm. ""Is it - Can it be entered?"" then of course  this is sort of binary as well. Yeah. And then um  there's also the question whether it may be entered. In the sense that  you know  if it's Tom - the house of Tom Cruise  you know  it's enterable but you may not enter it. You know? You're not allowed to. Yeah. Unless you are  whatever  his - his divorce lawyer or something. Yeah? Yeah. and um - Way- And these are very observable sort of from the - from the ontology sort of things. Does it actually help to distinguish between those two cases though? Whether it's practically speaking enterable  or actually physically enterable or not? It seems like it would for uh  uh determining whether they wanna go into it or not. y- y- Cuz they - If - Well I can see why - If you're running an errand you maybe more likely to be able to enter places that are usually not al- w- you're not usually - not allowed to uh- m- Let's get this uh b- clearer. S- so it's matrix between if it's not enterable  period. Whether it's a - Whether it's a public building  and whether it's - actually has a door. Yeah  exactly. O_K. This is sort of uh- Mm-hmm. So Tom Cruise's house is not a public building but it has a door. Right. Mm-hmm. But the thing is - O_K  sh- explain to me why it's necessary to distinguish between whether something has a door and is not public. Or  if something - It seems like it's equivalent to say that it doesn't have a door a- and it - Mm-hmm. Or ""not public"" and ""not a door"" are equivalent things  it seems like in practice. Yeah. Right. Yeah. So we would have - What does it mean  then  that we have to - we have an object type statue. That really is an object type. Right. So there is - there's gonna be a bunch of statues. And then we have  for example  an object type  hmm  that's a hotel. How about hotels? O_K. So  the most famous building in Heidelberg is actually a hotel. It's the hotel Zum Ritter  which is the only Renaissance building in Heidelberg that was left after the big destruction and for the Thirty Years War  blah-blah-blah. Hmm. Does it have nice walls? It has wonderful walls. Excellent. Um- And lots of detail  c- and carvings  engravings and so forth  so. But  um  it's still an unlikely candidate for the Tango mode I must say. But. Um. So - s- So if you are a d- Well it's very tricky. So I guess your question is - so far I have no really arg- no real argument why to differentiate between statues as - statues and houses of celebrities  from that point of view. Huh. O_K. Let - Let's do a - Can we add  just so I can see how it's done  uh  a ""has door"" property or - ? O_K. What would it  uh  connect to? Like  what would  uh  it affect? Um  I think  um  it might affect - Oh actually it's - it - it wouldn't affect any of our nodes  right? What I was thinking was if you had a - like - Oh it's - it affects th- The ""doing business"" is certainly not. You could affect - Theoretically you could affect ""doing business"" with ""has door"". Yeah. O_K. Hmm. It should  um  Right. Let's see. inhibit that  right? Yeah  I don't know if JavaBayes is nice about that. It might be that if you add a new thing pointing to a variable  you just like - it just overwrites everything. But you can check. Well  we have it saved. So. O_K. We can rel- open it up again. It's true. The safety net. I think you could just add it. I mean  I have before O_K. Whew! Well that's fine  but we have to see the function now. Has it become all point fives or not? Oh  right. Let's see. So this is ""has door"" Uh  true  false. That's acceptable. And I want to edit the function going to that  right? No. This is fine  this business. Oh no. Right. It was fine . 2x added this one. Yep. What would be nice if it - is if it just like kept the old function for either value but. This - Nope. Oh. Didn't do it. Oh wait  it might be - Did we w- Yes  that's not good. That's kind of annoying. O_K  so just dis- dismiss everything. Close it and - and load up the old state so it doesn't screw - screw that up. Let's see. Oops. Hmm. Maybe you can read in? Ha- So have you used JavaBayes a lot? Yes. Really I ha- I've - I haven't used it a lot and I haven't used it in the last you know many months so O_K. um  uh  we can ask someone. It might be worth uh - asking around. Like  Um. Yeah. we looked at sort of uh - a page that had like a bunch of - Srini - O_K. Yeah  S- I guess he'd be the person. Srini's the one to ask I would say. Um. He might know. Yeah. Cuz - Yeah. And. I mean in a way this is a lot of good features in Java it's cra- has a GUI and it's uh - Mm-hmm. I guess those are the main two things. It does learning  it has - Mm-hmm. Yeah. No it doesn't  actually. What? I didn't think it did learning. O_K. Maybe it did a little bit of learning  I don't remember. Oh right. Maybe you're right. O_K. Right. But uh - it's free. Which is w- quite positive  yeah. But uh  yeah. Maybe another thing that uh - But I mean its interface is not the greatest. So. Mm-hmm. But actually it had an interface. A lot of them were like  you know. Yep. Command line. Huh. What is the c- code? Can w- can we see that? How do you write the code or do you actually never have to write any code there? The c- Yeah. There is actually a text file that you can edit. But it's - You don't have to do that. There's like an X_M_L format for Bayes-nets. Is it X_M_L? The- there is one. I don't know if this uses it. Oh  I see. No this doesn't use it. But it - I didn't think it did. Yeah  the - the - You can look at the text file. Yeah. But do you have it here? Well  maybe you don't. Uh  yes I do actually. Let me see. Oh yes  of course. Like  there's the - Oh man  I didn't n- Is there an ampersand in DOS? Nope. Just s- l- start up a new D_O_S. We- That's alright. I can probably double cli- click on it. Or - Yeah  right. n- uh - Let's see. Yep. Let's see  come on. It'll ask you what you - what it wants - what you want to open it with and see what BAT   I guess. One of these days  it should open this  theoretically. Go - Right mouse. Open with. That's - Oh! Oh there we go. Maybe it was just - Oh! W- Oh. Ah  it was dead. To the world. God! O_K. Through the old Notepad. That's my favorite editor. I like - Wordpad? I like Word Pad because it has the uh - I - the returns  the carriage returns on some of them. Mm-hmm. O_K. You know how they get ""auto-fills"" I guess  or whatever you call it. Mmm-hmm. Anyway  there it is. So this is sort of LISP-y? No. Uh  Yeah. It just basically looks like it just specifies a bunch of STRUCTs. Mm-hmm. Yeah. That's how actual probability tables are specified. As  like  lists of numbers. Yeah. Mm-hmm. Mm-hmm. So theoretically you could edit that. Mm-hmm. But they're not very friendly. It just that - it's - Yeah the ordering isn't very clear on - Right. So you'd have to like figure out - The layout of the table. Yeah. Like you have to go and - Yeah. Well I - Actually we could write a program that could generate this. Yeah. I think so. You could. Yeah you could. it's not - We were doing it - Yeah we can maybe write an interface th- for uh entering probability distributions easily  something like - like a little script. That might be worth it. And that might do. Yeah. I actually seem to recall Srini complaining about something to do with Entering probability so this is probably Yeah  it's - Yeah. The other thing is it is in Java so. We could manipulate the source itself? Or - Yeah. Do you have the true source files or just the class? I don't know if he actually - Yeah. Uh  yeah. we do Does he - I - I saw directory called ""source""  or - Oh. Mm-hmm. I didn't e- Yeah. Go up one? Up one. Yeah. Ah yes  good. ""Source"". That's - that's quite nice. I don't know if it actually manipulate the source  though. That might be a bit complicated. I think it might - it might be simpler to just have a script that  you know - Mm-hmm. It's  like  friendly  it allows you enter things well. The d- the data tables. Yeah. Right. But if th- if there is an X_M_L file that - or format that it can also read - I mean it just reads this  right? When it starts. Mm-hmm. Yeah I know there is an - I was looking on the we- web page and he's updated it for an X_M_L version of I guess Bayes-nets. There's a Bayes-net spec for - in X_M_L. Mm-hmm. He's - Like this guy has? The JavaBayes guy? So - but  e- he doesn't use it. So in what sense has he updated it? Yeah. Well th- you can either - you ca- or you can read both. Oh. I see. To my understanding. O_K. That would be awesome. Oh. Because uh - Well at least the - uh - I could have misread the web page  I have a habit of doing that  but. O_K. O_K  wonderful. So you got more slides? Do I have more slides? Um yes  one more. ""Future Work"". I think every presentation have a should have a ""Future Work"" slide. But uh it's basically - we already talked about all this stuff  so. Um. The additional thing is I guess learning the probabilities  also. E- That's maybe  I don't know - If - Does - Uh that's future future work. That's - Yeah. Right. Very future. Mm-hmm. And of course if you have a presentation that doesn't - have something that doesn't work at all  then you have ""What I learned""  as a slide. Can't you have both? You could. My first approach failed. Right. What I learned. O_K  so I think that uh- our presentation's finished. Good. I know what I like about these meetings is one person will nod  and then the next person will nod  and then it just goes all the way around the room. So the uh - I missed my turn. No I - Earlier I went and Bhaskara went and you did it. You did it. It's like yawning. It's like yawning. And this announcement was in stereo. Ha. O_K. So this means um - Should I pull up the net again? Yes. Yeah. Could you put the - the um  net up again? Thanks. There we go. And actually I was - cuz I got a wireless mike on. So a more general thing than ""discussed admission fee"" um  could be - I - I'm just wondering whether the context  the background context of the discourse might be - I don't know  if there's a way to define it or maybe you know generalize it some way um  there might be other cues that  say  um  in the last few utterances there has been something that has strongly associated with say one of the particular modes uh  I don't know if that might be - Mm-hmm. uh  and - and into that node would be various - various things that - that could have specifically come up. I think we - I think a - a sort of general strategy here - You know  this is - this is excellent because - um it gets you thinking along these terms - is that maybe we ob- we could observe a couple of um discourse phenomena such as the admission fee  and something else and something else  that happened in the discourse before. Mm-hmm. Right. And um - let's make those four. And maybe there are two um - So maybe this could be sort of a separate region of the net  which has two - has it's own middle layer. Maybe this  you know  has some kind of um  funky thing that di- if this and this may influence these hidden nodes of the discourse which is maybe something that is uh  a more general version of the actual phenomenon that you can observe. So things that point towards - So instead of single node  for like  if they said the word ""admission fee"" - Exactly. Yeah. ""admission fee""  or maybe  you know  ""how much to enter"" or you know something  other cues. Exactly. Opening hours or something like that. That would all f- funnel into one node that would constitute entrance requirements or something like that. So "" pay a visit"" - Mm-hmm. uh- uh- d- Sure. Yeah. Yeah? I mean it sort of get into plan recognition kinds of things in the discourse. I mean that's like the bigger um  Exactly. Yeah? And then maybe there are some version of it. discourse acts if they happened before  um it's more for um a cue that the person actually wants to get somewhere else and that you are in a - in a - in a route um  sort of proceeding past these things  so this would be just something that - where you want to pass it. Hmm? Is that it? However these are of course then the - the nodes  the observed nodes  for your middle layer. So this again points to ""final destination""  ""doing business""  ""tourist hurry"" and so forth. Mm-hmm. O_K. Yeah? And so then we can say  ""O_K. we have a whole region -"" That's a whole set of discourse related cues to your middle layer. Right? in a e- Yeah  exactly. And this is just - then just one. So e- because at the end the more we um - add  you know  the more spider-web-ish it's going to become in the middle and the more of hand editing. It's going to get very ugly. But with this way we could say ""O_K  these are the discourse phenomena. They ra- may have there own hidden layer that points to some of the - the real hidden layer  um or the general hidden layer. Sure. And the same we will be able to do for syntactic information  the verbs used  the object types used  modifiers. And maybe there's a hidden layer Yep. for that. And so forth and so forth. Then we have context. Yeah. So essentially a lot of those nodes can be expanded into little Bayes-nets of their own. Yep. Mm-hmm. Precisely. So. One thing that's kind of been bugging me when I - more I look at this is that the - I guess  the fact that the - there's a complete separation between the observed features and in the output. Yeah. I mean  it makes it cleaner  but then uh - I mean. That's true. For instance if the discourse does - well for instance  the ""discourse admission fee"" node What do you mean by that? Uh-huh. seems like it should point directly to the - or increase the probability of ""enter directly"" versus ""going there via tourist"". Yeah. Or we could like add more  uh  sort of middle nodes. Like we could add a node like do they want to enter it  which is affected by admission fee and by whether it's closed and by whether it has a door. Mm-hmm. So it's like - Right. There are - Those are the two options. Either like make an arrow directly or put a new node. Hmm. Yeah  that makes sense. Yeah. And if it - if you do it - If you could connect it too hard you may get such phenomenon that - like ""So how much has it cost to enter?"" and the answer is two hundred fifty dollars  and then the persons says um ""Yeah I want to see it."" Yeah? meaning ""It's way out of my budget"" um - There are places in Germany where it costs two hundred fifty dollars to enter? Um  nothing comes to mind. Without thinking too hard. Um  maybe  yeah of course  um opera premiers. Really? So you know. Hmm. Or - or any good old Pink Floyd concert. I see. If you want to see ""The Magic Flute"" or something. Yeah. Or maybe um  a famous restaurant. or  I don't know. There are various things that you might w- not want to eat a meal there but your own table . The Spagos of Heidelberg. I think that the h- I mean nothing beats the - the admission charge prices in Japan. So there  two hundred dollars is - is moderate for getting into a discotheque. You know. Really. Then again  everything else is free then once you're ins- in there. Food and drink and so forth. So. I mean. But i- you know  i- we can - Something - Somebody can have discussed the admission fee and u- the answer is s- if we - um  you know  um - still  Hmm. based on that result is never going to enter that building. You know? Because it's just too expensive. Oh yeah  I think I see. So the discourse refers to ""admission fee"" but it just turns out that they change their mind in the middle of the discourse. Yeah. you have to have some notion of not just - I mean there's a - there's change across several turns of discourse so Right. Mm-hmm. I don't know how - if any of this was discussed - but how i- if it all this is going to interact with whatever general uh  other - other discourse processing that might be happen. I mean. Mm-hmm. Yeah. What sort of discourse processing is uh - are the - How much is built into SmartKom and - It works like this. The uh  um - I mean. The first thing we get is that already the intention is sort of t- They tried to figure out the intention  right? simply by parsing it. And this um - m- won't differentiate between all modes  yeah? but at least it'll tell us ""O_K here we have something that - somebody that wants to go someplace  now it's up for us to figure out what kind of going there is - is - is happening  and um  if the discourse takes a couple of turns before everything - all the information is needed  what happens is you know the parser parses it and then it's handed on to the discourse history which is  um o- one of the most elaborate - elaborate modules. It's - it's actually the - the whole memory of the entire system  that knows what - wh- who said what  which was - what was presented. It helps an- an- anaphora resolution and it - and it fills in all the structures that are omitted  so  um  because you say ""O_K  how can I get to the castle? ""Oh  how - how much is it?"" and um ""yeah I would like uh - um - to g- let's do it"" and so forth. So even without an a- ana- anaphora somebody has to make sure that information we had earlier on is still here. Mm-hmm. Because not every module keeps a memory of everything that happened. so whenever the uh  um person is not actually rejecting what happened before  so as in ""No I really don't want to see that movie. I'd rather stay home and watch T_V"" um - What movie was selected in what cinema in what town is - is going to be sort of added into the disc- into the representations every di- at each dialogue step  by the discourse model - discourse model  Yeah  that's what it's called. and  um  it does some help in the anaphora resolution and it also helps in coordinating the gesture screen issues. So a person pointing to something on the screen  you know  Hmm. the discourse model actually stores what was presented at what location on the s- on the screen so it's a - it's a rather huge - huge thing but um - um - we can sort of - It has a very clear interface. We can query it whether admission fees were discussed in the last turn and - and the turn before that or you know how deep we want to search O_K. um - which is a question. How deep do we want to sear  you know? Um - but we should try to keep in mind that  you know  we're doing this sort of for research  so we - we should find a limit that's reasonable and not go  you know  all the way back to Adam and Eve. You know  did that person ever discuss admissions fee - fees in his entire life? And the dialogues are pretty - pretty you know concise and - Anyway. So one thing that might be helpful which is implicit in the use of ""admission fee discussion"" as a cue for entry  is thinking about the plans that various people might have. Like all the different sort of general schemas that they might be following O_K. This person is um  finding out information about this thing in order to go in as a tourist or finding out how to get to this place in order to do business. Um  because then anything that's a cue for one of the steps would be slight evidence for that overall plan. Um  I don't know. They're - in - in non- in sort of more traditional A_I kinds of plan recognition things you sort of have you know  some idea at each turn of agent doing something  ""O_K  wha- what plans is this a - consistent with?"" and then get s- some more information and then you see ""here's a sequence that this sort of roughly fits into"". It - it might be useful here too. I - I don't know how you know you'd have to Mm-hmm. I mean the - Hmm. figure out what knowl- what knowledge representation would work for that. u- u- It's in the - these - these - these plan schemas. I mean there are some - some of them are extremely elaborate  you know. ""What do you need - need to buy a ticket?"" You know? Mm-hmm. Mm-hmm. and it - it's fifty steps  huh? Mm-hmm. just for buying a ticket at a ticket counter  you know  and - and maybe that's helpful to look at it - to look at those. It's amazing what human beings can do. W- when we talked uh we had the example  you know  of you being uh - a s- a person on a ticket counter working at railway station and somebody r- runs up to you with a suitcase in his hands  says New York and you say Track seven  huh? And it's because you know that that person actually is following  you know - You execute a whole plan of going through a hundred and fifty steps  you know  without any information other than ""New York""  huh? inferring everything from the context. So  works. Um  even though there is probably no train from here to New York  right? Mmm. Not direct. You'd uh probably have to transfer in Chicago. Mm-hmm. But uh - it's possible. Um  no you probably have to transfer also somewhere else. Right? Is that t- San Francisco  Chicago? I think - Is that possible? One time I saw a report on trains  and I think there is a l- I don't know if - I thought there was a line that went from somewhere  maybe it was Sacramento to Chicago  but there was like a California to Chicago line of some sort. Mm-hmm. Hmm. I could be wrong though. It was a while ago. The Transcontinental Railroad  doesn't that ring a bell? I think it has to exist somewhere. Yeah but I don't know if it's still - They might have blown it up. Well it never went all the way  right? I mean you always had to change trains at Omaha  right? Well most of the way. Uh. One track ended there and the other one started at - five meters away from that Mm-hmm. Yeah. and sort of - Well. You seem to know better than we do so. yeah? Has anybody ever been on an Amtrak? I have. But not transcontinentally. Yeah. I'm frightened by Amtrak myself. What? Why? I just - They seem to have a lot of accidents on the Amtrak. Really? Their reputation is very bad. huh? Yeah. Yeah. It's not maybe reality. It's not like German trains. Like German trains are really great so. But - you know  I don't know whether it's - which ones are safer  you know  statistically. Um  but they're faster. @@ Yeah. Much faster. Mm-hmm. And there's much more of them. Yeah  they're Yeah  it's way better yeah I used - um Amtrak quite a bit on the east coast and I was surprised. Mm-hmm. It was actually O_K. You know  on Boston New York  New York Rhode Island  whatever  Boston. Yeah. Yeah. I've done that kind of thing. Mm-hmm. Yeah. But - That's a different issue. This is going to be an interesting transcript. I - I want to see what it does with uh ""landmark-iness"". Hmm? That's - Yeah. Let's all say it a few more times. Just kidding. So. It'd help it figure it out. Right. Yeah. So by the way tha- that structure that Robert drew on the board was like more um  cue-type-based  right  here's like we're gonna segment off a bit of stuff that comes from discourse and then some of the things we're talking about here are more - you know  we mentioned maybe if they talk about um  I don't know  entering or som- you know like they might be more task-based. Hmm. So I - I don't know if there - There's obviously some - m- more than one way of organizing the variables into something so. I think that um - What you guys did is really nicely sketching out different tasks  Mm-hmm. and maybe some of their conditions. One task is more likely you're in a hurry when you do that kind of s- Mm-hmm. doing business  and - and less in a hurry when uh - you're a tourist Um - tourists may have - never have final destinations  you know because they are eternally traveling around so maybe what - what - what happened - what might happen is that we do get this sort of task-based Mm-hmm. middle layer  Mm-hmm. and then we'll get these sub-middle layers  that are more cue-based. That feed into those? Mm-hmm. Nah? Might be - might be a nice dichotomy of - of the world. So  um I suggest w- to - for - to proceed with this in - in the sense that maybe throughout this week the three of us will - will talk some more about maybe segmenting off different regions  and we make up some - some toy a- observable ""nodes"" - is that what th- Refined y- re- just refine the - O_K. What's the technical term? For which? For the uh - nodes that are observable? The ""outer layer""? Just observable nodes  evidence nodes? The features  I don't know  whatever you - Feature ma- make up some features for those - Yeah. Identify four regions  maybe make up some features for each region and uh - and uh  uh - and uh - middle layer for those. And then these should then connect somehow to the more plan-based deep space Yeah. Basically just refine some of the Yep. Yeah  this is totally like - The probabilities and all are completely ad-hoc. We need to look at all of them. more general nodes. The- they - they will be aud- ad-hoc for - for - for some time to come. I mean but  they're even like I mean like  close to the end we were like  uh  you know we were like uh really ad-hoc. It's a even distribution. Like  whatever. Right? Cuz if it's like  uh - If it's four things coming in  right? And  say  some of them have like three possibilities and all that. So you're thinking like - like a hundred and forty four or something And - That's terrible. possible things - numbers to enter  right? So. Some of them are completely absurd too  like - That's uh - they want to enter  but it's closed  it's night time  you know there are tourists and all this weird stuff happens at the line up and you're like - Well - Yeah  the only like possible interpretation is that they are like - confused. come here just to rob the museum or something to that effect. In which case you're supposed to alert the authorities  and see appropriate action. Yeah. Yeah. Yeah  another thing to do  um  is also to  um - I guess to ask around people about other Bayes-net packages. Is Srini gonna be at the meeting tomorrow  do you know? Maybe. Wait - Quite possibly. Oh  oh  sorry. Sorry  Wednesday  yeah. The day after tomorrow. Wednesday. Day after tomorrow. Yeah. Maybe we can ask him about it. Who's talking on Wednesday? Mmm. I haven't - J- Jerry never sent out a - sent out an email  did he  ever ? No. But he mentioned at the last meeting that someone was going to be talking  I forget who. Uh. Oh  isn't Ben? Ben? I think it's Ben actually  yeah  Ben  then   Ben. Ah! um  giving his job talk I think. um  Sorry. I was just reading the screen. O_K. Oh. Yeah. So the uh - That will be one - one thing we could do. I actually uh  have - Um  also we can uh  start looking at the SmartKom tables and I will - Right. I actually wanted to show that to you guys now but um. Do you want to trade? Um  no I - I actually made a mistake because it - it fell asleep and when Linux falls asleep on my machine it's - it doesn't wake up ever  so I had to reboot Oh  no. And if I reboot without a network  I will not be able to start SmartKom  because I need to have a network. Uh - So we'll do that t- maybe uh - But. O_K. But once you start - sart- start SmartKom you can be on - You don't have to be on a network anymore. Is that the deal? Yep. Ah  interesting. Why does SmartKom need a network? Um - it looks up some stuff that  you know  is - is that - is in the - written by the operating system only if it - if you get a D_H_C_P request  so it - you know  my computer does not know its I_P address  Ah. you know? You know. So. Unless it boots up with networking. It's plugged in. Yeah. And I don't have an I_P address  they can't look up - they don't know who localhost is  and so forth and so forth. Hmm. Always fun. But it's a  um  simple solution. We can just um  go downstairs and - and - and look at this  but maybe not today. The other thing um - I will - oh yeah  O_K  I have to report um  data collection. We interviewed Fey  Mm-hmm. She's willing to do it  meaning be the wizard for the data collection  also maybe transcribe a little bit  if she has to  but also recruiting subjects  organizing them  and so forth. So that looks good. Jerry however suggested that we should uh have a trial run with her  see whether she can actually do all the uh spontaneous  eloquent and creativeness that we uh expect of the wizard. And I talked to Liz about this and it looks as if Friday afternoon will be the time when we have a first trial run So who would be the subject of this trial run? for the data. Pardon me? Who - Will there be a - Is one - Is you - one of you gonna be the subject? Like are you - Um Liz also volunteered to be the first subject  Good. which I think might be even better than us guys. One of us  yeah. If we do need her for the technical stuff  then of course one of you has to sort of uh - jump in. I like how we've - you guys have successfully narrowed it down. ""Is one of you going to be the subject?"" Is one of you - jump in. Reference. I haven't done it yet. Well I just figured it has to be someone who's  um  familiar enough with the data to cause problems for the wizard  so we can  uh  see if they're you know good. Oh plants? e- u- someone who can plant difficult Yeah. I mean that's what we wanna check  right? things. Um  Well  in this case it's a p- it's a sort of testing of the wizard rather than of the subject. It's uh - Isn't that what it is? yes w- we - we would like to test the wizard  but you know  if we take a subject that is completely unfamiliar with the task  or any of the set up  we get a more realistic I guess that would be reasonable. Yeah. Yeah. you know  set up as - I know. That's probably a good enough test of - Uh-huh. Sort of having an actively antagonistic  uh - Yeah. Yeah. That might be a little unfair. Um. Yeah. I'm sure if we uh  - You think there's a chance we might need Liz for  whatever  the technical side of things? I'm sure we can get other people around who don't know anything um  if we want another subject. You know. Like I can drag Ben into it or something. Yeah  yeah. Although he might cause problems but. So  is it a experimental setup for the I like that. ""Test the wizard."" um  data collection totally ready - I want that on a T_shirt. Um - I think it's - it's - it's - I mean - determined? experimental setup u- on the technical issue yes  except we st- I think we still need uh - a recording device for the wizard  just a tape recorder that's running in a room. Mm-hmm. But um - in terms of specifying the scenario  Mm-hmm. um - uh - uh - we've gotten a little further but um - we wanted to wait until we know who is the wizard  and have the wizard partake in the ultimate sort of definition probe . So - so if - if on Friday it turns out that she really likes it and - and we really like her  then nothing should stop us from sitting down next week and getting all the details completely figured out. Mm-hmm. And um - O_K. So the ideal task um  will have whatever I don't know how much the structure of the evolving Bayes-net will af- affect - Like we wanna - we wanna be able to collect as much of the variables that are needed for that  Mmm-yea-some. right? in the course of the task? Well not all of them but you know. Bu- e- e- e- I'm even - This - this Tango  Enter  Vista is sort of  itself  an ad-hoc scenario. Mm-hmm. Mm-hmm. The - the basic u- um idea behind the uh data collection was the following. The data we get from Munich is very command line  simple linguistic stuff. Mm-hmm. Hardly anything complicated. No metaphors whatsoever. Mm-hmm. Not a rich language. So we wanted just to collect data  to get - that - that - that elicits more  uh  Mm-hmm. that elicits richer language. And we actually did not want to constrain it Mm-hmm. too much  you know? Just see what people say. And then maybe we'll discover the phenomenon - the phenomena that we want to solve  you know  with whatever engine we - we come up with. Um. So this - this - this is a parallel track  O_K. you know  there - they hopefully meet  So in other words this data collection is more general. It could - it could be used for not just this task. but since - It should tell us  you know  what kind of phenomenon could occur  it should tell us also maybe something about the difference between people who think they speak to a computer versus people who think they speak to a human being and the sort of differences there. Mm-hmm. So it may get us some more information on the human-machine pragmatics  um  that no one knows anything about  as of yesterday. And uh - nothing has changed since then  so. Uh. And secondly  now that of course we have sort of started to lick blood with this  and especially since um - Johno can't stop Tango-ing  we may actually include  you know  those - those intentions. So now I think we should maybe have at least one navigational task with - with sort of explicit - Mm-hmm. uh not ex- it's implicit that the person wants to enter  Mm-hmm. and maybe some task where it's more or less explicit that the person wants to take a picture  Mm-hmm. or see it or something. So that we can label it. I mean  that's how we get a corpus that we can label. Mm-hmm. Exactly. Whereas  you know  if we'd just get data we'd never know what they actually wanted  we'd get no cues. Yep. Alrighty. O_K. That was that. So is this the official end of the meeting now? Yep. Looks like it. So what's ""Economics  the fallacy""? I just randomly label things. Ma- So that has nothing to do with economics or anything. Oh  really? O_K. Maybe we ought to switch off these things before we continue. O_K. Switching o- ",Although the members of ICSI's Meeting Recorder Group at Berkeley had little progress to report  there were still a number of issues relating to their work to discuss. These included making plans for upcoming experiments  clarifying definitions  and approaches which may or may not be against the rules of the Aurora project  alongside alternatives that would not be. There was also debate about the necessary continuation of a group report. Plans were also made with regard to a visitor from research partner OGI For next weeks meeting  speaker me018 will provide numbers on his experiments into adjusting insertion penalties. Speaker me013 feels that mn007 and fn002 should just put together the coherent bare-bones of their report  and move back to experimenting  leaving the report for the end of the project. He also feels that they should discuss some aspects of future work  for clarity's sake  with the visitor from OGI. The main project the group is working on  Aurora  has a number of rules attaches as to what developers can and cannot play with  but this needs to be clarified. The rules are adhered to in the small community  but make no sense from a broader research perspective. While writing their report  mn007 and fn002 have noticed some tables contain only partial results  and there are things they do not recall the reasoning behind. Speaker me018 has done a little initial research into the next area he is to look at  adjusting the scaling and the insertion penalties. Playing with the latter made little difference  though was coming from a different feature set. Speakers mn007 and fn002 have been working on their report  logically writing up everything they have done so far. Mn007 has also been working with a new dataset  preparing it for use as a more realistic source of noise  though it is not clear if this is allowed. Speaker me006 has continued to look at phonetic events  and has come up with a plan for his future work. 
"And we already got the crash out of the way. It did crash  so I feel much better  earlier. @@ Yeah. Interesting. Hmm. Will you get the door  and - ? @@ O_K. You collected an agenda  huh? O_K  so um. I did collect an agenda. So I'm gonna go first. Mwa-ha-ha! It shouldn't take too long. Yeah. Um  so we're pretty much out of digits. We've gone once through the set. Um  so the only thing I have to do No there's only ten. Yeah  that's right. so I - I just have to go through them and uh Well  O_K. pick out the ones that have problems  and either correct them or have them re-read. So we probably have like four or five more forms to be read  to be once through the set. I've also extracted out about an hour's worth. We have about two hours worth. I extracted out about an hour's worth which are the f- digits with - for which whose speaker have speaker forms  have filled out speaker forms. Not everyone's filled out a speaker form. So I extracted one for speakers who have speaker forms and for meetings in which the ""key"" file and the transcript files are parsable. Some of the early key files  it looks like  were done by hand  and so they're not automatically parsable and I have to go back and fix those. So what that means is we have about an hour of transcribed digits that we can play with. Um  So you think two - you think two hours is the - is the total that we have? Liz - Yep  yeah. And you think we- th- uh  I - I didn't quite catch all these different things that are not quite right  but you think we'll be able to retrieve the other hour  reasonably? Yes  absolutely. O_K. So it's just a question of a little hand-editing of some files and then waiting for more people to turn in their speaker forms. I have this web-based speaker form  and I sent mail to everyone who hadn't filled out a speaker form  and they're slowly s- trickling in. So the relevance of the speaker form here  s- It's for labeling the extracted audio files. Oh  O_K. By speaker I_D and microphone type. Wasn't like whether they were giving us permission to use their digits or something. No  I spoke with Jane about that and we sort of decided that it's probably not an issue that - We edit out any of the errors anyway. Yeah. Right? So the- there are no errors in the digits  you'll always read the string correctly. So I can't imagine why anyone would care. So the other topic with digits is uh  Liz would like to elicit different prosodics  and so we tried last week with them written out in English. And it just didn't work at all because no one grouped them together. So it just sounded like many many more lines instead of anything else. So in conversations with Liz and uh Jane we decided that if you wrote them out as numbers instead of words it would elicit more phone number  social security number-like readings. The problem with that is it becomes numbers instead of digits. When I look at this  that first line is ""sixty one  sixty two  eighteen  eighty six  ten."" Um  and so the question is does anyone care? Um  I've already spoken with Liz and she feels that  Mm-hmm. correct me if I'm wrong  that for her  connected numbers is fine  as opposed to connected digits. Um  I think two hours is probably fine for a test set  but it may be a little short if we actually wanna do training and adaptation and all that other stuff. Yeah Um  do um you want different prosodics  so if you always had the same groupings you wouldn't like that? Is that correct? Well  we actually figured out a way to - the - the groupings are randomly generated. Yeah  the - the - No but  I was asking if that was something you really cared about because if it wasn't  it seems to me if you made it really specifically telephone groupings that maybe people wouldn't  uh  go and do numbers so much. You know if it- if it's - Uh - I think they may still do it  um  Maybe some  but I- probably not so much. What about putting a hyphen between the numbers in the group? And - Right? So if you - if - if you have uh Six dash one  you mean? if you go six six six uh dash uh two nine three one. I - well O_K - I - it might help  I would like to g- get away from having only one specific grouping. Um  so if that's your question  but I mean it seems to me that  at least for us  we can learn to read them as digits if that's what people want. I - I'm That's what I was asking  yeah. Yeah. Yeah. Yeah. don't think that'd be that hard to read them as single digits. Um  I agree. and it seems like that might be better for you guys since then you'll have just more digit data  and that's always a good thing. Right. It's a little bit better for me too because the digits are easier to recognize. They're better trained than the numbers. Yep. Right. So we could just  uh  put in the instructions ""read them as digits"". Right. Right  read them as single digits  so sixty-one w- is read as six one  and if people make a mistake we - Mm-hmm . How about ""O_"" versus ""zero""? I mean  the other thing is we could just bag it because it's - it's - it's- I'm not worrying about it I mean  because we do have digits training data that we have from uh from O_G_I. I'm sorry  digits - numbers training that we have from O_G_I  we've done lots and lots of studies with that. And um. But it's nice to get it in this room with the acous- I mean - for - it's - Yeah. No  no  I guess what I'm saying is that Just let them read it how they read it. to some extent maybe we could just read them - have them read how - how they read it and it just means that we have to expand our - our vocabulary out to stuff that we already have. Yeah. Right. Well that's fine with me as long as - It's just that I didn't want to cause the people who would have been collecting digits the other way to not have the digits. So - We can go back to the other thing later. I mean we s- we - we've - O_K. We can do this for awhile and then go back to digits for awhile  or um. Do yo- I mean  do you want - do you want this - O_K. Do you need training data or adaptation data out of this? How much of this do you need? with uh the - It's actually unclear right now. I just thought well we're - if we're collec- collecting digits  and Adam had said we were running out of the T_I forms  I thought it'd be nice to have them in groups  and probably  all else being equal  it'd be better for me to just have single digits since it's  O_K. you know  a recognizer's gonna do better on those anyway  um  and it's more predictable. So we can know from the transcript what the person said and the transcriber  in general. But if they make mistakes  it's no big deal if the people say a hundred instead of ""one O_O"". O_K  well if you pre- and also w- maybe we can just let them choose ""zero"" versus ""O_"" as they - as they like because even the same person c- sometimes says ""O_"" and sometimes says ""zero"" in different context  and that's sort of interesting. Yeah. So I don't have a Specific need cuz if I did I'd probably try to collect it  you know  without bothering this group  but If we can try it - O_K so - so I can just add to the instructions to read it as digits not as connected numbers. Right  and you can give an example like  Mm-hmm. you know  ""six - sixty-one would be read as six one"". And I think people will get it. Right. Mm-hmm. And i- actually it's no more artificial than what we've been doing with words. I'm sure people can adapt to this  Right  right. It's just easier to read. read it single. The spaces already bias it toward being separated. Right. And I know I'm gonna find this easier than words. Oh yeah  absolutely  cognitively it's much easier. O_K- I also had a hard - hard time with the words  but then we went back and forth on that. O_K  so let's give that a try and - Yeah. O_K. And is the spacing alright or do you think there should be more space between digits and groups? O_K. I mean what do other people think cuz you guys are reading them. Or is that alright? I think that i- it's fine. I- it - it - to me it looks like you've got the func- the idea of grouping and you have the grou- the idea of separation and  you know  it's just a matter of u- i- the instructions  that's all. O_K. O_K. Great. O_K. Well let's give it a try. And I think there are about ten different gouping patterns isn't that right  Liz? Let's try it. Righ- right  and you just - they're randomly generated and randomly assigned to digits. That we did. I did - Mm-hmm. Go ahead. So we have - Sorry  I - I was just gonna say  so we have in the vicinity of forty hours of - of recordings now. And you're saying two hours  uh  is digits  so that's roughly the ratio then  something like twenty - twenty to one. Which I guess makes - Yep. makes sense. So if we did another forty hours of recordings then we could get another couple hours of this. Right. Um  yeah like you say  I think a couple hours for a - for a - for a test - test set's O_K. It'd be nice to get  you know  more later because we'll - we might use - use this up  uh  in some sense  but - but uh - Mm-hmm. Right. Yeah  I also would like to argue for that cuz it - it seems to me that  um  there's a real strength in having the same test replicated in - a whole bunch of times and adding to that basic test bank. Hmm? Cuz then you have  you know  more and more  Right. u- chances to get away from random errors. And I think  um  the other thing too is that right now we have sort of a stratified sample with reference to dialect groups  and it might be - there might be an argument to be made for having uh f- for replicating all of the digits that we've done  which were done by non-native speakers so that we have a core that totally replicates the original data set  which is totally American speakers  and then we have these stratified additional language groups overlapping certain aspects of the database. Right. I think that uh trying to duplicate  spending too much effort trying to duplicate the existing T_I-digits probably isn't too worthwhile because the recording situation is so different. It's gonna be very hard to be comparable. Yeah. Except that if you have the stimuli comparable  then it says something about the - the contribution of setting and - No it's - it's not the same. A little bit  but the other differences are so major. O_K. They're such major sources of variance that it's - it's - it's uh - Yeah I mean read versus not. What's an example of a - of m- some of the other differences? Any other a- difference? Well i- i- individual human glottis is going to be different for each one  you know  it's just - There's so many things. O_K. O_K. it's - it - and - and enunciation. Well  and not just that  I mean the uh the corpus itself. I mean  we're collecting it in a read digit in a particular list  and I'm sure that they're doing more specific stuff. I mean if I remember correctly it was like postman reading zipcodes and things like that. T_I-digits was? I thought - I thought it was read. I thought so. Was it read? Yeah  I think the reading zipcode stuff you're thinking of would be O_G_I. Oh  I may well be. Yeah  no T_I-digits was read in th- in read in the studio I believe. I haven't ever listened to T_I-digits. So I don't really know how it compares. Yeah. Yeah. But it - but - But - but regardless it's gonna - it's hard to compare cross-corpus. It- it's different people is the - is the core thing. And they're different circumstances with different recording environment and so forth  so it's - it's - it's really pretty different. But I think So. O_K  fine. the idea of using a set thing was just to give you some sort of framework  so that even though you couldn't do exact comparisons  it wouldn't be s- valid scientifically at least it'd give you some kind of uh frame of reference. Uh  you know it's not - O_K. Hey Liz  What - what do the groupings represent? You said there's like ten different groupings? Right  just groupings in terms of number of groups in a line  and number of digits in a group  and the pattern of groupings. Mm-hmm. Are the patterns - like are they based on anything or Um  I - I just roughly looked at what kinds of digit strings are out there  and they're usually grouped into either two  three  or four  Oh. four digits at a time. And they can have  I mean  actually  things are getting longer and longer. In the old days you probably only had three sequences  and telephone numbers were less  and so forth. So  there's between  um - Well if you look at it  there are between like three and five groups  and each one has between two and four groupings and - I purposely didn't want them to look like they were in any kind of pattern. So Mmm. And which group appears is picked randomly  and what the numbers are are picked randomly. Right. Mm-hmm. So unlike the previous one  which I d- simply replicated T_I-digits  this is generated randomly. Mmm  oh  O_K. Oh O_K. But I think it'd be great i- to be able to compare digits  whether it's these digits or T_I-digits  to speakers  um  and compare that to their spontaneous speech  and then we do need you know a fair amount of - of digit data because you might be wearing a different microphone and  I mean - so it's - it's nice to have the digits Mm-hmm. you know  replicated many times. Especially for speakers that don't talk a lot. So um  for adaptation. No  I'm serious  so Yeah. Yeah. Yeah all we have for some people is digits. we have a problem with acoustic adaptation  and we're not using the digit data now  but you know - Yeah. Oh  you're not. Not for adaptation  nope. v- W- we're not - we were running adaptation only on the data that we ran recognition on and I'd - As soon as someone started to read transcript number  that's read speech and I thought ""well  we're gonna do better on that  Oh I see. that's not fair to use "". But  it might be fair to use the data for adaptation  so. Oh yeah that's true  absolutely. O_K. So those speakers who are very quiet  shy - r- Right - That would be interesting to see whether that helps. Do you think that would help adapting on - Yeah. Yeah  I have a real problem with that. Like Adam? Yeah. Well  it sh- I mean it's the same micropho- see the nice thing is we have that in the - in the same meeting  Right. Same - same acoustics  same microphone  same channel. Yeah. and so you don't get - Yeah. Right  and so I still like the idea of having some kind of O_K. Good. digit data . Yeah I mean  for the - for the um acoustic research  for the signal-processing  far-field stuff  I see it as - as - as the place that we start. But  th- I mean  it'd be nice to have twenty hours of digits data  but - but uh the truth is I'm hoping that we - we through the - the stuff that - that you guys have been doing as you continue that  we get  uh  the best we can do on the spontaneous stuff uh  uh near-field  and then um  we do a lot of the testing of the algorithms on the digits for the far-field  and at some point when we feel it's mature and we understand what's going on with it then we - we have to move on to the spontaneous data with the far-field. So. Great. The only thing that we don't have  I know this sounds weird  and maybe it's completely stupid  but we don't have any overlapping digits. Yeah  we talked about that a couple times. An- yea- I know it's weird  but um - Overlapping digits! The - the problem I see with trying to do overlapping digits is the cognitive load. Alright everybody's laughing. O_K. Dueling digits. No it's - it's not stupid  it's just - I mean  try to do it. I'm just talkin- for the stuff that like Dan Ellis is gonna try  you know  cross-talk cancellation. O_K. I mean  here  let's try it. You read the last line  I'll read the first line. Let's try it. Oh! Wait - oh it - these are all the same forms. O_K So but - Sixty-one. So - so you read the last line  I'll read the first line. So you plu- you plug your ears. No  I'll p- Oh I guess if you plug you're ears you could do it  but then you don't get the - the same effects. Well  what I mean is actually no- Yeah. not the overlaps that are well-governed linguistically  but the actual fact that there is speech coming from two people and the beam-forming stuf- all the acoustic stuff that like Dan Ellis and - and company want to do. Yeah. Oh I see. Digits are nice and well behaved  I mean Anyway  it's just a thought. It - it would go faster. I guess we could try. We could try doing some. Parallel. It's the P_make of digit reading. It would take one around amount of ti- That's right. Well - Well O_K. Well let's try it. I - I mea- I'm - I was sort of serious  but I really  I mean  I'm - I don't feel strongly enough that it's a good idea  so. See  y- You do the last line  I'll do the first line. O_K. Six one  six two  one eight  eight six  one O_. Zero zero nine  six six three  nine one nine. That's not bad. No  I can do it. A- and that prosody was great  by the way. I couldn't understand a single thing you guys were saying. I think it was numbers  but I'm not sure. It - it sort of sounded like a duet  or something. Yeah. Performance art. Alright   let's try three at once you - you pick one in the middle. The Aurora theater. O_K. Go. Six one  six two  one eight  eight six  one O_. Zero zero nine  six six three  nine one nine. Five  six O_ six  five five I'm sorry. I'm mean I think it's doable  I'm just - The poor transcribers they're gonna hate us. So  we - we could have a round like where you do two at a time  and then the next person picks up when the first guy's done  or something. Like a  So pairwise. Oh like a round  yeah  like in a - a - yeah. Yeah  just pairwise  or yeah. what do you call it? Li- a r- like - yeah  like that. Row  row  row your boat. Yeah. A round. Round. Mm-hmm. O_K. It's gonna require some coordination. Then it would go like h- twice as fast  or a third as fast. Anyway  it's just a thought. I'm actually sort of serious if it would help people do that kind o- but the people who wanna work on it we should talk to them. So. Yeah. You have to have a similar pace. I don't think we're gonna collect vast amounts of data that way  but I think having a little bit might at least be fun for somebody like Dan to play around with  yeah. Mmm. O_K. I think maybe if we wanted to do that we would do it as a separate session  something like that rather than Yeah. doing it during a real meeting and you know  do two people at a time then three people at a time and things like that. So. Can try it out. If we have nothing - if we have no agenda we could do it some week. O_K. See - see what Dan thinks. Yeah  right. Yeah  yeah. Spend the whole time reading digits with different qu- quantities. c- c- Can I- can I have an- another - another question w- about this? So  um  there are these digits  which are detached digits  but there are other words that contain I thought this was gonna be fast. Oh well. the same general phon- phoneme sequences. Like ""wonderful"" has ""one"" in it and - and Victor Borge had a - had a piece on this where he inflated the digits. Well  I wonder if there's  um  an- if there would be a value in having digits that are in essence embedded in real words to compare in terms of like the articulation of ""one"" in ""wonderful"" versus ""one"" as a digit being read. That's ""two"" bad. Yeah. I'm all ""four"" it. There you go. Not after I ""eight"" though. Uh  they don't all work as well  do they? Hmm. What does nine work in? Uh  Uh. Nein! You scream it. Nein! You have to be German  yeah. Oh. In German  yeah. That's right! Oh  oh! It's great for the Germans. That's German  yeah. Nein. Yeah. Oh! It only sounds w- good when you scream it  though. I think everybody's a little punchy here today. Yes. So. Well  I mean  I just wanted to offer that as a possible task because  you know  if we were to each read his embedded numbers words in sent- in sentences cuz it's like an entire sketch he does and I wouldn't take the inflated version. So he talks about the woman being ""two-derful""  and - and - a- But  you know  if it were to be deflated  just the normal word  it would be like a little story that we could read. Mm-hmm. I don't know if it would be useful for comparison  but it's embedded numbers. Well I don't know. I think for something like that we'd be better off doing like uh TIMIT. Well I think the question is what the research is  so I mean  I presume that the reason that you wanted to have these digits this way is because you wanted to actually do some research Hmm. looking at the prosodic form here. Right  yeah. Yeah O_K. So if somebody wanted to do that  if they wanted to look at the - the - the difference of the uh phones in the digits in the context of a word versus uh the digits - a - a non-digit word versus in digit word  uh that would be a good thing to do  but I think someone would have to express interest in that. I see. O_K. I think  to - I mean if you were interested in it then we could do it  for instance. O_K  thank you. Huh. O_K  are we done with digits? Um  We have A_S_R results from Liz  transcript status from Jane  and disk space and storage formats from Don. Does - do we have any prefer- preference on which way we wanna - we wanna go? Well I was actually gonna skip the A_S_R results part  in favor of getting the transcription stuff Mm-hmm. talked about since I think that's more important to moving forward  but I mean Morgan has this paper copy and if people have questions  um  it's pretty preliminary in terms of A_S_R results because we didn't do anything fancy  but I think e- just having the results there  and pointing out some main conclusions like it's not the speaking style that differs  it's the fact that there's overlap that causes recognition errors. And then  the fact that it's almost all insertion errors  which you would expect but you might also think that in the overlapped regions you would get substitutions and so forth  um  leads us to believe that doing a better segmentation  like your channel-based segmentation  or some kind of uh  echo cancellation to get basically back down to the individual speaker utterances would be probably all that we would need to be able to do good recognition on the - on the close-talking mikes. Um  why don't you  if you have a hard copy  why don't you email it So these - So  that's about the summary - to the list. But this is - Morgan has this paper. I mean he - he - it - it's that paper. Yeah  yeah. Yeah  yeah. Yeah  so it's the same thing? It's the same thing I mailed to every- Oh it's in the paper. O_K. everybody that w- where it was  yeah. So  we basically  um  O_K then  it's already been mailed. did a lot of work on that and it's - Let's see  th- I guess the other neat thing is it shows for sure w- that the lapel  you know within speaker is Horrible? bad. And it's bad because it picks up the overlapping speech. So  your - your A_S_R results were run on the channels synchronized  O_K. Yes  cuz that's all that w- had been transcribed at the time  um O_K. O_K. but as we - I mean I wanted to here more about the transcription. If we can get the channel asynchronous or the - the closer t- that would be very interesting for us because we - Yeah. So if - Yeah  that's - that's why I only Yeah. used the part from use- which we had uh about uh about the alt- over all the channels or mixed channel rather mixed signal. Right. That's - Yeah. Yeah sure. Yeah. Yeah. cuz - So if there was a segment of speech this long Yeah. And someone said ""oh"" in the front - in the middle. and oh and someone said ""oh "" the whole thing was passed to the recognizer? That's why there's so many insertion errors? There were several speakers in it  yeah. That's right. In fact I - I pulled out a couple classic examples in case you wanna u- use them in your talk of Mm-hmm. Chuck on the lapel  so Chuck wore the lapel three out of four times. Mmm. I noticed that Chuck was wearing the lapel a lot. Um  yeah  and I wore the lapel once  and for me the lapel was O_K . I mean I still - Early on  yeah. and I don't know why. I'm - But um  for you it was - Or who was next to me or something like that. Probably how you wear it - wore it I would guess. Yeah  where you were sitting probably affected it. Yeah. Right  but when Chuck wore the lapel and Morgan was talking there're a couple really long utterances where Chuck is saying a few things inside  and it's picking up all of Morgan's words pretty well and so the rec- you know  there're error rates because of insertion - Insertions aren't bounded  so with a one-word utterance and ten insertions you know you got huge error rate. Uh-huh. Yeah. And that's - that's where the problems come in. So I- this is sort of what we expected  but it's nice to be able to - to show it. Right. And also I just wanted to mention briefly that  um  uh Andreas and I called up Dan Ellis who's still stuck in Switzerland  and we were gonna ask him if - if there're - you know  what's out there in terms of echo cancellation and things like that. Not that we were gonna do it  but we wanted to know And he said  ""Lots lots lots lots."" what would need to be done. And he - We've given him the data we have so far  so these sychronous cases where there are overlap. Yep. And he's gonna look into trying to run some things that are out there and see how well it can do because right now we're not able to actually report on recognition in a real paper  like a Eurospeech paper  because it would look sort of premature. So - So - So the idea is that you would take this big hunk where somebody's only speaking a small amount in it  and then try to figure out where they're speaking based on Right. Or who's - At any point in time who's the foreground speaker  who's the background speaker. the other peopl- I thought we were just gonna move the boundaries in. So yeah - So. Yeah  should it - Well that's with the hand stuff. So there's like - But how would you do that automatically? Right. Uh  I've actually done some experiments with cross-correlation and it seems to work pretty well to - to get rid of those - those overlaps  yeah. Well ther- there's - Mm-hmm. I mean that- that's the sort of thing that you would do. So. Yeah. Yeah. Exactly  so it's - it's a - So why do you want to do echo cancellation? Um  it would be techniques used from adaptive - adaptive echo cancellation which I don't know enough about to talk about. Um. Uh-huh. It - just - it just to r- to remove cross-talk. Yeah. Yeah. But  right  um  and that would be similar to what you're also trying to do  but using um  you know  more than energy - I - I don't know Yeah. what exactly would go into it. So the idea is to basically run this on the whole meeting. Yeah  sure. So it would be - and get the locations  which gives you also the time boundaries O_K. So do sort of what he's already - what he's trying to do. of the individual speak- Right. Except that there are many techniques for the kinds of cues  um  that you can use to do that. O_K  I s- I see. Yeah  in another way  yeah. Yeah. Yeah. I see. Yeah  Dave - Dave uh is  um  also gonna be doin- usin- playing around with echo cancellation for the near-field far-field stuff  so we'll be - So. And I guess Espen? This - is - uh - is he here too? May also be working - So it would just be ver- that's really the next step because we can't Yeah. do too much  you know  on term- in terms of recognition results knowing that this is a big problem Mm-hmm. um  until we can do that kind of processing. And so  once we have some - O_K. Yeah I'm working on it. some of yours  and @@ we'll move on. I think this also ties into one of the things that Jane is gonna talk about too. Um  O_K. Mm-hmm. Mm-hmm. I also wanted to say I have done all this chopping up of digits  so I have some naming conventions that we should try to agree on. Oh right. Yeah. Right. Definitely - So let's do that off-line  we don't need to do it during the meeting. O_K. Uh  and Don should - And - and I have scripts that will extract it out from ""key"" files and - and do all the naming automatically  so you don't have to do it by hand. O_K. Alright. Great. So that- that's it for the - You've compiled the list of  uh  speaker names? Speakers and - O_K. Mm-hmm. Not names  but Yep. Yeah  names - names in the - names to I_Ds  so you I_Ds. O_K. Great. and it does all sorts of matches because the way people filled out names is different on every single file so it does a very fuzzy sort of match. Right. Cool. So at this point we can sort of finalize the naming  and so forth  and we're gonna basically re- Yep. Mm-hmm. rewrite out these waveforms that we did because as you notice in the paper your ""M_O_-four"" in one meeting and ""M_O_-two"" in another meeting and it's - we just need to standardize the Yeah. That was my fault. um  no it's - it's - No  I didn't notice that actually. @@ um  that's why those comments are s- are in there. So - Yeah. Then disregard it then. Yep. So th- I now have a script that you can just say basically Right. Yeah. O_K. look up Morgan  and it will give you his I_D. So. Great  great. Terrific. O_K . Um  alright. Do we - Don  you had disk space and storage formats. Is that something we need to talk about at the meeting  or should you just talk with Chuck Um  at some other time? I had some general questions just about the compression algorithms of shortening waveforms and I don't know exactly who to ask. I thought that maybe you would be the - the person to talk to. So  is it a lossless compression when you compress  so - Mm-hmm. Entropy coding. So. It just uses entropy coding? O_K. So  I mean  I guess my question would be is I just got this new eighteen gig drive installed. Um  yeah  which is - And I assume half of it is scratch and half of it is - ? I'm not exactly sure how they partitioned it. But um  Probably  yeah. That's typical  huh. yeah  I don't know what's typical here  but um  it's local though  so - That doesn't matter. But - You can access it from anywhere in ICSI. O_K. In fact  this is an eighteen gig drive  or is it a thirty six gig drive with eighteen - Alright. How do you do that? N_ - Eighteen. Eigh- eighteen. It was a spare that Dave had around - Oh O_K. Slash N_ slash machine name  slash X_A_ in all likelihood. Oh I see. O_K. Alright  I did know that. Um  so the - the only question is how much of it - The distinction between scratch and non-scratch is whether it's backed up or not. Mm-hmm. Right. So what you wanna do is use the scratch for stuff that you can regenerate. O_K. So  the stuff that isn't backed up is not a big deal because disks don't crash very frequently  as long as you can regenerate it. Right. Right. I mean all of this stuff can be regenerated  it's just a question - Yeah it's - Well the - Then put it all on scratch because we're - ICSI is - is bottlenecked by backup. Yeah. Mm-hmm  very good point. O_K. Well I'd leave all the - All the transcript stuff shouldn't - should be backed up  but all the waveform - So we wanna put - Mm-hmm. Sound files should not be backed up  the ones that you write out. Yeah  I guess - Right. O_K. So  I mean  I guess th- the other question was then  should we shorten them  downsample them  or keep them in their original form? Um - It just depends on your tools. I mean  because it's not backed up and it's just on scratch  if your sc- tools can't take shortened format  I would leave them expanded  Right. so you don't have to unshorten them every single time you wanna do anything. O_K. We can downsample them  Do you think that'd be O_K? so. Yeah. To downsample them? Yeah  we get the same performance. I mean the r- the front-end on the S_R_I recognizer just downsamples them on the fly  so - O_K. Yeah  I guess the only argument against downsampling is to preserve just the original files in case we want to experiment with different filtering techniques. So that's - I - I - I'm sorry - Yeah  l- I mean over all our data  we - we want to not downsample. Yeah  if fe- You'd - you wanna not. O_K. So we're - what we're doing is we're writing out - Yeah. I mean  this is just a question. We're writing out these individual segments  that wherever there's a time boundary from Thilo  or - or Jane's transcribers  you know  we - we chop it there. Yeah. Mm-hmm. And the reason is so that we can feed it to the recognizer  and throw out ones that we're not using and so forth. Mm-hmm. Yeah. And those are the ones that we're storing. Yeah  as I said  since that's - it's regeneratable  what I would do is take - So - Yeah. downsample it  and compress it however you're e- the S_R_I recognizer wants to take it in. Yeah. So we can't shorten them  but we can downsample them. So. ye- Right. Yeah  I mean - yeah  I'm sorry. As - yeah  as long as there is a - a form that we can come from again  r- Yeah. that is not downsampled  then  Oh yeah th- Yeah those are gonna be kept. Yeah. Yeah. That - that's why we need more disk space cuz we're basically duplicating the originals  um - uuu Yeah. Then it's fine. But for - for - fu- future research we'll be doing it with different microphone positions and so on we would like to - Right. Oh yeah. No. We always have the original long ones. Yep. Right. Yeah. So the S_R_I front-end won't take a uh - an - an - a large audio file name and then a - a list of segments to chop out from that large audio file? They actually have to be chopped out already? Um  it's better if they're chopped out  and - and it - it will be - Uh-huh. yeah  y- we could probably write something to do that  but it's actually convenient to have them chopped out cuz you can run them  you know  in different orders. You c- you can actually move them around. And that's the whole point about the naming conventions is that you could Uh  you can get rid of- Yeah  it- it's a lot faster. run all the English speaking  all the native speakers  and all the non-native speakers  and all the men  and all the women. Yeah. Right. You can grab everything with the word ""the"" in it  and it's - That's a lot quicker than actually trying to access the wavefile each time  find the time boundaries and - So in principle  yeah  you could do that  but it's - I don't - I don't think that's really right. but it's um - ""That's just not right  man."" These are long - These are long - You know. This is an hour of speech. The - the point - So - so s- For example  what if you wanted to run - run all the native speakers. Right  so if - if you did it that way you would have to generate a program that looks in the database somewhere  extracts out the language  finds the time-marks for that particular one  do it that way. The way they're doing it  you have that already extracted and it's embedded in the file name. And so  you know  you just say - We- yeah that's - so that's part of it is - y- so you just say you know ""asterisk E_ asterisk dot wave""  and you get what you want. Right. And the other part is just that once they're written out it - it is a lot faster to - to process them. Rather than doing seeks So. through the file. Otherwise  you're just accessing - This is all just temporary access  so I don't - I think - it's all just - It's fine. You know. Fine to do it however is convenient. Right. I mean it just depends how big the file is. If the file sits in memory you can do extremely fast seeks but. Right. The other thing is that  believe it or not - Yeah and they don't. I mean  we have some - Two gig? So we're also looking at these in Waves like for the alignments and so forth. You can't load an hour of speech into X_Waves . Yeah. You need to s- have these small files  and in fact  even for the Transcriber program Yes you can. Um - Yeah  you - you can give Waves a start and an end time. And middle. Yeah  if you try to load s- really long waveform into X_Waves  you'll be waiting there for - No  I - I'm not suggesting you load a long wave file  I'm just saying you give it a start and an end time. And it'll just Oh- I th- go and pull out that section. w- The transcribers didn't have any problem with that did they Jane? What's th- u- w- in what respect? Loading the long - They loaded - they loaded the long No  with the Transcriber tool  it's no problem. long files into X_Waves. It takes a very long ti- In the - in- Yeah just to load a transcription Mm-hmm. @@ Right. It takes a l- very long time. takes a long time  but not for the wavefile. The wavefile is there immediately. Mm-hmm. Yeah. Huh. Are you talking about Transcriber or X_Waves ? Yeah. Oh  I'm tr- talking about Transcriber. Actually  you're talking about Transcriber  right? Yeah. It was also true of the digits task which was X_Waves. Because - because i- we used X_Waves to do the digits. Yeah. And they were loading the full mixed files then  and it didn't seem to be any problem. Very quickly. I agree. Huh. Well we - we have a problem with that  you know  time-wise on a - It- it's a lot slower to load in a long file  and also to check the file  so if you have a Hmm. Seemed really fast. transcript  um  Well regardless  it's - I mean it's - Yeah. I - I think overall you could get everything to work by accessing the same waveform and trying to find two - you know  the begin and end times. Um  but I think it's more efficient  if we have the storage space  to have the small ones. and  it's no problem  right? Because it's not backed up. Yeah  it's - Yeah. It's - it's just - So we just - If we don't have a spare disk sitting around we go out and we buy ourselves an eighty gigabyte drive and make it all scratch space. You know  it's not a big deal. You're right about the backup being a bottleneck. It's good to Right. Yeah  so these wouldn't be backed up  the - think towards scratch. Yeah. Yep. Right. So remind me afterward and I'll - And - and we'll look at your disk and see where to put stuff. O_K. Alright. I mean  I could just u- do a D_U on it right? And just see which - how much is on each - Yep. So. Each partition. And you wanna use  either X_A or scratch. O_K. Well X_ question mark  anything starting with X_ is scratch. O_K. With two - two digits. Two digits  right  X_A  X_B  X_C. O_K? Jane? So  @@ . O_K. So I got a little print-out here. So three on this side  three on this side. And I stapled them. O_K. Alright so  first of all  um  there was a - an interest in the transcribe- transcription  uh  checking procedures and - and I can tell you first  uh  to go through the steps although you've probably seen them. Um  as you might imagine  when you're dealing with  um  r- really c- a fair number of words  and uh  @@ natural speech which means s- self-repairs and all these other factors  that there're lots of things to be  um  s- standardized and streamlined and checked on. And  um  so  I did a bunch of checks  and the first thing I did was obviously a spell-check. And at that point I discovered certain things like  um  ""accommodate"" with one ""M_""  that kind of thing. And then  in addition to that  I did an exhaustive listing of the forms in the data file  which included n- detecting things like f- faulty punctuation and things - Yeah? I'm - I'm sorry to interrupt you could - could I just back up a little bit and - Sure  please  yeah  please  please. Yeah  yeah  yeah. So you're doing these - So the whole process is that the transcribers get the conversation and they do their pass over it. Yes. And then when they're finished with it  it comes to you  and you begin these sanit- these quality checks. O_K. O_K. That's right. Exactly. I do these checks. Uh-huh. Exactly. Yeah. Thank you. And so  uh  I do a - an exhaustive listing of the forms - Actually  I will go through this in - in order  so if - if we could maybe wait and stick keep that for a second cuz we're not ready for that. So on the fifth page  seven down - Yeah  yeah  yeah  yeah. Exactly! Exactly! Alright so  a spelling check first then an exhaustive listing of the  uh - all the forms in the data with the punctuation attached and at that point I pick up things like  oh  you know  word followed by two commas. And th- and then another check involves  uh  being sure that every utterance has an identifiable speaker. And if not  then that gets checked. Then there's this issue of glossing s- w- so-called ""spoken-forms"". So there - mo- for the most part  we're keeping it standard wo- word level transcription. But there's - w- And that- that's done with the assumption that pronunciation variants can be handled. So for things like ""and""  the fact that someone doesn't say the ""D_""  uh that's not important enough to capture in the transcription because a - a good pronunciation  uh  you know  model would be able to handle that. However  things like ""cuz"" where you're lacking an entire very prominent first syllable  and furthermore  it's a form that's specific to spoken language  those are r- reasons - f- for those reasons I - I kept that separate  and used the convention of using ""C_U_Z"" for that form  however  glossing it so that it's possible with the script to plug in the full orthographic form for that one  and a couple of others  not many. So ""wanna"" is another one  ""going -"" uh  ""gonna"" is another one  with just the assumption  again  that this - th- these are things which it's not really fair to a- c- consider - expect that - a pronunciation model  to handle. And Chuck  you in- you indicated that ""cuz"" is - is one of those that's handled in a different way also  didn't you? Did I - I don't remember. O_K. So - so- it might not have been - It might not have been you  but someone told me that in fact ""cuz"" is treated differently Hmm. in  um  i- u- in this context because of that r- reason that  um  it's a little bit farther than a pronunciation variant. O_K  so after that  let's see  um. So that was part of the spell-check  or was that - that was after the spell-check? Well so when I get the exhau- So the spell-check picks up those words because they're not in the dictionary. So it gets ""cuz"" and ""wanna"" and that - Uh-huh. And then you gloss them? Yeah  mm-hmm. Run it through - I have a sed - You know  so I do sed script saying whenever you see ""gonna"" you know  ""convert it to gonna""  you know  ""gloss equals quote going-to quote""  you know. And with all these things being in curly brackets so they're always distinctive. O_K  I also wrote a script which will  Mm-hmm. um  retrieve anything in curly brackets  or anything which I've classified as an acronym  and - a pronounced acronym. And the way I tag ac- pronounced acronyms is that I have underscores between the components. So if it's ""A_C_L"" then it's ""A_"" underscore ""C_"" underscore ""L_"". And the th- And so - so your list here  are these ones that actually occurred in the meetings? Yes. Uh-huh  yeah. Whew! O_K  so now. Uh and - a- We are acronym- loaded. Um  can I ask a question about the glossing  uh before we go on? So  Yeah. for a word like ""because"" is it that it's always predictably ""because""? I mean  is ""C_U_Z"" always meaning ""because""? Yes  but not the reverse. So sometimes people will say ""because"" in the meeting  and if - if they actually said ""because""  then it's written as ""because"" with no - w- ""cuz"" doesn't even figure into the equation. Beca- because - But - but in our meetings people don't say ""hey cuz how you doing?"" Right. Right. Except right there. Yeah. Yeah. Um  so  I guess - So  from the point of view of - That's a good point. The - the only problem is that with - for the recognition we - we map it to ""because""  and so if we know that ""C_U_Z"" - Well  That's fine. Well Don has a script. but they have the gloss. You have the gloss form so you always replace it. Yeah. but  we don't - Exactly. If that's how - what you wanna do. Uh-huh. And Don knows this  and he's bee- he has a glo- he has a script that - Yeah. I replace the ""cuz"" with ""because"" if it's glossed. S- Right. But  if it's - O_K. And - But then there are other glosses that we don't replace  right? Because - Yes. And that's why there're different tags on the glosses  O_K. So  then it's fine. on the different - on the different types of comments  which we'll - which we'll see in just a second. Right. O_K. So the pronounceable acronyms get underscores  the things in curly brackets are viewed as comments. There're comments of four types. So this is a good time to introduce that. The four types. w- And maybe we'll expand that but the - but the comments are  um  of four types mainly right now. One of them is  um  Um - Can - ca- the gloss type we just mentioned. Another type is  um - So a- are we done with acronyms? Cuz I had a question on what - what this meant. I'm still doing the overview. I haven't actually gotten here yet. O_K so  gloss is things like replacing the full form Oh I'm sorry. u- with the  um  more abbreviated one to the left. Uh  then you have if it's - uh  there're a couple different types of elements that can happen that aren't really properly words  and wo- some of them are laughs and breathes  so we have - uh that's prepended with a v- a tag of ""V_O_C"". Whew! @@ And the non-vocal ones are like door-slams and tappings  and that's prepended with a no- non-vocalization. So then it - just an ending curly brace there  or is there something else in there. Oh yeah  so i- e- this would - Let's just take one example. A comment  basically. Oh  oh  oh. And then the no- non-vocalization would be something like a door-slam. They always end. So it's like they're paired curly brackets. And then the third type right now  uh  is m- things that fall in the category of comments about what's happening. So it could be something like  you know  ""referring to so-and-so""  ""talking about such-and-such""  uh  you know  ""looking at so-and-so"". Yeah. So on the m- on the middle t- So  in the first case that gloss applies to the word to the left. But in the middle two - Yeah  and this gets substituted here. They're impulsive. Th- it's not applying to anything  right? O_K. Huh-uh. No  they're events. They're actually - They have the status of events. O_K. Well the ""QUAL"" can be - The ""QUAL"" is applying to the left. Right  I just meant the middle two ones  yeah. Yep. Well  and actually  um  it is true that  with respect to ""laugh""  there's another one which is ""while laughing""  and that is  uh  i- i- An argument could be made for this - ""While laughing"". tur- turning that into a qualitative statement because it's talking about the thing that preceded it  but at present we haven't been  um  uh  coding the exact scope of laughing  you know  and so to have ""while laughing""  you know that it happened somewhere in there which could well mean that it occurred separately and following  or  you know  including some of the utterances to the left. Haven't been awfully precise about that  but I have here  now we're about to get to the - to this now  I have frequencies. So you'll see how often these different things occur. But  um  uh  the very front page deals with this  uh  final c- pa- uh  uh  aspect of the standardization which has to do with the spoken forms like ""mm-hmm"" and ""mm-hmm"" and ""ha"" and ""uh-uh"" and all these different types. And  um  uh  someone pointed out to me  this might have been Chuck  about  um - about how a recognizer  if it's looking for ""mm-hmmm"" with three M_'s  and it's transcribed with two M_'s  that it might - uh  that it might increase the error rate which is - which would really be a shame because um  I p- I personally w- would not be able to make a claim that those are dr- dramatically different items. So  right now I've standardized across all the existing data with these spoken forms. I - I should say Oh good. So it's a small list. all existing data except thirty minutes which got found today. So  I'm gonna - I'm gonna - I'm gonna check - That - that's known as ""found data"". Yeah  yeah. Acsu- actually yeah. I got - It was stored in a place I didn't expect  It's like the z- Zapruder Film. so - and - and um  w- we  uh  sh- yea- reconstructed how that happened. I wanna work with lost data. Yeah. It's much easier. And this is - this'll be great. So I'll - I'll be able to get through that tonight  and then everyth- i- well  actually later today probably. Hmm. And so then we'll have everything following these conventions. But you notice it's really rather a small set of these kinds of things. And I made it so that these are  um  with a couple exceptions but  things that you wouldn't find in the spell-checker so that they'll show up really easily. Yeah. And  um - Jane  can I ask you a question? What's that very last one correspond to? I don't even know how to pronounce that. Sure. Yeah. Well  yeah. Now that - that s- only occurs once  and I'm thinking of changing that. Right. So- c- I haven't listened to it so I don't know. Uh  is that like someone's like burning or some such thing? Like their hair's on fire? I haven't heard it actually. I n- I need to listen to that one. Ah! Uh  it looks like that . Actually we - we gave this to our pronunciation person  she's like  ""I don't know what that is either "". So. It's the Castle of Ah! Did she hear the th- did she actually hear it? Cuz I haven't heard it. No  we just gave her a list of words that  you know  weren't in our dictionary and so of course it picked up stuff like this  and she just didn't listen so she didn't know. We just - we're waiting on that just to do the alignments. Yeah. Yeah I'm curious to se- hear what it is  but I didn't know - wanna change it to something else until I knew. Maybe it's ""argh""? Right. @@ Well  sss  you know - But that's not really like - Hhh. Yeah. No one really says ""argh "" you know  it's not - @@ Right  no one say- Well  you just did. Well  Yeah. That's right. @@ - there's another - there's another word error. Except for now! Yes  that's right. We're gonna have a big problem when we talk about that. Cha- ching. Ah. We're gonna never recognize this meeting. O_K. In Monty Python you say ""argh"" a lot. So. Well  or if you're a C_ programmer. Oh yeah? Mmm. Yeah  that's right. That's right. Yeah. You say arg-C_ and arg-V_ all the time. That's true. Yeah Yeah. But it has a different prosody. Arg. It does. Arg - arg-max  arg-min  yeah. Mm-hmm. Ah! Uh  So  Jane  what's the - d- I have one question about the Maybe he died while dictating. so. the ""E_H"" versus like the ""A_H"" and the ""U_H"". That's partly a nonnative-native thing  but I have found ""E_H"" in native speakers too. @@ O_K. But it's mostly non-native - S- O_K. H_ That's ""eh"" versus ""ah""? Eh. Eh? ""Eh "" yeah right  cuz there were - were some speakers that did definite ""eh's"" but right now we - Mm-hmm. They were the Canadians  right? Canadians  yeah  yeah  yeah. That's right. So  it - it's actually probably good for us to know the difference between the real ""eh"" and the one that's just like "" uh "" or transcribed ""aaa"" cuz in - like in Switchboard  you would see Exactly. e- all of these forms  but they all were like ""uh"". You mean just the single letter ""a"" as in the particle? The transcription or - No  no  I mean like the - the ""U_H""  or - the ""U_H""  ""E_H""  ""A_H"" were all the same. And then  we have this additional non-native version of - Article. ""U_H"". Oh. uh  like ""eeh"". All the ""E_H""'s I've seen have been like that. They've been like ""eh"" like that have bee- has been transcribed to ""E_H"". Mm-hmm  that's right. And sometimes it's stronger  like ""eeh"" which is like closer to ""E_H"". But. Mmm. Right. Yeah. I'm just - these poor transcribers  they're gonna hate this meeting. I know. We should go off-line. Well  we're not doing - We're not doing length. Quick Thilo  do a - do a filled pause for us. Yeah  that's right. Ooo no. But you're a native German speaker so it's not a - Yeah. not a issue for - It's only - @@ Them Canadians. Onl- yeah. No  only if you don't have lax vowels  I guess. Right. So it's - like Japanese and Spanish and - Oh. This makes sense. Yeah I - I think you've - uh-huh  yeah. Uh- huh. Oh I see. I didn't get that  O_K. That makes sense. Yeah  and so  you know  I mean  th- th- I have - there are some  um  Americans who - who are using this ""eh"" too  and I haven't listened to it systematically  maybe with some of them  uh  they'd end up being ""uh's"" but  uh  I- my spot-checking has made me think that we do have ""eh"" in also  um  American e- e- data represented here. But any case  that's the - this is reduced down from really quite a long- a much longer list  and this is Yeah this is great. This is really really helpful. Mm-hmm. Yeah  it's good  yeah. functionally pretty  you know  also - It was fascinating  I was listening to some of these  uh  I guess two nights ago  and it's just hilarious to liste- to - to do a search for the ""mm- hmm's "". And you get ""mm-hmm"" and diff- everybody's doing it. And just listen to them? Yeah. Just - I wanted to say - I w- think it would be fun to make a montage of it because there's a ""Mm-hmm. Mm-hmm. Mm-hmm."" Performance art  just extract them all. Right. It's really - it's really fun to listen to. Morgan can make a song out of it. All these different vocal tracts  you know  but it's - it's the same item. It's very interesting. O_K. Uh  then the acronyms y- and the ones in parentheses are ones which the transcriber wasn't sure of  and I haven't been able to listen to to - to clarify  but you can see that Oh I see. the parenthesis convention makes it very easy to find them cuz it's the only place where - where they're used. o- How about question mark? The question marks  yeah. What are those? Question mark is punctuation. So it - they said that @@ - Oh. Mm-hmm. um  ""D_C?"" So they - so it's ""P_L_P?"" Ah. Exactly. Exactly. Yeah  so the only - Well  and I do have a stress marker here. Sometimes the contrastive stress is showing up  and  um - I'm sorry  I - I got lost here. What- w- what's the difference between the parenthesized acronym and the non-parenthesized? The parenthesized is something that the transcriber thought was A_N_N  but wasn't entirely sure. So I'd need to go back or someone needs to go back  and say  you know  yes or no  and then get rid of the parentheses. But the parentheses are used only in that context in the transcripts  of Ah. Right. of noti- noticing that there's something uncertain. Yeah  P_make is - That's a good one. That's correct. Yeah I mean cuz they - they have no idea  right. If you hear C_T_P_D  I mean  they do pretty well but it's - Yeah. Mm-hmm. I - I don't recognize a lot of these. I - you know how are - how are they gonna know? I know! I - I was saying that I think a lot of them are the Networks meeting. Yeah. I think that's true. Yeah  absolutely. N_S_A  a lot of these are - are coming from them. I listened to some of that. Maybe. Yeah. I see a few. Although I see - I see plenty of uh Yeah  we don't have that many acronyms comparatively in this meeting. It's not so bad. Yeah. Yeah. I agree. Right. And Robustness has a fair amount  but the N_S_A group is just very very many. Yeah. Mmm. The recognizer  it is funny. Kept getting P_T_A for P_D_A. That's not bad. This is close  right  and the P_T_A was in these  uh  topics about children  so  anyway. Yeah. Yeah  that's pretty close. Yeah. That's interesting. Is the P_- P_T_A working? Right and sometimes  I mean  you see a couple of these that are actually ""O_K's"" so it's - it's - may be that they got to the point where - I mean it was low enough understandable - understandability that they weren't entirely sure the person said ""O_K."" You know  so it isn't really necessarily a an undecipherable acronym  but just n- needs to be double checked. Now we get to the comments. This - There's a lot of ""O_K's"". The number to the left is the number of incidences? Uh-huh. Count. Yep. Number of times out of the entire database  w- except for that last thirty minutes I haven't checked yet. So C_T_S is really big here  yeah. Yeah  I wonder what it is. Yeah. So what is the difference between ""papers rustling"" and ""rustling papers""? I_P  I know what I_P is. I'd have to listen. I - I- I agree. I w- I'd like to standardize these down farther but  um  uh  uh  to me that sounds equivalent. Yeah. But  I - I'm a little hesitant to - to collapse across categories unless I actually listen to them. Seems so. O_K. Oh I'm sure we've said X_M_L more than five times. Well  then  at least now. Yeah. Six. Now it's at least six times  yeah. S- s- six now  yeah. O_K well - I'm wai- Wh- the self-referential aspect of these - these p- Yes  it's very bad. Yeah. Well this is exactly how people will prove that these meetings do differ because we're recording  right? Yes. Y- no- normally you don't go around saying  ""Now you've said it six times. Now you've said-"" Yeah that's right. But did you notice that there were seven hundred and eighty five instances of ""O_K""? And that's just without the - without Yep. No  I didn't. Yeah. Seven hundred eighty-five instances. Yeah. punc- punctuation. Extra forty one if it's questioned. And that's an underestimate cuz they're Where's that? So th- Yep. On the page two of acronyms. Yeah. Is this after - like did you do some uh replacements for all the different form of ""O_K"" to this? O_K. Seven hundred eighty. Yeah. Of ""O_K""  yes. Mm-hmm. So that's the single existing convention for ""O_K"". Wait a minute  w- s- So now we're up to seven hundred and eighty eight. Yeah that's - Although  what's - there's one with a slash after it. Yeah. That's - that's - I looked for that one. I actually explicitly looked for that one  and I think that  um  That's kind of disturbing. Yeah  we'll have to look at it you know. Yeah. Anyway. Mm-hmm. I - I'm not exactly sure about that. Was that somewhere where they were gonna say ""new speaker"" or something? No  I looked for that  but that doesn't actually exist. And it may be  I don't - I can't explain that. That's alright. I'm just pointing that out. There's - I- i- it's the only - it's the only pattern that has a slash after it  and I think it's - it's an epiphenomenon. Well there's not @@ . So I'll just - I was just looking at the bottom of page three there  is that ""to be"" or ""not to be"". Yeah. Oh that's cute. That's funny. There's no tilde in front of it  so. Yeah. O_K. O_K anyways  sorry. There is th- one - ""Try to stay on topic  Adam."" Y- well  no  that's r- that's legitimate. So now  uh  comments  you can see they're listed again  same deal  with exhaustive listing of everything found in everything except for these final th- O_K so  um  on some of these QUALs  thirty minutes. Yeah. are they really QUALs  or are they glosses? So like there's a ""QUAL T_C_L"". ""T_C_L"". Where do you see that? Uh Oh  oh. The reason is because w- it was said ""tickle"". What's a QUAL? Oh I see  I see. So it's not gloss. O_K  I see. Hmm. Yep. It wasn't said ""T_C_L"". Of course. Sh- shouldn't it be ""QUAL T_I_C_K_L_E"" or something? Like - it's not - On the - in the actual script - in the actual transcript  I s- I - So this - this happens in the very first one. I actually wrote it as ""tickle"". Mm-hmm. O_K. Because we - they didn't say ""T_C_L""  they said ""tickle"". Yeah. Right. And then  following that is ""QUAL T_C_L"". Oh I see. O_K. I f- I forget  what's QUAL? Qual- qualifier. It's just comment about what they said. Comment. Comment or contextual comment. Yeah. It's not something you wanna replace with but - So they didn't mean ""tickle"" as in Yeah. Elmo  they meant ""tickle"" as in - Tickle? Yeah. Huh. Right. But at some point - I mean  we probably shoul- We'll probably add it to the language model. But we should add it to the dictionar- No  to the pronunciation model. Yeah. What did I say? Language  uh - To the language model - model. Well both. We can go on lan- lan- add it to both dictionary and language model. Oh lan- Oh O_K- we- O_K - it's in the language model  Add what  Liz? Yeah. w- yeah  but it- so it's the pronunciation model that has to have a pronunciation of "" tickle"". Well ""tickle"" was pronounced ""tickle"". Right? It's pronounced the same - it's pronounced the same as the verb. So I think it's the language model that makes it different. What are you saying? ""tickle"" is pronounced ""tickle""? I'm sorry! Oh  sorry. What I meant is that there should be a pronunciation ""tickle"" for T_C_L as a word. And that word in the - in  you know  it stays in the language model wherever it was. Oh I see. Yeah. Mm- hmm. Right. Right. Yeah you never would put ""tickle"" in the language model in that form  yeah. Right. @@ Right. Right. There's actually a bunch of cases like this with people's names and - So how w- there'd be a problem for doing the language modeling then with our transcripts the way they are. Yes. Yeah. Yeah so th- th- there- there's a few cases like that where the um  the word needs to be spelled out in - in a consistent way as it would appear in the language  but there's not very many of these. Tcl's one of them. And - and you'll ha- you'll have to do it sychronously. Um  y- yeah. Right  so y- so  whoever's creating the new It's just disturbing. models  will have to also go through the transcripts and change them synchronously. Right. Right. We have this - there is this Hmm. thing I was gonna talk to you about at some point about  you know  what do we do with the dictionary as we're up- updating the dictionary  these changes have to be consistent with what's in the - Like spelling people's names and so forth. If we make a spelling correction to their name  like someone had Deborah Tannen's name misspelled  and since we know who that is  you know  we could correct it  but - You can correct it. Yeah. but we need to make sure we have the misspel- If it doesn't get corrected we have to have a pronunciation as a mispelled word in the dictionary. Things like that. Mm-hmm. These are so funny to read. Well  of course now the - the Tannen corre- the spelling c- change. Uh  that's what gets - I - I picked those up in the frequency check. So. Right. Right. So if there's things that get corrected before we get them  it's - it's not an issue  but if there's things that Mm-hmm. um  we change later  then we always have to keep our - the dictionary up to date. And then  yeah  in the case of ""tickle"" I guess we would just have a  you know  word ""T_C_L"" which - Mm-hmm. You add it to the dictionary. which normally would be an acronym  you know  ""T_C_L"" Right. but just has another pronunciation. Yep. "" ICSI "" is - is one of those that sometimes people pronounce and sometimes they say ""I_C_S_I."" So  Mm-hmm. Oh yeah. those that are l- are listed in the acronyms  I actually know they were said as letters. The others  um  e- those really do need to be listened to cuz I haven't been able to go to all the IC- ICSI things  and - Right  exactly. and until they've been listened to they stay as ""I_C_S_I"". Mm-hmm. Right. Don and I were just noticing  love this one over on page three  ""vocal - vocal gesture mimicking sound of screwing something into head to hold mike in place."" It's this  ""rrre-rrre-rrre"". That's great. It was me. It was! In fact  it was! Yeah! A lot of these are me the - the ""beep is said with a high pit- high pitch and lengthening."" He - he s- he said - he said get - To head . Yeah  that's it. That was the - I was imitating uh  beeping out - Beep. Perfect. Yeah that's it. That's it. Oh there is something spelled out ""B_E_E_E_E_E_E_P"" Yeah. Um - Yeah  that's - that's been changed. Yeah. in the old - Thank you. Because he was saying  ""How many E_'s do I have to allow for?"" What I meant was ""beep"". You need a lot of - You need a lot of qualification Adam. That's been changed. So  exactly  that's where the lengthening comment c- came in. I guess so. Anyway. Right  thanks  yeah. s- chan- brought it down. Subtext. So they're vocalization  Right. And those of course get - get picked up in the frequency check because you see ""beep"" and you know - I mean it gets kicked out in the spelling  and it also gets kicked out in the  uh  freq- frequency listing. glosses. Right. Right. Right. I have the - there're various things like ""breathe"" versus ""breath"" versus ""inhale"" and  hhh  you know  I don't know. I - I think they don't have any implications for anything else so it's like I'm tempted to leave them for now an- and - It's easy enough to find them when they're in curly brackets. We can always get an exhaustive listing of these things and find them and change them. Yeah. ""Sings finale-type song"" that's - that's good. Yeah. Yeah  that was in the first meeting. Um  Yeah  but I don't actually remember what it was. But that was - Eric did that. Yeah. Yeah. So on - Tah-dah! I don't know. Something like that maybe  yeah. I think maybe something like that. Well  that'd qualify. On the glosses for numbers  Yeah. it seems like there are lots of different ways it's being done. There's a - O_K. Interesting question. Yes. O_K  now first of all - Ooo-ooo! Very important. Uh Chuck - Chuck led to a refinement here which is to add ""NUMS"" if these are parts of the read numbers. Now you already know i- ""Ooo-ooo."" that I had  uh  in places where they hadn't transcribed numbers  I put ""numbers"" in place of any kind of numbers  but there are places where they  um  it - th- this convention came later an- and at the very first digits task in some transcripts they actually transcribed numbers. And  um  d- Chuck pointed out that this is read speech  and it's nice to have the option of ignoring it for certain other prob- uh p- uh  things. And that's why there's this other tag here which occurs a hundred and five - or three hundred and five times right now which is just - well n- n- ""NUMS"" by itself which means this is part of the numbers task. I may change it to ""digits"". I mean  i- with the sed command you can really just change it however you want ""NUMS""  yeah. because it's systematically encoded  you know? Have to think about what's the best for - for the overall purposes  but in any case  Yep. um  ""numbers"" and ""NUMS"" are a part of this digits task thing. Um  now th- Then I have these numbers that have quotation marks around them. Um  I didn't want to put them in as gloss comments because then you get the substitution. And actually  th- um  the reason I b- did it this way was because I initially started out with the other version  you have the numbers and you have the full form and the parentheses  however sometimes people stumble over these numbers they're saying. So you say  ""Seve- seventy eight point two""  or whatever. And there's no way of capturing that if you're putting the numbers off to the side. You can't have the seven and - So what's to the left of these? The left is i- so example the very first one  it would be  spelled out in words  Mm-hmm. O_K  that's what I was asking. ""point five"". Right. Only it's spelled out in words. So i- this is also spelled out in - in words. ""Point five."" Point F_I_V_E  yeah. Good. And then  in here  ""NUMS""  so it's not going to be mistaken as a gloss. It comes out as ""NUMS quote dot five"". O_K now  the other example is  in the glosses right there  Thank you. ""gloss one one one dash one three zero"". What - what's to the left of that? Well now - Right. In that case it's people saying things like ""one one one dash so-and-so"" or they're saying uh ""two - I mean zero"" whatever. And in that case  it's part of the numbers task  and it's not gonna be included in the read digits anyway  so - I m- in the uh - O_K. So there will be a ""NUMS"" tag on those lines? There is. Yeah. I've added that all now too. Yeah. Good. There's a ""numbers"" tag - I'm sorry I'm - I didn't follow that last thing. Wait. So  so gloss - in the same line that would have ""gloss quote one one one dash one thirty""  you'd have a gloss at the end of the line saying  Right. uh  ""curly bracket NUMS curly bracket"". So if you - if you did a  uh  a ""grep minus V_ nums"" and you get rid of anything that was read. Oh  so you could do ""grep minus V_ nums"". So that's the - yeah. So there wouldn't be something like O_K. i- if somebody said something like  ""Boy  I'm really tired  O_K."" and then started reading that would be on a separate line? Yes. O_K great. Cuz I was doing the ""grep minus V_"" quick and dirty and looked like that was working O_K  but - Mm-hmm. Good. Great. Yep. Now why do we - what's the reason for having like the point five have the ""NUMS"" on it? Is that just like when they're talking about their data or something? Or - This is more because - Yeah. Oh these are all these  the ""NUMS point""  this all where they're saying ""point"" something or other. These are all like inside the spontaneous - And the other thing too is for readability of the transcript. I mean if you're trying to follow this while you're reading it it's really hard to read  you know - eh  ""so in the data column five has""  you know  ""one point five compared to seventy nine point six""  it's like when you see the words it's really hard to follow the argument. And this is just really a - a way of someone who would handle th- the data in a more discourse-y way to be able to follow what's being said. So this is where Chuck's  um  overall h- architecture comes in  where Oh O_K. Label it. I see. we're gonna have a master file of the channelized data. Um  there will be scripts that are written to convert it into these t- these main two uses and th- some scripts will take it down th- e- into a f- a for- ta- take it to a format that's usable for the recognizer an- uh  other scripts will take it to a form that's usable for the - for linguistics an- and discourse analysis. And  um  the implication that - that I have is that th- the master copy will stay unchanged. These will just be things that are generated  and Right O_K. e- by using scripts. Master copies of superset. When things change then the - the script will cham- change but the - but there won't be stored copies of - in different versions of things. Good. So  I guess I'd have one request here which is just  um  maybe to make it more robust  th- that the tag  whatever you would choose for this type of ""NUMS"" where it's inside the spontaneous speech  is different than the tag that you use for the read speech. Right. Right. That would argue for changing the other ones to be "" digits "" or something. Um  that way w- if we make a mistake parsing  or something  we don't see the ""point five""  or - or it's not there  then we Mm-hmm. a- Just - an- And actually for things like ""seven eighths""  or people do fractions too I guess  you - maybe you want one overall tag for sort of that would be similar to that  or - Except - As long as they're sep- as they're different strings that we - that'll make our p- sort of Well - processing more robust. Cuz we really will get rid of everything that has the ""NUMS"" string in it. I suppose what you could do is just make sure that you get rid of everything that has ""curly brace NUMS curly brace"". Well - Ex- exactly. Exactly. I mean that would be the - That was - that was my motivation. Yeah. And i- these can be changed  like I said. You know  I mean  as I said I was considering changing it to "" digits "". And  it just - i- you know  it's just a matter of deciding on whatever it is  and being sure the scripts know. Right. It would probably be safer  if you're willing  to have a separate tag just because um  then we know for sure. And we can also do counts on them without having to do the processing. But you're right  we could do it this way  it - it should work. Um  Yeah  and it makes it - I guess the thing about - but it- it's probably not hard for a person to tell the difference because one's in the context of a - Yeah. you know  a transcribed word string  and - Right. The other thing is you can get really so minute with these things and increase the size of the files and the re- and decrease the readability to such an extent by So - simply something like "" percent "". Now I - I could have adopted a similar convention for "" percent ""  but somehow percent is not so hard  you know? i- It's just Hmm. when you have these points and you're trying to figure out where the decimal places are - And we could always add it later. Percent's easy to detect. Point however is - is uh a word that has a couple different meanings. And you'll find both of those in one of these meetings  where he's saying ""well the first point I wanna make is so-and-so"" and he goes through four points  and also has all these decimals. So Liz  what does the recognizer do  So. uh  Hmm. what does the S_R_I recognizer output for things like that? ""seven point five"". Does it output the word - ""Seven point five"". Right  the word ""seven""? The number ""seven""? Well  the numbers? The word. The word ""seven""  O_K. Yeah. Yeah. So I'd - so ""I'd like - I'd like to talk about point five"". And - and actually  you know the language - it's the same point  actually  the - the p- you know  the word ""to"" and the word Yeah. y- th- ""going to"" and ""to go to"" those are two different "" to's "" and so there's no distinction there. Mm-hmm. It's just - just the word "" point "" has - Yeah  every word has only one  yeah e- one version even if - even if it's - A- actually even like the word ""read"" and ""read"" . Those are two different words. They're spelled the same way  right? And they're still gonna be transcribed as R_E_A_D. Mm-hmm. Right. Mm-hmm. So  yeah  I - I like the idea of having this in there  I just - I was a little bit worried that  um  the tag for removing the read speech - because i- What if we have like ""read letters"" or  I don't know  like ""read something"" like ""read"" yeah  basically. We might wanna - just a separate tag that says it's read. Mm-hmm. But other than that I- it sounds great. Yeah. O_K? Are we done? Well I wanted to say also regarding the channelized data  that  um  Thilo requested  um  that Oh  I guess we're not done. Yeah. we ge- get some segments done by hand to e- e- s- reduce the size of the time bins wh- like was Chuc- Chuck was mentioning earlier that  um  that  um  if you - if you said  ""Oh"" and it was in part of a really long  s- complex  overlapping segment  that the same start and end times would be held for that one as for the longer utterances  and - Well - We did that for one meeting  right  so you have that data don't you? Yeah  that's the training data. And he requested that there be  uh  similar  uh  samples done for five minute stretches c- involving a variety of speakers and overlapping secti- sections. He gave me - he did the - very nice  he - he did some shopping through the data and found segments that would be useful. Yeah. And at this point  all four of the ones that he specified have been done. In addition the I've - I have the transcribers expanding the amount that they're doing actually. So right now  um  Oh great. I know that as of today we got an extra fifteen minutes of that type  and I'm having them expand the realm on either side of these places where they've already started. Oh great. O_K. But if - if - you know  and I - and he's gonna give me some more sections that - that he thinks would be useful for this purpose. Yeah. Yeah. Because it's true  I mean  if we could do the - the more fine grained tuning of this  uh  using an algorithm  that would be so much more efficient. And  um. So this is gonna be useful to expand this. So I - I thought we - we sh- we sh- perhaps we should try to - to start with those channelized versions just to - just to try it. Give it - Give one tr- transcriber the - the channelized version of - of my speech-nonspeech detection and look if - if that's helpful for them  or just let them try if - if that's better or If they - if they can - You mean to start from scratch f- in a brand new transcript? That'd be excellent. Yeah  that'd be really great. Yeah. Yeah. Yeah. As it stands we're still in the phase of sort of  um  cleaning up the existing data getting things  uh  in i- m- more tight- tightly time - uh  aligned. I also wanna tell - um  I also wanted to r- raise the issue that - O_K so  there's this idea we're gonna have this master copy of the transcript  it's gonna be modified by scripts t- into these two different functions. And actually the master - Two or more. Two or more different functions. Two - two or more. And that the master is gonna be the channelized version. Right. So right now we've taken this i- initial one  it was a single channel basically the way it was input. And now  uh  thanks to the advances made in the interface  we can from now on use the channelized part  and  um  any changes that are made get made in the channelized version kind of thing. But I wanted to get all the finished - all the checks - Yeah  so that has implications for your script. Yeah. So  uh  have those - e- e- the vis- the ten hours that have been transcribed already  have those been channelized? Yes  they have. And I know - I've seen @@ - I've seen they've been channelized  but All ten hours? Except for the missing thirty minutes. Great. have they uh - have they been - has the time - have the time markings been adjusted  uh  p- on a per channel - Uh  for - for a total of like twenty m- f- for a total of - Let's see  four times - total of about an - thirty minutes. That's - that's been the case. And plus the training  whatever you have. So  I guess  I mean  I don't know if we should talk about this now  or not  but I- Well it's just we're missing tea. So. Yeah  I know. No  but I mean my question is like should I wait until all of those are processed  and channelized  like the time markings are adjusted before I do all the processing  and we start like branching off into the - into the - our layer of uh Well  you know the problem - the problem is that some - some of the adjustments that they're making are to bring - are to combine transcripts. bins that were - time bins which were previously separate. And the reason they do that is sometimes there's a word that's cut off. Right. And so  i- i- i- it's true that it's likely to be adjusted in the way that the words are more complete. And  O_K. No I know - I know that adjusting those things are gonna - is gonna make it better. so I - it's gonna be a more reliable thing and I'm not sure - Yeah. I mean I'm sure about that  but do you have like a time frame when you can expect like all of it to be done  or when you expect them to finish it  or - Well partly it depends on how - um  how e- effective it will be to apply an algorithm because Yeah. i- this takes time  you know  it takes a couple hours t- to do  uh  ten minutes. Yeah. Yeah  I don't doubt it. Um  so. So right now the - what you're doing is you're taking the - uh  the o- original version and you're sort of channelizing yourself  right? Yeah. I'm doing it myself. I mean i- if the time markings aren't different across channels  like the channelized version really doesn't have any more information. Mm-hmm. So  I was just - I mean  originally I had done before like the channelized versions were coming out. Um  Right. Right. and so it's a question of like what - So I - I th- I think probably the way it'll go is that  you know  when we make this first general version and then start working on the script  that script Mm-hmm. @@ that will be ma- you know primarily come from what you've done  um  we'll need to work on a channelized version of those originals. Mm-hmm. Mm-hmm. And so it should be pretty much identical to what you have t- except for the one that they've already tightened the boundaries on. Yep. Mm-hmm. Right. Um  So Yeah  I mean - yeah. uh  and then probably what will happen is as the transcribers finish tightening more and more  you know  that original version will get updated and then we'll rerun the script and produce better O_K. uh versions. But the - I guess the ef- the effect for you guys  because you're pulling out the little wave forms into separate ones  that would mean these boundaries are constantly changing you'd have to constantly re- rerun that  so  maybe - I know . Right. But that - Mm-hmm. But that - that's not hard. No. I- I think the harder part is making sure that the transc- the transcription - O_K. So if you b- merge two things  then you know that it's the sum of the transcripts  but if you split inside something  you don't where the word - which words moved. Mm-hmm. Mm-hmm. And that's wh- that's where it becomes a little bit - uh  having to rerun the processing. The cutting of the waveforms is pretty trivial. Mm-hmm. Yeah. I mean as long as it can all be done automatically  I mean  then that's not a concern. You know  if I just have to run three scripts to extract it all and let it run on my computer Right. Mm-hmm. Yeah. Uh-huh. for an hour and a half  or however long it takes to parse and create all the reference file  that's not a problem. Mm-hmm. Um  so yeah. As long as we're at that point. And I know exactly like what the steps will work - what's going on  in the editing process  so. Yeah. O_K. So that's - I- I mean I could - there were other checks that I did  but it's - I think that we've - unless you think there's anything else  I think that I've covered it. Yeah. I can't think of any of the - other ones. O_K. Great. O_K. Oop! Man! ","The main topics of the agenda were a paper submitted to Eurospeech and the organising of the recording transcriptions to be done by IBM. The results presented in the former show a significant percentage of overlapping speech even without counting in backchanneling. Additionally  the high error rate in the recognition of such overlapping speech by the SRI recogniser was minimised simply by changing the scoring method used. Finally  a strong correlation between pauses and interruptions was confirmed. All these measurements were based on the sample of available transcripts. Other features  like prosody  will be studied in the near future. An FTP directory containing such experimental data is being set up for the benefit of other researchers. Regarding the transcriptions to be carried out by IBM  the discussion mainly concerned the format of the recordings that should be sent to them. Suggestions included sending only the channels with the dominant speakers for transcription  but it was finally agreed on sending the original files with minimal modifications  as there will be extensive in-house post-processing. Within this discussion  the rationale behind the coding of the time bins according to the flow of discourse was also explained. The files made available in the FTP directory will be the original ones (before down-sampling)  as these seem to be wanted by other parties. Moreover  as files may have been modified through different processing  tests will be carried out in order to ensure the generation of beep files in a consistent way. Also towards this goal  some of the time bins will need to be merged. On the other hand  the two meetings where time bins have been hand-coded in detail will be used to fine-tune the forced alignments. Recordings will be sent to IBM for transcription. Before that  the files will be automatically pre-segmented into speech/non-speech bins and the beeps will be inserted. In order to make things easier for the transcribers  breathy channels  which are erroneously marked as speech  will be re-classified correctly with other methods. All this pre-processing will have to be evaluated first by checking a sample of the output files. Other issues  like whether and how synthesised speech off a laptop needs be transcribed  will be resolved during the in-house post-processing of the transcriptions. There is a slight worry about the acceptance of the paper submitted to Eurospeech as the deadline was exceeded. As to the content of the paper  the overlap statistics have not been normalised against the number of participants in the conversation  although the dependency is probably going to be a weak one. Additionally  the correlation between pauses in speech and interruptions does not provide a cause-and-effect link for these phenomena. The preparation of files for transcription by IBM is facing some minor difficulties  as some features (hand-coded time boundaries  multiplicity of channels etc) may complicate the generation of beep files. Besides this  the automatic pre-segmentation has been deemed to be good  but there are still no specific measurements to verify this. The pre-segmentation tool also classifies synthesised speech used in a recording as ""normal speech"" and assigns a random channel to it. The transcribers at IBM may not be able to differentiate between the two. A paper has been submitted to Eurospeech  which also includes a section on new corpora. The statistics in the paper are based on the transcripts of two meeting and two telephone conversation corpora. In the first two  the overlapped words vary between 9% and 18%. The telephone conversation results were in-between and very similar to each other. On the other hand  the automatic recognition errors affected by overlaps were reduced dramatically by focusing on regions with the foreground speech. Furthermore  it was shown that after spurts  backchannels  disfluencies and discourse markers  the likelihood of interruption by other speakers was much higher. Files with the recordings  as well as some experimental data will be available for other researchers in an FTP directory that is being set up. Parts of the recordings will have to be beeped out by a script that has already been developed. Finally  EDU meetings already recorded have now been pre-segmented and assigned to the transcribers at ICSI. "
"O_K we're on and we seem to be working. O_K. Yes. O_K. We didn't crash - we're not crashing anymore and it really bothers me. One  two  three  four  f- I do. Yeah? No crashing. I crashed when I started this morning. You crashed - crashed this morning? I did not crash this morning. Yeah? Oh! Well maybe it's just  you know  how many t- u- u- u- u- Really? how many times you crash in a day. Yeah. Maybe  yeah. First time - first time in the day  you know. Or maybe it's once you've done enough meetings it won't crash on you anymore. Yeah. Hmm. No? Yeah. It's a matter of experience. Yeah. Yeah. Self-learning  yeah. That's - that's great. Yeah. Yeah. Uh. @@ Do we have an agenda? Liz - Liz and Andreas can't sh- I do. can't - uh  can't come. So  they won't be here. I have agenda and it's all me. Cuz no one sent me anything else. Did - Did they send  uh  the messages to you about the meeting today? I have no idea but I just got it a few minutes ago. Oh. Oh. Right when you were in my office it arrived. O_K  cuz I checked my mail. I didn't have anything. So  does anyone have any a- agenda items other than me? I actually have one more also which is to talk about the digits. Uh  right  so - so I - I was just gonna talk briefly about the N_S_F I_T_R. Mm-hmm. Yeah. Oh  great. Uh  and then  you have - Can w- I mean  I won't say much  but - uh  but then  uh  you said - wanna talk about digits? I have a short thing about digits and then uh I wanna talk a little bit about naming conventions  although it's unclear whether this is the right place to talk about it. So maybe just talk about it very briefly and take the details to the people who - for whom it's relevant. Right. Yeah. I could always say something about transcription. I've been - but - but - uh  well - Well if we - Yeah  we shouldn't add things in just to add things in. I'm actually pretty busy today  so if we can - Yeah. Yeah  yeah  yeah. we - a short meeting would be fine. This does sound like we're doing fine  yeah. That won't do. So the only thing I wanna say about digits is  we are pretty much done with the first test set. There are probably forms here and there that are marked as having been read that weren't really read. So I won't really know until I go through all the transcriber forms and extract out pieces that are in error. So I wa- Uh. Two things. The first is what should we do about digits that were misread? My opinion is  um  we should just throw them out completely  and have them read again by someone else. You know  the grouping is completely random  so it - it's perfectly fine to put a - a group together again Uh-huh. of errors and have them re-read  just to finish out the test set. Oh! By - throw them out completely? Um  the other thing you could do is change the transcript to match what they really said. Mm-hmm. So those are - those are the two options. Yeah. But there's often things where people do false starts. I know I've done it  where I say - say a - What the transcribers did with that is if they did a correction  and they eventually did read the right string  you extract the right string. Oh  you're talking about where they completely read the wrong string and didn't correct it? Yeah. Yeah. And didn't notice. Ah. Yeah. Which happens in a few places. Yeah. Well  and s- and you're talking string-wise  you're not talking about the entire page? So - so - Yeah. Correct. I get it . And so the - the two options are change the transcript to match what they really said  but then - but then the transcript isn't the Aurora test set anymore. I don't think that really matters because the conditions are so different. And that would be a little easier. Well how many are - how - how often does that happen? Mmm  five or six times. Oh  so it's not very much. No  it's not much at all. Seems like we should just change the transcripts Yeah. O_K. to match. Yeah  it's five or six times out of thousands? Yeah. Four thousand. Four thous- Ah! Four thousand. Four thousand? Yeah  it's - Yeah  I would  uh  tak- do the easy way  yeah. Yeah. O_K. Yeah. It - it's kinda nice - I mean  wh- who knows what Mmm. studies people will be doing on - on speaker-dependent things and so I think having - Yeah. having it all - the speakers who we had is - is at least interesting. So you - um  how many digits have been transcribed now? Four thousand lines. And each line Four thousand lines? is between one and about ten digits. I didn't - Hmm. I didn't compute the average. I think the average was around four or five. So that's a couple hours of - Wow. Yep. of  uh  speech  probably. Yep. Which is a yeah reasonable - reasonable test set. Mm-hmm. Mm-hmm. And  Jane  I do have a set of forms which I think you have copies of somewhere. Mm-hmm. Yeah  true. Oh you do? Mm-hmm. Mm-hmm. Oh O_K  good  good. Yeah  I was just wond- I thought I had - had all of them back from you. No  not yet. And then the other thing is that  uh  the forms in front of us here that we're gonna read later  were suggested by Liz because she wanted to elicit some different prosodics from digits. Mm-hmm. And so  uh  I just wanted people to  Eight eight two two two nine. take a quick look at the instructions and the way it wa- worked and see if it makes sense and if anyone has any comments on it. I see. And the decision here  uh  was to continue with uh the words rather than the - the numerics. Uh  yes  although we could switch it back. The problem was O_ and zero. Oh - Although we could switch it back and tell them always to say ""zero"" or always to say ""O_"". Or neither. Yeah. But it's just two thing - ways that you can say it. Mm-hmm. Right? Oh. Sure. Yeah. Um - um  that's the only thought I have because if you t- start talking about these  you know u- tr- She's trying to get at natural groupings  Right. but it - there's - there's nothing natural about reading numbers this way. I mean if you saw a telephone number you would never see it this way. The - the problem also is she did want to stick with digits. I mean I'm speaking for her since she's not here. Yeah. But  um  the other problem we were thinking about is if you just put the numerals  Yeah. Mmm. they might say forty-three instead of four three. Yeah. Yeah. Yeah. Well  if there's space  though  between them. I mean  you can - With - when you space them out they don't look like  uh  Yeah. forty-three anymore. Well  she and I were talking about it  and she felt that Yeah. it's very  very natural to do that sort of She's right. It's - it - it's a different problem. chunking. I mean it's a - it's a - it's an interesting problem - I mean  we've done stuff with numbers before  and yeah sometimes people - If you say s- ""three nine eight one"" sometimes people will say ""thirty-nine eighty-one"" or ""three hundred - three hundred eighty-nine one""  or - I don't think they'd say that  but - Yeah. Not very frequently but  they certainly could. but th- no - But - Yeah. Uh  th- thirty-eight ninety-one is probably how they'd do it. So. I mean  this is something that Liz and I spoke about and  since But - I see. this was something that Liz asked for specifically  Mm-hmm. I think we need to defer to her. Yeah. O_K. Well  we're probably gonna be collecting meetings for a while and if we decide we still wanna do some digits later we might be able to do some different ver- different versions  but this is the next suggestion  so. Do something different  yeah. O_K. O_K  so uh e- l- I guess  let me  uh  get my - my short thing out about the N_S_F. I sent this - actually this is maybe a little side thing. Um  I - I sent to what I thought we had  uh  in some previous mail  as the right joint thing to send to  which was ""M_ - M_T_G It was. R_C_D_R hyphen joint "". Joint. Yep. But then I got some sort of funny mail saying that the moderator was going to - It's - Hmm. That's because they set the one up at U_W - that's not on our side  that's on the U_dub side. Oh. And so U_- U_W set it up as a moderated list. Yeah. Oh  O_K. And  I have no idea whether it actually ever goes to anyone so you might just wanna mail to Mari No - no  th- I got - I got  uh  little excited notes from Mari and Jeff and so on  so it's - and - O_K  good. Yeah. So the moderator actually did repost it. Yeah. Cuz I had sent one earlier - Actually the same thing happened to me - I had sent one earlier. The message says  ""You'll be informed"" and then I was never informed but I got replies from people indicating that they had gotten it  so. Right. It's just to prevent spam. I see. Yeah so O_ - O_K. Well  anyway  I guess - everybody here - Are y- are - you are on that list  right? So you got the note? Yeah? O_K. Mm-hmm. Yeah. Um  so this was  uh  a  uh  proposal that we put in before on - on more - more higher level  uh  issues in meetings  from - I guess higher level from my point of view. Uh  and  uh  meeting mappings  and  uh - so is i- for - it was a proposal for the I_T_R program  uh  Information Technology Research program's part of National Science Foundation. It's the second year of their doing  uh  these grants. They're - they're - a lot of them are - some of them anyway  are larger - larger grants than the usual  small N_S_F grants  and. So  they're very competitive  and they have a first phase where you put in pre-proposals  and we - we  uh  got through that. And so th- the - the next phase will be - we'll actually be doing a larger proposal. And I'm - I - I hope to be doing very little of it. And - uh  which was also true for the pre-proposal  so. Uh  there'll be bunch of people working on it. So. When's - when's the full proposal due? Uh  I think April ninth  or something. p- s- So it's about a month. Yep. u- Um - And they said end of business day you could check on the reviewer forms  is that - Tomorrow. Tomorrow. Tomorrow? Yeah. March second  I said. Tomorrow. I've been a day off all week. I guess that's a good thing cuz that way I got my papers done early. It would be interesting - So that's amazing you showed up at this meeting! It is. It is actually quite amazing. Yeah. It'll be interesting to see the reviewer's comments. Yeah. Yeah. My favorite is was when - when - when one reviewer says  uh  ""you know  this should be far more detailed""  and the nex- the next reviewer says  ""you know  there's way too much detail"". Yep. Or ""this is way too general""  and the other reviewer says  ""this is way too specific"". Yeah. Yeah. Yeah. Yeah. ""This is way too hard""  ""way too easy"". We'll see. Maybe there'll be something useful. And - and  uh - Well it sounded like they - they - the first gate was pretty easy. Is that right? That they didn't reject a lot of the pre-proposals? Do you know anything about the numbers? No. It's just from his message it sounded like that. Just - just th- Yeah. Yeah. I said something  yeah. Gary Strong's - there was a sentence at the end of one of his paragraphs I - I- Yeah. I should go back and look. I didn't - I don't think that's true. Yeah  O_K. Mmm. He said the next phase'll be very  Very - competitive because we didn't want to weed out very  yeah. Yeah. much in the first phase. Or something like that  so. Well we'll have to see what the numbers are. Mm-hmm. Hmm. Yeah. But they - they have to weed out enough so that they have enough reviewers. Right. Yeah. So  uh  you know  maybe they didn't r- weed out as much as usual  but it's - it's usually a pretty - But it - Yeah. It's - it's certainly not - I'm sure that it's not down to one in two or something Right. of what's left. I'm sure it's  How - how many awards are there  do you know? you know - Well there's different numbers of w- awards for different size - They have three size grants. This one there's  um - See the small ones are less than five hundred thousand total over three years and that they have a fair number of them. Um  and the large ones are  uh  boy  I forget  I think  more than  uh  more than a million and a half  more than two million or something like that. And - and we're in the middle - Mm-hmm. middle category. I think we're  uh  uh  I forget what it was. But  um - Uh  I don't remember  but it's pr- probably along the li- I - I could be wrong on this yeah  but probably along the lines of fifteen or - that they'll fund  or twenty. I mean when they - Do you - do you know how many they funded when they f- in - in Chuck's  that he got I don't - I don't know. last year? Yeah. I thought it was smaller  that it was like four or five  wasn't it? I - I'm - Well they fund - I don't remember. they - yeah. Uh it doesn't matter  we'll find out one way or another. I mean - Yeah. I mean last time I think they just had two categories  small and big  and this time they came up with a middle one  so it'll - Mm-hmm. there'll be more of them that they fund than - of the big. If we end up getting this  um  what will it mean to ICSI in terms of  w- wh- where will the money go to  what would we be doing with it? Uh. Exactly what we say in the proposal. I - I mean uh which part is ICSI though. I mean - You know  it - i- None of it will go for those yachts that we've talking about. Dang! Um  well  no  I mean it's - u- It's just for the research - to continue the research on the Meeting Recorder stuff? It - It's extending the research  right? Because the other - Yeah. Yeah it's go- higher level stuff than we've been talking about for Meeting Recorder. Yeah. Yeah the other things that we have  uh  been working on with  uh  the c- with Communicator - uh  especially with the newer things - with the more acoustically-oriented things are - are - are - are lower level. And  this is dealing with  uh  mapping on the level of - of  um  the conversation - Mm-hmm. of mapping the conversations Right  right. to different kind of planes. So. Um. But  um. So it's all- it's all stuff that none- none of us are doing right now  or none of us are funded for  so it's - so it's - it would be new. So assuming everybody's completely busy now  it means we're gonna hafta  hire more students  or  something? Well there's evenings  and there's weekends  and - Uh. Yeah  there - there would be - there would be new hires  and - and there - there would be expansion  but  also  there's always - for everybody there's - there's always things that are dropping off  grants that are ending  or other things that are ending  so  Right. Mm-hmm. there's - there's a continual need to - Right. Yep. to bring in new things. But - I see. but there definitely would be new - new - new  uh  students  and so forth  both at - at U_W and here. Are there any students in your class who are expressing interest? Um  not clear yet. Other than the one who's already here. Not clear yet. I mean we got - we have - yeah  two of them are - two in the c- There're two in the class already here  and then - and - and  uh - uh  then there's a third who's doing a project here  Mm-hmm. who  uh - But he - he - he won't be in the country that long  and  maybe another will Yep. end up. Actually there is one other guy who's looking - that - that's that Hmm. guy  uh  Jeremy? I think. Mm-hmm. Cool. Anyway  yeah that's - that's all I was gonna say is that - that that's - you know  that's nice and we're sorta preceding to the next step  and  it'll mean some more work  uh  you know  in - in March in getting the proposal out  and then  it's  uh  you know - We'll see what happens. Uh  the last one was - that you had there  was about naming? Yep. It just  uh - we've been cutting up sound files  in - for ba- both digits and for  uh  doing recognition. And Liz had some suggestions on naming and it just brought up the whole issue that hasn't really been resolved about naming. So  uh  one thing she would like to have is for all the names to be the same length so that sorting is easier. Yeah. Um  same number of characters so that when you're sorting filenames you can easily extract out bits and pieces that you want. And that's easy enough to do. And I don't think we have so many meetings that that's a big deal just to change the names. So that means  uh  instead of calling it ""M_R one""  ""M_R two""  you'd call it ""M_R_M zero zero one""  ""M_R_M zero zero two""  things like that. Just so that they're - they're all the same length. But  you know  when you  do things like that you can always - as long as you have - uh  you can always search from the beginning or the end of the string. You know  so ""zero zero two"" - The problem is that they're a lot of fields. Yeah. Alright  so we - we have th- we're gonna have the speaker I_D  the session  uh - uh  Yeah  well  your example was really - information on the microphones  information on the speak- on the channels and all that. i- Uh-huh. O_K. And so if each one of those is a fixed length  the sorting becomes a lot easier. She wanted to keep them the same lengths across different meetings also. So like  the N_S_A meeting lengths  all filenames are gonna be the same length as the Meeting Recorder meeting names? Yep. And as I said  the- it's - we just don't have that many Cuz of digits. that that's a big deal. And so  uh  um  at some point we have to sort of take a few days off  let the transcribers have a few days off  make sure no one's touching the data and reorganize the file structures. And when we do that we can also rationalize some of the naming. I - I would think though that the transcribe - the transcripts themselves wouldn't need to have such lengthy names. So  I mean  you're dealing with a different domain there  and with start and end times and all that  and Right. channels and stuff  so  Right. So the only thing that would change with that is just the directory names  I would change them to match. So instead of being M_R one it would be M_R_M zero zero one. But I don't think that's a big deal. it's a different set. Fine. Fine. So for - for m- the meetings we were thinking about three letters and three numbers for meeting I_Ds. Uh  for speakers  M_ or F_ and then three numbers  For  uh - and  uh  that also brings up the point that we have to start assembling a speaker database so that we get those links back and forth and keep it consistent. Um  and then  uh  the microphone issues. We want some way of specifying  more than looking in the "" key "" file  what channel and what mike. What channel  what mike  and what broadcaster. Or - I don't know how to s- say it. So I mean with this one it's this particular headset with this particular transmitter w- as a wireless. Yeah. Yep. And you know that one is a different headset and different channel. And so we just need some naming conventions on that. Yeah. And  uh  Uh-huh. that's gonna become especially important once we start changing the microphone set-up. We have some new microphones that I'd like to start trying out  um  once I test them. And then we'll - we'll need to specify that somewhere. So I was just gonna do a fixed list of  uh  microphones and types. Yeah. O_K. So  as I said - That sounds good. Yeah. Um  since we have such a short agenda list I guess I wi- I will ask how - how are the transcriptions going? Yeah. The - the news is that I've - I uh - s- So - in s- um - So I've switched to - Start my new sentence. I - I switched to doing the channel-by- channel transcriptions to provide  uh  the - uh  tighter time bins for - partly for use in Thilo's work and also it's of relevance to other people in the project. And  um  I discovered in the process a couple of - of interesting things  which  um  one of them is that  um  it seems that there are time lags involved in doing this  uh  uh  using an interface that has so much more complexity to it. And I - and I wanted to maybe ask  uh  Chuck to help me with some of the questions of efficiency. Maybe - I was thinking maybe the best way to do this in the long run may be to give them single channel parts and then piece them together later. And I - I have a script  I can piece them together. I mean  so it's like  I - I know that I can take them apart and put them together and I'll end up with the representation which is where the real power of that interface is. And it may be that it's faster to transcribe a channel at a time with only one  Mm-hmm. uh  sound file and one  uh  set of - of  uh  Yeah. utterances to check through. I'm a little confused. I thought that - that one of the reason we thought we were so much faster than - than  uh  the - the other transcription  uh  thing was that - that we were using the mixed file. Oh  yes. O_K. But  um  with the mixed  when you have an overlap  you only have a - a choice of one start and end time for that entire overlap  which means that you're not tightly  uh  tuning the individual parts th- of that overlap by different speakers. So someone may have only Mm-hmm. Yeah. said two words in that entire big chunk of overlap. Yeah. And for purposes of - of  uh  things like - well  so things like training the Yeah. speech-nonspeech segmentation thing. Th- it's necessary to have it more tightly tuned than that. O_K. And w- and w- and  you know  is- a- It would be wonderful if  uh  it's possible then to use that algorithm to more tightly tie in all the channels after that but  um  you know  I've - th- the - So  I- I don't know exactly where that's going at this point. But m- I was experimenting with doing this by hand and I really do think that it's wise that we've had them start the way we have with  uh  m- y- working off the mixed signal  um  having the interface that doesn't require them to do the ti- uh  the time bins for every single channel at a t- uh  Mm-hmm. through the entire interaction. Um  I did discover a couple other things by doing this though  and one of them is that  um  um  once in a while a backchannel will be overlooked by the transcriber. As you might expect  because when it's Mm-hmm. Sure. a b- backchannel could well happen in a very densely populated overlap. And if we're gonna study types of overlaps  which is what I wanna do  an analysis of that  then that really does require listening to every single channel all the way through the entire length for all the different speakers. Now  for only four speakers  that's not gonna be too much time  but if it's nine speakers  then that i- that is more time. So it's li- you know  kind of wondering - And I think again it's like this - it's really valuable that Thilo's working on the speech-nonspeech segmentation because maybe  um  we can close in on that wi- without having to actually go to the time that it would take to listen to every single channel from start to finish through every single meeting. Yeah  but those backchannels will always be a problem I think. Uh especially if they're really short and they're not very loud and so it - it can - it - it will always happen that also the automatic s- detection system will miss some of them  so. O_K. Well so then - then  maybe the answer is to  uh  listen especially densely in places of overlap  just so that they're - they're not being Yeah. overlooked because of that  and count on accuracy during the sparser phases. Cuz there are large s- spaces of the - That's a good point. Yeah. There are large spaces where there's no overlap at all. Someone's giving a presentation  or Yeah. whatever. That's - that's a good - that's a good thought. And  um  let's see  there was one other thing I was gonna say. I - I think it's really interesting data to work with  I have to say  it's very enjoyable. I- really  not - not a problem spending time with these data. Really interesting. And not just because I'm in there. No  it's real interesting. Uh  well I think it's a short meeting. Is true. Uh  you're - you're - you're still in the midst of what you're doing from what you described last time  I assume  and - Huh. I haven't results  eh  yet but  eh  Yeah. I - I'm continue working with the mixed signal now  after the - the last experience. Yeah. Yeah. And - and I'm tried to - to  uh  adjust the - to - to improve  eh  an harmonicity  eh  detector that  eh  Yeah. I - I implement. But I have problem because  eh  I get  eh  eh  very much harmonics now. Yeah. Um  harmonic - possi- possible harmonics  uh  eh  and now I'm - I'm - I'm trying to - to find  eh  some kind of a  um - of h- of help  eh  using the energy to - to distinguish between possible harmonics  and - and other fre- frequency peaks  that  eh  corres- not harmonics. And  eh  I have to - to talk with y- with you  with the group  eh  about the instantaneous frequency  because I have  eh  an algorithm  and  I get  mmm  eh  t- t- results - similar results  like  eh  the paper  eh  that I - I am following. But  eh  the - the rules  eh  that  eh  people used in the paper to - to distinguish the harmonics  is - doesn't work well. Mm-hmm. And I - I - I - I not sure that i- eh  the - the way - o- to - ob- the way to obtain the - the instantaneous frequency is right  or it's - it's not right. Eh  Yeah. I haven't enough file- feeling to - to - to distinguish what happened. Yeah  I'd like to talk with you about it. If - if - if  uh - If I don't have enough time and y- you wanna discuss with someone else - some- someone else besides us that you might want to talk to  uh  might be Stephane. Yeah. I talked with Stephane and - and Thilo and  Yeah and - and Thilo  yeah. Yeah  but - they - nnn they - they - they didn't - I'm not too experienced with harmonics and - they think that the experience is not enough to - I see. Is - is this the algorithm where you hypothesize a fundamental  and then No  no it's - No - get the energy for all the harmonics of that fundamental? No. And then hypothesize a new fundamental and get the energy - No. Yeah  that's wh- No. I - I - I - I don't proth- process the - the fundamental. I - I  ehm - I calculate the - the phase derivate Yeah. using the F_F_T. And - The algorithm said that  eh  if you - if you change the - the - the  eh  nnn - the X_- the frequency ""X_""  eh  using the in- the instantaneous frequency  you can find  eh  how  eh  in several frequencies that proba- probably the - the harmonics  eh  Uh-huh. the errors of peaks - the frequency peaks  eh  eh  move around these  eh - eh frequency harmonic - the frequency of the harmonic. And  eh  if you - if you compare the - the instantaneous frequency  eh  of the - of the  eh  continuous  eh  eh  filters  Mm-hmm. that  eh - that  eh  they used eh  to - to - to get  eh  the - the instantaneous frequency  it probably too  you can find  eh  that the instantaneous frequency for the continuous  eh  eh - the output of the continuous filters are very near. And in my case - i- in - equal with our signal  it doesn't happened. Yeah. I'd hafta look at that and think about it. It's - it's - And - it's - I haven't worked with that either so I'm not sure - The way - the simple-minded way I suggested was what Chuck was just saying  is that you could make a - a sieve. Yeah. You know  y- you actually say that here is - Let's - let's hypothesize that it's this frequency or that frequency  and - and  uh  maybe you - maybe you could use some other Yeah. cute methods to  uh  short cut it by - by uh  making some guesses  but - but uh - uh - uh  I would  uh - I mean you could make some guesses from  uh - from the auto-correlation or something but - but then  given those guesses  try  um  uh  only looking at the energy at multiples of the - of that frequency  and - and see how much of the - take the one that's maximum. Call that the - Yeah. Using the energy of the - of the multiple of the frequency. But - Of all the harmonics of that. Yeah. Yeah. Do you hafta do some kind of  uh  low-pass filter before you do that? I don't use. Or - But  No. I - I know many people use  eh  low-pass filter to - to - to get  eh  the pitch. To get the pitch  yes. I don't use. To get the pitch  yeah. To get the pitch  yes. But the harmonic  no. But i- But the harmonics are gonna be  uh  uh  I don't know what the right word is. Um  they're gonna be dampened by the uh  vocal tract  right? The response of the vocal tract. Yeah? Yeah? And so - just looking at the energy on those - at the harmonics  is that gonna - ? Well so the thing is that the - This is for  uh  a  um - I m- what you'd like to do is get rid of the effect of the vocal tract. Right? Yeah. And just look at the - at - Yeah. at the signal coming out of the glottis. Uh  well  yeah that'd be good. Yeah. But  uh - but I - but - but I don't know that you need to - Open wide! but I don't need you - know if you need to get rid of it. I mean that'd - that'd be nice but I don't know if it's ess- if it's essential. Uh-huh. Um  I mean - cuz I think the main thing is that  uh  you're trying - wha- what are you doing this for? You're trying distinguish between the case where there is  uh - where - where there are more than - uh  where there's more than one speaker Sorry. and the case where there's only one speaker. So if there's more than one speaker  um - yeah I guess you could - I guess - yeah you're - so you're not distinguished between voiced and unvoiced  so - so  i- if you don't - Yeah. if you don't care about that - See  if you also wanna just determine - if you also wanna determine whether it's unvoiced  then I think you want to look - look at high frequencies also  because the f- the fact that there's more energy in the high frequencies is gonna be an ob- sort of obvious cue that it's unvoiced. Yeah. But  i- i- uh - I mean i- i- but  um  other than that I guess as far as the one person versus two persons  it would be primarily a low frequency phenomenon. And if you looked at the low frequencies  yes the higher frequencies are gonna - there's gonna be a spectral slope. The higher frequencies will be lower energy. But so what. I mean - that's - that's w- I will prepare for the next week eh  all my results about the harmonicity and will - will try to come in and to discuss here  because  eh  I haven't enough feeling to - to u- many time to - to understand what happened with the - with  eh  so many peaks  eh  eh  and I - I see the harmonics there many time but  eh  there are a lot of peaks  eh  that  eh  they are not harmonics. Um  Yeah. I have to discover what - what is the - the w- the best way to - to - to c- to use them Well  but - yeah I don't think you can - I mean you're not gonna be able to look at every frame  so I mean - I - I mean I - I really - I- I really thought that the best way to do it  and I'm speaking with no experience on this particular point  but  my impression was that the best way to do it was however you - You've used instantaneous frequency  whatever. However you've come up - you - with your candidates  Yeah. Yeah. you wanna see how much of the energy is in that as coppo- as opposed to all of the - all - the total energy. And  um  if it's voiced  I guess - so - so y- I think maybe you do need a voiced-unvoiced determination too. But if it's voiced  Yeah. um  and the  uh - e- the fraction of the energy that's in the harmonic sequence that you're looking at is relatively low  Is height. then it should be - then it's more likely to be an overlap. Yeah. This - this is the idea - the idea I - I - I had to - to compare the - the ratio of the - the energy of the harmonics with the - eh  with the  eh  total energy in the spectrum and try to get a ratio to - to distinguish between overlapping and speech. Mmm. But you're looking a- y- you're looking at - Let's take a second with this. Uh  uh  you're looking at f- at the phase derivative  um  in - in  uh  what domain? I mean this is - this is in - in - in - in bands? Or - or - No  no  no. It's a - it's a - o- Just - just overall - i- w- the band - the band is  eh  from zero to - to four kilohertz. And I - I ot- I - And you just take the instantaneous frequency? Yeah. I u- m- t- I - I used two m- two method - two methods. Eh  one  eh  based on the F_ - eh  F_T_T. to F_F_T to - to obtain the - or to study the harmonics from - from the spectrum directly  and to study the energy and the multiples of Yeah. Yeah. frequency. And another - another algorithm I have is the - in the instantaneous frequency  based on - on - on the F_F_T to - to - to calculate the - the phase derivate in the time. Eh  uh n- the d- I mean I - I have two - two algorithms. Right. But  eh  in m- i- in my opinion the - the - the instantaneous frequency  the - the - the behavior  eh  was - th- it was very interesting. Because I - I saw eh  how the spectrum concentrate  eh  Oh! around the - the harmonic. But then when I apply the - the rule  eh  of the - in the - the instantaneous frequency of the ne- of the continuous filter in the - the near filter  the - the rule that  eh  people propose in the paper doesn't work. And I don't know why. But the instantaneous frequency  wouldn't that give you something more like the central frequency of the - you know  of the - where most of the energy is? I mean  I think if you - Does i- does it - Why would it correspond to pitch? Yeah. I - I - I not sure. I - I - I try to - to - Yeah. When first I - I calculate  eh  using the F_F_T  Di- digital camera. the - the - Keep forgetting. I get the - the spectrum  Yeah. and I represent all the frequency. Yeah. And - when ou- I obtained the instantaneous frequency. And I change the - the - the @@   using the instantaneous frequency  here. Oh  so you scale - you s- you do a - I use - a scaling along that axis according to instantaneous - It's a kinda normalization. Yeah. Yeah. Yeah. Because when - when - O_K. eh  when i- I - I use these - these frequency  eh  the range is different  and the resolution is different. Yeah. And I observe more - more or less  thing like this. And the paper said that  eh  these frequencies are probably  eh  harmonics. I see. But  eh  they used  Huh. eh  a rule  eh  based in the - in the - because to - to calculate the instantaneous frequency  they use a Hanning window. Yeah. And  they said that  eh  if these peak are  eh  harmonics  the f- instantaneous frequency  of the contiguous  eh - w- eh eh  filters are very near  or have to be very near. But  eh  phh! I don't - I - I - I - I don- I- I - and I don't know what is the - what is the distance. And I tried to - to put different distance  eh  to put difference  eh - eh  length of the window  eh  different front sieve   Pfff! and I - I not sure what happened. O_K  yeah well I - I guess I'm not following it enough. I'll probably gonna hafta look at the paper  but - which I'm not gonna have time to do in the next few days  but - Yeah. but I'm - I'm curious about it. Um  uh  @@ O_K. I- I did i- it did occur to me that this is - uh  the return to the transcription  that there's one third thing I wanted to - to ex- raise as a to- as an issue which is  um  how to handle breaths. So  I wanted to raise the question of whether people in speech recognition want to know where the breaths are. And the reason I ask the question is  um  aside from the fact that they're obviously very time-consuming to encode  uh  the fact that there was some - I had the indication from Dan Ellis in the email that I sent to you  and you know about  Yeah. that in principle we might be able to  um  handle breaths by accessi- by using cross-talk from the other things  be able that - in principle  maybe we could get rid of them  so maybe - And I was - I - I don't know  I mean we had this an- and I didn't - couldn't get back to you  but the question of whether Yeah. it'd be possible to eliminate them from the audio signal  which would be the ideal situation  cuz - I don't know - think it'd be ideal. Uh-uh. We- See  we're - we're dealing with real speech and we're trying to have it be as real as possible and breaths are part of real speech. Yeah. Well  except that these are really truly - I mean  ther- there's a segment in o- the one I did - n- the first one that I did for - Yeah. i- for this  where truly w- we're hearing you breathing like - as if we're - you're in our ear  you know  and it's like - it's like - Yeah. I- y- i- I mean  breath is natural  but not It is - Yeah. but it is if you record it. Except that we're - we're trying to mimic - Oh  I see what you're saying. You're saying that the P_D_A application would have - uh  have to cope with breath. Yeah. But - Well An- any application may have to. No. The P_D_ A might not have to  but No - i- Yeah. more people than just P_D_A users are interested in this corpus. O_K  then the - then - I have two questions. So - so mean you're right we could remove it  Yeah? but I - I think - we don't wanna w- remove it from the corpus  in terms of delivering it because the - people will want it in there. O_K  so maybe the question is notating it. Yeah? Yeah. If it gets - Yeah - i- Right. If - if it gets in the way of what somebody is doing with it then you might wanna have some method which will allow you to block it  but you - it's real data. You don't wanna b- but you don't - O_K  well - If s- you know  if there's a little bit of noise out there  and somebody is - is talking about something they're doing  that's part of what we accept as part of a real meeting  even - And we have the f- uh - the uh - the - the fan and the - in the projector up there  and  uh  this is - it's - this is actual stuff that we - we wanna work with. Well this is in- very interesting because i- it basically has a So. i- it shows very clearly the contrast between  uh  speech recognition research and discourse research because in - in discourse and linguistic research  what counts is what's communit- communicative. Mm-hmm. And - breath  you know  everyone breathes  they breathe all the time. And once in a while breath is communicative  but r- very rarely. O_K  so now  I had a discussion with Chuck about the data structure and the idea is that Mm-hmm. the transcripts will - that - get stored as a master- there'll be a master transcript which has in it everything that's needed for both of these uses. Mm-hmm. And the one that's used for speech recognition will be processed via scripts. You know  like  Don's been writing scripts and - and  Mm-hmm. uh  to process it for the speech recognition side. Discourse side will have this - this side over he- the - we- we'll have a s- ch- Sorry  not being very fluent here. But  um  this - the discourse side will have a script which will stri- strip away the things which are non-communicative. O_K. So then the - then - let's - let's think about the practicalities of how we get to that master copy with reference to breaths. So what I would - r- r- what I would wonder is would it be possible to encode those automatically? Could we get a breath detector? Oh  just to save the transcribers time. Well  I mean  you just have no idea. I mean  if you're getting a breath several times every minute  Mm-hmm. and just simply the keystrokes it takes to negotiate  to put the boundaries in  to - to type it in  Mm-hmm. i- it's just a huge amount of time. And you wanna be sure it's used  and you wanna be sure it's done as efficiently as possible  and if it can be done automatically  that would be ideal. Oops. Yeah. Wh- what - what if you put it in but didn't put the boundaries? Well  but - So you just know it's between these other things  right? Well  O_K. So now there's - there's another - another possibility which is  um  the time boundaries could mark off words from nonwords. And that would be extremely time-effective  if that's sufficient. Yeah I mean I'm think- if it's too - if it's too hard for us to annotate the breaths per se  we are gonna be building up models for these things and these things are somewhat self- aligning  so if - so  we - i- i- if we say there is some kind of a thing which we call a ""breath"" or a ""breath-in"" or ""breath-out""  the models will learn that sort of thing. Uh  so - but you - but you do want them to point them at some region where - where the breaths really are. O_K. But that would maybe include a pause as well  and that wouldn't be a problem to have it  uh  pause plus breath plus laugh plus So - Well  there's a- there's - Yeah  i- You know there is - there's this dynamic tension between - between marking absolutely everything  as you know  and - and - sneeze? and marking just a little bit and counting on the statistical methods. Basically the more we can mark the better. But if there seems to be a lot of effort for a small amount of reward in some area  and this might be one like this - Although I - I - I'd be interested to h- get - get input from Liz and Andreas on this to see if they - Cuz they've- they've got lots of experience with the breaths in - in  uh  They have lots of experience with breathing? I - uh  their transcripts. Actually - Well  yes they do  but we - we can handle that without them here. But - but - but  uh  you were gonna say something about - Yeah  I - I think  um  one possible way that we could handle it is that  um  you know  as the transcribers are going through  and if they get a hunk of speech that they're gonna transcribe  u- th- they're gonna transcribe it because there's words in there or whatnot. If there's a breath in there  Yeah. they could transcribe that. That's what they've been doing. So  within an overlap segment  they - they do this. Yeah. Right. But - Right. But if there's a big hunk of speech  let's say on Morgan's mike where he's not talking at all  Yeah. um  don't - don't worry about that. So what we're saying is  there's no guarantee that  um - So for the chunks that are transcribed  everything's transcribed. But outside of those boundaries  there could have been stuff that wasn't transcribed. So you just - somebody can't rely on that data and say ""that's perfectly clean data"". Uh - do you see what I'm saying? So I would say don't tell them to transcribe anything that's outside of Yeah  you're saying it's - uncharted territory. That sounds like a reasonable - a grouping of words. reasonable compromise. Yeah  and that's - that - that quite co- corresponds to the way I - I try to train the speech-nonspeech detector  as I really try to - not to detect those breaths which are not within a speech chunk but with - which are just in - in a silence region. Yeah. And they - so they hopefully won't be marked in - in those channel-specific files. Yeah  so - But - u- I - I wanted to comment a little more just for clarification about this business about the different purposes. Mm-hmm. See  in a - in a way this is a really key point  that for speech recognition  uh  research  uh  um  e- a - it's not just a minor part. In fact  the - I think- I would say the core thing that we're trying to do is to recognize the actual  meaningful components in the midst of other things that are not meaningful. So it's critical - it's not just incidental it's critical for us to get these other components that are not meaningful. Because that's what we're trying to pull the other out of . That's our problem. Yeah. If we had nothing - if we had only linguistically-relevant things - if - if we only had changes in the spectrum that were associated with words  with different spectral components  and  uh  we - we didn't have noise  we didn't have convolutional errors  we didn't have extraneous  uh  behaviors  and so forth  and moving your head and all these sorts of things  then  actually speech recognition i- i- isn't that bad right now. I mean you can - you know it's - Yeah. it's - the technology's come along pretty well. The - the - the reason we still complain about it is because is - when - when you have more realistic conditions then - then things fall apart. O_K  fair enough. I guess  um  I - uh  what I was wondering is what - what - at what level does the breathing aspect enter into the problem? Because if it were likely that a P_D_A would be able to be built which would get rid of the breathing  so it wouldn't even have to be processed at thi- at this computational le- well  let me see  it'd have to be computationally processed to get rid of it  but if there were  uh  like- likely on the frontier  a good breath extractor then  um  and then you'd have to - But that's a research question  you know? And so - Yeah  well  see and that's what I wouldn't know. that - And we don't either. I mean so - so the thing is it's - it - right now it's just raw d- it's just data that we're collecting  and so we don't wanna presuppose that people will be able to get rid of particular degradations because that's actually the research that we're trying to feed. O_K. So  you know  an- and maybe - maybe in five years it'll work really well  and - and it'll only mess-up ten percent of the time  but then we would still want to account for that ten percent  so. I guess there's another aspect which is that as we've improved our microphone technique  we have a lot less breath in the - in the more recent  uh  recordings  so it's - in a way it's an artifact that there's so much on the - on the earlier ones. Uh-huh. I see. One of the - um  just to add to this - one of the ways that we will be able to get rid of breath is by having models for them. I mean  that's what a lot of people do nowadays. And so in order to build the model you need to have some amount of it marked  Right. Right. Yeah. so that you know where the boundaries are. Hmm. Yeah. So - I mean  I don't think we need to worry a lot about breaths that are happening outside of a  you know  conversation. We don't have to go and search for them to - to mark them at all  but  I mean  if they're there while they're transcribing some hunk of words  I'd say put them in if possible. O_K  and it's also the fact that they differ a lot from one channel to the other because of the way the microphone's adjusted. Yeah. Mm-hmm. O_K. Should we do the digits? Yeah. Yep. O_K  this is Transcript L_ one seven three. Four three  six eight  one three  six five  nine three. Seven  three zero eight  five nine  one seven six  one. Five six O_  one O_  six three nine five. O_ three eight  five one zero  eight six eight. Six four six  five one  eight seven seven three  four. Eight two four three four  six four three  two. Zero two five  nine four  two zero eight six. Two eight nine  six seven one  two nine  one six. Four  three nine nine  seven six  three zero zero  three. Two seven seven  five five  six three nine zero. Transcript L_ dash two five seven. Six two O_ O_  zero one two nine  zero two three six. Seven zero three four  six  three six eight. Four six three  two three  six eight four one. O_ six  four nine  five eight  two three  one six. Seven  seven eight five  nine zero  seven two four  four. O_ one four  two six four  eight four three five. One  eight seven seven  four five  eight seven two  two. Four nine five four  one O_ one five  five one O_ five. O_ two two  O_ zero  zero two zero six. Six five five six  one  eight five eight. Transcript L_ dash th- four two three. Nine two zero  two six  seven one three two. Four one O_  five zero  three one nine three. Four eight eight  four five one  two four eight seven. Eight four seven  two three four  six O_  three. Nine three nine  two five  six six - six six  four zero. Three  five one three  nine eight  six five three  three. Seven one three O_  five six zero one  four nine eight seven. Seven two nine  six one one  four six three. Four three five  one six  nine one  O_ zero. O_ two  nine zero  O_ zero  three one  seven three. Transcript L_ five five nine. Four two  nine one  eight eight  four two  two nine. Five two  three five  two eight  three six  three eight. Six eight  four nine  one four  seven one  five six. Four eight three  seven two  four one eight O_. O_ one one three  five  two one six. Three four  seven five  O_ seven  three five  two zero. One  one nine one  zero nine  seven O_ five  one. Two five one three  five  five two seven. Three four six three  five three O_ seven  four zero one two. One eight zero  five nine five  nine three nine. Transcript L_ four nine three. Nine three four  six eight  five two six eight. O_ four three  eight six four  two three nine five. Four nine two  one one O_  eight seven one zero. Nine five three  zero three O_  six eight four. Eight five seven one  five zero nine two  nine three eight nine. Eight three two one  three  one two nine. Two seven three six  nine nine eight nine  nine five seven four. Seven three  zero five  four three  five three  three one. Six six one  three nine  five five four two. Six five  one nine  five eight  nine three eight five. Transcript L_ dash thirty-one. Three two seven one  nine  six six five. Six O_ eight  three one six  eight five one one. Seven  O_ nine four  nine two  one six nine  seven. Four seven  O_ one  four nine  nine zero  two five. Three seven nine  two four two  seven nine O_. Eight six  one six  four zero  seven three  three eight. Five eight  six four  four four  three eight  seven seven. Two eight seven  six eight  five nine nine two. Zero four  three four  eight  six eight  five. Zero eight one  six three nine  one six eight nine. Transcript L_  six two one. Zero two three  one nine five  eight five four. One five zero one  one  eight eight three. Nine six  six three  two five  seven eight  seven nine  two O_ six  four three  four six six zero. Three six  two three  six eight  three five  two four. One seven  two six  one nine  seven two  six five. Eight nine  eight seven  three two  two one  three eight. Two three three seven  seven  one nine five. Four eight eight seven  six one three five  three three five seven. Nine five eight  five one five  four two two. O_K. O_K. Mmm. Alright. And - ",The Berkeley Meeting Recorder group discussed research aims and corresponding concerns for future data collection. It was agreed that a substantial amount of meeting data is required from different domains  and comprising several speakers  to perform the types of discourse and acoustic analyses desired. Ongoing efforts by speaker mn005 to automatically  detect regions of speaker overlap were considered. It was suggested that speaker mn005 focus on a small set of acoustic parameters  e.g. energy and harmonics-related features  to distinguish regions of overlap from those containing the speech of just one speaker. Disk space issues were discussed. And  finally  the problem of speaker anonymization was explored. Recordings must be of existing meetings that are conducted in English. Participants should ideally consist of professors and doctoral students  but no undergraduate students  who are willing to record their meetings at ICSI. The Meeting Recorder corpus should comprise data from a large number of speakers representing different domains. Attempts should also be made to optimize the speaker population for generating good language models. Speaker me011 will pursue volunteers from the Haas Business School to record their weekly meetings at ICSI. A tentative decision was made to offer participants a recorded version of their meeting on a cd rom once the transcript screening phase is complete for that meeting. Non-native speakers with a low proficiency in English are problematic for language modelling. The prospect of creating another recording setup requires the elimination certain more complicated dimensions of the existing setup  e.g. the use of close-talking and far-field microphones. Speaker anonymization poses problems for the transcription proccess  and also discourse analysis  as it makes it more difficult to track who is speaking and to whom a particular utterance is being addressed. As the current version of transcriptions does not include speaker identification labels  no multiple speaker transcription conventions are in use. Research aims and corresponding concerns for future data collection were discussed. A student researcher will be working with speaker fe016 to investigate different strategies for automatically summarizing meetings  and identifying discussional 'hotspots'. Efforts by speaker mn005 are ongoing to detect regions of speaker overlap in the signal. A total of 230 regions of overlapping speech have been manually transcribed for a subset of meeting data. Supervised clustering and neural networks are being considered as means for classifying overlap. It was suggested that speaker mn005 focus on a small set of acoustic parameters  e.g. energy and harmonics-related features  to use the mixed signal to distinguish regions of overlap from those containing the speech of just one speaker. Future work may also involve focussing on additional signals  and using a Markov model to analyze acoustic parameters over larger time frames. Beam-forming was suggested as an alternate method of detecting overlapping speech. Efforts are ongoing to select an optimal method for anonymizing speakers. More disk space is gradually being made available for the storage of new Meeting Recorder data. 
"Hey  you're not supposed to be drinking in here dude. So you can just um  you know  say the number at the transcript top  the thing that says ""transcript"". O_K. Transcript number three two seven one three two nine O_. nine seven five nine zero two zero O_ zero one three two six seven six four four seven five five six eight four seven eight nine zero one zero one one eight one three one two three O_ one two seven five three four nine nine five six O_ nine eight two nine five four two one Do we have to read them that slowly? O_K. Sounded like a robot. Um  this is t- transcript three eight nine one three nine one zero. three three six zero zero four two zero zero one one seven five six six nine seven eight O_ three zero five three one five six six two eight seven six five three four five zero seven two seven nine O_ three eight eight three nine nine nine O_ eight eight nine two zero eight two eight one two zero seven three This is transcript three nine five one three nine seven O_. five six zero five one one eight one eight nine five O_ five seven O_ seven zero one two three zero five one two three four six five two five eight six O_ seven seven six six zero eight nine one seven nine O_ zero zero seven two two one four three three zero two two four three five eight zero three seven four five Great. I'm thinking  why don't you guys go ahead and start - I mean  the data of the - of you guys talking to each other is still useful. And then when your colleagues come in you know  it'll - it's recording now  so it'll just keep recording. And then if - Would you do me a big favor  um  maybe Robert  cuz you know how to do this  that's just hitting the square? the black square when you're done? But make sure not to hit the ""record"" again. It will erase everything. If you hit that black square  that's great. I'm gonna be in the meeting that might run over to four o'clock @@ finished . And then when you leave  just turn off the microphones  the batteries go up and down and - That's great. And if Jerry and Nancy are willing to read the numbers that would be great as well. At least we got yours. O_K. Thanks. Have a good meeting. O_K. When you read the numbers it kind of reminded me of beat poetry. I tried to go for the E_E Cummings sort of feeling  but - Three three six zero zero. Four two zero zero one seven. That's what I think of when I think of beat poetry. Beat poetry. You ever seen ""So I married an axe murderer""? Uh parts of it. Mm-hmm. There's a part wh- there's parts when he's doing beat poetry. Oh yeah? And he talks like that. That's why I thi- That uh probably is why I think of it that way. Hmm. No  I didn't see that movie. Who did - who made that? Mike Meyers is the guy. Oh. O_K. It- it's his uh - it's his cute romantic comedy. That's - that's - That's his cute romantic comedy  yeah. The other thing that's real funny  I'll spoil it for you. is when he's - he works in a coffee shop  in San Francisco  and uh he's sitting there on this couch and they bring him this massive cup of espresso  and he's like ""excuse me I ordered the large espresso?"" Uh. We're having  a tiramisu tasting contest this weekend. Wait - do are y- So you're trying to decide who's the best taster of tiramisu? No? Um. There was a - a - a fierce argument that broke out over whose tiramisu might be the best and so we decided to have a contest where those people who Ah. claim to make good tiramisu make them  and then we got a panel of impartial judges that will taste - do a blind taste and then vote. Hmm. Should be fun. Seems like - Seems like you could put a s- magic special ingredient in  so that everyone know which one was yours. Then  if you were to bribe them  you could uh - Mm-hmm. Well  I was thinking if um - y- you guys have plans for Sunday? We're - we're not - it's probably going to be this Sunday  but um we're sort of working with the weather here because we also want to combine it with some barbecue activity where we just fire it up and what - whoever brings whatever you know  can throw it on there. So only the tiramisu is free  nothing else. Well  I'm going back to visit my parents this weekend  so  I'll be out of town. So you're going to the west Bay then? No  south Bay? No  the South Bay  yeah. South Bay. Well  I should be free  so. O_K  I'll let you know. I'm sorry. I was looking for you guys downstairs. I didn't know that you had moved the meeting. Alright. O_K. So  are we recording ourselves? We are. Alright. Is Nancy s- uh gonna show up? She's got a student with her now. So  she probably won't. Mmm. Wonder if these things ever emit a very  like  piercing screech right in your ear? No  no. The Human Subjects Committee wouldn't allow it. They are gonna get more comfortable headsets. Good. They already ordered them. Uh-huh. O_K. Let's get started. Uh - The uh - Should I go first  with the uh  um  data. Can I have the remote control. Thank you. O_K. So. On Friday we had our wizard test data test and um these are some of the results. This was the introduction. I actually uh  even though Liz was uh kind enough to offer to be the first subject  I sort of felt that she knew too much  so I asked uh Litonya. just Mm-hmm. on the spur of the moment  and she was uh kind enough to uh serve as the first subject. So  this is what she saw as part of - as uh for instr- introduction  this is what she had to read aloud. Uh  that was really difficult for her and uh - Because of l- all the names  you mean? The names and um this was the uh first three tasks she had to - to master after she called the system  and um then of course the system broke down  and those were the l- uh uh I should say the system was supposed to break down and then um these were the remaining three tasks that she was going to solve  with a human - Um. There are - here are uh the results. Mmm. And I will not - Culture on every corner  and more than ten museums @@ five playhouses  city theater Zwinger three. Visit our parts of the civi- We will skip the reading now. D- Um. And um. The reading was five minutes  exactly. And now comes the - This is the phone-in phase of - Thank you for calling Heidelberg Tourist Information. How may I help you? Wait  can I - I have a question. So. Hi. I want to um So there's no system  right? Like  there was a wizard for both uh - both parts  is this right? Yeah. It was bo- it both times the same person. O_K. One time  pretending to be a system  one time  to - pretending to be a human  which is actually not pretending. I should - O_K. And she didn't - I mean. Well. Isn't this kind of obvious when it says ""O_K now you're talking to a human"" and then the human has the same voice? No no no. We u- Wait. O_K  good question  but uh you - you just wait and see. O_K. It's - You're gonna l- learn. And um the wizard sometimes will not be audible  Because she was actually - they - there was some uh lapse in the um wireless  we have to move her closer. I'm deciding on going to the concert of the Hindelberg - for the Hindelberg Symphony Orchestra. I want to buy something elegant in one of the designer boutiques which is located on Friedrich Ebert um Anlage? Do you understand that? In order to get to the Friedrich Ebert Anlage  turn around and go towards the church. Next you have to t- at the church into the Friedrich Ebert Anlage. O_K. And - Is she mispronouncing ""Anlage""? Is it ""Anlaga"" or ""Anlunga"" I want @@ what a um sunken castle is. They're mispronouncing everything  but it's - O_K. A sunken castle differs from the regular sort of castle in that it is built in a flat region  rather than on a mountain. O_K. Oh  and I also remember that someone told me about the historic graffiti in the old student prison. And I wanted to know how to get there. In order to get to the student prison  from the university square  turn right  and walk one block. O_K. This is the system breaking down  actually. Hello? Yes. Uh  I'm sorry  something uh seems to have happened to the system. Um  - see if I can help you. Are you a live person? or is this a recording? Th- I'm a live person. O_K. Um  thing I was gonna ask you  um  O_K  @@ I need to know where you are  first of all- W- where am I? Um  I don't know. See - c- figure it out? I see a building that's um under construction. O_K  uh-huh. That's p- that's probably the university square. O_K. So  that's where I am. O_K. So  uh  what do you need to do? Um. Can you give me a description of the Powder-Tower  at the Hindelberg castle? Yeah  in order to get there you have to take a bus  you've got to take the number forty-two Uh-huh. To the town hall station. And then you have to transfer to the funicular train. And take the castle exit and turn right. And after about half a mile you'll see the - the Powder-Tower on your right hand side. O_K. And - How much is it for the train and the bus? Um  you can get a - a transfer on the - on the bus for the train for about uh four marks. O_K. O_K  that's it. That's all you need to do? Mm-hmm  thanks for your help. Sure  bye-bye. Bye. I thought it was a recording. @@ At first I thought it was like a recording that could pick up what you're saying and give you directions. Yeah. But it was a real person. Did I call Europe? ""Did I call Europe?"" So  this is it. @@ @@ Well  if we - we um We had her fooled in the beginning. She thought it was a recording in the beginning and was surprised that a human came on. Um  maybe she had some doubts later on whether the first part was a recording or not  but while she was speaking to it  she didn't. So  are - are you trying to record this meeting? There was a strange reflex. I have a headache. I'm really sort of out of it. O_K  the uh lessons learned. The reading needs to be shorter. Five minutes is just too long. Um  that was already anticipated by some people suggested that if we just have bullets here  they're gonna not - they're - subjects are probably not gonna - going to follow the order. And uh she did not. She - No. She - she jumped around quite a bit. Really? Oh  it's surprising. S- so if you just number them ""one""  ""two""  ""three"" it's Yeah  and make it sort of clear in the uh - O_K. Right. Um. We need to - So that's one thing. And we need a better introduction for the wizard. That is something that Fey actually thought of a - in the last second that sh- the system should introduce itself  when it's called. Mm-hmm. True. And um  um  another suggestion  by Liz  was that we uh  through subjects  switch the tasks. So when - when they have task-one with the computer  the next person should have task-one with a human  and so forth. So we get nice um data for that. Mm-hmm. Um  we have to refine the tasks more and more  which of course we haven't done at all  so far  in order to avoid this rephrasing  so where  even though w- we don't tell the person ""ask blah-blah-blah-blah-blah"" they still try  or at least Litonya tried to um Say exactly what's on there? Yeah. repeat as much of that text as possible. And uh my suggestion is of course we - we keep the wizard  because I think she did a wonderful job  in the sense that she responded quite nicely to things that were not Great. asked for  ""How much is a t- a bus ticket and a transfer"" so this is gonna happen all the time  we d- you can never be sure. Mm-hmm. Um. Johno pointed out that uh we have maybe a grammatical gender problem there with wizard. So um. Yes. I wasn't - wasn't sure whether wizard was the correct term for uh ""not a man"". But uh - There's no female equivalent of - Are you sure? No  I don't know. Not that I know of. Right. Well  there is witch and warlock  and uh - Yeah  that's what I was thinking  but - Yeah  that's so @@ . Right. Right. O_K. Uh. And um - So  some - some work needs to be done  but I think we can uh - And this  and - in case no - you hadn't seen it  this is what Litonya looked at during the uh - Ah. um while taking the - while partaking in the data collection. O_K  great. So first of all  I agree that um we should hire Fey  and start paying her. Probably pay for the time she's put in as well. Um  do you know exactly how to do that  or is uh Lila - I mean  you know what exactly do we do to - to put her on the payroll in some way? I'm completely clueless  but I'm willing to learn. O_K. Well  you'll have to. Right. So anyway  um N- So why don't you uh ask Lila and see what she says about you know exactly what we do for someone in th- Student-type worker  or - ? Well  yeah she's un- she's not a - a student  she just graduated but anyway. So i- if - Yeah  I agree  she sounded fine  she Hmm. a- actually was uh  more uh  present and stuff than - than she was in conversation  so she did a better job than I would have guessed from just Yeah. talking to her. So I think that's great. This is sort of what I gave her  so this is for example h- how to get to the student prison  and I didn't even spell it out here and in some cases I - I spelled it out a little bit um Yeah. Right. more thoroughly  this is the information on - on the low- sunken castle  and the amphitheater that never came up  and um  so i- if we give her even more um  instruments to work with I think the results are gonna be even better. Oh  yeah  and then of course as she does it she'll - she'll learn @@ . So that's great. Um And also if she's willing to take on the job of organizing all those subjects and stuff that would be wonderful. Mmm. And  uh she's - actually she's going to graduate school in a kind of an experimental paradigm  so I think this is all just fine in terms of h- her learning things she's gonna need to know Mmm. uh  to do her career. So  I - my guess is she'll be r- r- quite happy to take on that job. And  so - Yep. Yeah she - she didn't explicitly state that so. Great. And um I told her that we gonna um figure out a meeting time in the near future to refine the tasks and s- look for the potential sources to find people. She also agrees that you know if it's all just gonna be students the data is gonna be less valuable because of that so. Well  as I say there is this s- set of people next door  it's not hard to We're already - Yeah. uh - However  we may run into a problem with a reading task there. And um  we'll see. Yeah. We could talk to the people who run it and um see if they have a way that they could easily uh tell people that there's a task  pays ten bucks or something  but Mm-hmm. Yeah. um you have to be comfortable reading relatively complicated stuff. And - and there'll probably be self-selection to some extent. Mmm. Yep. Uh  so that's good. Um. O_K. Now  I signed us up for the Wednesday slot  and part of what we should do is this. So  my idea on that was uh  partly we'll talk about system stuff for the computer scientists  but partly I did want it to get the linguists involved in some of this issue about what the task is and all - um you know  what the dialogue is  and what's going on linguistically  because to the extent that we can get them contributing  that will be good. Yep. So this issue about you know re-formulating things  maybe we can get some of the linguists sufficiently interested that they'll help us with it  uh  other linguists  if you're a linguist  but in any case  um  Yep. the linguistics students and stuff. So my idea on - on Wednesday is partly to uh - you - I mean  what you did today would - i- is just fine. You just uh do ""this is what we did  and here's the thing  and here's s- some of the dialogue and - and so forth."" But then  the other thing of course is we should um give the computer scientists some idea of - of what's going on with the system design  and where we think the belief-nets fit in and where the pieces are and stuff like that. Yep. Is - is this make sense to everybody? Yeah. So  I don't - I don't think it's worth a lot of work  particularly on your part  to - to - to make a big presentation. I don't think you should - you don't have to make any new uh PowerPoint or anything. I think we got plenty of stuff to talk about. And  then um just see how a discussion goes. Mm-hmm. Sounds good. The uh other two things is um we've - can have Johno tell us a little about this Great. and we also have a l- little bit on the interface  M_-three-L_ enhancement  and then um that was it  I think. So  what I did for this - this is - uh  a pedagogical belief-net because I was - I - I took - I tried to conceptually do what you were talking about with the nodes that you could expand out - so what I did was I took - I made these dummy nodes called Trajector-In and Trajector-Out that Yep. would isolate the things related to the trajector. And then there were the things with the source and the path and the goal. Yep. And I separated them out. And then I um did similar things for our - our net to - uh with the context and the discourse and whatnot  um  so we could sort of isolate them or whatever in terms of the - the top layer. Mm-hmm. And then the bottom layer is just the Mode. So  let's - let's - Yeah  I don't understand it. Let's go - So. Slide all the way up so we see what the p- the p- very bottom looks like  or is that it? Yeah  there's just one more node and it says ""Mode"" which is the decision between the - Yeah. O_K  great. Alright. So - So basically all I did was I took the last belief-net Mm-hmm. and I grouped things according to what - how I thought they would fit in to uh image schemas that would be related. And the two that I came up with Yep. were Trajector-landmark and then Source-path-goal as initial ones. Mm-hmm. And then I said well  uh the trajector would be the person in this case probably. Right  yep. Um  you know  we have - we have the concept of what their intention was  whether they were trying to tour or do business or whatever  Right. or they were hurried. That's kind of related to that. And then um in terms of the source  the things - uh the only things that we had on there I believe were whether - Oh actually  I kind of  - I might have added these cuz I don't think we talked too much about the source in the old one but Mm-hmm. uh whether the - where I'm currently at is a landmark might have a bearing on whether - or the ""landmark-iness"" of where I'm currently at. And ""usefulness"" is basi- basically means is that an institutional facility like a town hall or something like that that's not - something that you'd visit for tourist's - tourism's sake or whatever. ""Travel constraints"" would be something like you know  maybe they said they can - they only wanna take a bus or something like that  right? And then those are somewhat related to the path  so that would Mm-hmm. determine whether we'd - could take - we would be telling them to go to the bus stop or versus walking there directly. Um  ""Goal"". Similar things as the source except they also added whether the entity was closed and whether they have somehow marked that is was the final destination. Um  and then if you go up  Robert  Yeah  so - um  in terms of Context  what we had currently said was whether they were a businessman or a tourist of some other person. Um  Discourse was related to whether they had asked about open hours or whether they asked about where the entrance was or the admission fee  or something along those lines. Mm-hmm. Uh  Prosody I don't really - I'm not really sure what prosody means  in this context  so I just made up you know whether - whether what they say is - or h- how they say it Right  O_K. is - is that. Um  the Parse would be what verb they chose  and then maybe how they modified it  in the sense of whether they said ""I need to get there quickly"" or whatever. Mm-hmm. And um  in terms of World Knowledge  this would just basically be like opening and closing times of things  the time of day it is  and whatnot. What's ""tourbook""? Tourbook? That would be  I don't know  the ""landmark-iness"" of things  Mm-hmm. whether it's in the tourbook or not. Ch-ch-ch-ch. Now. Alright  so I understand what's - what you got. I don't yet understand how you would use it. So let me see if I can ask a s- Well  this is not a working Bayes-net. Right. No  I understand that  but - but um So  what - Let's slide back up again and see - start at the - at the bottom and Oop-bo-doop-boop-boop. Yeah. So  you could imagine w- Uh  go ahead  you were about to go up there and point to something. Well I - O_K  I just - Say what you were gonna say. O_K. Good  do it! No no  go do it. Uh - I - I'd - No  I was gonna wait until - Oh  O_K . So  so if you - if we made - if we wanted to make it into a - a real uh Bayes-net  that is  you know  with fill - you know  actually f- uh  fill it @@ in  then uh - So we'd have to get rid of this and connect these things directly to the Mode. Well  I don't - That's an issue. So  um - Cuz I don't understand how it would work otherwise. Well  here's the problem. And - and uh - Bhaskara and I was talking about this a little earlier today - is  if we just do this  we could wind up with a huge uh  combinatoric input to the Mode thing. And uh - Well I - oh yeah  I unders- I understand that  I just - uh it's hard for me to imagine how he could get around that. Well  i- But that's what we have to do. O_K. O_K  so  so  uh. There - there are a variety of ways of doing it. Uh. Let me just mention something that I don't want to pursue today which is there are technical ways of doing it  uh I- I slipped a paper to Bhaskara and - about Noisy-OR's and Noisy-MAXes and there're ways to uh sort of back off on the purity of your Bayes-net-edness. Mmm. Uh  so. If you co- you could ima- and I- now I don't know that any of those actually apply in this case  but there is some technology you could try to apply. So it's possible that we could do something like a summary node of some sort that - Yeah. O_K. Yeah. And  um So. So in that case  the sum- we'd have - we - I mean  these wouldn't be the summary nodes. We'd have the summary nodes like where the things were - I guess maybe if thi- if things were related to business or Yeah. some other - So what I was gonna say is - is maybe a good at this point Yeah. is to try to informally - I mean  not necessarily in th- in this meeting  but to try to informally think about what the decision variables are. So  if you have some bottom line uh decision about which mode  you know  what are the most relevant things. Mmm. And the other trick  which is not a technical trick  it's kind of a knowledge engineering trick  is to make the n- each node sufficiently narrow that you don't get this combinatorics. So that if you decided that you could characterize the decision as a trade-off between three factors  whatever they may be  O_K? then you could say ""Aha  let's have these three factors""  O_K? and maybe a binary version f- for each  or some relatively compact decision node just above the final one. Mmm. And then the question would be if - if those are the things that you care about  uh can you make a relatively compact way of getting from the various inputs to the things you care about. So that y- so that  you know  you can sort of try to do a knowledge engineering thing O_K. given that we're not gonna screw with the technology and just always use uh sort of orthodox Bayes-nets  then we have a knowledge engineering little problem of how do we do that. Um and So what I kind of need to do is to take this one and the old one and merge them together? So that - ""Eh-eh-eh."" Yeah. Well  mmm  something. I mean  so uh  Robert has thought about this problem f- for a long time  cuz he's had these examples kicking around  so he may have some good intuition about you know  what are the crucial things. Mmm. and  um  I understand where this - the uh - this is a way of playing with this abs- Source-path-goal trajector exp- uh uh abstraction and - and sort of sh- displaying it in a particular way. Yeah. Uh  I don't think our friends uh on Wednesday are going to be able to - Well  maybe they will. Well  let me think about whether - whether I think we can present this to them or not. Um  Uh  Well  I think this is still  I mean  ad-hoc. This is sort of th- the second version and I - I - I - look at this maybe just as a  you know  a - a - whatever  U_M_L diagram or  you know  as just a uh screen shot  not really as a Bayes-net as John - Johno said. We could actually  y- yeah draw it in a different way  in the sense that it would make it more abstract. Yeah. But the uh - the - the nice thing is that you know  it just is a - is a visual aid for thinking about these things which has comple- clearly have to be specified m- more carefully and uh Alright  well  le- let me think about this some more  and uh see if we can find a way to present this to this linguists group that - that is helpful to them. I mean  ultimately we - we may w- w- we regard this as sort of an exercise in - in thinking about the problem and maybe a first version of uh a module  if you wanna call it that  that you can ask  that you can give input and it- it'll uh throw the dice for you  uh throw the die for you  because um I integrated this into the existing SmartKom system in - in the same way as much the same way we can um sort of have this uh - this thing. Close this down. So if this is what M_-three-L_ um will look like and what it'll give us  um - And a very simple thing. We have an action that he wants to go from somewhere  which is some type of object  to someplace. Mm-hmm. And this - these uh - this changed now only um  um - It's doing it twice now because it already did it once. Um  we'll add some action type  which in this case is ""Approach"" Mm-hmm. Good. and could be  you know  more refined uh in many ways. Or we can uh have something where the uh goal is a public place and it will give us then of course an action type of the type ""Enter"". So this is just based on this one - um  on this one feature  and that's - that's about all you can do. And so in the f- if this pla- if the object type um here is - is a m- is a landmark  of course it'll be um ""Vista"". And um this is about as much as we can do if we don't w- if we want to avoid uh uh a huge combinatorial explosion where we specify ""O_K  if it's this and this but that is not the case""  and so forth  it just gets really really messy. O_K  I'm sorry. You're - you're - Hmm? It was much too quick for me. O_K  so let me see if I understand what you're saying. So  I - I do understand that uh you can take the M_-three-L_ and add not - and it w- and you need to do this  for sure  we have to add  you know  not too much about um object types and stuff  and what I think you did is add some rules of the style that are already there that say ""If it's of type ""Landmark""  then you take - you're gonna take a picture of it."" Exactly. F- full stop  I mean  that's what you do. Ev- every landmark you take a picture of  Every public place you enter  and statue you want to go as near as possible. you enter - You approach. O_K. Uh  and certainly you can add rules like that to the existing SmartKom system. And you just did  right? O_K. Yeah. And it - it would do us no good. That - Ultimately. Ah. W- Well. So  s- well  and let's think about this. Um  that's a - that's another kind of baseline case  that's another sort of thing ""O_K  here's a - another kind of minimal uh way of tackling this"". Add extra properties  a deterministic rule for every property you have an action  ""pppt!"" You do that. Um  then the question would be Uh Now  if that's all you're doing  then you can get the types from the ontology  O_K? because that's all - you're - all you're using is this type - the types in the ontology and you're done. Hmm? Right? So we don't - we don't use the discourse  we don't use the context  we don't do any of those things. No. Alright  but that's - but that's O_K  and I mean it- it's again a kind of one minimal extension of the existing things. And that's something the uh SmartKom people themselves would - they'd say ""Sure  that's no problem - you know  no problem to add types to the ont-"" Right? Yeah. No. And this is - just in order to exemplify what - what we can do very  very easily is  um we have this - this silly uh interface and we have the rules that are as banal as of we just saw  and we have our content. Now  the content - Hmm. I - whi- which is sort of what - what we see here  which is sort of the Vista  Schema  Source  Path  Goal  whatever. This will Yeah. Yeah. um be um a job to find ways of writing down Image schema  X_schema  constructions  in some - some form  and have this be in a - in a - in the content  loosely called ""Constructicon"". And the rules we want to throw away completely. And um - and here is exactly where what's gonna be replaced with our Bayes-net  which is exactly getting the input feeding into here. This decides whether it's an- whether action - the - the Enter  the Vista  or the whatever Uh  "" approach ""  you called it  I think this time. uh Approach um construction should be activated  That's what you said - Yeah  that's fine. I_E just pasted in. Yeah  but - Right. But it's not construction there  it's action. Construction is a d- is a different story. Yeah. Right. This is uh - so what we'd be generating would be a reference to a semantic uh like parameters for the - For - for - for - for the X_schema? Yes. O_K. Yeah. So that - that uh i- if you had the generalized ""Go"" X_schema and you wanted to specialize it to these three ones  then you would have to supply the parameters. Right. And then uh  although we haven't worried about this yet  you might wanna worry about something that would go to the G_I_S and use that to actually get you know  detailed route planning. So  you know  where do you do take a picture of it and stuff like that. Mm-hmm. But that's not - It's not the immediate problem. Right. But  presumably that - that - that functionality's there when - when we - So the immediate problem is just deciding w- which - Aspects of the X_schema to add. Yeah  so the pro- The immediate problem is - is back t- t- to what you were - what you are doing with the belief-net. You know  Yeah. uh what are we going to use to make this decision - Right and then  once we've made the decision  how do we put that into the content? Yeah. Right. Right. Well  that - that actually is relatively easy in this case. O_K. The harder problem is we decide what we want to use  how are we gonna get it? And that the - the - that's the hardest problem. So  the hardest problem is how are you going to get this information from some combination of the - what the person says and the context and the ontology. The h- So  I think that's the hardest problem at the moment is - is O_K. where are you gonna - how are you gonna g- get this information. Um  and that's - so  getting back to here  uh  we have a d- a technical problem with the belief-nets that we - we don't want all the com- There's just too many factors right now. too many factors if we - if we allow them to just go combinatorially. Right. So we wanna think about which ones we really care about and what they really most depend on  and can we c- you know  clean this - this up to the point where it - So what we really wanna do i- cuz this is really just the three layer net  we wanna b- Right. make it - expand it out into more layers basically? We might. Uh  I mean that - that's certainly one thing we can do. Uh  it's true that the way you have this  a lot of the times you have - what you're having is the values rather than the variable. So uh - Right. O_K? So you - So instead of in- instead it should really be - just be ""intention"" as a node instead of ""intention business"" or ""intention tour"". Yeah  right  and then it would have values  uh  ""Tour""  ""Business""  or uh ""Hurried"". Right. But then - but i- it still some knowledge design to do  about i- how do you wanna break this up  what really matters. Right. I mean  it's fine. You know  we have to - it's - it's iterative. We're gonna have to work with it some. I think what was going through my mind when I did it was someone could both have a business intention and a touring intention and the probabilities of both of them happening at the same time - Well  you - you could do that. And it's perfectly O_K to uh insist that - that  you know  th- um  they add up to one  but that there's uh - that - that it doesn't have to be one zero zero. Mmm. O_K. O_K. So you could have the conditional p- So the - each of these things is gonna be a - a - a probability. So whenever there's a choice  uh - so like landmark-ness and usefulness  O_K - Well  see I don't think those would be mutually - it seems like something could both be - Absolutely right. O_K. And so that you might want to then have those b- Th- Then they may have to be separate. They may not be able to be values of the same variable. Object type  mm-hmm. So that's - but again  this is - this is the sort of knowledge design you have to go through. Yeah  this is just more me taking last - the last one  and saying ""how can I formalize this?"" @@ Right. It's - you know  it's great - is - is  you know  as one step toward uh - toward where we wanna go. Also it strikes me that we - we m- may want to approach the point where we can sort of try to find a - uh  a specification for some interface  here that um takes the normal M_-three-L_  looks at it. Then we discussed in our pre-edu - E_D_U meeting um how to ask the ontology  what to ask the ontology um the fact that we can pretend we have one  make a dummy until we get the real one  and so um we - we may wanna decide we can do this from here  but we also could do it um you know if we have a - a - a belief-net interface. So the belief-net takes as input  a vector  right? of stuff. And it - Yeah. And um it- Output is whatever  as well. But this information is just M_-three-L_  and then we want to look up some more stuff in the ontology and we want to look up some more stuff in the - maybe we want to ask the real world  maybe you want to look something up in the G_R_S  but also we definitely want to look up in the dialogue history um some s- some stuff. Based on we - we have uh - I was just made some examples from the ontology and so we have for example some information there that the town hall is both a - a - a building and it has doors and stuff like this  but it is also an institution  so it has a mayor and so forth and so forth and we get relations out of it and once we have them  we can use that information to look in the dialogue history  ""were any of these things that - that are part of the town hall as an institution mentioned?""  ""were any of these that make the town hall a building mentioned?""  Mm-hmm. Right. and so forth  and maybe draw some inferences on that. So this may be a - a sort of a process of two to three steps before we get our vector  that we feed into the belief-net  and then - Yeah. I think that's - I think that's exactly right. There will be rules  but they aren't rules that come to final decisions  they're rules that gather information for a decision process. Yeah. Yeah  no I think that's - that's just fine. Uh  yeah. So they'll - they - presumably there'll be a thread or process or something that ""Agent""  yeah  ""Agent""  whatever you wan- wanna say  yeah  that uh is rule-driven  and can - can uh - can do things like that. And um there's an issue about whether there will be - that'll be the same agent and the one that then goes off and uh carries out the decision  so it probably will. My guess is it'll be the same basic agent that um can go off and get information  run it through a - a c- this belief-net that - Turn the crank  yeah. turn a crank in the belief-net  that'll come out with s- uh more - another vector  O_K  which can then be uh applied at what we would call the simulation or action end. So you now know what you're gonna do and that may actually involve getting more information. So on- once you pull that out  it could be that that says ""Ah! Now that we know that we gonna go ask the ontology something else."" Mmm. O_K? Now that we know that it's a bus trip  O_K? we didn't - We didn't need to know beforehand  uh how long the bus trip takes or whatever  but - but now that we know that's the way it's coming out then we gotta go find out more. So I think that's O_K. Mm-hmm. Mm-hmm. So this is actually  s- if - if we were to build something that is um  and  uh  I had one more thing  the - it needs to do - Yeah. I think we - I - I can come up with a - a code for a module that we call the ""cognitive dispatcher""  O_K. which does nothing  but it looks of complect object trees and decides how - are there parts missing that need to be filled out  there's - this is maybe something that this module can do  something that this module can do and then collect uh sub-objects and then recombine them and put them together. So maybe this is actually some - some useful tool that we can use to rewrite it  and uh Oh  O_K. Uh. get this part  then. Yeah. I confess  I'm still not completely comfortable with the overall story. Um. I- i- This - this is not a complaint  this is a promise to do more work. So I'm gonna hafta think about it some more. Um. In particular - see what we'd like to do  and - and this has been implicit in the discussion  is to do this in such a way that you get a lot of re-use. So. What you're trying to get out of this deep co- cognitive linguistics is the fact that w- if you know about source - source  paths and goals  and nnn all this sort of stuff  that a lot of this is the same  for different tasks. And that - uh there's - there's some - some important generalities that you're getting  so that you don't take each and every one of these tasks and hafta re- do it. And I don't yet see how that goes. There're no primitives upon which uh yeah. Alright. u- u- What are the primitives  and how do you break this - So I y- I'm just - just there saying eee well you - I know how to do any individual case  Right. right? but I don't yet - see what's the really interesting question is can you use uh deep uh cognitive linguistics to get powerful generalizations. And um Yep. Maybe we sho- should we a- add then the ""what's this?"" domain? N- I mean  we have to ""how do I get to X_"". Then we also have the ""what's this?"" domain  Right. Could. Uh. where we get some slightly different - Um Johno  actually  does not allow us to call them ""intentions"" anymore. So he - he dislikes the term. Yeah. I didn't @@ . Well  I - I don't like the term either  so I have n- i- uh i- i- y- w- i- i- It uh - But um  I'm sure the ""what's this?"" questions also create some interesting X_schema aspects. Could be. I'm not a - I'm not op- particularly opposed to adding that or any other task  I mean  eventually we're gonna want a whole range of them. So. Mm-hmm. That's right. Uh  I'm just saying that I'm gonna hafta do some sort of first principles thinking about this. I just at the moment don't know. H- Mm-hmm. Do you mean in the sense of ""is there a way we can generalize the Bayes-net""  or - No. Well  no the Bayes - the Bayes-nets - Seems like it has to be specific. The Bayes-nets will be dec- specific for each decision. But what I'd like to be able to do O_K. is to have the way that you extract properties  that will go into different Bayes-nets  be the - uh general. Mm-hmm. So that if you have sources  you have trajectors and stuff like that  and there's a language for talking about trajectors  you shouldn't have to do that differently Right. O_K. for uh uh going to something  than for circling it  for uh telling someone else how to go there  whatever it is. Getting out of - So that - that  the - the decision processes are gonna be different What you'd really like of course is the same thing you'd always like which is that you have um a kind of intermediate representation which looks the same o- over a bunch of inputs and a bunch of outputs. So all sorts of different tasks and all sorts of different ways of expressing them use a lot of the same mechanism for pulling out what are the fundamental things going on. And that's - that would be the really pretty result. Right. And pushing it one step further  when you get to construction grammar and stuff  what you'd like to be able to do is say you have this parser which is much fancier than the parser that comes with uh SmartKom  i- that - that actually uses constructions and is able to tell from this construction The SmartKom. that there's uh something about the intent - you know  the actual what people wanna do or what they're referring to and stuff  Mm-hmm. in- independent of whether it - about - what is this or where is it or something  that you could tell from the construction  you could pull out deep semantic information which you're gonna use in a general way. Somehow you'd know that you needed to find out whether there - something was a landmark or something like that? So that's the - You might. Yeah. You might. You might be able to - to uh say that this i- this is the kind of construction in which the - there's - Let's say there's a uh cont- there - the - the land- the construction implies the there's a con- this thing is being viewed as a container. Mmm. O_K. So just from this local construction you know that you're gonna hafta treat it as a container you might as well go off and get that information. And that may effect the way you process everything else. So if you say ""how do I get into the castle"" O_K  Right. then um - Or  you know  ""what is there in the castle"" or - so there's all sorts of things you might ask that involve the castle as a container and you'd like to have this orthogonal so that anytime the castle's referred to as a container  you crank up the appropriate stuff. Right. Independent of what the goal is  and independent of what the surrounding language is. Mm-hmm. Alright  so that's - that's the - that's the thesis level uh - Mm-hmm. It's unfortunate also that English has sort of got rid of most of its spatial adverbs because they're really fancy then  in - in - for these kinds of analysis. But uh. Well  you have prepositional phrases that - Yeah  but they're - they're easier for parsers. Parsers can pick those up but - but the - with the spatial adverbs  Right. they have a tough time. Because the - mean- the semantics are very complex in that. Right. O_K  yeah? I had one more thing. I don't remember. I just forgot it again. No . Oh yeah  b- But an architecture like this would also enable us maybe to - to throw this away and - and replace it with something else  or whatever  so that we have - so that this is sort of the representational formats we're - we're - we're talking about that are independent of the problem  that generalize over those problems  and are oh  t- of a higher quality than an- any actual whatever um belief-net  or ""X_"" that we may use for the decision making  ultimately. Should be decoupled  yeah. O_K. Right. So  are we gonna be meeting here from now on? I'm - I'm happy to do that. We - we had talked about it  cuz you have th- th- the display and everything  that seems fine. Yeah  um  Liz also asks whether we're gonna have presentations every time. I don't think we will need to do that but it's - so far I think it was nice as a visual aid for some things and - and - Right. Oh yeah. No I - I think it's worth it to ass- to meet here to bring this  and assume that something may come up that we wanna look at. Yeah. I mean. Why not. And um. Yeah  that was my - She was good. Litonya was good. Yeah? The uh - um  she w- she was definitely good in the sense that she - she showed us some of the weaknesses Right. and um also the um - the fact that she was a real subject you know  is - is - Right. Yeah  and - and - and - yeah and - and she took it seriously and stuff l- No  it was great. Yeah. Yeah. So I think that um - I mean  w- Looking - just looking at this data  listening to it  what can we get out of it in terms of our problem  for example  is  you know  she actually m- said - you know  she never s- just spoke about entering  she just wanted to get someplace  and she said for buying stuff. Nuh? So this is definitely interesting  and - Yeah  right. Um  and in the other case  where she wanted to look at the stuff at the graffiti  also  of course  not in the sentence ""How do you get there?"" was pretty standard. Nuh? except that there was a nice anaphora  you know  for pointing at what she talked about before  and there she was talking about looking at pictures that are painted inside a wall- on walls  so Right. Actually  you'd need a lot of world knowledge. This would have been a classical um uh ""Tango""  actually. Um  Yeah. because graffiti is usually found on the outside and not on the inside  but O_K. So the mistake would have make a mistake - the system would have made a mistake here. Yep. Click? Alright. ",ICSI's Meeting Recorder Group at Berkeley meets to discuss  for the most part  progress on the Aurora Project. The main areas being worked on were the voice activity detector and the tandem data streams. The group discussed possible further investigations that arose from these areas  including better linking the two. They also consider how aspects of an absent member's work might be applied to the current project. The meeting closed with a discussion of upcoming absences  and how meetings would continue. Speaker me018 must confirm what is needed to work with the new software in terms of adjustments with someone further up the project chain. The system at it's current stage employs the neural networks and second stream  but the group leader would like the network investigated separately  incase it is hurting performance. There are worries regarding the need to make adjustments so the new software can handle the group's different feature set. The system  whilst improved  also has increased latency  and while the limit has not been set  the group need to reduce it. Likewise the number of features the use in their system  since this has been set at an arbitrarily low value. There has been an increase in the number of deletion in the errors  which is of some concern. Speaker mn007 has been implementing a new voice activity detector on noise compensated data  and it performs much better. He has also been working on the tandem neural network. Speaker me013  along with a student  submitted work on reverberation for a speech workshop. Speaker me018 has downloaded and compiled the software he was asked to work with in the previous meeting. 
"Oh. So  uh - Alright. Um  so I wanted to discuss digits briefly  Oh good. but that won't take too long. Right. O_K  agenda items  Uh  we have digits  What else we got? New version of the presegmentation. New version of presegmentation. Um  do we wanna say something about the  Yeah  why don't you summarize the - an update of the  uh  transcript? Update on transcripts. And I guess that includes some - the filtering for the  the A_S_I refs  too. Mmm. Filtering for what? For the references that we need to go from the - the fancy transcripts to the sort of brain-dead. It'll - it'll be - basically it'll be a re-cap of a meeting that we had jointly this morning. Uh-huh. With Don  as well. Mm-hmm. Got it. Anything else more pressing than those things? So - So  why don't we just do those. You said yours was brief  so - O_K. O_K well  the  w- uh as you can see from the numbers on the digits we're almost done. The digits goes up to about four thousand. Um  and so  uh  we probably will be done with the T_I-digits in  um  another couple weeks. um  depending on how many we read each time. So there were a bunch that we skipped. You know  someone fills out the form and then they're not at the meeting and so it's blank. Um  but those are almost all filled in as well. And so  once we're - it's done it would be very nice to train up a recognizer and actually start working with this data. So we'll have a corpus that's the size of T_I-digits? And so - One particular test set of T_I-digits. Test set  O_K. So  I - I extracted  Ther- there was a file sitting around which people have used here as a test set. It had been randomized and so on and that's just what I used to generate the order. Oh! Great. of these particular ones. Great. Um - So  I'm impressed by what we could do  Is take the standard training set for T_I-digits  train up with whatever  you know  great features we think we have  uh for instance  and then test on uh this test set. And presumably uh it should do reasonably well on that  and then  presumably  we should go to the distant mike  and it should do poorly. Yeah. Right. And then we should get really smart over the next year or two  and it - that should get better. And inc- increase it by one or two percent  yeah. Yeah  Yeah. Um  but  in order to do that we need to extract out the actual digits. Right. Um  so that - the reason it's not just a transcript is that there're false starts  and misreads  and miscues and things like that. And so I have a set of scripts and X_Waves where you just select the portion  hit R_  um  it tells you what the next one should be  and you just look for that. You know  so it - it'll put on the screen  ""The next set is six nine  nine two two"". And you find that  and  hit the key and it records it in a file in a particular format. So is this - And so the - the question is  should we have the transcribers do that or should we just do it? Well  some of us. I've been do- I've done  eight meetings  something like that  just by hand. Just myself  rather. So it will not take long. Um - Uh  what - what do you think? My feeling is that- we discussed this right before coffee and I think it's a - it's a fine idea partly because  um  it's not un- unrelated to their present skill set  but it will add  for them  an extra dimension  it might be an interesting break for them. And also it is contributing to the  uh  c- composition of the transcript cuz we can incorporate those numbers directly and it'll be a more complete transcript. So I'm - I think it's fine  that part. There is - there is - So you think it's fine to have the transcribers do it? Mm-hmm. Yeah  O_K. There's one other small bit  which is just entering the information which at s- which is at the top of this form  Good. onto the computer  to go along with the - where the digits are recorded automatically. Yeah. And so it's just  you know  typing in name  times - time  date  and so on. Um  which again either they can do  but it is  you know  firing up an editor  or  again  I can do. Or someone else can do. And  that  you know  I'm not  that - that one I'm not so sure if it's into the - the  things that  I  wanted to use the hours for  because the  the time that they'd be spending doing that they wouldn't be able to be putting more words on. Mmm. But that's really your choice  it's your - So are these two separate tasks that can happen? Or do they have to happen at the same time before - No they don't have - this - you have to enter the data before  you do the second task  but they don't have to happen at the same time. So it's - it's just I have a file whi- which has this information on it  O_K. and then when you start using my scripts  for extracting the times  it adds the times at the bottom of the file. And so  um  I mean  it's easy to create the files and leave them blank  and so actually we could do it in either order. Oh  O_K. Um  it's - it's sort of nice to have the same person do it just as a double-check  to make sure you're entering for the right person. But  either way. Yeah. Yeah just by way of uh  uh  a uh  order of magnitude  uh  um  we've been working with this Aurora  uh data set. And  uh  the best score  on the  nicest part of the data  that is  where you've got training and test set that are basically the same kinds of noise and so forth  uh  is about  uh - I think the best score was something like five percent  uh  error  per digit. So  that - Per digit. Per digit. You're right. So if you were doing ten digit  uh  recognition  you would really be in trouble. Mm-hmm. So - So the - The point there  and this is uh car noise uh  uh things  but - but real - real situation  well  ""real""  Um  the - uh there's one microphone that's close  that they have as - as this sort of thing  close versus distant. Uh but in a car  instead of - instead of having a projector noise it's - it's car noise. Uh but it wasn't artificially added to get some - some artificial signal-to-noise ratio. It was just people driving around in a car. So  that's - that's an indication  uh that was with  many sites competing  and this was the very best score and so forth  so. Although the models weren't  More typical numbers like that good  right? I mean  the models are pretty crappy? You're right. I think that we could have done better on the models  but the thing is that we got - this - this is the kind of typical number  for all of the  uh  uh  things in this task  all of the  um  languages. And so I - I think we'd probably - the models would be better in some than in others. Um  Hmm. so  uh. Anyway  just an indication once you get into this kind of realm even if you're looking at connected digits it can be pretty hard. Hmm. It's gonna be fun to see how we  compare at this. How did we do on the T_I-digits? Yeah. Very exciting. Well the prosodics are so much different s- it's gonna be  strange. I mean the prosodics are not the same as T_I-digits  for example. Yeah. So I'm - I'm not sure how much of effect that will have. H- how do - What do you mean  the prosodics? Um  just what we were talking about with grouping. That with these  the grouping  there's no grouping at all  and so it's just - the only sort of discontinuity you have is at the beginning and the end. So what are they doing in Aurora  are they reading actual phone numbers  or  Aurora I don't know. I don't know what they do in Aurora. a - a digit at a time  or - ? Uh  I'm not sure how - no  no I mean it's connected - it's connected  uh  Cuz it's - Connected. digits  yeah. So there's also the - But. But - not just the prosody but the cross - the cross-word modeling is probably quite different. Right. But in T_I-digits  H- they're reading things like zip codes and phone numbers and things like that  so it's gonna be different. Right. How do we do on T_I-digits? I don't remember. I mean  very good  right? Yeah  I mean we were in the. One and a half percent  two percent  something like that? Uh  I th- no I think we got under a percent  but it was - but it's - but I mean. Oh really? O_K. s- The very best system that I saw in the literature was a point two five percent or something that somebody had at - at Bell Labs  or. Alright. Uh  but. Hmm. But  uh  sort of pulling out all the stops. But I think a lot of systems sort of get half a percent  or three-quarters a percent  and we're - we're in there somewhere. Right. But that - I mean it's really - it's - it's close-talking mikes  no noise  clean signal  just digits  I mean  Yeah. every- everything is good. It's the beginning of time in speech recognition. Yes  exactly. Yeah. And we've only recently got it to anywhere near human. It's like the  single cell  Pre- you know  prehistory. it's the beginning of life  yeah. And it's still like an order of magnitude worse than what humans do. Right. Yeah. So. When - When they're wide awake  yeah. Yeah. Um  after coffee  you're right. After coffee. Not after lunch. O_K  so  um  what I'll do then is I'll go ahead and enter  this data. And then  hand off to Jane  and the transcribers to do the actual extraction of the digits. Yeah. Yeah. One question I have that - that- I mean  we wouldn't know the answer to now but might  Hmm. do some guessing  but I was talking before about doing some model- modeling of arti- uh  uh  marking of articulatory  features  with overlap and so on. And  and  um  On some subset. One thought might be to do this uh  on - on the digits  or some piece of the digits. Uh  it'd be easier  uh  and so forth. The only thing is I'm a little concerned that maybe the kind of phenomena  in w- i- i- The reason for doing it is because the - the argument is that certainly with conversational speech  the stuff that we've looked at here before  um  just doing the simple mapping  from  um  the phone  to the corresponding features that you could look up in a book  uh  isn't right. It isn't actually right. In fact there's these overlapping processes where some voicing some up and then some  you know  some nasality is - comes in here  and so forth. And you do this gross thing saying ""Well I guess it's this phone starting there"". So  uh  that's the reasoning. But  It could be that when we're reading digits  because it's - it's for such a limited set  that maybe - maybe that phenomenon doesn't occur as much. I don't know. Di- an- anybody - ? Do you have any - ? Anybody have any opinion about that  or - ? @@ . It s- strikes me that there are more - each of them is more informative because it's so  random  and that people might articulate more  and you that might end up with more - a closer correspondence. Mm-hmm. Yeah - that's - I - I agree. That - it's just - Yeah. Sort of less predictability  and - Mm-hmm. Yeah. Well it's definitely true that  when people are  It's a - reading  even if they're re-reading what  they had said spontaneously  that they have very different patterns. Mitch showed that  and some  Right. dissertations have shown that. So the fact that they're reading  first of all  whether they're reading in a room of  people  or rea- you know  just the fact that they're reading will make a difference. Yeah. And  Well - depends what you're interested in. Would  this corpus really be the right one to even try that on? See  I don't know. So  may- maybe the thing will be do - to take some very small subset  I- mean not have a big  program  but take a small set  uh  subset of the conversational speech and a small subset of the digits  and look and - and just get a feeling for it. Um  just take a look. Really. H- Cuz I don't think anybody is  I- at least  I don't know  of anybody  uh  well  I don't know  That could - could be an interesting design  too  cuz then you'd have the com- the comparison of the  You hafta - the answers . Hey. uh  predictable speech versus the less predictable speech and maybe you'd find that it worked in  Yeah. in the  case of the pr- of the  uh  non-predictable. Yeah. Hafta think about  the particular acoustic features to mark  too  because  Mm-hmm. I mean  some things  they wouldn't be able to mark  like  uh  you know  uh  tense lax. Some things are really difficult. You know  just listening. M- I think we can get Ohala in to  Well. give us some advice on that. Yeah. Also I thought you were thinking of a much more restricted set of features  that - Yeah  but I - I - I - I was  like he said  I was gonna bring John in and ask John what he thought. Yeah  sure. Sure. Yeah. Right. But I mean you want - you want it be restrictive but you also want it to - to - to have coverage. Right. You know i- you should. Yeah It should be such that if you  if you  uh  if you had o- um  all of the features  determined that you - that you were uh ch- have chosen  that that would tell you  uh  in the steady-state case  uh  the phone. So  O_K. um. Even  I guess with vowels that would be pretty hard  wouldn't it? To identify actually  you know  which one it is? It would seem to me that the points of articulation would be m- more  g- uh  I mean that's - I think about articulatory features  I think about  points of articulation  which means  uh  rather than vowels. Yeah. Points of articulation? What do you mean? So  is it  uh  bilabial or dental or is it  you know  palatal. Mm-hmm. @@ Which - which are all like where - where your tongue comes to rest. Uvular. Place of ar- place of articulation. Place  place. Place. Place. Thank you  what - whatev- whatever I s- said  that's - I really meant place. Yeah. O_K. Yeah. O_K  I see. Yeah. O_K we got our jargon then  O_K. Yeah. Uh. Well it's also  there's  really a difference between  the pronunciation models in the dictionary  and  the pronunciations that people produce. And  so  You get  some of that information from Steve's work on the - Right. on the labeling and it really  Right. I actually think that data should be used more. That maybe  although I think the meeting context is great  that he has transcriptions that give you the actual phone sequence. And you can go from - not from that to the articulatory features  but that would be a better starting point for marking  the gestural features  then  data where you don't have that  because  we - you wanna know  both about the way that they're producing a certain sound  and what kinds of  you know what kinds of  phonemic  differences you get between these  transcribed  sequences and the dictionary ones. Well you might be right that mi- might be the way at getting at  what I was talking about  but the particular reason why I was interested in doing that was because I remember  when that happened  and  John Ohala was over here and he was looking at the spectrograms of the more difficult ones. Uh  he didn't know what to say  about  what is the sequence of phones there. They came up with some compromise. Because that really wasn't what it look like. It didn't look like a sequence of phones it look like this blending thing happening here and here and here. Right. Right. Right. Yeah  so you have this feature here  and  There was no name for that. Yeah. But - overlap  yeah. Right. But it still is - there's a - Yeah. there are two steps. One - you know  one is going from a dictionary pronunciation of something  And - Right. like  ""gonna see you tomorrow""  it could be ""going to"" or ""gonna"" or "" gonta s-"" you know. Yeah. Right. Or "" gonta "". And  yeah. ""Gonna see you tomorrow""  uh  "" guh see you tomorrow"". And  that it would be nice to have these  intermediate  or these - some - these reduced pronunciations that those transcribers had marked or to have people mark those as well. Mm-hmm. Because  it's not  um  that easy to go from the  dictionary  word pronuncia- the dictionary phone pronunciation  to the gestural one without this intermediate or a syllable level kind of  representation. Well I don't think Morgan's suggesting that we do that  though. Do you mean  Yeah. Yeah  I mean  I- I- I'm jus- at the moment of course we're just talking about what  to provide as a tool for people to do research who have different ideas about how to do it. So for instance  you might have someone who just has a wor- has words with states  and has uh - uh  comes from articulatory gestures to that. And someone else  might actually want some phonetic uh intermediate thing. So I think it would be - be best to have all of it if we could. But um  But - What I'm imagining is a score-like notation  Yeah. where each line is a particular feature. Right  so you would say  you know  it's voiced through here  and so you have label here  and you have nas- nasal here  and  Yeah. they - they could be overlapping in all sorts of bizarre ways that don't correspond to the timing on phones. I mean this is the kind of reason why - I remember when at one of the Switchboard  workshops  that uh when we talked about doing the transcription project  Dave Talkin said  ""can't be done"". Right. He was - he was  what - what he meant was that this isn't  you know  a sequence of phones  and when you actually look at Switchboard that's  not what you see  and  you know. And in - in fact the inter-annotator agreement was not that good  right? And. On the harder ones? It  yeah I mean it was- It depends how you look at it  and I - I understand what you're saying about this  Yeah. kind of transcription exactly  because I've seen - you know  where does the voicing bar start and so forth. Yeah. All I'm saying is that  it is useful to have that - the transcription of what was really said  and which syllables were reduced. Uh  if you're gonna add the features it's also useful to have some level of representation which is  is a reduced - it's a pronunciation variant  that currently the dictionaries don't give you because if you add them to the dictionary and you run recognition  you  Mm-hmm. Right. you add confusion. So people purposely don't add them. Right. So it's useful to know which variant was - was produced  at least at the phone level. So it would be - it would be great if we had  either these kind of  labelings on  the same portion of Switchboard that Steve marked  or  Steve's type markings on this data  Right. That's all  I mean . Exactly. with these. Yeah. Exactly. Yeah  no I - I don't disagree with that. And Steve's type is fairly - it's not that slow  uh  uh  I dunno exactly what the  timing was  but. Yeah u- I don't disagree with it the on- the only thing is that  What you actually will end - en- end up with is something  i- it's all compromised  right  so  the string that you end up with isn't  actually  what happened. But it's - it's the best compromise that a group of people scratching their heads could come up with to describe what happened. Mm-hmm. And it's more accurate than  But. And it's more accurate than the - than the dictionary or  phone labels. The word. Yeah. if you've got a pronunciation uh lexicon that has three or four  this might be have been the fifth one that you tr- that you pruned or whatever  so sure. So it's like a continuum. It's - you're going all the way down  yeah. Right. Right. Right. That's what I meant is - an- and in some places it would fill in  Yeah. Yeah. Well - So - the kinds of gestural features are not everywhere. So there are some things that you don't have access to either from your ear or the spectrogram  Right. Mm-hmm. but you know what phone it was and that's about all you can - all you can say. And then there are other cases where  Right. It's basically just having  nasality  voicing - multiple levels of - Right. of  information and marking  Right. Yeah. on the signal. Well the other difference is that the - the features  Right. are not synchronous  right. They overlap each other in weird ways. Mm-hmm. Mm-hmm. So it's not a strictly one-dimensional signal. Right. So I think that's sorta qualitatively different. Right. You can add the features in  uh  but it'll be underspecified. Th- there'll be no way for you to actually mark what was said completely by features. Hmm. Well not with our current system but you could imagine designing a system  And i- if you're - that the states were features  rather than phones. Well  That's - we - we've probably have a separate  um  Yeah. discussion of  uh - of whether you can do that. Well  isn't that - I thought that was  well but that - wasn't that kinda the direction? I thought Yeah  so I mean  what  what - where this is  I mean  I- I want- would like to have something that's useful to people other than those who are doing the specific kind of research I have in mind  so it should be something broader. But  The - but uh where I'm coming from is  uh  we're coming off of stuff that Larry Saul did with - with  um  uh  John Dalan and Muzim Rahim in which  uh  they  uh  have  um  a m- a multi-band system that is  uh  trained through a combination of gradient learning an- and E_M  to um  estimate  uh  the  uh  value for m- for - for a particular feature. O_K. And this is part of a larger  image that John Dalan has about how the human brain does it in which he's sort of imagining that  individual frequency channels are coming up with their own estimate  of - of these  these kinds of - something like this. Might not be  you know  exact features that  Jakobson thought of or something. But I mean you know some  something like that. Some kind of low-level features  which are not  fully  you know  phone classification. And the - the - th- this particular image  of how thi- how it's done  is that  then given all of these estimates at that level  there's a level above it  then which is - is making  some kind of sound unit classification such as  you know  phone and - and  you know. You could argue what  what a sound unit should be  and - and so forth. But that - that's sort of what I was imagining doing  um  and - but it's still open within that whether you would have an intermediate level in which it was actually phones  or not. You wouldn't necessarily have to. Um  but  Again  I wouldn't wanna  wouldn't want what we - we produced to be so  know  local in perspective that it - it was matched  what we were thinking of doing one week  And - and  and  you know  what you're saying is absolutely right. That  that if we  can we should put in  uh  another level of  of description there if we're gonna get into some of this low-level stuff. Well  you know  um - I mean if we're talking about  having the  annotators annotate these kinds of features  it seems like  You know  you - The - the question is  do they do that on  meeting data? Or do they do that on  Switchboard? That's what I was saying  maybe meeting data isn't the right corpus. W- Well it seems like you could do both. I mean  I was thinking that it would be interesting  Mm-hmm. to do it with respect to  parts of Switchboard anyway  in terms of  uh - partly to see  Mm-hmm. if you could  generate first guesses at what the articulatory feature would be  based on the phone representation at that lower level. It might be a time gain. But also in terms of comparability of  um  Mm-hmm. Well cuz the- yeah  and then also  if you did it on Switchboard  you would have  what you gain the full continuum of transcriptions. You'd have it  Yep. from the lowest level  the ac- acoustic features  then you'd have the  you know  the phonetic level that Steve did  and  Mm-hmm. Yeah that - that's all I was thinking about. it is telephone band  so  yeah. And you could tell that - It'd be a complete  And you get the relative gain up ahead . the bandwidth might be - set then. It's so it's a little different. Yeah. So I mean i- we'll see wha- how much we can  Mm-hmm. uh  get the people to do  and how much money we'll have and all this sort of thing  but  Mm-hmm. But it - it might be good to do what Jane was saying uh  you know  Might be do both. seed it  with  guesses about what we think the features are  based on  you know  the phone or Steve's transcriptions or something. to make it quicker. Alright  so based on the phone transcripts they would all be synchronous  but then you could imagine  Adjusting? Yeah  exactly. nudging them here and there. Scoot the voicing over a little  because - Yeah. Right. Well I think what - I mean I'm - I'm a l- little behind in what they're doing  now  and  uh  the stuff they're doing on Switchboard now. But I think that  Steve and the gang are doing  something with an automatic system first and then doing some adjustment. As I re- as I recall. So I mean that's probably the right way to go anyway  is to - is to start off with an automatic system with a pretty rich pronunciation dictionary that  that  um  you know  tries  to label it all. And then  people go through and fix it. So in - in our case you'd think about us s- starting with maybe the regular dictionary entry  and then? Well  Or would we - regular dictionary  I mean  this is a pretty rich dictionary. It's got  got a fair number of pronunciations in it Or you could start from the - if we were gonna  But - do the same set  of sentences that Steve had  done  we could start with those transcriptions. Mm-hmm. Yeah. That's actually what I was thinking  is tha- - the problem is when you run  So I was thinking - Right. Yeah. uh  if you run a regular dictionary  um  even if you have variants  in there  Yeah. which most people don't  you don't always get  out  the actual pronunciations  so that's why the human transcriber's giving you the - Yeah. that pronunciation  and so y- Actually maybe they're using phone recognizers. Oh. they - they - I thought that they were - we should catch up on what Steve is  uh - I think that would be a good i- good idea. They are. Is that what they're doing? Oh  O_K. Yeah. Yeah  so I think that i- i- we also don't have  I mean  we've got a good start on it  but we don't have a really good  meeting  recorder or recognizer or transcriber or anything yet  so. Yeah. So  I mean another way to look at this is to  is to  uh  do some stuff on Switchboard which has all this other  stuff to it. And then  um  As we get  further down the road and we can do more things ahead of time  we can  Mm-hmm. O_K. do some of the same things to the meeting data. Yeah. And I'm - and these people might - they - they are  s- Yeah most of them are trained with I_P_A. They'd be able to do phonetic-level coding  or articulatory. Are they busy for the next couple years  or - ? Well  you know  I mean they  they - they're interested in continuing working with us  so - I mean - I  and this would be up their alley  so  we could - when the - when you d- meet with  with John Ohala and find  you know what taxonomy you want to apply  then  they'd be  Yeah. good to train onto it. Yeah. Anyway  this is  Yeah. not an urgent thing at all  just it came up. It'd be very interesting though  to have that data. Yeah. I wonder  how would you do a forced alignment? I think so  too. Might - Interesting idea. To - to - I mean  you'd wanna iterate  somehow. Yeah. It's interesting thing to think about. It might be - Hmm. I was thinking it might be n- I mean you'd - you'd want models for spreading. Of the f- acoustic features? Yeah. Mm-hmm. Mm-hmm. Yeah. Well it might be neat to do some  phonetic  features on these  nonword words. Are - are these kinds of words that people never - the ""huh""s and the ""hmm""s and the ""huh-"" and the uh - These k- No  I'm serious. There are all these kinds of functional  uh  elements. I don't know what you call them. But not just fill pauses but all kinds of ways of interrupting and so forth. Uh-huh. And some of them are  yeah  ""uh-huh""s  and ""hmm""s  and  ""hmm!"" ""hmm"" ""O_K""  ""uh"" Grunts  uh  that might be interesting. He's got lip - lipsmacks. In the meetings. We should move on. Yeah. Uh  new version of  uh  presegmentation? Uh  oh yeah  um  I worked a little bit on the - on the presegmentation to - to get another version which does channel-specific  uh  speech-nonspeech detection. And  what I did is I used some normalized features which  uh  look in- into the - which is normalized energy  uh  energy normalized by the mean over the channels and by the  minimum over the  other . within each channel. And to - to  mm  to  yeah  to normalize also loudness and - and modified loudness and things and that those special features actually are in my feature vector. Oh. And  and  therefore to be able to  uh  somewhat distinguish between foreground and background speech in - in the different - in - each channel. And  eh  I tested it on - on three or four meetings and it seems to work  well yeah  fairly well  I - I would say. There are some problems with the lapel mike. Of course. Yeah. Uh  yeah. Wow that's great. And. So I - I understand that's what you were saying about your problem with  Yeah. minimum. And. Yeah  and - and I had - I had  uh  specific problems with. I get it. So new- use ninetieth quartile  rather than  Yeah. Yeah. minimum. Wow. Yeah - yeah  then - I - I did some - some - some things like that  Interesting. as there - there are some - some problems in  when  in the channel  there - they - the the speaker doesn't - doesn't talk much or doesn't talk at all. Then  the  yeah  there are - there are some problems with - with - with n- with normalization  and  then  uh  there the system doesn't work at all. So  I'm - I'm glad that there is the - the digit part  where everybody is forced to say something  Right. so  that's - that's great for - for my purpose. And  the thing is I - I  then the evaluation of - of the system is a little bit hard  as I don't have any references. Well we did the hand - the one by hand. Yeah  that's the one - one wh- where I do the training on so I can't do the evaluation on So the thing is  can the transcribers perhaps do some  Uh. some - some meetings in - in terms of speech-nonspeech in - in the specific channels? Well won't you have that from their transcriptions? Well  I have - Well  O_K  so  now we need - so  um  No  cuz we need is really tight. Yeah. I think I might have done what you're requesting  though I did it in the service of a different thing. Oh  great. I have thirty minutes that I've more tightly transcribed with reference to individual channels. O_K. O_K  that's great. That's great for me. Yeah  so. And I could - And - Hopefully that's not the same meeting that we did. And - No  actually it's a different meeting. Good. So  um  e- so the  you know  we have the  O_K. th- they transcribe as if it's one channel with these - with the slashes to separate the overlapping parts. And then we run it through - then it - then I'm gonna edit it and I'm gonna run it through channelize which takes it into Dave Gelbart's form- format. And then you have  Yeah. Yeah. all these things split across according to channel  and then that means that  if a person contributed more than once in a given  overlap during that time bend that - that two parts of the utterance end up together  it's the same channel  and then I took his tool  and last night for the first thirty minutes of O_K. one of these transcripts  I  tightened up the  um  boundaries on individual speakers' channels  cuz his - his interface allows me to have total flexibility in the time tags across the channels. O_K. Yeah. Yeah. And um  So  current - so  yeah - yeah  that - that - that's great  but what would be nice to have some more meetings  not just one meeting to - to be sure that - that  so. Yes. Might not be what you need. there is a system  Yeah  so if we could get a couple meetings done with that level of precision I think that would be a good idea. O_K. Oh  O_K. Yeah. Uh  how - how m- much time - so the meetings vary in length  what are we talking about in terms of the number of minutes you'd like to have as your - as your training set? It seems to me that it would be good to have  a few minutes from - from different meetings  so. But I'm not sure about how much. O_K  now you're saying different meetings because of different speakers or because of different audio quality or both or - ? Both - both. Different - different number of speakers  different speakers  different conditions. O_K. Yeah  we don't have that much variety in meetings yet  uh  I mean we have this meeting and the feature meeting and we have a couple others that we have uh  Yeah  m- couple examples of. Yeah. Mm-hmm. But - but  uh  Even probably with the gains differently will affect it  you mean - Uh  not really as - uh  because of the normalization  yeah. Yeah. Poten- potentially. Oh  cuz you use the normalization? O_K. Oh  O_K. We can try running - we haven't done this yet because  um  uh  Andreas an- is - is gonna move over the S_R_I recognizer. i- basically I ran out of machines at S_R_I  cuz we're running the evals and I just don't have machine time there. O_K. But  once that's moved over  uh  hopefully in a - a couple days  then  we can take  um  what Jane just told us about as  Oh  shoot! the presegmented  the - the segmentations that you did  Yeah. Yeah. at level eight or som- at some  The pre- presegment- yeah. threshold that Jane  Yeah. tha- right  and try doing  forced alignment. um  With the recognizer? Yeah. on the word strings. And if it's good  then that will - that may give you a good boundary. Of course if it's good  we don't - then we're - we're fine  but  Yeah. M- I don't know yet whether these  segments that contain a lot of pauses around the words  I - will work or not. I would quite like to have some manually transcribed references for - for the system  as I'm not sure if - if it's really good to compare with - with some other automatic  Yeah. Right. found boundaries. Well  no  if we were to start with this and then tweak it h- manually  would that - that would be O_K? Yeah. Right. Yeah sure. They might be O_K. It - you know it really depends on a lot of things  but  O_K. Yeah. I would have maybe a transcriber  uh  look at the result of a forced alignment and then adjust those. That might save some time. Yeah. To a- adjust them  or  yeah. Yeah  yeah. If they're horrible it won't help at all  but they might not be horrible. Yeah  great. Yeah. So - but I'll let you know when we  O_K  great. uh  have that. How many minutes would you want from - I mean  we could easily  get a section  you know  like say a minute or so  from every meeting that we have so f- from the newer ones that we're working on  everyone that we have. And then  should provide this. If it's not the first minute of - of the meeting  that - that's O_K with me  but  in - in the first minute  uh  Often there are some - some strange things going on which - which aren't really  well  for  which - which aren't re- re- really good. So. What - what I'd quite like  perhaps  is  to have  some five minutes of - of - of different meetings  so. Somewhere not in the very beginning  five minutes  O_K. Yeah. And  then I wanted to ask you just for my inter- information  then  would you  be trai- cuz I don't quite unders- so  would you be training then  um  the segmenter so that  it could  on the basis of that  segment the rest of the meeting? So  if I give you like five minutes is the idea that this would then be applied to  uh  to  I - I could do a - a retraining with that  yeah. providing tighter time bands? Wow  interesting. That's - but - but I hope that I - I don't need to do it. O_K. So  uh it c- can be do in an unsupervised way. Uh-huh. Excellent. Excellent  O_K. So. I'm - I'm not sure  but  for - for - for those three meetings whi- which I - which I did  it seems to be  quite well  but  there are some - some - as I said some problems with the lapel mike  but  perhaps we can do something with - with cross-correlations to  to get rid of the - of those. And. Yeah. That's - that's what I - that's my future work. Well - well what I want to do is to - to look into cross-correlations for - for removing those  false overlaps. Wonderful. Are the  um  wireless  different than the wired  mikes  at all? I mean  have you noticed any difference? I'm - I'm not sure  um  if - if there are any wired mikes in those meetings  or  uh  I have - have to loo- have a look at them but  I'm - I'm - I think there's no difference between  So it's just the lapel versus everything else? Yeah. Yeah. O_K  so then  if that's five minutes per meeting we've got like twelve minutes  twelve meetings  roughly  that I'm - that I've been working with  then - Of - of - of the meetings that you're working with  how many of them are different  No. tha- are there any of them that are different than  these two meetings? Well - oh wa- in terms of the speakers or the conditions or the? Yeah  speakers. Sorry. So. Um  Yeah  that - we have different combinations of speakers. I mean  just from what I've seen  uh  there are some where  um  you're present or not present  and  then - then you have the difference between the networks group and this group Yeah  I know  some of the N_S_A meetings  yeah. Yeah. So I didn't know in the group you had if you had - so you have the networks meeting? Yeah. Yep  we do. Yeah. Do you have any of Jerry's meetings in your  pack  er  Um  no. No? We could  I mean you - you recorded one last week or so. I could get that new one in this week - I get that new one in. Yep. This week. We're gonna be recording them every Monday  u- Yeah. Cuz I think he really needs variety  and - and having as much variety for speaker certainly would be a big part of that I think. so - Great. Yeah. Yeah. O_K  so if I  O_K  included - include  O_K  then  uh  if I were to include all together samples from twelve meetings that would only take an hour and I could get the transcribers to do that right - I mean  what I mean is  that would be an hour sampled  and then they'd transcribe those - that hour  right? That's what I should do? Yeah. And. Right. Ye- But you're - y- That's - that's. I don't mean transcribe I mean - I mean adjust. So they get it into the multi-channel format and then adjust the timebands so it's precise. So that should be faster than the ten times kind of thing  yeah. Absolutely. I did - I did  um  uh  so  last night I did  uh  Oh gosh  well  last night  I did about half an hour in  three hours  Yeah. which is not  terrific  but  um  Yeah. Well  that's probably. anyway  it's an hour and a half per - So. Well  I can't calculate on my  Do the transcribers actually start wi- with  uh  transcribing new meetings  or are they? on my feet. Well  um they're still working - they still have enough to finish that I haven't assigned a new meeting  but the next  O_K. m- m- I was about to need to assign a new meeting and I was going to take it from one of the new ones  and I could easily give them Jerry Feldman's meeting  no problem. O_K. And  O_K. then - So they're really running out of  data  prett- I mean that's good. Mm-hmm. Uh  that first set. Um  O_K. They're running out of data unless we s- make the decision that we should go over and start  So - uh  transcribing the other set. There - the first - the first half. And so I was in the process of like editing them but this is wonderful news. Yeah. O_K. Alright. We funded the experiment with  uh - also we were thinking maybe applying that that to getting the  Yeah  that'll be  very useful to getting the overlaps to be more precise all the way through. So this  blends nicely into the update on transcripts. Yes  it does. O_K. So  um  um  Yeah  please. Go ahead. Liz  and - and Don  and I met this morning  in the BARCO room  with the lecture hall  And this afternoon. and this afternoon  it drifted into the afternoon  uh  concerning this issue of  um  the  well there's basically the issue of the interplay between the transcript format and the processing that  they need to do for  the S_R_I recognizer. And  um  well  so  I mentioned the process that I'm going through with the data  so  you know  I get the data back from the transcri- Well  s- uh  metaphorically  get the data back from the transcriber  and then I  check for simple things like spelling errors and things like that. And  um  I'm going to be doing a more thorough editing  with respect to consistency of the conventions. But they're - they're generally very good. And  then  I run it through  uh  the channelize program to get it into the multi-channel format  O_K. And the  what we discussed this morning  I would summarize as saying that  um  these units that result  in a - a particular channel and a particular timeband  at - at that level  um  vary in length. And  um  their recognizer would prefer that the units not be overly long. But it's really an empirical question  whether the units we get at this point through  just that process I described might be sufficient for them. So  as a first pass through  a first chance without having to do a lot of hand-editing  what we're gonna do  is  I'll run it through channelize  give them those data after I've done the editing process and be sure it's clean. And I can do that  pretty quickly  Mm-hmm. with just  that minimal editing  without having to hand-break things. And then we'll see if the units that we're getting  uh  with the - at that level  are sufficient. And maybe they don't need to be further broken down. And if they do need to be further broken down then maybe it just be piece-wise  maybe it won't be the whole thing. So  that's - that's what we were discussing  this morning as far as I - Right. Among - also we discussed some adaptational things  so it's like  Then lots of - Right. uh - You know I hadn't  uh  incorporated  a convention explicitly to handle acronyms  for example  but if someone says  P_Z_M it would be nice to have that be directly interpretable from  the transcript what they said  or Pi- uh Tcl - T_C_L I mean. It's like Mm-hmm. y- it's - and so  um  I've - I've incorporated also convention  with that but that's easy to handle at the post editing phase  and I'll mention it to  transcribers for the next phase but that's O_K. And then  a similar conv- uh  convention for numbers. So if they say one-eighty-three versus one eight three. Um  and also I'll be  um  encoding  as I do my post-editing  the  things that are in curly brackets  which are clarificational material. And eh to incorporate  uh  keyword  at the beginning. So  it's gonna be either a gloss or it's gonna be a vocal sound like a  laugh or a cough  or  so forth. Or a non-vocal sound like a doors- door-slam  and that can be easily done with a  you know  just a - one little additional thing in the  in the general format. Yeah we j- we just needed a way to  strip  you know  all the comments  all the things th- the - that linguist wants but the recognizer can't do anything with. Um  but to keep things that we mapped to like reject models  or  you know  uh  mouth noise  or  cough. And then there's this interesting issue Jane brought up which I hadn't thought about before but I was  realizing as I went through the transcripts  that there are some noises like  um  well the - good example was an inbreath  where a transcriber working from  the mixed  signal  doesn't know whose breath it is  and they've been assigning it to someone that may or may not be correct. Right. And what we do is  if it's a breath sound  you know  a sound from the speaker  we map it  to  a noise model  like a mouth-noise model in the recognizer  and  yeah  it probably doesn't hurt that much once in a while to have these  but  if they're in the wrong channel  that's  not a good idea. And then there's also  things like door-slams that's really in no one's channel  they're like - it's in the room. Yeah. Right. And uh  Jane had this nice  uh  idea of having  like an extra  An extra channel. uh couple tiers  yeah. Yeah. I've been - I've been adding that to the ones I've been editing. And we were thinking  that is useful also when there's uncertainties. So if they hear a breath and they don't know who breath it is it's better to put it in that channel than to put it in the speaker's channel because maybe it was someone else's breath  or - Uh  so I think that's a good - you can always clean that up  post-processing. Yeah. So a lot of little details  but I think we're  coming to some kinda closure  on that. So the idea is then  uh  Don can take  uh  Jane's post-processed channelized version  and  with some scripts  you know  convert that to - to a reference for the recognizer and we can  can run these. So when that's  ready - you know  as soon as that's ready  and as soon as the recognizer is here we can get  twelve hours of force-aligned and recognized data. And  you know  start  working on it  so we're  And - I dunno a coup- a week or two away I would say from  uh  if - if that process is automatic once we get your post-process  transcript. Mm-hmm. And that doesn't - the amount of editing that it would require is not very much either. I'm just hoping that the units that are provided in that way  will be sufficient cuz I would save a lot of  uh  time  dividing things. Yeah  some of them are quite long. Just from - I dunno how long were - you did one? I saw a couple  around twenty seconds  and that was just without looking too hard for it  so  Right. Well n- I would imagine that there might be some that are longer. One question  e- w- would that be a single speaker or is that multiple speakers overlapping? No. No  but if we're gonna segment it  like if there's one speaker in there  that says ""O_K"" or something  right in the middle  it's gonna have a lot of dead time around it  so it's not - Right. It's not the - it's not the fact that we can't process a twenty second segment  it's the fact that  there's twenty seconds in which to place one word in the wrong place- Yeah. You know  if - Yeah. if someone has a very short utterance there  and that's where  we  might wanna have this individual  you know  Yep. ha- have your pre- pre-process input. Yeah. Sure. I - I - I thought that perhaps the transcribers could start then from the - those mult- multi-channel  And I just don't know  I have to run it. That's very important. uh  speech-nonspeech detections  if they would like to. Right. In - in doing the hand-marking? Yeah that's what I was thinking  too. Yeah. Right. So that's probably what will happen  but we'll try it this way and see. Yeah. Yeah. I mean it's probably good enough for force-alignment. If it's not then we're really - then we def- definitely Yeah. uh  but for free recognition I'm - it'll probably not be good enough. We'll probably get lots of errors because of the cross-talk  and  Yep. noises and things. Good s- I think that's probably our agenda  or starting up there. Oh I wanted to ask one thing  Yeah? the microphones - the new microphones  when do we get  uh? Uh  they said it would take about a week. Oh  exciting. K_. K_. You ordered them already? Mm-hmm. Great. So what happens to our old microphones? They go where old microphones go. Um - Do we give them to someone  or - ? Well the only thing we're gonna have extra  We don't have more receivers  we just have - for now  Right  we don- so the only thing we'll have extra now is just the lapel. Right. Not - not the  bodypack  just the lapel. Just the lapel itself. Um  and then one of the - one of those. Since  what I decided to do  on Morgan's suggestion  was just get two  new microphones  um  and try them out. Mm-hmm. And then  if we like them we'll get more. O_K. Yeah. Since they're - they're like two hundred bucks a piece  we won't  uh  at least try them out. So it's a replacement for this headset mike? Yep. Yep. Yeah. And they're gonna do the wiring for us. What's the  um  style of the headset? It's  um  it's by Crown  and it's one of these sort of mount around the ear thingies  and  uh  when I s- when I mentioned that we thought it was uncomfortable he said it was a common problem with the Sony. And this is how apparently a lot of people are getting around it. Hmm. And I checked on the web  and every site I went to  raved about this particular mike. It's apparently comfortable and stays on the head well  so we'll see if it's any good. But  uh  I think it's promising. You said it was used by aerobics instructors? Yep. Yep  so it was - Hmm. That says a lot. it was advertised for performers and - For the recor- for the record Adam is not a paid employee or a consultant of Crown. Excuse me? Oh. Excuse me? Right. I said ""For the record Adam is - is not a paid consultant or employee of Crown"". That's right. However  he may be solicited after these meetings are distributed. Well we're using the Crown P_Z_Ms. These are Crown aren't they? Don't worry about finishing your dissertation. Yeah. Right. The P_Z_Ms are Crown  aren't they? Yeah. Yeah  I thought they were. You bet. You bet. And they work very well. Yes. So if we go to a workshop about all this - this it's gonna be a meeting about meetings about meetings. O_K. So. And then it - we have to go to the planning session for that workshop. Oh  yeah  what - Which'll be the meeting about the meeting about the meeting. Yeah? Oh  god. Cuz then it would be a meeting about the meeting about the meeting about meetings. Ooo. Just start saying ""M_ four"". Yeah  O_K. Yeah. M_ to the fourth. Should we do the digits? Yep  go for it. O_K. Transcript two six one one  two six three zero. three two zero five six five three four four five zero five seven five six six six six two seven eight nine zero two one five two six three two five one two three seven four seven zero six eight zero zero five six eight one nine three O_ zero four zero one two. Transcript two five nine one dash two six one zero. two four nine seven nine eight O_ three four O_ six S- s- four six five six seven zero eight  seven six two one nine three O_ two O_ five zero four one three one zero zero one seven five four two eight zero  two eight one four three four zero five six two zero seven three nine one three eight four one nine seven two  zero one zero zero O_ zero one zero six seven nine three one Transcript three seven three one  three seven five zero. seven two O_ O_  four nine nine eight eight eight nine eight O_ zero zero two one one six three three seven four seven five nine three O_ four six nine nine two two seven eight O_ one zero four one nine five five seven two nine nine O_ three four five zero nine seven one four one two zero zero. Transcript three seven one one  three seven three O_. six seven nine O_ three seven eight nine zero four five four one two four three nine two six three four five six O_ nine five eight one nine six six three eight one zero O_ five seven zero one two three zero seven zero eight five three one four eight O_ three six four two. Pause between the lines  remember? Two one seven one dash two one nine O_. six two nine seven seven two seven O_ seven nine seven eight six nine nine O_ O_ eight nine zero one zero seven three two three one four two two four five seven seven eight four six eight zero seven zero seven eight O_ three four seven nine zero two zero one seven seven O_ seven two three O_ eight three nine four nine five. Transcript two one five one dash two one seven O_ five two four eight six three seven six eight seven  five six nine eight nine O_ O_ seven eight one one two three six three eight four eight one three O_ O_ two five six seven nine two eight  three five two eight O_ three O_ zero nine three one eight zero  eight two three two two three O_ four O_  six O_  nine six five Transcript two three five one dash two three seven zero. two three five four zero four two six four one seven seven four eight seven two one eight nine O_ zero zero two three three three O_ four eight two five eight nine six O_ nine on four O_ two seven eight zero nine eight zero four zero O_ one seven zero four three zero two eight one one seven eight five seven two nine six two O_ - Excuse me. two nine six two one one O_ O_K. O_K. Huh. ",The Berkeley Meeting Recorder group talked about the ongoing transcription effort and issues related to the Transcriber tool  which despite its limitations for capturing tight time markings for overlapping speech  will continue to remain in use. Speaker mn014 explained his efforts to pre-segment the signal into speech and non-speech portions for facilitating transcriptions. Recording equipment and procedures were discussed  with a focus on audible breathing and the need for standards in microphone wear and use. And  finally  it was determined that speaker mn005's efforts to detect speaker overlap using energy should instead be focussed on pitch- and harmonicity-related features or be guided by a non-featural  statistical approach  i.e. via the use of Markov models. In the interest of time  it was decided that the group should continue using the existing Transcriber tool and perform a forced alignment on the close-talking microphones that will  it is hoped  help to recover some of the time information indicating where different speaker overlaps occurred in the signal. A meeting will be arranged with NIST to decide on a common standard and format for doing transcriptions. One or two meetings will be assigned to multiple transcribers to check for inter-annotator agreement. To cut down on audible breaths during recordings  the group will institute some level of standards for microphone wear and use. Speaker mn005 will feed his hand-segmented data into the speech segmenter developed by Javier to train it to identify different types of speech (i.e. that of single versus multiple speakers)  as well as focussing on pitch- and harmonicity-related features for identifying overlapping speech. There is no channel identifier to help in encoding speaker overlaps. Speech uttered while laughing is problematic for ASR. So far  speaker mn005's attempts to detect speaker overlap have been unsuccessful  as it has not been possible to normalize energy as a reliable indicator of overlap. Speaker mn014's efforts to detect speech/non-speech portions in the mixed signal (using an HMM-based detector with Gaussian mixtures) have produced pre-segmentations that facilitate the transcription effort. Speaker mn014 also trained the system to identify speech from loud versus quiet speakers. Such pre-segmentation modifications allow the experimenter to specify the minimum length of speech and silence portions desired  and also facilitate the identification of pauses and utterance boundaries. The transcriber pool is making quick progress  and may be used in the future to perform other types of coding  e.g. a more detailed analysis of speaker overlap. Transcribers are coding non-speech gestures  such as audible breaths and laughter  both of which are useful for improving recognition results. Recent modifications to the Transcriber tool allow transcribers to listen to speech from different channels  as well as helping to preserve portions of overlapping speech  and enabling the creation of different output files for each channel for a cleaner and more segmentable transcript. The Praat software package was discussed as an alternative transcription tool capable of representing multiple channels of speech. Cross-correlation was discussed as a means of enabling speaker identification  and may be integrated into future work. 
"@@ How many batteries do you go through? Thank you. Alright. Good. Sure. Yeah. O_K so  let's get started. Nancy said she's coming and that means she will be. Um. My suggestion is that Robert and Johno sort of give us a report on last week's adventures uh to start. So everybody knows there were these guys f- uh from Heidelber- uh  uh  actually from uh D_F_K_I uh  part of the German SmartKom project  who were here for the week and  I think got a lot done. Yeah  I think so too. Um. The - we got to the point where we can now speak into the SmartKom system  and it'll go all the way through and then say something like ""Roman numeral one  am Smarticus."" It actually says  ""Roemisch einz  am Smarticus "" which means it's just using a German synthesis module for O_K. English sentences. So uh  O_K. It doesn't know ""I"". O_K. Um  the uh Oh  ""Am Spartacus."" Verstehe. O_K. ""I am Sm- I am Smarticus"" is what it's saying. I gue- Right. The uh synthesis is just a question of um  hopefully it's just a question of exchanging a couple of files  once we have them. And  um  it's not going to be a problem because we decided to stick to the so-called concept to speech approach. So I'm - I'm - I'm going backwards now  so ""synthesis"" is where you sort of make this - uh  make these sounds  and ""concept to speech"" is feeding into this synthesis module giving it what needs to be said  and the whole syntactic structure so it can pronounce things better  presumably. Then  just with text to speech. Mm-hmm. And  uh  Johno learned how to write X_M_L tags. Uh  and did write the tree adjoining grammar for some - some sentences. No  right? Yeah  for a couple - Yeah. So. Bu- Uh  i- The way the uh  the dialogue manager works is it dumps out what it wants to know  or what it wants to tell the person  to a - er in X_M_L and there's a conversion system for different uh  to go from X_M_L to something else. And th- so  the knowledge base for the system  that generates the syntasti- syntactic structures for the ge- generation is uh  in a LISP-like - the knowledge base is in a LISP-like form. And then the thing that actually builds these syntactic structures is something based on Prolog. So  you have a - basically  a goal and it  you know  says ""O_K  well I'm gonna try to do the Greet-the-person goal  so it just starts - uh  it binds some variables and it just decides to  you know  do some subscold . Basically  it just means ""build the tree."" Mm-hmm. O_K. And then it passes the tree onto  uh  the ge- the generation module. But I think that the point is that out of the twelve possible utterances that the German system can do  we've already written the - the syntax trees for We- yeah. So  the syntax trees are very simple. It's like most of the sentences in one tree  three or four. Mm-hmm. and instead of  you know  breaking down to  like  small units and building back up  they basically took the sentences  and basically cut them in half  or you know  into thirds or something like that  and made trees out of those. And so uh  uh Tilman wrote a little tool that you could take LISP notation and generate an X_M_L  uh  tree. Uh  S- what do ca- structure from the - from the LISP. And so basically you just say  you know  ""noun goes to""  you know  Er  nah  I don't re- I've never been good at those. So there's like the V_P goes to N_ and those things O_K. N_  V_ yeah  O_K. in LISP  and it will generate for you. Alright. And because we're sticking to that structure  the synthesis module doesn't need to be changed. So all that f- fancy stuff  and the Texas speech version of it  which is actually the simpler version  is gonna be done in October which is much too late for us. So. This way we - we worked around that. The  uh - the system  um - I can show you the system. I actually want  at least  maybe  you should be able to start it on your own. If you wanna play around with it  in th- in the future. Right now it's brittle and you need to ch- start it up and then make ts- twenty changes on - on - on - on seventeen modules before they actually can stomach it  anything. And send in a - a - a couple of side queries on some dummy center set-up program so that it actually works because it's designed for this seevit thing  where you have the gestural recognition Mm-hmm. running with this s- Siemens virtual touch screen  which we don't have here. And so we're doing it via mouse  but the whole system was designed to work with this thing and it was - It was a lot of engineering stuff. No science in there whatsoever  but it's working now  and um  that's the good news. So everything else actually did prove to be language independent except for the parsing and the generation. Why - I had - I did need to chan- generate different trees than the German ones  mainly because you know like uh  the gerund in - in German is automatically taken care of with just a regular verb  You have to switch it on. Mm-hmm. so I'd uh have to add ""am walking "" or I'd have to add a little stem for the ""am""  O_K. O_K. when I build the - built the tree. Yeah  I noticed that um  that some of the examples they had  had you know  non-English word orders and so on  you know. And then all that good stuff. So. Like. Alright. Yeah. So it might be worth  Keith  you looking at this  Yeah. um Well Tilman s- I - I still don't - I still don't really understand e- like - I mean we sort of say  um - You know  I - I still don't exactly understand sort of the information flow uh in - in this thing  or what the modules are and so on. So  you know  like just that such-and-such module uh um decides that it wants to achieve the goal of greeting the user  and then magically it sort of s- Yeah - I mean  how does it know which syntactic structure to pull out  and all that? I thi- Yeah. So. I think it's not worth going over in the group  but sort of when you get R- uh Sure. free and you have the time Yeah  soon. uh either Robert or Johno or I can walk you through it. O_K. And you can ask all the questions about how this all fits together. That's fine. It's eee messy but once you understand it you understand it. It's - it's - There's nothing really complicated about it. O_K. No. And I remember one thing that - that came up in the talk last Wednesday. Um  was this  I - I think he talked about the idea of like  um - He was talking about these lexicalized O_K  you know how to do it? uh  uh  tree adjoining grammars where you sort of - for each word you  um - For each lexical item  the lexical entry says what all the uh trees are that it can appear in. And of course  that's not v- That's the opposite of constructional. That's  you know  that's - that's H_P_S_G or whatever. You know? Right. Right. Now  we're - we're not committed for our research Yeah. to do any of those things. So uh we are committed for our funding. Mm-hmm. Right. O_K? to uh - Make our stuff fit to that. Yeah  to - n- no  to just get the dem- get the demos they need. Uh-huh. O_K? So between us all we have t- to get th- the demos they need. If it turns out we can also give them lots more than that by  you know  tapping into other things we do  that's great. You should probably move the microphone closer to your face. Mm-hmm. But i- it turns out not to be in an- any of the contracts There's like a little - The twisty thing  you can move it with. O_K. and  s- deliberately. So  the reason I'd like you to understand uh what's going on in this demo system is not because it's important to the research. It's just for closure. So that if we come up with a question of ""could we fit this deeper stuff in there?"" or something. You know what the hell Right. we- we're talking about fitting in. So it's just  uh in the sam- same actually with the rest of us O_K. we just need to really understand what's there. Is there anything we can make use of? Uh  is there anything we can give back  beyond th- the sort of minimum requirements? But none of that has a short time fuse. O_K. So th- the demo- the demo requirements for this Fall are sort of taken care of as of later this week or something. And then - So  it's probably fifteen months or something until there's another serious demo requirement. That doesn't mean we don't think about it for fifteen months  but it means we can not think about it for six months. Oh O_K. Right. Right  yeah. Right. So. The plan for this summer uh  really is to step back from the applied project  keep the d- keep the context open  but actually go after the basic issues. Hmm. Oh O_K. And  so The idea is there's this uh  other subgroup that's worrying about formalizing the nota- getting a notation. But sort of in parallel with that  uh  the hope is tha- in particularly you will work on constructions in English Ge- and German for this domain  Mm-hmm. but y- not worry about parsing them or fitting them into SmartKom or Yeah. any of the other - anything lik- any other constraints O_K. for the time being. Got it. It's hard enough to get it semantically and syntactically right and then - and get the constructions in their form and stuff. Yeah. And  I don- I don't want you f- feeling that you have to somehow meet all these other constraints. Right  O_K. Um. And similarly with the parsing  uh we're gonna worry about parsing uh  the general case you know  construction parser for general constructions. And  if we need a cut-down version for something  or whatever  we'll worry about that later. O_K. So I'd like to  for the summer turn into science mode. O_K. And I assume that's also  uh  your plan as well. Right. So I mean  the - the point is that like the meetings um so far that I've been at have been - sort of been geared towards this demo  Yeah. Yeah. But - but we- we're swit- and then that's going to go away pretty soon. Right. O_K. And then we'll sort of shift gears a- Yeah. It's - Yeah. Fairly substantially  huh? It's got . What I - what I think is - is a good idea that I can - can show to anyone who's interested  we can even make a - sort of an internal demo  and I - I show you Yeah. what I do  Mm-hmm. I speak into it and you hear it talk  O_K. and I can sort of walk f- through the information. So  this is like in half hour or forty-five minutes. Just fun. O_K. And so you - when somebody on the streets com- comes up to you and asks you what is SmartKom so you can  sort of  give a sensible answer. Right. O_K. So  c- sh- we could set that up as actually an institute wide thing? Just give a talk in the big room  and - and so peo- people know what's going on? when you're ready? Absolutely. Yeah I mean  that's the kind of thing - That's the level at which you know we can just li- invite everybody and say ""this is a project that we've been working on and here's a demo version of it"" and stuff like that. Yeah. O_K. Well d- we - we do wanna have all the bugs out b- where you have to sort of pipe in extra X_M_L messages from left and right before you're - Uh-huh. Indeed. Yeah. O_K. Makes sense. But any- so that - e- e- It's clear  then  I think. Actually  roughly starting uh let's say  nex- next meeting  cuz this meeting we have one other thing to tie up besides the trip report. Yeah. O_K. But uh starting next meeting I think we want to flip into this mode where - Uh. I mean there are a lot of issues  what's the ontology look like  you know what do the constructions look like  what's the execution engine look like  mmm lots of things. Mm-hmm. Mm-hmm. But  more focused on uh an idealized version than just getting the demo out. Now before we do that  let's get back in - Oh! But  it's still  I think  useful for you to understand the demo version Yeah. enough  so that you can - can see what - what it is that - that uh it might eventually get retro-fitted into or something. O_K  right. And Johno's already done that  Wa- uh - uh  looked at the dem- uh the - looked at the SmartKom stuff. To some de- uh what - what part of th- the SmartKom stuff? Well  the parser  and that stuff. Oh yeah - yeah. O_K. Anyway. So  the trip - the report on these - the last we- we sort of interrupted you guys telling us about what happened last week. Yeah. It's alright. Um. Maybe you're done  then. Well it was just amazing to - to see uh how - how instable the whole thing is  and if you just take the - And I g- I got the feeling that we are the only ones right now who have a running system. I don't know what the guys in Kaiserslautern have running because e- the version - that is  the full version that's on the server d- does not work. And you need to do a lot of stuff to make it work. And so it's - And even Tilman and Ralf sort of said ""yeah there never was a really working version that uh did it without th- all the shortcuts that they built in for the uh October @@ version"". So we're actually maybe ahead of the System Gruppe by now  the system - the integration group. And it was  uh - It was fun to some extent  but the uh the outcome that is sort of of scientific interest is that I think both Ralf and Tilman - um  I know that they enjoyed it here  and they r- they - they liked  uh  a lot of the stuff they saw here  what - what we have been thinking about  and they're more than willing to - to um  cooperate  by all means. And um  part of my responsibility is uh to use our internal ""group-ware"" server at E_M_L  make that open to all of us and them  so that whatever we discuss in terms of parsing and - and generating and constructions w- we - we sort of uh put it in there and they put what they do in there and maybe we can even um  get some overlap  get some synergy out of that. And um  the  uh - If I find someone at - in E_M_L that is interested in that  um I - I may even think that we could look - take constructions and - and generate from them because the tree adjoining grammars that - that Tilman is using is as you said nothing but a mathematical formalism. And you can just do anything with it  whether it's syntactic trees  H_P_S_ G-like stuff  or whether it's construction. So if you ever get to the generation side of constructing things and there might be something of interest there  but in the moment we're of course definitely focused on the understanding  um  pipeline. Anyth- any other uh repo- visit reports sort of stories? uh we - so we now know I think  what the landscape is like. Mm-hmm. And so we just push on and - and uh  do what we need to do. And one of the things we need to do is the um  and this I think is relatively tight - tightly constrained  is to finish up this belief-net stuff. So. Uh. And I was going to switch to start talking about that unless there're m- other more general questions. O_K so here's where we are on the belief-net stuff as far as I understand it. Um. Going back I guess two weeks ago uh Robert had laid out this belief-net  missing only the connections. Right? That is - So  he'd put all th- all the dots down  and we went through this  and  I think  more or less convinced ourselves that at least the vast majority of the nodes that we needed for the demo level we were thinking of  were in there. Yeah we may run across one or two more. But of course the connections weren't. So  uh Bhaskara and I went off and looked at some technical questions about were certain operations sort of legitimate belief-net computations and was there some known problem with them or had someone already uh  solved you know how to do this and stuff. And so Bhaskara tracked that down. The answer seems to be uh  ""no  no one has done it  but yes it's a perfectly reasonable thing to do if that's what you set out to do"". And  so the current state of things is that  again  starting now  um we'd like to actually get a running belief-net for this particular subdomain done in the next few weeks. So Bhaskara is switching projects as of the first of June  and uh  he's gonna leave us an inheritance  which is a uh - hopefully a belief-net that does these things. And there're two aspects to it  one of which is  you know  technical  getting the coding right  and making it run  and uh stuff like that. And the other is the actual semantics. O_K? What all - you know  what are the considerations and how- and what are the ways in which they relate. So he doe- h- he doesn't need help from this group on the technical aspects or if he does uh we'll do that separately. But in terms of Mm-hmm. what are the decisions and stuff like that  that's something that we all have to work out. Is - is that right? I mean that's - that's both you guys' understanding of where we are? O_K. Absolutely. So  I guess  um - Is there like a latest version of the belief-net - of the proposed belief-net? Like - We had um decided - like - Um. Well  no  we didn't decide. We wanted to look into maybe getting it  the visualization  a bit clearer  but I think if we do it  um  sort of a paper version of all the nodes and then Mm-hmm. the connections between them  Yeah  that should be fine. that should suffice. Yeah I mean  that's a separate problem. We do in the long run wanna do better visualization and all that stuff. Yeah  I - Yeah. That's separable  yeah. I did look into that  uh in terms of  you know  exploding the nodes out and down ag- Yep. Right. JavaBayes does not support that. I can imagine a way of hacking at the code to do that. It'd probably take two weeks or so to actually go through and do it  Not - not at this point. and I went through all the other packages on Murph- Kevin Murphy's page  and I couldn't Right. find the necessary mix of free and uh with the GUI and  with this thing that we want. Well  we can p- If it's - If we can pay - Yeah. If you know it's paying a thousand dollars or something we can do that. O_K? So - so don't view free as - as a absolute constraint. O_K. O_K  so then I'll go back and look at the ones on the list that - O_K. And you can ask Kevin. Yeah. But - Yeah  the one that uh people seem to use is uh Hugin or whatever? How exp- I don't think it's - Is it free? Because I've seen it advertised in places so I - it seems to - Mmm. But - Hugin  yeah that's free. Uh  it may be free to academics. Like I - I don't know. I have a co- I have a copy that I l- I downloaded. So  at one point it was free. O_K. O_K. Uh but yo- I noticed people do use Hugin so um  How do you spell that? Why H_U_G_I_N. And Bhaskara can give you a pointer. So then  in any case  um - But paying a lit- You know  if i- if it's uh - Probably for university  it's - it's gonna be real cheap anyway. But um  you know  if it's fifty thousand dollars we aren't gonna do it. I'm mean  we have no need for that. I - I also s- would suggest not to d- spend two weeks in - in - in changing the - the JavaBayes code. I - I will send you a pointer to a Java applet No  he's not gonna do that. Yeah. O_K. that does that  it's sort of a fish-eye. You - you have a node  and you click on it  and it shows you all the connections  and then if you click on something else that moves away  that goes into the middle. Mmm. And maybe there is an easy way of interfacing those two. If that doesn't work  it's not a problem we - we need to solve right now. What I'm - what my job is  I will  um  give you the input in terms of - of the internal structure. Maybe node by node  or something like this? Or should I collect it all and - Mm-hmm. Doesn't matter. Um  just any like - like sort of rough representation of the entire belief-net is probably best. O_K. And um you're gonna be around? t- again  always Tuesdays and Thursdays afternoon-ish? As usual? Yeah - Or will that change? I mean  yeah  I can - like I c- Um. This week I guess um  kind of - I have a lot of projects and stuff but after that I will generally be more free. So yes  I might - I can be around. And g- I mean  generally if you email me also I can be around on other days. Yeah. O_K. Yeah and this is not a crisis that - I mean  you do  e- everybody who's a student should  you know do their work  get their c- courses all in good shape and - and - and - and then we'll dig - d- dig down on this. Yeah  that's - Yeah. O_K. No  that's good. That means I have I h- I can spend this week doing it. So. O_K. How do you go about this process of deciding what these connections are? I know that there's an issue of how to weight the different things too  and stuff. Right? I mean do you just sort of guess and see if it sort of - Right. Well there - there - there It's - There're two different things you do. One is you design and the other is you learn. O_K? So uh what we're gonna do initially is - is do design  and  i- if you will  guess. O_K. O_K. Uh that is you know use your best knowledge of - of the domain to Right. uh  hypothesize what the dependencies are and stuff. O_K. If it's done right  and if you have data Yeah. then  there are techniques for learning the numbers given the structure and there are even techniques for learning the structure  although that takes a lot more data  and it's not as @@ and so forth and so on. So uh but for the limited amount of stuff we have for this particular exercise I think we'll just design it. Alright. Yeah. Fo- Hopefully as time passes we'll get more and more data from Heidelberg and O_K. from people actually using it and stuff. So but this is the long run. Yeah. But to solve our problems ag- uh a mediocre design will do I think in the beginning. Yeah  that's right. Yeah  oh  and by the way  speaking of data  um  are there I could swore - uh  I could swear I saw it sitting on someone's desk at some point  but is there a - um a transcript of any of the  sort of  initial interactions of people with the - with the system? Mm-hmm. Cuz you know  I'm still sort of itching to - to look at what - look at the stuff  and see what people are saying. Yeah. Yeah make yourself a note. So and - and  of course Keith would like the German as well as the English  so whatever you guys can get. The German. Oh yeah  of course  German. Yeah. Yeah  the y- your native language  right? You remember that one. O_K. That's important  yeah. Yeah  u- So he'll get you some data. O_K. Yeah  I mean I - I sort of um found the uh  uh the audio of some of those  and um  Hmm. it kind of sounded like I didn't want to trudge through that  you know. It was just - Strange  but. Yep. We probably will not get those to describe because they were trial runs. Oh yeah  O_K. Um  but uh that's th- but we have data in English and German already. So. - Transcribed. I will send you that. O_K  yeah  I mean. O_K. O_K  so while we're still at this sort of top level  anything else that we oughta talk about today? Ho- how was your thingy. Oh  um  I just wanted to  uh  s- like mention as an issue  um  you know last meeting I wasn't here because I went to a linguistics colloquium on the fictive motion stuff  and that was pretty interesting and you know  I mean  seems to me that that will fairly obviously be of relevance to uh - Oh right. to what we're doing here because you know people are likely to give descriptions like you know  ""What's that thing uh right where you start to go up the hill "" or something like that  you know  meaning a few feet up the hill or whatever from some reference point and all that stuff so I mean  I'm sure in terms of you know  people trying to state locations or  you know  all that kind of stuff  this is gonna be very relevant. So  um  now that was - the talk was about English versus Japanese  um  which obviously the Japanese doesn't affect us directly  except that  um  some of the construction - he'd - what he talked about was that you know in English we say things like th- you know  ""your bike is parked across the street"" and we use these prepositional phrases  you know  ""well  if you were to move across the street you would be at the bike""  but um in - in Japanese the - the more conventionalized tendency is to use a - sort of a description of ""where one has crossed to the river  there is a tree"". Um  and you know  you can actually say things like  um  ""there's a tree where one has crossed the river  but no one has ever crossed the river""  or something like that. So the idea is that this really is you know that's supposed show that's it's really fictive and so on. But um - But the point is that that kind of construction is also used in English  you know  like ""right where you start to go up the hill""  or ""just when you get off the train""  or something like that to - Mmm. uh  to indicate where something is. So we'll have to think about - So - how much is that used in German? Um. The uh - Well - I wa- I was on a uh - on a - on a different sidetrack. I mean  Oh  O_K. the - the Deep Map project which um is undergoing some renovation at - at the moment  but this is a - a three language project: German  English  Japanese. O_K. And um  we have a uh  uh - I have taken care that we have the - the Japanese generation and stuff. And so I looked into uh spatial description. So we can generate spatial descriptions  how to get from A_ to B_. Mm-hmm. And - and information on objects  in German  English  and Japanese. And there is a huge uh project on spatial descriptions uh - differences in spatial descriptions. Well  if yo- if you're interested in that  so how - how  I mean it does sort of go d- all the way down to the conceptual level to some extent. So. O_K. Um. So  where is this huge project? It's KLEIST. It's the uh Bielefeld Mm-hmm. generation of uh spatial descriptions and whatever. @@ Well  that may be another thing that Keith wants to look at. O_K. But um  I - I think we should leave Japanese constructions maybe outside of the scope for - for now  but um Yeah. definitely it's interesting to look at - at Mm-hmm. cross the bordered there. Are - are you going to p- pay any attention to the relative position - of - of the direction relative - relative to the speaker? For example  there are some differences between Hebrew and English. We can say um ""park in front of the car"" as you come beh- you drive behind the car. In Hebrew it means ""park behind the car""  because to follow the car is defined as it faces you. Mm-hmm. Intrinsic  yeah. While in English  front of the car is the absolute front of the car. O_K. Right  so the canonical direction of motion determines where the front is. O_K. So. Right. Right. So  i- i- i- is German Mm-hmm. uh closer to - to E- uh  uh  uh  uh - to E- I mean uh I don't think it - it's related to syntax  though  so it may be entirely different. Um  as a matter of fact - No  it's not. Right. Yeah. Um. I think - Did you ever get to look at the - the rou- paper that I sent you on the - on that problem in English and German? Carroll  ninety-three. Um. I - There is a - a study on the differences between English and German on exactly that problem. Hmm. So it's - they actually say ""the monkey in front of the car  where's the monkey?"" Mm-hmm. And  um  they found statistically very significant differences in English and German  so I - I - I - It might be  since there are only a finite number of ways of doing it  that - that German might be more like Hebrew in that respect. Hmm. The solution they proposed was that it was due to syntactic factors. That - but it wasn't - was - That syntactic facto- factors do - do play a role there  wh- whether you're more likely  Right. Mm-hmm. Right. you know   to develop uh  choices that lead you towards using uh intrinsic versus extrinsic reference frames. Hmm. I mean   it seems to me that you can get both in - in English depending o- You know  like  ""in front of the car"" could you know - Like  here's the car sideways to me in between me and the car or something's in front of the car  or whatever. I could see that  but - Absolutely. But anyway  so you know  I mean  this was - this was a - a very good talk on those kinds of issues and so on. So uh. I can also give you uh  a pointer to a paper of mine which is the - the ultimate taxonomy of reference frames. So. I'm the only person in the world who actually knows how it works. Alright! Cool! Oh. Oh. Not really. Great. No  I've not seen that. It's called a - What do you mean. Um. ""reference frames""? It's - it's spatial reference frames. You actually have only - uh uh Um. If you wanna have a - This is usually um - I should - there should be an ""L_""  though. Well actually you have - only have two choices. You can either do a two-point or a three-point which is you- You're familiar with th- with the ""origo""? where that's the center - ""Origo"" is the center of the f- frame of reference. And then you have the Hmm. reference object and the object to be localized. Hmm. O_K? In some cases the origo Mm-hmm. is the same as the reference object. This was like - So that would be ""origin"" in English  right? The origin. Yeah. Right ""Origo"" is a Terminus technikus. in that sense  that's even used in the English literature. ""Origo."" Oh  O_K. I never heard it. O_K. Alright. O_K. And um  so  this video tape is in front of me. I'm the origo and I'm also the reference object. Those are two-point. Mm-hmm. Mm-hmm. Right. Mm-hmm. And three-point relations is if something has an intrinsic front side like this chair then your f- Yeah. shoe is behind the chair. Mm-hmm. And  reference object and - Um. No  from - from my point of view your shoe is Right. left of the chair. You - you can actually say things like  um  ""it's behind the tree from me"" or something like that  I think  in - in - in certain circumstances in English  right? As sort of ""from where I'm standing it would appear that"" - Yeah. Yeah. So  Looks a little bit like Reichenbach for time. Yeah  it sounds like it  doesn't it  yeah. It's a lot like it. Um. Yeah. And then - and then here you - On this scale  you have it either be ego or allocentric. Mm-hmm. And that's - that's basically it. So. Egocentric two-point  egocentric three-point  or you can have allocentric. So  ""as seen from the church  the town hall is right of that Oh  O_K. um  fire station"". aa-huh It's hardly ever used but it's w- I'd love to see it if you - if you have a copy kind of . Uh. Yeah. I see this is - this is getting into Ami's thing. Yeah. Mm-hmm. He's - he's very interested in that. Here O_K. So. Uh. Yeah. Well  why don't you just put it on the web page? There's this E_D_U - Right? Me too. Yeah it's - or - or just - Yeah. It's also all on my - my home page at E_M_L. It's called ""An Anatomy of a Spatial Description"". But I'll send that link. Or a link to it. Just O_K  great. Maybe just put a link on. Yeah. By the way  there - something that I didn't know until about Yep. Yep. a week ago or so  is apparently  there are separate brain areas for things within reach  and things that are out of reach. Huh. So there's - there's uh all this linguistic stuff about you know  near and far  or yon and - and so forth. So this is all - This is - There's this linguistic facts. But apparently  Mm-hmm. the - Uh. Here's the way the findings go. That  you know they do M_R_I  and - and if you're uh - got something within reach then there's one of your areas lights up  and if something's out of reach uh a different one. But here's the - the amazing result  um  they say. You get someone with a - with a deficit so that they have a perfectly normal ability at distance things. So the s- typical task is subdivision. So there's a - a line on the wall over there  and you give them a laser pointer  and you say  "" Where's the midpoint?"" And they do fine. If you give them the line  and they have to touch it  they can't. There's just that part of the brain isn't functioning  so they can't do that. Here's the real experiment. The same thing on the wall  you give them a laser  ""where is it?""  they do it. Give them a stick  long stick  and say ""do it""  they can't do it. Mm-hmm. So there's a remapping of distant space into nearby space. Right. So they doubled - the - the end - the end of this - Because it's within reach now? It's not within reach and you use the Within-Reach Yeah  yeah. Oh. Wow. uh  mechanism. So I'll d- I'll dig you up this reference. Circuits. Right. That's cool. And so this doe- This is  uh - First of all  it explains something that I've always wondered about and I'll do this - this test on you guys as well. So. Uh. How- I have had an experience  not often  but a certain number of times  when  for example  I'm working with a tool  a screwdriver or something  for a long time  I start feeling the tip directly. Not indirectly  but you actually can feel Yeah yeah. the tip. And people who are uh accomplished violinists and stuff like that  claim they also have this kind of thing where you Yeah. get a direct sensation of  physical sensation  of the end affector. What's going on at the end of the tool  yeah. The ext- the - the - The extension  right. Huh? What's going on at the end of the tool  or whatever. Yeah  within - Huh? The extension of - of your hand  right. Yeah  right. Have you hav- y- h- had this? The - I - I think so. I mean i- i- it's not exactly the th- same thing  but - but s- it - it - it's getting close to that. W- what does it feel like? Yeah. Oh i- it feels like your - as if your uh neurons had extended themselves out to this tool  and you're feeling forces on it and so forth and - and you deal directly with it. I once - I - I was playing you know with those um uh devices that allow you to manipulate objects when it's dangerous to get close? Right  yeah - yeah - yeah. Yeah. Oh  O_K. So you can insert your hand something and there's a correspondence between - Yeah. So I played with it. After a while  you don't feel the difference anymore. Yeah  right. I - I mean it's kind of - Mm-hmm. Very - kind of - you stop back and suddenly it goes away Yeah. and you have to kind of work again Right  right. to recapture it  but yeah. Yeah  so anyway  so - So this was the first actual experimental evidence I'd seen that was consistent with this anecdotal stuff. That's cool. And of course it makes a lovely def- uh story about why languages uh  make this distinction. Of course there are behavioral differences too. Things you can reach are really quite different than things you can't. Yeah. But there seems to be an actu- really deep embodied neural difference. And i- this is  um - So. In addition to the e- This is more proximal-distal. Yeah uh exactly. So in addition to e- ego and allocentric uh which appear all over the place  you also apparently have this proximal-distal thing which is very deeply uh embedded. S- Well  Dan Montello sort of  he - he does the uh uh - th- the cognitive map world   down in Santa Barbara. And he - he always talks about these - He - he already - well - i- probably most likely without knowing this - this evidence uh is talking about these small scale spaces that you can manipulate versus large scale environmental spaces. Yeah. Well there's - there's uh been a lot of behavioral things o- on this  but that was the first neur- neuro-physiological thing I saw. Anyway yeah  so we'll - we'll look at this. And. So  all of these issues now - are now starting to come up. So  now - we're now done with demos. We're starting to do science  right? And so these issues about uh  reference  and - spatial reference  discourse reference  uh-uh-uh-uh all this sort of stuff  Mm-hmm. uh  deixis which is part of what you were talking about  um - So  all of this stuff is Mm-hmm. coming up essentially starting now. So we gotta do all this. So there's that. And then there's also a set of system things that come up. So "" O_K   we're not using their system. That means we need our system."" Right? It - it follows. And so  Mm-hmm. Yeah. uh  in addition to the business about just getting the linguistics right  and the formalism and stuff  we're actually gonna build something and uh  Johno is point person on the parser  analyzer  whatever that is  and we're gonna start on that in parallel with the um  the grammar stuff. But Alright. to do that we're gonna need to make some decisions like ontology  so  um - And so this is another thing where we're gonna  you know  have to get involved and make s- relatively early I think  make some decisions on uh  ""is there an ontology A_P_I that - that"" - There's a sort of standard way of getting things from ontologies and we build the parser and stuff around that  or is there a particular ontology that we're gonna standardize on  and if so - For example  is there something that we can use there. i- Does uh either the uh SmartKom project or one of the projects at E_M_L have something that we can just p- pull out  for that. Uh  so there are gonna be some - some - some things like that  which are not science but system. But we aren't gonna ignore those cuz we're - we're not only going - The plan is not only to lay out this thing  but to actually uh build some of it. And how much we build  and - and so forth. I - Uh. Part of it  if it works right  is wh- It looks like we're now in a position that the construction analyzer that we want for this applied project can be the same as the construction analyzer that Nancy needs for the child language modeling. So. It's always been out of phase but it now seems that um  there's a good shot at that. So we've talked about it  and the hope is that we can make these things the same thing  O_K. and of course it's only w- In both cases it's only one piece of a bigger system. Mm-hmm. But it would be nice if that piece were exactly the same piece. It was just this Right. uh construction analyzer. And so we think - we think we have a shot at - at that. O_K. So. The for- So. To - to come full circle on that  this formalization task  O_K? is trying to get the formalism into - into a shape where it can actually Yeah. uh d- Be of use to someone who's trying to do this  right? Well  yeah  where it actually is - is - covers the whole range of things. And the - the - the - the thing that got Mark into the worst trouble is he had a very ambitious thing he was trying to do  and he insisted on trying to do it with a limited set of mechanisms. It turned out  inherently not to cover the space. O_K. And it just - it was just terribly frustrating for him  and he seemed fully committed to both Yeah. sides of this I see. i- i- irreconcilable thing. Right. And. Uh. Johno is much more pragmatic. O_K. Uh. Good to know. Huh? Is - This is true  is it not? Yes. O_K. So there's you know sort of  yeah  deep  really deep  emotional commitment to a certain theory being Oh  O_K. uh  complete. You don't have a hidden purist streak? Oh no. O_K. Just checking. We- well it hasn't it - it certainly hasn't been observed  in any case. No sir. Alright. Um. Now  you do  but that's O_K. Uh. So. For - for - Cuz I don't have to implement anything. Exactly right. I have a problem  then. Exactly. It's - So. Whether I do depends on whether I'm talking to him or him probably. Which meeting I'm in. Hmm. Yeah  right. Right. Why - a- actually  uh  the thing is  you - you do but  th- the thing you have to im- implement is so small that - Uh. It's O_K to be purist within that context. Yes  good. Within that  yeah  and uh  it's - a- and still  I think  you know  get something done. Cool! But to try to do something upscale and purist Yay. Mm-hmm. Particularly if - if um what you're purist about doesn't actually work  is real hard. Yeah. O_K. And then the other thing is @@ It's possible yeah. while we're doing this uh Robert's gonna pick a piece of this space  O_K. O_K  uh  for his absentee thesis. I think you all know that - that you can just  in Germany - almost just send in your thesis. Just a drive up. Ca-chuk! There you go. Um Yeah right. O_K. The- th- There - there's a drive-in thesis uh sh- joint over in Saarbruecken. Exactly. Drive through  yeah. It costs a lot. The - the amount - You put in your credit card and - as well. But  uh  - But anyway  so  uh  that's um  also gotta be worked out  hopefully over the next few weeks  so that - that it becomes clear uh  what piece uh  Robert wants to jump into. And  while we're at this level  uh  there's at least one new doctoral student in computer science who will be joining the project  either next week or the first of August  depending on the blandishments of Microsoft. So  de- O_K. Uh. And her name is Eva. O_K. It really is. Nobody believed th- th- that - Yeah  I thought it had to be a joke  of your part  you know like - ""Johno made it up  I'm sure. "" Yeah. Is this person someone who's in first-year this year  or No  first year coming. So  she's - she's now out here she's moved  and she'll be a student as of O_K. then. And probably she'll pick up from you on the belief-net stuff  so sh- she'll be chasing you down and stuff like that. O_K. Document. Uh. Right. Uh  against all traditions. And actually I talked today to a uh undergraduate who wants to do an honors thesis on this. Uh - Someone from the class? No  interestingly enough. We always get these people who are not in the class  who - It's interesting. Some of th- some of them  yeah. So anyway  uh  but uh she's another one of these ones with a three point nine average and so forth and so on. Mm-hmm. Uh  so  um  I've give- I've given her some things to read. So we'll see how this goes. Oh there's yet another one of the incoming first - incoming first-year graduate students who's expressed interest  so we'll see how that goes. Um  anyway  so  I think as far as this group goes  um  it's certainly worth continuing for the next few weeks to get closure on the uh belief-net and the ideas that are involved in that  and what are th- what are the concepts. We'll see whether it's gonna make sense to have this be separate from the other bigger effort with the formalization stuff or not  I'm not sure. It partly depends on w- what your thesis turns out to be and how that goes. S- so  we'll see. And then  Ami  you can decide  you know  how much time you wanna put into it and uh  O_K. it- it's beginning to take shap- shape  so uh Right and  I think you will find that if you want to look technically at some of the - your traditional questions in this light  uh Keith  who's buil- building constructions  will be quite happy to uh see what  you know  you envision as the issues and the problems and um  how they might uh get reflected in constructions. Sure. I suspect that's right. Yeah. Yeah. I - I may have to go to Switzerland for - in June or beginning of July for between two weeks and four weeks  but uh  after that or before that. O_K  fine. And  um  if it's useful we can probably arrange for you to drop by and visit either at Heidelberg or at the German A_I center  while you're in - in the neighborhood. Right. Yeah be- uh actu- actually I'm invited to do some consulting with a bank in Geneva which has Yeah. Yep. an affiliation with a research institute in Geneva  which I forgot the name of. E- o- do y- Well  we- we're connected to Yeah. uh - There's a - there's a - a very significant connection between - We'll - we'll go through this  Yeah. I_C_S_I and E_P_F_L  which is the  uh - It's the - Fr- Ge- Germany's got two big technical institutes. There's one in - in Zurich  Mm-hmm. E_T_ H  and then there's one  the French speaking one  in Lausanne  O_K? which is uh E_P_F_ L. Oh  so in Switzerland. So find out who they are associated with in Geneva. Probably we're connected to them. Great. Right. Great. I'll let you know. O_K. S- I'll send you email. Yeah  and so anyway we c- uh - We can m- undoubtedly get Ami uh to give a talk at uh E_M_L or something like that. While he's in - in uh - Hmm. Uh. I - I think the one you - you gave here a couple of weeks ago would be of interest there  too. Sure  yeah. A lot of interest. Actually  either place  D_F_K_I or uh - Yeah  so  and - and if there is a book  that you'll be building up Yeah. Right. some audience for it. And you'll get feedback from these guys. Cuz they've actually - Great  yeah. these D_F_K_I guys have done as much as anyone over the last decade in trying to build them. So we'll set that up. Cool. O_K. So  uh  unless we wanna start digging into the - uh the belief-net and the decisions now  which would be fine  it's probably - I - I tho- It's probably better if I come next week with the um version O_ point nine O_K. So  how about if you two guys between now and next week of the structure. come up with something that is partially proposal  and partially questions  saying ""here's what we think we understand  here are the things we think we don't understand"". And that we as a group will try to - to finish it. What I'd like to do is shoot f- for finishing all this next Monday. Sure. O_K? Uh  ""these are the decisions"" - I don't think we're gonna get lots more information. It's a design problem. Mm-hmm. Yeah. You know. We - Yeah. And let's come up with a first cut at what this should look like. And then finish it up. Does that so- make sense? O_K. O_K. And um  the - the sem- semester will be over next week but then you have projects for one more week to come? No  I - I think I'll be done - everything by this uh - by the end of this week. Same with you? No. Nnn. This - Well  I've - I have projects  but then the - my prof- professor of one of my classes also wa- has a final that he's giving us. And he's giving us five days to do it which means it going to be hard. Yeah. Yeah. Oh. is it a take-home final? Who's doing this? Yeah. Aikin  Alex  yeah. Yeah  figured. That would have been i- my guess. Hmm. Right. Um  But anyway  yeah. Pretty soon. O_K. O_K  so I guess that's So  the seventeenth will definitely be the last day  like it or not for me. Right. right. So let's do this  and then we- we- well there's gonna be some separate co- these guys are talking  uh we have a group on the formalization  uh Nancy and Johno and I are gonna talk about parsers. So there're various kinds of O_K. uh - Of course  nothing gets done even in a meeting of seven people  right? So  um  Right. Mmm. two or three people is the size in which actual work gets done. Yeah. So we'll do that. Great. Oh  the other thing we wanna do is catch up with uh  Ellen and see what she's doing because the um image schemas Yeah. are going to be um  an important pa- We - we want those  right? And we want them formalized and stuff like that. Quite relevant  yeah. Yeah  oh yeah. Yeah. So let me - let me make a note to do that. O_K. Yeah  I'm actually probably going to be in contact with her uh pretty soon anyway because of various of us students were going to have a reading group about O_K. precisely that sort of thing over the summer  so. Oh right! Right right right! That's great! Yeah  I - I - Shweta mentioned that  O_K. although she said it's a secret. Hi @@ Right  no faculty! Th- the faculty aren't - faculty aren't supposed to know. Wednesday's much better for me  yeah. But um  I'm sufficiently clueless that I count as a - Yeah  right. It's as if we didn't tell anyone at all  right. Bhaskara. ",The Berkeley Meeting Recorder group discussed recognition results generated for 20 minutes of close-talking microphone data. Recognition performance was very good  indicating promising results for forced alignment procedures and the ability to analyze other important signal information  e.g. prosody and overlapping speech. It was decided that close-talking data should be downsampled and fed to the SRI recognizer to compare recognition performance  and that data from the far-field microphones should be tested on the recognizer as soon as possible. The group also discussed recording setup and equipment issues. A decision was made to purchase an additional head-mounted crown microphone. A tentative decision was also made to integrate the use of a hand-held wireless microphone to help compensate for the lack of available close-talking microphones. The collection of Meeting Recorder data is ongoing  and will include meetings by the Berkeley Even Deeper Understanding research group and  possibly  an organized discussion by members of the transcriber pool. Following close-talking microphone recognition procedures  it was decided that data from the far-field microphones (or PZMs) should be tested on the recognizer as soon as possible. Speaker mn017 will compare close-talking microphone recognition results with those obtained for downsampled data. The SRI recognizer will be set up at ICSI to enable researchers to run their own variants. The group decided to purchase one additional head-mounted crown microphone. A tentative decision was also made to acquire a hand-held wireless microphone to pass around to additional meeting participants should the installation of more close-talking microphones prove too difficult. The suggestion of incorporating the use of cross pads during meeting recordings received favorable comments from participants. It was also tentatively decided to elicit meeting data from members of the transcriber pool discussing the Meeting Recorder corpus. During recognition procedures  a fixable acoustic glitch was discovered that causes speech channels to become slightly asynchronized. Poorer recognition performance was yielded for speech recorded via the use of lapel microphones. Recognition errors were largely due to misrecogniton of the plural -s and out-of-vocabulary items. The current recording setup is limited in that not all BMR meeting participants have their own close-talking microphone. A 'no-frills' recognizer  trained on Switchboard acoustic models and both Switchboard and Broadcast News language models  was used to test data for one transcribed meeting. Despite a glitch affecting the synchronized output of remaining data  recognition results for the first 20 minutes were very positive. The recognizer was successful in making accurate gender distinctions among speakers. It is anticipated that subsequent forced alignment procedures will also generate good results  enabling the group to analyze other types of signal information  such as prosody and overlapping speech segments. Future work will involve getting assessments of forced aligned data from linguists on the accuracy of time marks. The collection of Meeting Recorder data is ongoing  and will include meetings by the Berkeley Even Deeper Understanding research group. Transcriptions are also ongoing  with nearly 10 hours of Meeting Recorder data transcribed so far. Future work will involve working out a system for getting subjects to approve transcriptions so that confidentiality agreements are upheld and data may be shared with other research groups. A visiting student from Norway is working with a member of ICSI to conduct a related project on echo cancellation for handling segments of overlapping speech. Finally  the group is expecting a visit from a representative of NIST. 
"As usual. Yes. Whew! I almost forgot about the meeting. I woke up twenty minutes ago  thinking  Oy. what did I forget? It's great how the br- brain sort of does that. Something's not right here. Internal alarms. O_K. So the news for me is A_  my forthcoming travel plans in two weeks Yes. from today? Yeah? More or less? I'll be off to Sicily and Germany for a couple  three days. Now what are y- what are you doing there? I forgot? O_K  I'm flying to Sicily basically to drop off Simon there with his grandparents. And then I'm flying to Germany t- to go to a MOKU-Treffen which is the meeting of all the module-responsible people in SmartKom  Mmm. and  represent I_C_I and myself I guess there. And um. That's the mmm actual reason. And then I'm also going up to E_M_L for a day  and then I'm going to meet the very big boss  Wolfgang Walster  in Saarbruecken and the System system integration people in Kaiserslautern and then I'm flying back via Sicily pick up my son come back here on the fourth of July. What a great time to be coming back to the U_S_-of-A_. And uh. God bless America. You'll see maybe - see the fireworks from your plane coming in. And I'm sure all the - the people at the airport will be happy to work on that day. Yeah. You'll get even better service than usual. Mm-hmm. Wait  aren't you flying on Lufthansa though? Alitalia. Oh. Well then the - you know  it's not a big deal. Once you get to the United States it'll be a problem  but Yeah. And um  that's that bit of news  and the other bit of news is we had - you know  uh  I was visited by my German project manager who A_  did like what we did - what we're doing here  and B_  is planning to come here either three weeks in July or three weeks in August  to actually work. On - ? Oh. With us. And we sat around and we talked and he came up - we came up - with a pretty strange idea. And that's what I'm gonna lay on you now. And um  maybe it might be ultimately the most interesting thing for Eva because she has O_K. been known to complain about the fact that the stuff we do here is not weird enough. So this is so weird it should even Uh. O_K. make you happy. Oh great. Imagine if you will  that we have a system that does all that understanding that we want it to do Mm-hmm. based on utterances. It should be possible to make that system produce questions. So if you have the knowledge of how to interpret ""where is X_?"" Mm-hmm. under given conditions  situational  user  discourse and ontological conditions  you should also be able to make that same system ask ""where is X_?"" in a sper- certain way  based on certain intentions. So in- instead of just being able to Hmm. observe phenomenon  um  and  guess the intention we might be able just to sort of give it an intention  and make it produce an utterance. Well  like in A_I they generally do the take in  and then they also do the generation phase  like Nancy's thing. Or uh  you remember   in the - the hand thing in one-eighty-two  like not only was it able to recognize but it was also to generate based upon situations. You mean that sort of thing? Absolutely. O_K. And once you've done that what we can do is have the system ask itself. And answer  understand the answer  ask something else  and enter a dialogue with itself. So the - the ba- basic - the same idea as having two chess computers play against each other. Except this smacks a little bit more of a schizophrenic computer than A_I. Yeah you c- if you want  you can have two parallel machines um  asking each other. What would that give us? Would A_ be something completely weird and strange  and B_   i- That's good. if you look at all the factors  we will never observe people let's say  in wheelchairs under - you know  in - under all conditions  you know  when they say ""X_""  and there is a ride at the goal  and the parking is good  we can never collect enough data. It's - it's - it's not possible. Mm-hmm. Right  right. But maybe one could do some learning. If you get the system to speak to itself  you may find n- break downs and errors and you may be able to learn. And make it more robust  maybe learn new things. And um  so there's no - no end of potential things one could get out of it  if that works. And he would like to actually work on that with us. So- Well then  he probably should be coming back a year from now. Yeah  I w- See the - the generation bit  making the system generate - generate something  is - shouldn't be too hard. Well  once the system understands things. Yeah. No problem. I just don't think - I think we're probably a year away from getting the system to understand things. Yeah. Well  if we can get it to understand one thing  like our ""where is"" run through we can also  maybe  e- make it say  or ask ""where is X_?"" Or not. @@ Mmm  I don't know. e- I'm sort of - have the impression that getting it to say the right thing in the right circumstances is much more difficult than getting it to understand something given the circumstances and so on  you know  I mean just cuz it's sort of harder to learn to speak correctly in a foreign language  rather than learning to understand it. Right? I mean @@ just the fact that we'll get - The point is that getting it to understand one construction doesn't mean that it will n- always know exactly when it's correct to use that construction. Right? It's - it's uh - Well  I've - I've done generation and language production research for fo- four - four and a half years. And so it's - it's - you're right  it's not the same as the understanding. It's in some ways easier and some ways harder. nuh? But  um  Yeah. I think it'd be fun to look at it  or into that question. Nnn  yeah. It's a pretty strange idea. And so that's - that's - But - The basic idea I guess would be to give - allow the system to have intentions  basically? Cuz that's basically what needs to be added to the system for it. Well  look at th- eee  I think even - think even - What it - would be the - the prior intention. So let's uh - uh  let's say we have this - Well we'd have to seed that  I mean. No. Let's - we have to - we have some - some top-down processing  given certain setting. O_K  now we change nothing  and just say ask something. Right? @@ What would it ask? It wouldn't know what to ask. I mean. It shur- Unless it was in a situation. We'd have to set up a situation where  Yeah! it didn't know where something was and it wanted to go there. Mm-hmm. Yeah. Which means that we'd need to set up an intention inside of the system. Right? Which is basically  ""I don't know where something is and I need to go there"". Eh  n- Yeah. Ooh  do we really need to do that? Because  Well  no I guess not. Excel- s- It's - i- I know it's - it's strange  but look at it - look at our Bayes-net. If we don't have - Let's assume we don't have any input from the language. Right? So there's also nothing we could query the ontology  but we have a certain user setting. If you just ask  what is the likelihood of that person wanting to enter some - something  it'll give you an answer. Right? That's just how they are. Sure. And so  @@ whatever that is  it's the generic default intention. That it would find out. Which is  wanting to know where something is  maybe nnn - and wanting - I don't know what it's gonna be  but there's gonna be something that Well you're not gonna - are you gonna get a variety of intentions out of that then? I mean  you're just talking about like given this user  what's the th- what is it - what is that user most likely to want to do? And  have it talk about - Well you can observe some user and context stuff and ask  what's the posterior probabilities of all of our decision nodes. O_K. You could even say  ""let's take all the priors  let's observe nothing ""  and query all the posterior probabilities. It- it's gonna tell us something. Right? Well  it will d- And - r- assign values to all the nodes. Yes. Yes. And come up with posterior probabilities for all the values of the decision nodes. Which  if we have an algorithm that filters out whatever the - the best or the most consistent answer out of that  will give us the intention ex nihilo. And that is exactly what would happen if we ask it to produce an utterance  it would be b- based on that extension  ex nihilo  which we don't know what it is  but it's there. So we wouldn't even have to - t- to kick start it by giving it a certain intention or observing anything on the decision node. And whatever that - maybe that would lead to ""what is the castle?""  or ""what is that whatever"". I'm just - I guess what I'm afraid of is if we don't  you know  set up a situation  we'll just get a bunch of garbage out  like you know  everything's exactly thirty percent. No - Mmm. Yeah. So what we actually then need to do is - is write a little script that changes all the settings  you know  go- goes through all the permutations  which is - we did a - didn't we calculate that once? It's a - Well that was - that was absurdly low  in the last meeting  cuz I went and looked at it cuz I was thinking  that could not be right  and it would - it was on the order of Uh  twenty output nodes and something like twenty - And like thirty input nodes or some - thirty input nodes. So to test every output node  uh  would at least - Let's see  so it would be two to the thirty for every output node? Which is very th- very large. Oh! That's n- Oh. that's - that's nothing for those neural guys. I mean  they train for millions and millions of epochs. So. Well  I'm talking about Oh  I was gonna take a drink of my water. I'm talking about billions and billions and billions and a number - two to the thirty is like a- Bhaskara said  we had calculated out and Bhaskara believes that it's larger than the number of particles in the universe. I don't know if that's right or not. And if i- Th- that's big. That's just - That's uh - It's a billion  right? Two to the thirty? Well  two to the thirty is a billion  but if we have to do it two to the twenty times  Right. Argh . then that's a very very large number. Oh  O_K. Yeah. Yeah  that's big. Cuz you have to query the node  for every a- Sure. Alright. uh  or query the net two to the twenty times. Or not two to th- excuse me  twenty times. O_K. So  is it t- comes to twenty billion or something? Yes. That's pretty big  though. As far as - That's @@ - That's big. Actually - Oh! We calculated a different number before. How did we do that? Hmm. I remember there being some other one floating around. But anyway  uh. I don't really know. Yeah  it's g- Anyway  the point is that given all of these different factors  it's uh e- it's - it's still going to be impossible to run through all of the possible Ooo  it's just big. situations or whatever. But I mean  this'll get us a bit closer at least  right? I mean. If it takes us a second to do  for each one  and let's say it's twenty billion  then that's twenty billion seconds  which is - Yeah. Eva  do the math. Long! Can't . @@ Hours and hours and hours and hours. But we can do randomized testing. Tah-dah! Which probabilistically will be good enough. Mm-hmm. Yeah. So  it be it- it's an idea that one could n- for - for example run - run past  um  what's that guy's name? You know? He- he's usually here. Tsk. Here in the group? J- J- Jer- Jerj- Jerry Feldman. Oh  yeah. That's the guy. We - we - we - we g- Wait  who? Yeah  i- that would the g- the bald guy. Oh! My advisor! And um. so this is just an idea that's floating around and we'll see what happens. And um  hmm  what other news do I have? Well we fixed some more things from the SmartKom system  but that's not really of general interest  Um  Oh! Questions  yeah. I'll ask Eva about the E_Bayes and she's working on that. How is the generation X_M_L thing? I'm gonna work on that today and tomorrow. O_K. No need to do it today or tomorrow even. Do it next week or - I'm gonna finish it today  uh hopefully. O_K. I wanna do one of those things where I stay here. Cuz uh  if I go home  I can't finish it. I've tried about five times so far  where I work for a while and then I'm like  I'm hungry. So I go home  and then I think - I'm not going back. Yeah. Either that or I think to myself  I can work at home. And then I try to work at home  but I fail miserably. Yeah. Like I ended up at Blakes last night. Non-conducive. No. I almost got into a brawl. But I did not finish the uh  SmartKom. But I've been looking into it. I th- @@ It's not like it's a blank slate. I found everything that I need and stu- and uh  But st- At the b- uh furthermore  I told Jerry that I was gonna finish it before he got back. So. O_K. That's approaching. He's coming back when? Uh next - Well  I think - we think we'll see him definitely on Tuesday for the next - Or  no  wait. The meetings are on Thursday. Maybe. Who knows. Maybe. O_K. Well  we'll see him next week. Alright. That's good. Yeah. The paper. Hmm. Hmm. I was thinking about that. I think I will try to work on the SmartKom stuff and I'll - if I can finish it today  I'll help you with that tomorrow  if you work on it? I don't have a problem with us working on it though? So. O_K. So you would say it's And it - funky cool. I mean we just - I mean it wouldn't hurt to write up a paper  cuz then  I mean  yeah - I was talking with Nancy and Nancy said  you don't know whether you have a paper to write up until you write it up. So. Yeah. Well And since Jerry's coming back  we can run it by him too. So. Yep. Um  what's your input? Well  um  I don't have much experience with uh  conference papers for compu- in the computer science realm  and so when I looked at what you had  which was apparently a complete submission  I just sort of said what - just - I - I didn't really know what to do with it  like  this is the sort of the basic outline of the system or whatever  or - or ""here's an idea""  right? That's what that paper was  ""here's - here's one possible thing you could do""  Mm-hmm. short  eight pages  and I just don't know what you have in mind for expanding. Like I'd - I - what I didn't do is go to the web site of the conference and look at what they're looking for or whatever. Mm-hmm. Well  it seems to me that um - Wait  is this a computer science conference or is it a - Um  well it's more - It's both  right? It's - it's sort of t- cognitive  neural  psycho  linguistic  but all for the sake of doing computer science. So it's sort of cognitive  psycho  neural  plausibly motivated  architectures of natural language processing. So it seems pretty interdisciplinary  and I mean  w- w- the keynote speaker is Tomasello and blah-blah-blah  so  Right. Oh  yeah. W- the - the question is what could we actually do and - and - and keep a straight face while doing it. And i- Well  I really can't keep a straight face doing anything. My idea is  Setting that aside. well  you can say we have done a little bit and that's this  and uh sort of the rest is position paper  ""we wanna also do that "". Which is not too good. Might be more interesting to do something like let's assume um  we're right  we have as Jerry calls it  a delusion of adequacy  and take a ""where is X_"" sentence  Mm-hmm. and say  ""we will just talk about this  and how we cognitively  neurally  psycho-linguistically  construction grammar-ally  Mmm. motivated  envision uh  understanding that"". So we can actually show how we parse it. That should be able to - we should be able to come up with  you know  a sort of a - a parse. Right. I'm sorry. I'm sorry. I'm sorry. It's on  just - just put it on. O_K. Did Ben harass you? Hi. Yes. Good. Was he supposed to harass me? Yes. Well  he just told me that you came looking for me. You don- @@ Oh. @@ figure this out. You will suffer in hell  you know that. Backwards. There's a s- diagram somewhere which tells you how to put that - I know  I didn't understand that either! This is it. No wait. You have to put it on exactly like that  so put that - those things over your ears like that. Yeah. O_K. See the p- how the plastic things ar- arch out like that? There we go. O_K. It hurts. It hurts. It hurts real bad. It does! But that's what you get for coming late to the meeting. I'm sorry I didn't mean to - I'm sorry. I'm sorry  oh these are all the same. O_K! th- this is not very on target. Is your mike on? An- Yeah  it is. Shoot. O_K. Alright  you guys can continue talking about whatever you were talking about before. Um  We're talking about this um  alleged paper that we may  just  sort of w- Oh! Which Johno mentioned to me. Uh-huh. Yeah. And I just sort of brought forth the idea that we take a sentence  ""Where is the Powder-Tower""  Mm-hmm. and we - we p- pretend to parse it  we pretend to understand it  @@ and we write about it. Hmm. About how all of these things - What's the part that's not pretend? The writing? O_K  then we pretend to write about. The submitting to a major international conference. Yeah. Tha- Which conference is it for? It's the whatever  architectures  eh you know  where - There is this conference  it's the seventh already international conference  on neu- neurally  cognitively  motivated  architectures of natural language processing. Oh. Wow. And Interesting. the keynote speakers are Tomasello  MacWhinney? We- MacWhinney  I think. Whinney. MacWhinney. Uh-huh. MacWhinney. So  interesting  both  like  child language people. Yeah. Yep. O_K. So maybe you wanna write something too. Yeah  maybe I wanna go. Mmm. Mmm. Um  why are they speaking at it if it - is - is it normally like - like  dialogue systems  or  you know  other N_L_P-ish things? No no no no no no no no. It's - it's like a - Oh  it's cognitive. O_K. Yeah. Yeah. Even neuro. And uh  both learning and like  comprehension  production  that kinda stuff. Psycho. @@ You could look at the web site. O_K. I'll - O_K. And the ad- and - and the deadline is the fifteenth of June. I don't know about it. Yeah that's pretty soon. Mmm. Hey. Plenty of time. Why  we've got over a week! It would be nice to go write two papers actually. Yeah. And one - one from your perspective  and one from our peve- per- per- Mm-hmm. I mean  th- that's the kinda thing that maybe like  um  the general uh con- sort of like N_T_L-ish like  whatever  the previous simulation based pers- maybe you're talking about the same kind of thing. A general paper about the approach here would probably be appropriate. Yeah. And good to do at some point anyway. Yeah. Um. Well  I - I also think that if we sort of write about what we have done in the past six months  we - we - we could sort of craft a nice little paper that if it gets rejected  which could happen  Mm-hmm. doesn't hurt because it's something we eh - Having it is still a good thing. having it is a good - good thing. It's a nice exercise  it's - Yeah. I usually enjoy writing papers. It's not - I don't re- regard it as a painful thing. Mm-hmm. It's fun. And um  we should all do more for our publication lists. And. It just never hurts. And Keith and-or Johno will go  probably. Will I? When is it and where? Hmm! In case of - It's on the twenty second of September  in Saarbruecken Germany. Ah  it's in Germany. Ah  O_K. I s- I see. Tomasello's already in Germany anyway  so makes sense. O_K. Just - Um. O_K. So  is the - What - Are you just talking about you know  the details of how to do it  or whether to do it  or what it would be? What would one possibly put in such a paper? What to write about. What is our - what's our take home message. What - what do we actually - Or what to write about? Because I mean  it - I don't like papers where you just talk about what you plan to do. I mean  it's obvious that we can't do any kind of evaluation  and have no - you know  we can't write an A_C_L type paper where we say  ""O_K  we've done this and now we're Mm-hmm. whatever percentage better than everybody else"". You know. Mm-hmm. It's far too early for that. But uh  we - we can tell them what we think. I mean that's - never hurts to try. And um  maybe even - That's maybe the time to introduce the - the new formalism that you guys have cooked up. Mm-hmm. Are in the process of - But that - How many pages? don't they need to finish the formalism? It's just like four pages. I mean it's - it's not even a h- Yeah. Four pages? O_K  so it's a little thing. Mm-hmm. Oh. Well  you said it was four thousand lines? Is that what you s- Oh. O_K. Four pages is  like  really not very much I don't know w- Did you look at it? Yeah  it depends on the format. space. Oh my gosh. Oh  I thought you were - I thought we were talking about something which was much No that's - I mean that's actually a problem. It's difficu- it's more difficult to write on four pages than on eight. more like ten or something. It's - Yeah. Yeah. And it's also difficult to - even if you had a lot of substance  it's hard to demonstrate that in four pages  basically. Um. Yeah. That would be hard. Well I uh maybe it's just four thousand lines. I do- I don't - They don't want any - I mean it's still - it's still - They don't have a TeX f- style @@ guide. They just want ASCII. Pure ASCII Uh-huh  uh-huh. O_K. lines  whatever. Why  for whatever reason  I don't know. Not including figures and such? I don't know. Very unspecific unfortunately. O_K. Well  We'll just uh - I would say that's closer to six pages actually. Four thousand lines of ASCII? O_K then. It's - Four thousand lines. I mean. Isn't a- isn't it about fifty s- fifty five  sixty lines to a page? I d- don't quote me on this. This is numbers I - I have from looking o- How many characters are on a line? O_K. Let's - let's - wh- wh- what should we - should - should we uh  um  discuss this over tea and all of us look at the web? Oh  I can't. I'm wizarding today. ASCII? O_K  look at the web page? Um. Wha- w- Look at the web page and let's talk about it maybe tomorrow afternoon? More cues for us to find it are like  neural cons- Johno will send you a link. Oh  you have a link. O_K. O_K. I got an email. O_K. By the way  Keith is comfortable with us calling him ""cool Keith"". Oh. Cool. Keith. Cool  ""cool Keith"". He - he decided I'm chilling in the five-one-O_. Yeah. Excellent. O_K. That's a very cool T_shirt. Thank you. And I'm also flying - I got this from the two one two. Yeah. New York? Excellent. Sorry. Yes? I'm flying to Sicily next - in a w- two weeks from now  Oh  lucky you. w- and a week of business in Germany. I should mention that for you. And otherwise you haven't missed much  except for a really weird idea  but you'll hear about that The idea that you and I already know about? soon enough. That you already told me? Not that - O_K. No  no  no. Yeah  that is something for the rest of the gang to - to g- The thing with the goats and the helicopters? Change the watchband. It's time to walk the sheep. like O_K. Um. Did you catch that allusion? It's time to walk the sheep? No. It's a- a- uh presumably one of the Watergate codes they uh - Oh. Anyways  th- um  um  don't make any plans for spring break next year. That's - Oh  shoot. That's the other thing . We're gonna do an int- E_D_U internal workshop in Sicily. That's what - That's what he says. I've already got the funding. So  I mean. I kn- That's great! Does that mean - We'll see. No  that's - Does that mean you'll get - you'll fly us there? Yeah  that's what it means. Hhh! O_K  cool. Uh- a- a- Huh. And he'll put us up  too. I know - I know about that part. I know about the - the almond trees and stuff. Not joking. O_K. Name a vegetable  O_K. Oh  um  kiwi? Yeah. Mmm  too easy. Coconut. Ki- Pineapple. See? Mango? Too easy. O_K. O_K. Too easy? Yeah  mangos go everywhere. So do kiwi. Really? Oh. O_K  but I was trying to find something that he didn't grow on his farm. But coconut anana- pineapple  that's - that's tricky  yeah. Sorry. Anyway. Cantaloupe. So  but we have to decide what  like  sort of the general idea of - Potatoes. So. Sorry! @@ . Um  I mean  we're gonna have an example case um  right? I m- the - the point is to - like this ""where is"" case  or something. Yeah  maybe you have - It would be kind of - The paper ha- would have  in my vision  a nice flow if we could say  well here is th- the - th- here is parsing if you wanna do it c- right  here is understanding if you wanna do it right  and you know - without going into technical - Mm-hmm. But then in the end we're not doing like those things right yet  right? Would that be clear in the paper or not? That would be clear  we would - I - I mailed around a little paper that I have - O_K. It would be like  this is the idea. Oh  I didn't get that  did I? Oops. w- we could sort of say  this is - Did I? Oops. Sorry. No  No  y- I don't think you got it. See this  if you- if you're not around  and don't partake in the discussions  and you don't get any email  and I'm sorry. I'm sorry  I'm sorry. Sorry. O_K  go on. So parsing done right is like chicken done right. Su- So we could - we could say this is what - what's sort of state of the art today. O_K. Nuh? And say  this is bad. Nuh? And then we can say  uh well what we do is this. Yeah. Yeah. O_K. Parsing done right  interpretation done right  example. Mm-hmm. Yeah. And And how much to get into the cognitive neural part? We- That's the only - That's the question mark. Don't you need to reduce it if it's a - or reduce it  if it's a cognitive neuro - Well  you don't have t- I mean the conference may be cognitive neural  doesn't mean that every paper has to be both. Yeah  and you can - you can just point to the - to the literature  you can say that construction-based @@ Like  Mmm. N_L_P cognitive neural. You know - So i- so this paper wouldn't particularly deal with that side although it could reference the N_T_L-ish sort of  like  um  approach. Yeah. Mm-hmm. Yeah. The fact that the methods here are all compatible with or designed to be compatible with whatever  neurological - neuro- Mm-hmm. neuro-biol- su- stuff. Yeah  I guess four pages you could - @@ I mean you could definitely - it's definitely possible to do it. It's just - It'd just be small. Like introducing the formalism might be not really possible in detail  but you can use an example of it. Well  l- looking at - yeah  looking at that paper that - that you had  I mean you know  like  you didn't really explain in detail what was going on in the X_M_L cases or whatever you just sorta said well  you know  here's the general idea  some stuff gets put in there. You know  hopefully you can - you can say something like constituents tells you what the construction is made out of  you know  without going into this intense detail. Yeah  yeah. So it be like using the formalism rather than Yeah. you know  introducing it per se. So. Give them the one paragraph whirlwind tour of w- w- what this is for  and - Yeah. Yeah. Mm-hmm. And people will sort of figure out or ask about the bits that are Yeah. So this will be sort of documenting what we think  and documenting what we have in terms of the Bayes-net implicit. Mm-hmm. stuff. And since there's never a bad idea to document things  no? That's th- that's definitely a good idea. That would be my  uh - We - we should sketch out the details maybe tomorrow afternoon-ish  if everyone is around. I don't know. You probably I think so. wouldn't be part of it. Maybe you want? Think about it. Um  You may - may ruin your career forever  if you appear. Yeah  you might get blacklisted. And um  the uh  other thing  yeah we actually - Have we made any progress on what we decided  uh  last week? I'm sure you read the transcript of last week's meeting in red so sh- so you're up to dated - caught up. No. Sorry. We decided t- that we're gonna take a ""where is something"" question  and pretend we have parsed it  and see what we could possibly hope to observe on the discourse side. Remember I came in and I started asking you about how we were sor- going to sort out the uh  decision nodes? Yes! What'd you say? I remember you talking to me  just not what you said. I do remember you talking to me. Um  a few more bits. Well  there was like we needed to - or uh  in my opinion we need to design a Bayes - another sub-Bayes-net - You know  it was whether - it was whether we would have a Bayes-net on the output and on the input  or whether the construction was gonna be in the Bayes-net  Oh. Oh  yeah. O_K. a- and outside of it  and - O_K. So that was - was that the question? Was that what - Well that was related to what we were talking about. Should I introduce it as SUDO-square? Yeah sure. We have to put this in the paper. If we write it. This is - this is my only constraint. The - th- So. The SUDO-square is  Oh I saw the diagram in the office  yeah. ""Situation""  ""User""  ""Discourse""  right? ""Ontology"". Oh my god  that's amazing! Mmm. Yeah. Whatever. No way. Way! Is it? Someone's gonna start making Phil Collins jokes. Yeah. Oh  god  I hope not. Hmm? Sorry. What? You guys are too young. You know like ""Sussudio""  that horrible  horrible song that should never have been created. Yeah  come on. Oh  oh  oh  oh. I know  that was horrible. Sussudio. I've blocked every aspect of Phil Collins out of my mind. What? I'm sorry  I haven't. Not on purpose. @@ in here Oh - Well  also he's talking about suicide  and that's - that's not a notion I wanna have evoked. No  he's not. He is. Really? Oops. I didn't really listen to it  I was too young. Anyway . The - Hmm. It sounds too rocking for that. Anyway. So  what's going on here? Yeah. So what are - what - So  Was wollte der Kuenstler uns damit sagen? Stop excluding me. O_K  so we have tons of little things here  and we've I can't believe that that's never been thought of before. Wait  what are the dots? I don't remember what the dots were. Those are little bugs. O_K. Cool Keith. You know  these are our  whatever  belief-net decision nodes  and they all contribute to these @@ Oh  oh. things down here. Wait  wait  what's the middle thing? That's E_D_U. e- e- Our e- e- e- That's a c- But wh- I mean - That's - You. We. Us. But what is it? Well  in the moment it's a Bayes-net. And it has sort of fifty not-yet-specified interfaces. O_K. Eh I have taken care that we actually can build little interfaces  to other modules that will tell us whether the user likes these things and  n- the - or these things  and he - whether O_K. he's in a wheelchair or not  Is that supposed to be the international sign for interface? I think so  yeah. Mmm. O_K. I'd - I'd never seen it before either. O_K. Just t- Cool. Mmm. So. Cuz things fit onto that  see? Yeah. In a vaguely obscene fashion. Cool. No  this is a R_M_E core by agent design  I don't know. That's so great. There's maybe a different So wait  what a- what are these letters again  Situr- Situation  User  Discourse and Situation  user  d- ontology. User? Ontology. What about the utterance? That's here. It's - Discourse. Oh  discourse. Yeah. Discourse is all things linguistic  yeah. So that's not like context  O_K. So this - this includes the - the current utterance plus all the previous utterances. Interesting  uh-huh. User. And for example w- i- s- I- Irena Gurevich is going to be here eh  end of July. She's a new linguist working for E_M_L. User. And what she would like to do for example is great for us. She would like to take the ent- ontolog- So  Ouch. we have discussed in terms of the EVA - uh - Grateful for us? Did you just say grateful for us? O_K  sorry. Anyway. Think of - back at the EVA vector  and Johno coming up with the idea that if the person discussed the - discussed the admission fee  in - eh previously  that might be a good indication that  Mm-hmm. ""how do I get to the castle?""  actually he wants to enter. Or  you know  ""how do I get to X_?"" discussing the admission fee in the previous utterance  is a good indication. Mm-hmm. @@ So we don't want a hard code  a set of lexemes  or things  that person's you know  sort of filter  or uh search the discourse history. Mm-hmm. So what would be kind of cool is that if we encounter concepts that are castle  tower  bank  hotel  we run it through the ontology  and the ontology tells us it has um  admission  opening times  it has admission fees  it has this  it has that  and then we - we - we make a thesaurus lexicon  look up  and then search dynamically through the uh  discourse history for occurrences of these things in a given window of utterances. Mm-hmm. And that might  you know  give us additional input to belief A_ versus B_. Or E_ versus A_. So it's not just a particular word's - O_K  so the - you're looking for a few keys that you know are cues to - sorry  a few specific cues to some intention. You can dynamically look up keys  yeah. Yeah. Uh  so  wait - so um  since this - since this sort of technical stuff is going over my head  the - the point is that you uh - that O_K. And then grep  basically. when someone's talking about a castle  you know that it's the sort of thing that people are likely to wanna go into? Or  is it the fact that if there's an admission fee  then one of the things we know about admission fees is that you pay them in order to go in? And then the idea of entering is active in the discourse or something? And then Well blah-blah-blah? I mean. the - the idea is even more general. The idea is to say  we encounter a certain entity in a - in a- in a utterance. So le- let's look up everything we - the ontology gives us about that entity  what stuff it does  what roles it has  what parts  whatever it has. Functions. And  then we look in the discourse  whether any of that  or any surface structure corresponding to these Oh  O_K. roles  functions aaa has ever occurred. And then  the discourse history can t- tell us  ""yeah""  or ""no"". O_K. And then it's up for us to decide what to do with it. O_K. t- So i- So - No  go ahead. So  we may think that if you say um  ""where is the theater""  um  whether or not he has talked about tickets before  Mm-hmm. then we - he's probably wanna go there to see something. O_K. Or ""where is the opera in Par- Paris?  yeah? Lots of people go to the opera to take pictures of it and to look at it  and lots of people go to attend a performance. Mm-hmm. O_K. Mm-hmm. And  the discourse can maybe tell us w- what's more likely if we know what to look for in previous statements. And so we can hard code ""for opera  look for tickets  look for this  look for that  O_K. O_K. or look for Mozart  look for thi-"" but the smarter way is to go via the ontology and dynamically  then look up u- stuff. O_K. But you're still doing look up so that when the person - So the point is that when the person says  ""where is it?"" then you sort of say  let's go back and look at other things and then decide  rather than the other possibility which is that all through discourse as they talk about different things - You know like w- prior to the ""where is it"" question they say  you know  ""how much does it cost to get in  you know  to - to see a movie around here""  um  ""where is the closest theater"" - The - the - the point is that by mentioning admission fees  that just sort of stays active now. Yeah. You know. That becomes part of like  their sort of current ongoing Mm-hmm. active conceptual structure. And then  um  over in your Bayes-net or whatever  when - when the person says ""where is it""  you've already got  you know since they were talking about admission  and that evokes the idea of entering  um  then when they go and ask ""where is it""  then you're Enter node is already active because that's what the person is thinking about. I mean that's the sort of cognitive linguistic-y way  and probably not practical. Mm-hmm. Yeah. Yeah  e- ultimately that's also what we wanna get at. I think that's - that's the correct way. So  of course we have to keep memory of what was the last intention  and how does it fit to this  and what does it tell us  in terms of - of the - the - what we're examining. Mm-hmm. Mmm  yeah. And furthermore  I mean we can idealize that  you know  people don't change topics  Mm-hmm. but they do. But  Right. even th- for that  there is a student of ours who's doing a dialogue act um  recognition Mm-hmm. module. So  maybe  we're even in a position where we can take your approach  which is of course much better  as to say how - how do these pieces - Mmm. And much harder to r- program. Hmm? And much harder to p- to program. Yeah. How - how do these pieces fit together? Uh-huh. And um. But  O_K  nevertheless. So these are issues but we - what we actually decided last week  is to  and this is  again  for your benefit - is to um  pretend we have observed and parsed an utterance such as ""where is the Powder-Tower ""  or ""where is the zoo""  and specify um  what - what we think the - the output uh  observe  out - i- input nodes for our Bayes-nets for the sub- sub-D_  for the discourse bit  should be. So that - And I will - I will then come up with the ontology side uh  bits and pieces  so that we can say  O_K we - we always just look at this utterance. That's the only utterance we can do  it's hard coded  like Srini  sort of hand parsed  hand crafted  but this is what we hope to be able to observe in general from utterances  and from ontologies  and then we can sort of fiddle with these things to see what it actually produces  in terms of output. Uh So we need to find out what the ""where is X_"" construction will give us in terms of semantics and Simspec type things. Just - O_K. Just ""where is X_""? Mm-hmm. Or any variants of that. Yeah. No! Um  look at it this way  i- Yeah. What did we decide. We decided sort of the - Well we were the prototypical ""where is X_""  where you know  we don't really know  does he wanna go there  or just wanna know where it is. So the difference of ""where is the railway station""  versus where - where - ""where is Greenland"". Nuh? Mm-hmm. Uh s- I was just dancing  sorry. We're not videotaping any of this. So. Uh - ah - So  um  we're supposed to - I mean we're talking about sort of anything that has the semantics of request for location  right? actually? Or  I mean  anyway  the node in the uh - the ultimate  uh  in - in the Bayes-net thing when you're done  the - the node that we're talking about um  is one that says ""request for location  true""  or something like that  right? Um  and - and exactly how that gets activated  you know  like whether we want Yeah  but it - the sentence ""how do I get there?"" to activate that node or not  you know  that's - that's sort of the issue that sort of the linguistic-y side has to deal with  right? Yea- Nnn- Well actually more - m- more the other way around. We wanted something that represents uncertainty uh we- in terms of going there or just wanting to know where it is  for example. Some generic information. O_K. And so this is prototypically @@ found in the ""where is something"" question  surface structure  O_K. We- which can be p- you know  should be maps to something that activates both. I mean the idea is to - Alright  O_K. I don't - Hhh. let's have it fit nicely with the paper. The - I guess. I don't - I don't see unde- how we would be able to distinguish between the two intentions just from the g- utterance  though. I mean  uh bef- or  before we don't - before we cranked it through the Bayes-net. I mean. Yeah  we - we wouldn't. That's exactly what we want. We would? We want to get - No. We wouldn't. O_K  but then so basically it's just a - for every construction we have a node in the net  right? And we turn on Yeah. that node. Oy. What - what is this gonna - Exactly. What is the uh - Well - And then given that we know that the construction has these two things  we can set up probabilities - we can s- basically define all the tables for ev- for those - Yeah  it should be - So we have um  i- let's assume we - we call something like a loc-X_ node and a path-X_ node. And what we actually get if we just look at the discourse  ""where is X_"" should activate or should - Mmm. Hmm. Should be both  whereas maybe ""where is X_ located ""  we find from the data  is always just asked when the person wants to know where it is  and ""how do I get to"" is always asked when the person just wants to know how to get there. Right? So we want to sort of come up with what gets uh  input  and how inter- in case of a ""where is"" question. So what - what would the outcome of - of your parser look like? And  what other discourse information from the discourse history could we hope to get  squeeze out of that utterance? So define the - the input into the Bayes-net based on what the utterance  ""where is X_""  gives us. So definitely have an Entity node here s- which is activated via the ontology  so ""where is X_"" produces something that is s- stands for X_   whether it's castle  bank  restroom  toilet  whatever. And then the ontology will tell us - That it has a location or something like that? - or th- the ontology will tell us where actually it is located? No. O_K. Not at all. Where it is located  we have  a user proximity node here somewhere  O_K. O_K. e- which tells us how far the user - how far away the user is in respect to that uh entity. O_K. So you're talking about  for instance  the construction obviously involves this entity or refers - refers to this entity  Mm-hmm. and from the construction also you know that it is a location - is - or a thing - thing that can be located. Right? Ontology says this thing has a location slot. Sh- and that's the thing that is being - that is the content of the question that's being queried by one interpretation of ""where is X_"". And another one is  um  path from current - user current location to Mm-hmm. that location. So. So is the question - I mean it's just that I'm not sure what the - Is the question  for this particular construction how we specify that that's the information it provides? Or - or asked for? b- Both sides  right? Yeah  you don't need to even do that. It's just sort of what what would be @@ observed in uh - in that case. Observed when you heard the speaker say ""where is X_""  or when - when that's been parsed? Mm-hmm. So these little circles you have by the D_? Is that - ? That's exactly what we're looking for. O_K. O_K. I d- I just - I don't like having - characterizing the constructions with location and path  or li- characterizing them like that. Cuz you don't - It seems like in the general case you wouldn't know You wouldn't. how - how to characterize them. I mean - or  for when. There could be an interpretation that we don't have a node for in the - I mean it just seems like @@ has to have uh - a node for the construction and then let the chips fall where they may. Versus uh  saying  this construction either can mean location or path. And  in this cas- and since - since it can mean either of those things  it would light both of those up. It's the same. Thoughts? Questions? I'm thinking about it. Um - It will be the same. So I think r- in here we have ""I'll go there""  right? Answers? And we have our Info-on. So in my c- my case  this would sort of make this happy  and this would make the Go-there happy. What you're saying is we have a Where-X_ question  Where-X_ node  that makes both happy. Right? That's what you're proposing  which is  in my mind just as fine. So w- if we have a construction node  ""where is X_""  it's gonna both get the po- posterior probability that - it's Info-on up  Mmm  yeah. Info-on is True-up  and that Go-there is True-up  as well. Which would be exactly analogous to what I'm proposing is  this makes - uh makes something here true  and this makes something - also something here true  and this makes this True-up  and this makes this True-up as well. I kinda like it better without that extra level of indirection too. You know with - with this points to this points to that  and so on because I don't know  it - Yeah  because we get - we get tons of constructions I think. Because  Is- uh  Yeah. you know  mmm people have many ways of asking for the same thing  and - Yeah  sure. Yeah. So un- I change- I changed my mind actually. O_K. So I agree with that. I have a different kinda question  might be related  which is  O_K so implicitly everything in E_D_U  we're always inferring the speaker intent  right? Like  what they want either  the information that they want  or - It's always information that they want probably  of some kind. Right? Or I - I don't know  or what's something that they - The system doesn't massage you  no. No. I - I - I don't - O_K. So  um  let's see. So I don't know if the - I mean i- if th- just there's more s- here that's not shown that you - it's already like part of the system whatever  but  ""where is X_""  like  the fact that it is  you know  a speech-act  whatever  it is a question. It's a question that  um  queries on some particular thing X_  and X_ is that location. There's  like  a lot of structure in representing that. Yep. Yeah. So that seems different from just having the node ""location-X_"" and that goes into EDU  right? Yeah. Precisely. That's - that's - So  w- So tha- is that what you're t- Exactly. We have su- we have specified two. talking about? wh- what kinds of structure we want. O_K  the next one would be here  just for mood. Mm-hmm. Mm-hmm. The next one would be what we can squeeze out of the uh I don't know  maybe we wanna observe the uh  um  Mmm. uh the length of - of the words used  and  or the prosody and g- a- and t- make conclusions about the user's intelligence. I don't know  yeah. O_K. So in some ways - um  so in some ways in the other sort of parallel set of mo- more linguistic meetings we've been talking about possible semantics of some construction. Mm-hmm. Right? Where it was the simulation that's  according to it - you know  that - that corresponds to it  and as well the - as discourse  whatever  conte- infor- in- discourse information  such as the mood  and  you know  other stuff. So  Mm-hmm. are we looking for a sort of abbreviation of that  that's tailored to this problem? Cuz that - that has  you know  basically  you know  s- it's in progress still it's in development still  but it definitely has various feature slots  attributes  Mm-hmm. um  bindings between things - Yeah. U- that's exactly r- um  why I'm proposing - It's too early to have - to think of them - of all of these Uh-huh. discourse things that one could possibly observe  Mm-hmm. so let's just assume For the subset of - human beings are not allowed to ask anything but ""where is X_"". O_K. This is the only utterance in the world. What could we observe from that? O_K. That exactly ""where is X_""  not In ter- the - the choices of ""where is X_"" or ""how do I get to X_"". Just "" where is X_"". O_K. Yeah. Just - just ""where is X_"". And  but you know  do it - do it in such a way that we know that people can also say  ""is the town hall in front of the bank ""  so that we need something like a w- W_H focus. Nuh? Should be - should be there  that  you know  this - the - whatever we get from the - Wait  so do  or do not take other kinds of constructions into account? Well  if you - if you can  oh definitely do  O_K. Where possible. O_K. where possible. Right? If i- if - if it's not at all triggered by our thing  then it's irrelevant  and it doesn't hurt to leave it out for the moment. Mm-hmm. Um  but - O_K. Um  it seems like for instance  ""where is X_""  the fact that it might mean um  ""tell me how to get to X_""  like - Do y- So  would you wanna say that those two are both  like - Those are the two interpretations  right? the - the ones that are location or path. So  you could say that the s- construction is a question asking about this location  and then you can additionally infer  if they're asking about the location  it's because they wanna go to that place  Mm-hmm. in which case  the - you're jumping a step - step and saying  ""oh  I know where it is but I also know how to get - they wanna seem - they seem to wanna get there so I'm gonna tell them"". So there's like structure Yeah. Right  th- this - it's not - it's not that this is sort of like semantically ambiguous between these two. It's really about this but why would you care about this? Well  i- do you kn- sort of uh  that - Mm-hmm. it's because you also want to know this  or something like that right? Mm-hmm. So it's like you infer the speaker intent  and then infer a plan  a larger plan from that  for which you have the additional information  you're just being extra helpful. Um. Yeah. Mm-hmm. Yep. Think - Uh  well this is just a mental exercise. If you think about  Yeah. focus on this question  how would you design that? Mm-hmm. Is it - do you feel confident about saying this is part of the language already to - to detect those plans  and why would anyone care about location  if not  you know Mmm. and so forth. Or do you actually  I mean this is perfectly legitimate  and I - I would not have any problems with erasing this and say  that's all we can activate  based on the utterance out of context. Mm-hmm. And just by an additional link - Oh. What? Right. Right  like  And then the - the - the miracle that we get out the intention  Go-there  with context and enough user information  yeah. Yeah. happens  based on what we know about that entity  about the user  about his various beliefs  goals  desires  blah-blah-blah. Absolutely fine. But this is the sort of thing  I - I propose that we think about  O_K. so that we actually end up with um  um  nodes for the discourse and ontology so that we can put them into our Bayes-net  never change them  so we - all there is is ""where is X_""  and  Eva can play around with the observed things  and we can run our better JavaBayes  and have it produce some output. And for the first time in th- in - in the world  we look at our output  and um - and see uh whether it - it's any good. O_K. You know? I mean  Here's hoping. Hmm? Here's hoping. Right? Now cross your fingers. Yeah  I - I mean  for me this is just a ba- matter of curiosity  I wanna - Yeah. Yeah. would like to look at uh  what this ad-hoc process of designing a belief-net would actually produce. Mmm. If - if we ask it where is something. And  maybe it also h- enables you to think about certain things more specifically  um  come up with interesting questions  to which you can find interesting answers. And  additionally it might fit in really nicely with the paper. Um-hmm. Because if - if - if we want an example for the paper  I suggest there it is. Yeah. So th- this might be a nice opening paragraph for the paper as saying  ""you know people look at kinds of - at ambiguities""  and um  in the literature there's ""bank"" and Mm-hmm. whatever kinds of garden path phenomenon. And we can say  well  that's all nonsense. A_  uh these things are never really ambiguous in discourse  B_  don't ever occur really in discourse  but normal statements that seem completely unambiguous  such as ""where is the blah-blah""  Mm-hmm. actually are terribly complex  and completely ambiguous. Mm-hmm. And so  what every- everybody else has been doing so far in - in - in - you know  has been completely nonsensical  and can all go into the wastepaper bin  That's always a good way to begin. Yeah. Yeah. and the only - Yeah. And the - the - the only - I am great. All others are useless. Yeah. That's good. Yeah. Nice overture  but  you know  just not really - O_K  I'm eja- exaggerating  but that might be  you know  saying ""hey""  you know  some stuff is - is actually complex  if you look at it in - in - in the vacuum Mm-hmm. and - and ceases to be complex in reality. And some stuff that's as - that's absolutely straightforward in the vacuum  is actually terribly complex in reality. Would be nice sort of  uh  also  nice  um bottom-up linguistics  um  type message. Mm-hmm. True. Versus the old top-down school. I'm running out of time. O_K. When do you need to start wizarding? At four ten. O_K  this is the other bit of news. The subjects today know Fey  so she can't be here  and do the wizarding. Huh. So I'm gonna do the wizarding and Thilo's gonna do the instructing. Also we're getting a - a person who just got fired Mmm. uh  from her job. Uh a person from Oakland who is interested in maybe continuing the wizard bit once Fey leaves in August. And um  she's gonna look at it today. Which is good news in the sense that if we want to continue  after the thir- thir- after July  we can. We could. And  um - and that's also maybe interesting for Keith and whoever  if you wanna get some more stuff into the data collection. Remember this  we can completely change the set-up any time we want. Mm-hmm. O_K. Look at the results we've gotten so far for the first  whatever  fifty some subjects? Fifty? You've had fifty so far  or - ? No  we're approaching twenty now. But  until Fey is O_K. Yeah. leaving  we surely will hit the - some of the higher numbers. Hmm. And um  so that's cool. Can a- do more funky stuff. Sure. Yeah  I'll have to look more into that data. Is that around? Like  cuz that's pretty much getting posted or something right away when you get it? Or - ? I guess it has to be transcribed  huh? Um. We have uh  eh found someone here who's hand st- hand transcribing the first twelve. O_K. First dozen subjects Uh-huh. just so we can build a - a language model for the recognizer. O_K. But  um - So those should be available soon. O_K. The first twelve. And I can ch- ch- st- e- You know - I mean you know that I - that I looked at the first - the first one and got enough data to keep me going for  you know  probably most of July. So. But  um. Yeah  a- probably not the right way to do it actually. But you can listen to - a- y- y- y- You can listen to all of them from your Solaris box. O_K. If you want. Right. It's always fun. ",The Berkeley Meeting Recorder group discussed the aims  methods  timing  and outsourcing issues concerning transcription of the Meeting Recorder corpus. The Transcriber software tool was introduced  along with a set of transcription conventions for coding different speech events. The prospect of sending the data to an external transcription service was weighed against that of hiring a graduate student transcriber pool. It was tentatively decided that the latter option would be less costly and allow BMR to maintain greater control over the transcription process. Methods for distributing the data were briefly discussed  along with an initiative for creating a BMR project website. The group received an update on the meeting room recording setup and electronics. The group will obtain a copy of the Corpus of Spoken American English (CSAE) from the LDC to compare the methods and conventions used by UC Santa Barbara with those being considered for the Meeting Recorder project. Speakers fe008 and me011 will experiment with pre-segmentation procedures in hopes of facilitating the transcription process. Speaker fe008 will perform the transcription for one meeting (40-60 minutes of data) as a pilot project. A tentative decision was made to put speaker fe008 in charge of the in-house transcription effort. Modifications to the Transcriber source code  e.g. adjusting the delay between the audio play function and the inputting of time boundaries  may be undertaken by the BMR group. An initiative for creating an internal BMR project website was discussed  along with ideas for providing web access to external organizations  such as the University of Washington. A cabinet will be built to house wires and other electronic equipment used in the recording setup. A laptop and wireless modem will be avaiable to participants for monitoring the recording progress. How should transcripts and corresponding audio files be formatted and distributed? External transcription services are expensive  difficult to monitor  and are unlikely to be able to handle multi-channel data. It is unclear which levels of transcription should be encoded. What size of segment is most useful for doing transcriptions? Which level of segmentation is most suitable for the aims of the project? Transcriber's interface only allows the user to view two overlapping speakers. Decisions must be made regarding the security of electronic recording equipment. Efforts are in progress to collect and transcribe 30-40 hours of Meeting Recorder data. The Transcriber tool is being used to do word-level transcriptions  and was reported to work well  when used with supplementary scripts  for specifying multiple speakers. Other levels of transcription being considered include marking stress  repairs  and false starts. A set of transcription conventions have been formulated for marking colloquial forms  the continuity of utterances  etc. A cost assessment was made for sending Meeting Recorder data to an external transcription service. It was agreed that hiring linguistics graduate students would be cheaper and allow the group to maintain greater control over the transcription process. Tentative pre-segmentation efforts will enable the automatic generation of phrase boundaries and speaker identity coding  and will be extendable for use at UW. Perl and XWaves scripts are available for extracting and aligning digits data. Other suggestions for future work included performing multi-channel speech/non-speech detection  and linking an ASR system to the Transcriber tool. The recording room electronics setup has been diagrammed and will take approximately one week to configure. It was suggested that such efforts will enable researchers at UW and other collaborating institutions to create their own recording setups more cheaply. 
"We're  @@ Yeah. I mean we - Yeah. O_K. We didn't have a house before. We're on again? O_K. Mm-hmm. That is really great. Yeah  so if uh - O_K- Oh  yeah! So if anyone hasn't signed the consent form  That's terrific. please do so. The new consent form. The new and improved consent form. Uh. Now you won't be able to walk or ride your bike  huh? O_K. O_K. Right. And uh  shall I go ahead and do some digits? Uh  we were gonna do that at the end  remember? O_K  whatever you want. Yeah. Just - just to be consistent  from here on in at least  that - The new consent form. that we'll do it at the end. It's uh - Yeah  it doesn't matter. O_K. O_K Testing  one  two  three. Um Well  it ju- I mean it might be that someone here has to go  and - Right? That was - that was sort of the point. So  uh I had asked actually anybody who had any ideas for an agenda to send it to me and no one did. So  So we all forgot. Uh  From last time I wanted to - Uh The - An iss- uh one topic from last time. Right  s- O_K  so one item for an agenda is uh Jane has some uh uh some research to talk about  research issues. Um and And I have some short research issues. Uh  Adam has some short research issues. Um  I have a list of things that I think were done over the last three months I was supposed to send off  uh and  um I - I sent a note about it to uh - to Adam and Jane but I think I'll just run through it also and see if someone thinks it's inaccurate or uh insufficient. A list that you have to send off to who? Uh  to uh uh  I_B_M. Oh. @@ O_K. They're  you know - So. Um  So  uh so  I'll go through that. Um  And  Anything else? anyone wants to talk about? No. O_K. What about the  um - your trip  yesterday? Um. Sort of off-topic I guess. Cuz that's Cuz that was all - all about the  uh - Oh  O_K. I - I - I can chat with you about that off-line. That's another thing. Um  And  Anything else? Nothing else? Uh  there's a - I mean  there is a - a  um uh telephone call tomorrow  which will be a conference call that some of us are involved in for uh a possible proposal. Um  we'll talk - we'll talk about it next week if - if something - Do you want me to be there for that? I noticed you C_ C'ed me  but I wasn't actually a recipient. I didn't quite know what to make of that. Uh Well  we'll talk - talk about that after our meeting. O_K. O_K. Uh  O_K. So it sounds like the - the three main things that we have to talk about are  uh this list  uh Jane and - Jane and Adam have some research items  and  other than that  anything  as usual  anything goes beyond that. O_K  uh  Jane  since - since you were sort of cut off last time why don't we start with yours  make sure we get to it. O_K  it's - it's very eh - it's very brief  I mean - just let me - just hand these out. Oops. Is this the same as the email or different? It's slightly different. I - Thanks. O_K. basically the same. Same idea? But  same idea. So  if you've looked at this you've seen it before  so Basically  um as you know  uh part of the encoding includes a mark that indicates an overlap. It's not indicated with  um uh  tight precision  it's just indicated that - O_K  so  It's indicated to - to - so the people know what parts of sp- which - which stretches of speech were in the clear  versus being overlapped by others. So  I used this mark and  um and  uh uh  divided the - I wrote a script which divides things into individual minutes  of which we ended up with forty five  and a little bit. And  uh you know  minute zero  of course  is the first minute up to sixty seconds. O_K . And  um What you can see is the number of overlaps and then to the right  whether they involve two speakers  three speakers  or more than three speakers. And  um and  what I was looking for sp- sp- specifically was the question of whether they're distributed evenly throughout or whether they're bursts of them. Um. And it looked to me as though - uh  you know - y- this is just - eh - eh  this would - this is not statistically verified  but it did look to me as though there are bursts throughout  rather than being localized to a particular region. The part down there  where there's the maximum number of - of  um overlaps is an area where we were discussing whether or not it would be useful to indi- to s- to code stress  uh  sentence stress as possible indication of  uh information retrieval. So it's like  you know  rather  lively discussion there. What was - what's the - the parenthesized stuff that says  like - Oh  th- e- the first one that says six overlaps and then two point eight? That's the per cent. Mmm. So  six is  uh two point eight percent of the total number of overlaps in the session. Mm-hmm. Ah. Mm-hmm. At the very end  this is when people were  you know  packing up to go basically  there's this final stuff  I think we - I don't remember where the digits fell. I'd have to look at that. But the final three there are no overlaps at all. And couple times there are not. So  i- it seems like it goes through bursts Mm-hmm. but  um that's kind of it. Now  Mm-hmm. Another question is is there - are there individual differences in whether you're likely to be overlapped with or to overlap with others. And  again I want to emphasize this is just one particular um - one particular meeting  and also there's been no statistical testing of it all  but I  um I took the coding of the - I  you know  my - I had this script figure out  um who was the first speaker  who was the second speaker involved in a two-person overlap  I didn't look at the ones involving three or more. And  um this is how it breaks down in the individual cells of who tended to be overlapping most often with who - who else  and if you look at the marginal totals  which is the ones on the right side and across the bottom  you get the totals for an individual. So  um If you look at the bottom  those are the  um numbers of overlaps in which um Adam was involved as the person doing the overlapping and if you look - I'm sorry  but you're o- alphabetical  that's why I'm choosing you And then if you look across the right  then that's where he was the person who was the sp- first speaker in the pair and got overlap- Hmm! overlapped with by somebody. Mm-hmm. And  then if you look down in the summary table  then you see that  um th- they're differences in whether a person got overlapped with or overlapped Is this uh just raw counts or is it - by. Raw counts. So it would be interesting to see how much each person spoke. Mm-hmm. Yes  Yeah Yeah. very true - very true it would be good to normalize with respect to that. Now on the table I did Normalized to how much - Yeah take one step toward  uh away from the raw frequencies by putting  uh percentages. So that the percentage of time of the - of the times that a person spoke  what percentage eh  w- so. Of the times a person spoke and furthermore was involved in a two- two-person overlap  what percentage of the time were they the overlapper and what percent of the time were they th- the overlappee? And there  it looks like you see some differences  um  that some people tend to be overlapped with more often than they're overlapped  but  of course  uh i- e- this is just one meeting  uh there's no statistical testing involved  and that would be required for a - for a finding of any kind of S- so  i- it would be statistically incorrect to conclude from this that Adam talked too much or something. scientific reliability. No - no actually  that would be actually statistically correct  but No  no  no. Yeah  yeah. Yeah  yeah. Yeah  yeah. Yeah  that's right. That's right. And I'm you know  I'm - I don't see a point of singling people out  Yeah. Excuse me. now  this is a case where obviously - B- I - I - I rather enjoyed it  but - but this But the numbers speak for themselves. He's - Yeah  yeah  yeah. Well  you know  it's like - I'm not - I'm not Yes  that's right  so you don't nee- O_K. saying on the tape who did better or worse because I don't think that it's - I Sure. you know  and - and th- here's a case where of course  human subjects people would say be sure that you anonymize the results  and - and  so  might as well do this. Yeah  when - this is what - Yeah. This is actually - when Jane sent this email first  is what caused me to start thinking about anonymizing the data. Well  fair enough. Fair enough. And actually  you know  the point is not about an individual  it's the point about tendencies toward Yeah. you know  different styles  different speaker styles. Oh sure. And it would be  you know of course  there's also the question of what type of overlap was this  and w- what were they  and i- and I - and I know that I can distinguish at least three types and  probably more  I mean  the general cultural idea which w- uh  the conversation analysts originally started with in the seventies was that we have this strict model where politeness involves that you let the person finish th- before you start talking  and and you know  I mean  w- we know that - an- and they've loosened up on that too s- in the intervening time  that that that's - that's viewed as being a culturally- relative thing  I mean  that you have the high-involvement style from the East Coast where people will overlap often as an indication of interest in what the other person is saying. Uh-huh. And Yeah  exactly! Well  there you go. Fine  that's alright  that's O_K. Exactly! Yeah And - and  you know  in contrast  so Deborah - d- and also Deborah Tannen's thesis she talked about differences of these types  that they're just different styles  and it's um you - you can't impose a model of - there - of the ideal being no overlaps  and you know  conversational analysts also agree with that  so it's now  universally a- ag- agreed with. And - and  als- I mean  I can't say universally  but anyway  the people who used to say it was strict  um now  uh don't. I mean they - they also you know  uh uh  ack- acknowledge the influence of sub- of subcultural norms and cross-cultural norms and things. So  um Then it beco- though - so - just - just superficially to give um a couple ideas of the types of overlaps involved  I have at the bottom several that I noticed. So  uh  there are backchannels  like what Adam just did now and  um um  anticipating the end of a question and simply answering it earlier  and there are several of those in this - in these data where - Mm-hmm. because we're people who've talked to each other  um we know basically what the topic is  what the possibilities are and w- and we've spoken with each other so we know basically what the other person's style is likely to be and so and t- there are a number of places where someone just answered early. No problem. And places also which I thought were interesting  where two or more people gave exactly th- the same answer in unison - different words of course but you know  the - basically  you know everyone's saying ""yes"" or - you know  or ev- even more sp- specific than that. So  uh  the point is that  um overlap's not necessarily a bad thing and that it would be im- i- useful to subdivide these further and see if there are individual differences in styles with respect to the types involved. And that's all I wanted to say on that  unless people have questions. Well  of course th- the biggest  um result here  which is one we've - Yep. we've talked about many times and isn't new to us  but which I think would be interesting to show someone who isn't familiar with this is just the sheer number of overlaps. Yes  yes! Oh  O_K - interesting. That - that - Yeah. Right? that - that  um here's a relatively short meeting  it's a forty - forty plus minute Hundred ninety-seven. meeting  and not only were there two hundred and fifteen overlaps but  uh I think there's one - one minute there where there - where - where there wasn't any overlap? I mean  it's - S- n- are - uh throughout this thing? Well  at the bottom  you have the bottom three. It's - It'd be interesting - Yeah. So four - four minutes all together with none - none. You have - @@ Oh  so the bottom three did have s- stuff going on? But it w- Yes  uh-huh. Yeah. But just no overlaps. There was speech? O_K  so if - the - this - It'd be interesting to see what the total amount of time is in the overlaps  versus - Yes  exactly and that's - that's where Jose's pro- project comes in. I was about to ask - Yeah  yeah  Yeah. Yeah. I h- I have this- that infor- I have th- that information Hmm. now. Oh  about how much is it? The - the duration of eh - of each of the overlaps. O- oh  what's - what's the - what's the average length? M- I - I haven't averaged it now but  uh I - I will  uh You don't know? I will do the - the study of the - with the - with the program with the - uh  the different  uh the  nnn  distribution of the duration of the overlaps. O_K  you - you don- you don't have a feeling for roughly how much it is? Yeah. mmm  Because the - the uh  @@ is @@ . The duration is  uh the variation - the variation of the duration is uh  very big Yeah. Mm-hmm. on the dat- I suspect that it will also differ  but eh - depending on the type of overlap involved. So backchannels will be very brief and - Yeah. Oh  I'm sure. Because  on your surface eh a bit of zone of overlapping with the duration eh  overlapped and another very very short. Yeah. Yeah. Uh  i- probably it's very difficult to - to - because the - the overlap is  uh on- is only the - in the final ""S_"" of the - of the - the fin- the - the end - the end word of the  um previous speaker Mm-hmm. with the - the next word of the - the new speaker. Um  I considered that's an overlap but it's very short  it's an ""X_"" with a - and - the idea is probably  eh when eh - when eh  we studied th- th- that zone  eh eh  we h- we have eh eh confusion with eh eh noise. Mm-hmm. With eh that fricative sounds  but uh I have new information but I have to - to study . Yeah. Yeah  but I - I'd - Can I - u- go ahead . Yeah. You split this by minute  um so if an overlap Yes. straddles the boundary between two minutes  that counts towards both of those minutes. Mm-hmm. Actually  um um actually not. Uh  so le- let's think about the case where A_ starts speaking and then B_ overlaps with A_  and then the minute boundary happens. And let's say that after that minute boundary  um B_ is still speaking  and A_ overlaps with B_  that would be a new overlap. But otherwise um  let's say B_ comes to the conclusion of - of that turn without anyone overlapping with him or her  in which case there would be no overlap counted in that second minute. No  but suppose they both talk simultaneously both a - a portion of it is in minute one and another portion of minute two. O_K. In that case  um my c- the coding that I was using - since we haven't  uh incorporated Adam's  uh coding of overlap yets  the coding of Yeah  ""yets"" is not a word. Uh since we haven't incorporated Adam's method of handling overl- overlaps yet um then that would have fallen through the cra- cracks. It would be an underestimate of the number of overlaps because  um I wou- I wouldn't be able to pick it up from the way it was encoded so far. We just haven't done th- the precise second to sec- you know  I- I- second to second coding of when they occur. I- I- I'm - I'm - I'm confused now. So l- l- let me restate what I thought Andreas was saying and - and see. Uh-huh. Let's say that in - in second fifty-seven Yep. of one minute  you start talking and I start talking O_K. and we ignore each other and keep on talking for six seconds. Mm-hmm. So we go over - So we were - we were talking over one another  and it's just - in each case  it's just sort of one interval. Right? Mm-hmm? So  um we talked over the minute boundary. Is this considered as one overlap in each of the minutes  the way you have done this. No  it wouldn't. It would be considered as an overlap in the first one. O_K  so that's good  i- I think  in the sense that I think Andreas meant the question  right? That's - that's good  yeah  cuz the overall rate is - Yeah. @@ Statistical. Yeah. Mm-hmm. Yeah. They're not double counted. Yep. Other- otherwise you'd get double counts  here and there. Yeah. Ah but  yeah. @@ And then it would be harder - Yeah. Yeah. I should also say I did a simplifying  uh count in that if A_ was speaking B_ overlapped with A_ and then A_ came back again and overlapped with B_ again  I - I didn't count that as a three-person overlap  I counted that as a two-person overlap  Mm-hmm. and it was A_ being overlapped with by D_ . Because the idea was the first speaker had the floor and the second person started speaking and then the f- the first person reasserted the floor kind of thing. These are simplifying assumptions  didn't happen very often  there may be like three overlaps affected that way in the whole thing. Yeah. I want to go back and listen to minute forty-one. Yeah  yeah. Cuz i- i- I find it interesting that there were a large number of overlaps and they were all two-speaker. Yeah. I mean what I thought - what I would have thought in That's interesting. is that when there were a large number of overlaps  it was because everyone was talking at once  That's interesting. Yeah. Yeah. but uh apparently not. That's really neat. Mmm. Yeah  there's a lot of backchannel  a lot o- a lot of - Yeah. This is really interesting data. Yeah  it is. I think so too  I think - I think what's really interesting though  it is before d- saying "" yes  meetings have a lot of overlaps "" is to actually find out how many more we have than two-party. Cuz in two-party conversations  like Switchboard  there's an awful lot too if you just look at backchannels  if you consider those overlaps? it's also ver- it's huge. It's just that people haven't been looking at that because they've been doing single-channel processing for Mm-hmm. Mm-hmm? speech recognition. So  the question is  you know  how many more overlaps do you have of  say the two-person type  by adding more people. to a meeting  and it may be a lot more but i- it may - it may not be. Well  but see  I find it interesting even if it wasn't any more  So. because since we were dealing with this full duplex sort of thing in Switchboard where it was just all separated out we just - everything was just nice  so that - so the issue is in - in a situation Mm-hmm? Well  it's not really "" nice "". where th- that's - It depends what you're doing. So if you were actually having  uh - depends what you're doing  if - Right now we're do- we have individual mikes on the people in this meeting. Mm-hmm? So the question is  you know - ""are there really more overlaps happening than there would be in a two-person party "". Let - let m- And - and there well may be  but - let me rephrase what I'm saying cuz I don't think I'm getting it across. What - what I - what - I shouldn't use words like "" nice "" because maybe that's too - i- too imprecise. But what I mean is that  um in Switchboard  despite the many - many other problems that we have  one problem that we're not considering is overlap. And what we're doing now is  aside from the many other differences in the task  we are considering overlap and one of the reasons that we're considering it  you know  one of them not all of them  one of them is that w- uh at least  you know I'm very interested in the scenario in which  uh both people talking are pretty much equally audible  and from a single microphone. And so  in that case  it does get mixed in  and it's pretty hard to jus- to just ignore it  to just do processing on one and not on the other. I - I agree that it's an issue here but it's also an issue for Switchboard and if you think of meetings being recorded over the telephone  which I think  Mm-hmm. you know  this whole point of studying meetings isn't just to have people in a room but to also have meetings over different phone lines. Maybe far field mike people wouldn't be interested in that but all the dialogue issues still apply  Mm-hmm. so if each of us was calling and having a meeting that way you kn- you know like a conference call. And  just the question is  y- you know  in Switchboard you would think that's the simplest case of a meeting Mm-hmm. of more than one person  and I'm wondering how much more overlap of the types that - that Jane described happen with more people present. So it may be that having three people is very different from having two people or it may not be. That's an important question to ask. I think what I'm - So. All I'm s- really saying is that I don't think we were considering that in Switchboard. Not you  me. But uh - Were you? Though it wasn't in the design. but - but Were you - were you - were you - were you measuring it? I mean  w- w- were - There - there's actually to tell you the truth  the reason why it's hard to measure is because of so  from the point of view of studying dialogue  I mean  which Dan Jurafsky and Andreas and I had some projects on  Yeah. you want to know the sequence of turns. So what happens is if you're talking and I have a backchannel in the middle of your turn  Yeah. and then you keep going what it looks like in a dialogue model is your turn and then my backchannel  even though my backchannel occurred completely inside your turn. Yeah? So  for things like language modeling or dialogue modeling it's - We know that that's wrong Yeah? in real time. But  because of the acoustic segmentations that were done and the fact that some of the acoustic data in Switchboard were missing  people couldn't study it  but that doesn't mean in the real world that people don't talk that way. Yeah  I wasn't saying that. So  Right? I was just saying that w- now we're looking at it. it's - um And - and - and  you - you maybe wanted to look at it before but  for these various technical reasons in terms of how the data was you weren't. Well  we've als- Right. We're looking at it here. So that's why it's coming to us as new even though it may well be you know  if your - if your hypothes- The hypothesis you were offering Um. eh - Right? - if it's the null poth- hypothesis  and if actually you have as much overlap in a two-person  we don't know the answer to that. The reason we don't know the answer to is cuz it wasn't studied and it wasn't studied because it wasn't set up. Right? Yeah  all I meant is that if you're asking the question from the point of view of what's different about a meeting  Mm-hmm? studying meetings of  say  more than two people versus what kinds of questions you could ask with a two-person meeting. It's important to distinguish that  you know  this project is getting a lot of overlap but other projects were too  but we just couldn't study them. May have been. And and so uh May have been. Right? We do kn- we don't know the numbers. Well  there is a high rate  So. It's - but I don't know how high  in fact Well  here- I have a question. that would be See  I mean  i- i- le- let me t- interesting to know. I mean  my point was just if you wanted to say to somebody  "" what have we learned about overlaps here?"" just never mind comparison with something else  Mm-hmm. what we've learned about is overlaps in this situation  is that - the first - the first-order thing I would say is that there's a lot of them. Yeah. Right? In - in the sense that i- if you said Yeah  if - i- i- i- I - I don't di- In a way  I guess what I'm comparing to is more the I agree with that. common sense notion of how - how much people overlap. Uh you know the fact that when - when - when  uh  Adam was looking for a stretch of - of speech before  that didn't have any overlaps  and he w- he was having such a hard time and now I look at this and I go  ""well  I can see why he was having such a hard time"". It's happening a lot. Right. That's also true of Switchboard. It may not be - I wasn't saying it wasn't. Right. So it's just  Right? um I was commenting about this. O_K. I'm saying if I - All I'm saying is that from the I'm saying if I have this complicated thing in front of me  and we sh- which  you know we're gonna get much more sophisticated about when we get lots more data  But - Then  if I was gonna describe to somebody what did you learn right here  about  you know  the - the modest amount of data that was analyzed I'd say  ""Well  the first-order thing was there was a lot of overlaps "". In fact - and it's not just an overlap - bunch of overlaps - second-order thing is it's not just a bunch of overlaps in one particular point  Right. but that there's overlaps  uh throughout the thing. And that's interesting. That's all. Right. No  I - I agree with that. I'm just saying that it may - the reason you get overlaps may or may not be due to sort of the number of people in the meeting. Oh yeah. Yeah. And Yeah  I wasn't making any statement about that. that's all. And - and it would actually be interesting to find out because Yeah. some of the data say Switchboard  which isn't exactly the same kind of context  I mean these are two people who don't know each other and so forth  But we should still be able to somehow say what - what is the added contra- contribution to sort of overlap time of each additional person  or something like that. Yep. I could certainly see it going either way. O_K  now. Yeah  that would be good to know  but w- we - What - Wh- yeah  I - I agree - I agree with Adam. And the reason is because I think there's a limit - But yeah. Yeah. there's an upper bound on how many you can have  simply from the standpoint of audibility. When we speak we - we do make a judgment of Mm-hmm. Right. ""can -"" you know  as adults. I mean  children don't adjust so well  I mean  if a truck goes rolling past  adults will Mm-hmm. well  depending  but mostly  adults will - will - will hold off to what - to finish the end of the sentence till the - till the noise is past. And I think we generally do monitor things like that  about - whether we - whether our utterance will be in the clear or not. Right. And partly it's related to rhythmic structure in conversation  so  you know  you - you t- Yeah  this is d- also um  people tend to time their - their - Well - their  um when they come into the conversation based on the overall rhythmic  uh Right. uh  ambient thing. So you don't want to be c- cross-cutting. And - and  just to finish this  that um That I think that there may be an upper bound on how many overlaps you can have  simply from the standpoint of audibility and how loud the other people are who are already in the fray. But I - you know  of certain types. Now if it's just backchannels  people may be doing that with less intention of being heard  just sort of spontaneously doing backchannels  in which case that - those might - there may be no upper bound on those. I - I have a feeling that backchannels  which are the vast majority of overlaps in Switchboard  uh  don't play as big a role here  because it's very unnatural I think  to backchannel if - in a multi-audience - you know  in a multi-person If you can see them  actually. audience. It's interesting  so if you watch people are going like - Right. Right - right  like this here  but Yeah. u- That may not be the case if you couldn't see them. But - but  it's sort of odd if one person's speaking and everybody's listening  and it's unusual to have everybody going ""uh-huh  uh-huh"" Actually  I think I've done it a fair number of times today. @@ But. Yeah. There's a lot of head-nodding  Um. in this Yeah. Yep  we need to put trackers on it. @@ In - in the two-person - Yeah  yeah  yeah. Plus - plus - plus the - He could  he could. Yeah. So - so actually  um That's in part because the nodding  if you have visual contact  the nodding has the same function  but on the phone  in Switchboard you - you - that wouldn't work. So Yeah  you don't have it. so you need to use the backchannel. Your mike is - So  in the two-person conversations  when there's backchannel  is there a great deal of That is an earphone  so if you just put it so it's on your ear. overlap in the speech? or - Cuz my impression is sometimes There you go. Yes. Yeah. Thank you. E- for example. it happens when there's a pause  you know  like you - you Yes. get a lot of backchannel  when somebody's pausing Right. She's doing that. Sorry  what were you saying? It's hard to do both  huh? Um no  when - when - when there's backchannel  I mean  just - Oh. I was just listening  and - and when there's two people talking and there's backchannel it seems like  um the backchannel happens when  you know  the pitch drops and the first person - and a lot of times  the first person actually stops talking and then there's a backchannel and then they start up again  and so I'm wondering about - h- I just wonder how much overlap there is. Is there a lot? I think there's a lot of the kind that Jose was talking about  where - I mean  this is called ""precision timing"" in conversation analysis  where they come in overlapping  but at a point where the information is mostly complete. So all you're missing is some last syllables or something or the last word or some highly predictable words. Mmm. Mm-hmm. So technically  it's an overlap. But But maybe a - just a small overlap? you know  from information flow point of view it's not an overlap in the predictable information. More  yeah. It'd be interesting if we could do prediction. I was just thinking more in terms of alignment  alignment overlap. Yeah. Language model prediction of overlap  that would be really interesting. So - so - Yeah. Well  that's exactly  exactly why we wanted to study the precise timing of overlaps ins- in Right. Right. uh Switchboard  say  because there's a lot of that. So - so here's a - here's a first interesting labeling task. Mm-hmm. Uh  to distinguish between  say  backchannels precision timing - Sort of you know  benevolent overlaps  and - and - and w- and - Let's pick a different word. and sort of  um I don't know  hostile overlaps  where someone is trying to grab the floor from someone else. Yeah. Uh  that - that might be an interesting  um problem to look at. Hostile takeovers. Yeah. Yeah. Yeah. Well  I mean you could do that. I ju- I - I think that Yeah. in this meeting I really had the feeling that wasn't happening  that the hostile - hostile type. These were - these were O_K. benevolent types  as people Mm-hmm. finishing each other's sentences  and Um  I could imagine that as - there's a fair number of stuff. um cases where  and this is sort of  not really hostile  but sort of competitive  where one person is finishing something and you have  like  two or three people jumping - trying to - Trying to get the floor. trying to - trying to  uh grab the next turn. Yeah. And so it's not against the person who talks first because actually we're all waiting for that person to finish. But they all want to be next. I have a feeling most of these things are - that - that are not a benevolent kind are - are are  uh um are - are competitive as opposed to real- really - really hostile. But. Right. Yeah  I agree. I agree. I wonder what determines who gets the floor? Well  there are various things  you - you have the - I mean - Uh a vote - vote in Florida. Yeah. It's been studied a lot. Voting for - Um  o- one thing - I - I wanted to - or you can tell a good joke and then everybody's laughing and you get a chance to g- break in. But. But. Seniority. Um. You know  the other thing I was thinking was that  um these - all these interesting questions are  of course  pretty hard to answer with  uh u- you know  a small amount of data. @@ Ach. So  um I wonder if what you're saying suggests that we should make a conscious attempt to have  um a - a fair number of meetings with  uh a smaller number of people. Right? I mean we - most of our meetings are uh  meetings currently with say five  six  seven  eight people Should we really try to have some two-person meetings  or some three-person meetings and re- record them just to - to - to beef up the - the statistics on that? That's a control. Well  it seems like there are two possibilities there  I mean i- it seems like if you have just two people it's not really  y- like a meeting  w- is not as similar as the rest of the - of the sample. It depends on what you're after  of course  but It seems like that would be more a case of the control condition  compared to  uh an experimental Mm-hmm. condition  with more than two. Well  Liz was raising the question of - of whether i- it's the number - there's a relationship between the number of people and the number of overlaps or type of overlaps there  Mm-hmm. and  um If you had two people meeting in this kind of circumstance then you'd still have the visuals. You wouldn't have that difference also that you have in the say  in Switchboard data. Mm-hmm. Uh Yeah  I'm just thinking that'd be more like a c- control condition. Yeah. Mm-hmm. Well  but from the acoustic point of view  Yeah. it's all good. Is the same. Yeah  acoustic is fine  but - If - if the goal were to just look at overlap you would - you could serve yourself - save yourself a lot of time but not even transcri- transcribe the words. Yep. Well  I was thinking you should be able to do this from the acoustics  on the close-talking mikes  right? Well  that's - the - that was my - my status report  so Yeah. Right  I mean Adam was - You've been working on that. Yeah. Yeah. right. I mean  not as well as what - I mean  you wouldn't be able to have any kind of typology  obviously  but Once we're done with this stuff discussing  Yeah. Mm-hmm. you'd get some rough statistics. So. But - what - what do you think about that? Do you think that would be useful? I'm just thinking that as an action item of whether we should try to record some two-person meetings or something. I guess my - my first comment was  um only that um we should n- not attribute overlaps only to meetings  but maybe that's obvious  maybe Yeah. everybody knew that  but that in normal conversation with two people there's an awful lot of the same kinds of overlap  and that it would be interesting Mm-hmm. to look at whether there are these kinds of constraints that Jane mentioned  that what maybe the additional people add to this competition that happens right after a turn  you know  because now you can have five people trying to grab the turn  but pretty quickly there're - they back off and you go back to this sort of only one person at a time with one person Mm-hmm. interrupting at a time. So  I don't know. To answer your question I it - I don't think it's crucial to have controls but I think it's worth recording all the meetings we Can. can. Well  O_K. Yeah. So  um I - I have an idea. you know. D- I wouldn't not record a two-person meeting just because it only has two people. Right. Could we - Could we  um - we have - have in the past and I think continue - will continue to have a fair number of uh phone conference calls. Uh-huh. Yeah  we talked about this repeatedly. And  uh  and as a - to  um as another c- c- comparison condition  we could um see what - what what happens in terms of overlap  when you don't have visual contact. So  um - Can we actually record? It just seems like that's a very different thing than what we're doing. Uh I mean physically can we record the o- the other - Well  we'll have to set up for it. Yeah. Well  we're not really set up for it to do that. But. Or  Yeah. this is getting a little extravagant  we could put up some kind of blinds or something to - Barriers! Yeah. to remove  uh That's what they did on Map Task  visual contact. you know  this Map Task corpus? They ran exactly the same pairs of people with and without visual cues and it's quite interesting. Well  we - we record this meeting so regularly it wouldn't be that - I mean a little strange. O_K  we can record  but no one can look at each other. Well  we could just put b- blindfolds on. Yeah. Well y- no you - f- Yeah  Yeah. Close your eyes. Blindf- Turn off the lights. and we'd take a picture of everybody sitting here with blindfolds. That would - Oh  th- that was the other thing  weren't we gonna take a picture at the beginning of each of these meetings? Um  what - I had thought we were gonna do is just take pictures of the whiteboards. rather than take pictures of the meeting. Well  linguistic - And  uh - Yeah. Yes. Linguistic anthropologists would - would suggest it would be useful to There's a head nodding here vigorously  yeah. also take a picture of the meeting. The - Why - why do we want to have a picture of the meeting? Ee- you mean  transc- @@ because you get then the spatial relationship of the speakers. no - Well  you could do that by just noting on the enrollment sheet the - And that Yeah could be Yeah. Yeah. Seat number  that's a good idea. the seat number. I'll do that. I'll do that on the next set of forms. Yeah. Yeah. Is possible to get information from the rhythmic - f- from the ge-   eh uh  files. So you'd number them somehow. I finally remembered to put  uh We can - can't you figure it out from the mike number? put native language on the newer forms. No. O_K. The wireless ones. And even the jacks  I mean  I'm sitting here and the jack is over in front of you. Oh. But probably from these you could've infer it. Yeah  but It would be another task. It's - it would be trivial - It would be a research task. Having - having ground tu- truth would be nice  so seat number would be good. Yeah  yeah. You know where you could get it? Yeah. Beam-forming during the digit uh stuff. So I'm gonna put little labels on all the chairs with the seat number. That's a good idea. Mm-hmm. Not the chairs. The chairs are - But you have to keep the chairs in the same pla- like here. But  uh - Chairs are movable. Put them - The chair- Like  put them on the table where they - Yep. Yeah. Yep. Yeah. But you know  they - the - s- the linguistic anthropologists would say it would be good to have a digital picture anyway  because you get Just remembered a joke. a sense also of posture. Posture  and we could like  What people were wearing. Yeah. you know  block out the person's face or whatever but - The fashion statement. but  you know  these are important cues  I mean the - Oh  Andreas was - How big their heads are. the - how a person is sitting is - Yeah. But if you just f- Andreas was wearing that same old sweater again. But from one picture  I don't know that you really get that. Right? You'd want a video for that  I think. It'd be better than nothing  is - is - A video  yeah. i- Just from a single picture I think you can tell some aspects. Think so? I mean I - I could tell you I mean  if I- if I'm in certain meetings I notice that there are certain people who And - And - really do - eh - The body language is very uh - Hmm. is very interesting in terms of the dominance aspect. Yeah. And - and Morgan had that funny hair again. Yeah. Well  I mean you black out the - that part. But it's just  you know  the - the body Hmm. He agreed. you know? Of course  the - where we sit at the table  I find is very interesting  that we do tend to cong- to gravitate to the same place each time. Yeah. and it's somewhat coincidental. I'm sitting here so that I can run into the room if the hardware starts  you know  catching fire or something. Oh  no  you - you just like to be in charge  that's why you're sitting - I just want to be at the head of the table. Yeah. Take control. Speaking of taking control  you said you had some research to talk about . Yeah. Yeah  I've been playing with  um Yeah. uh  using the close-talking mike to do - to try to figure out who's speaking. So my first attempt was just using thresholding and filtering  that we talked about - about two weeks ago  and so I played with that a little bit  and it works O_ K  except that it's very sensitive to your choice of your filter width and your threshold. So if you fiddle around with it a little bit and you get good numbers you can actually do a pretty good job of segmenting when someone's talking and when they're not. But if you try to use the same parameters on another speaker  it doesn't work anymore  even if you normalize it based on the absolute loudness. But does it work for that one speaker throughout the whole meeting? It does work for the one speaker throughout the whole meeting. Um Pretty well. Pretty well. How did you do it Adam? How did I do it? What do you mean? Yeah. I mean  wh- what was the - The algorithm was  uh Yeah. take o- every frame that's over the threshold  and then median-filter it  Mm-hmm. and then look for runs. So there was a minimum run length  so that - Every frame that's over what threshold? A threshold that you pick. In terms of energy? Yeah. Ah! O_K. Say that again? Frame over fres- threshold. So you take a - each frame  and you compute the energy and if it's over the threshold you set it to one  and if it's under the threshold you set it to zero  Hmm. so now you have a bit stream of zeros and ones. O_K. And then I median - filtered that using  um a fairly long filter length. Uh well  actually I guess depends on what you mean by long  you know  tenth of a second sorts of numbers. Um and that's to average out you know  pitch  you know  the pitch contours  and things like that. And then  uh looked for long runs. O_K And that works O_ K  if you fil- if you tune the filter parameters  if you tune how long your median filter is and how high you're looking for your thresholds. Did you ever try running the filter before you pick a threshold? No. I certainly could though. But this was just I had the program mostly written already so it was easy to do. O_K and then the other thing I did  was I took Javier's speaker-change detector - acoustic- change detector  and I implemented that with the close-talking mikes  and unfortunately that's not working real well  and it looks like it's - the problem is - he does it in two passes  the first pass is to find candidate places to do a break. And he does that using a neural net doing broad phone classification and he has the the  uh one of the phone classes is silence. And so the possible breaks are where silence starts and ends. And then he has a second pass which is a modeling - a Gaussian mixture model. Um looking for uh whether it improves or - or degrades to split at one of those particular places. And what looks like it's happening is that the - even on the close-talking mike the broad phone class classifier's doing a really bad job. Who was it trained on? Uh  I have no idea. I don't remember. Hmm. Does an- do you remember  Morgan  was it Broadcast News? I think so  yeah. Um So  at any rate  my next attempt  which I'm in the midst of and haven't quite finished yet was actually using the uh  thresholding as the way of generating the candidates. Because one of the things that definitely happens is if you put the threshold low you get lots of breaks. All of which are definitely acoustic events. They're definitely someone talking. But  like  it could be someone who isn't the person here  but the person over there or it can be the person breathing. And then feeding that into the acoustic change detector. And so I think that might work. But  I haven't gotten very far on that. But all of this is close-talking mike  so it's  uh just - just trying to get some ground truth. Only with eh uh  but eh I - I - I think  eh when - when  y- I - I saw the - the - the - the speech from P_D_A and  eh close talker. I - I think the there is a - a great difference in the - in the signal. Oh  absolutely. So s- my intention for this is - is as an aide for ground truth. Um but eh I - not - but eh I - I - I mean that eh eh in the - in the mixed file you can find  uh zone with  eh great different  eh level of energy. Um I - I think for  eh algorithm based on energy  eh  that um h- mmm  - more or less  eh  like eh eh  mmm  first sound energy detector. Say it again? eh nnn. When y- you the detect the - the - the first at - at the end of - of the detector of  ehm princ- um. What is the - the name in English ? the - the  mmm  the de- detector of  ehm of a word in the - in the s- in - an isolated word in - in the background I'm - I'm not sure what you're saying  can you try - That  uh I mean that when - when you use  eh eh any I think he's saying the onset detector. Yeah. Onset detector  O_K. I - I think it's probably to work well eh  because  eh you have eh  in the mixed files a great level of energy. eh and great difference between the sp- speaker. And probably is not so easy when you use the - the P_D_A  eh that - Right. Because the signal is  eh the - in the e- energy level. in - in that  eh eh speech file is  eh more similar. Right. between the different eh  speaker  um But different speakers. I - I think is - eh  it will i- is my opinion. It will be  eh more difficult to - to detect bass-tone energy. the - the change. I think that  um In the P_D_A. Yeah. Yeah. And the - the another question  that when Ah  in the clo- in the P_D_ A  you mean? Absolutely. Yeah  no question. It'll be much harder. Much harder. Yeah. I review the - the - the work of Javier. I think the  nnn  the  nnn  that the idea of using a neural network to - to get a broad class of phonetic  eh from  eh uh a candidate from the - the - the speech signal. If you have  eh uh  I'm considering  only because Javier  eh only consider  eh like candidate  the  nnn  eh the silence  because it is the - the only model  eh - eh  he used that  eh eh nnn  Right. to detect the - the possibility of a - a change between the - between the speaker  Um another - another research thing  different groups  eh working  eh on Broadcast News prefer to  eh to consider hypothesis eh between each phoneme. Mm-hmm. Yeah  when a phone changes. Because  I - I - I think it's more realistic that  uh only consider the - the the - the silence between the speaker. Eh there - there exists eh silence between - between  eh a speaker. is - is  eh eh acoustic  eh event  important to - to consider. Mm-hmm. Mm-hmm. I - I found that the  eh silence in - in many occasions in the - in the speech file  but  eh when you have  eh eh  two speakers together without enough silence between - between them   eh I think eh is better to use the acoustic change detector basically and I - I - I I_X or  mmm  BIC criterion for consider all the frames Mm-hmm. in my opinion. Yeah  the - you know  the reason that he  uh just used silence was not because he thought it was better  Yeah. it was - it was - it was the place he was starting. Yep. So  he was trying to get something going  and  uh e- e- you know  as - as - Yeah. Yeah  yeah  yeah  yeah. as is in your case  if you're here for only a modest number of months you try to Yeah. Do something. pick a realistic goal  Yeah  But his - his goal was always to proceed from there to then allow broad category change also. yeah  yeah  yeah. Uh-huh. But  eh do - do you think that if you consider all the frames to apply the - the  eh the BIC criterion to detect the - the - the different acoustic change  eh between speaker  without  uh with  uh silence or with overlapping  uh  I think like - like  eh eh a general  eh eh way of process the - the acoustic change. In a first step  I mean. Mm-hmm. Mm-hmm. An- and then  eh eh without considering the you - you - you  um you can consider the energy like a- another parameter in the - in the feature vector  eh. Right. Absolutely. Mm-hmm. This - this is the idea. And if  if you do that  eh eh  with a BIC uh criterion for example  or with another kind of  eh of distance in a first step  and then you  eh you get the  eh the hypothesis to the - this change acoustic  eh to po- process Right. Because  eh eh  probably you - you can find the - the eh a small gap of silence between speaker with eh eh a ga- mmm  small duration Mm-hmm. Less than  eh two hundred milliseconds for example and apply another - another algorithm  another approach like  eh eh detector of ene-   eh detector of bass-tone energy to - to consider that  eh that  eh zone. of s- a small silence between speaker  or another algorithm to - to process  eh the - the segment between marks Mm-hmm. eh founded by the - the the BIC criterion and applied for - for each frame. I think is  eh Mm-hmm. nnn  it will be a- an - an - a more general approach the if we compare - with use  eh a neural net or another  eh speech recognizer with a broad class or - or narrow class  because  in my opinion eh it's in my opinion  eh Mm-hmm. if you - if you change the condition of the speech  I mean  if you adjust to your algorithm with a mixed speech file and to  eh Mm-hmm. to  eh adapt the neural net  eh used by Javier with a mixed file. With the what file? uh With a m- mixed file  ""Mixed"". with a - the mix  ""Mixed."" ""Mixed?"" mix. Sorry. And Mm-hmm. and then you - you  eh you try to - to apply that  eh  eh  eh  speech recognizer to that signal  to the P_D_A  eh speech file  I - I think you will have problems  because the - the - the - the condition you - you will need t- t- I - I suppose that you will need to - Well  I - Oh  absolutely. This is - this is not what I was suggesting to do. I - u- to - to retrain it. Look  I - I think this is a - Really? One - once - It's a - I used to work  like   on voiced - on voice silence detection  you know  and this is this kind of thing. Yeah. Um If you have somebody who has some experience with this sort of thing  and they work on it for a couple months  they can come up with something that gets most of the cases fairly easily. Then you say  ""O_K  I don't just wanna get most of the cases I want it to be really accurate."" Then it gets really hard no matter what you do. So  the p- the problem is is that if you say  ""Well I - I have these other data over here  that I learn things from  either explicit training of neural nets or of Gaussian mixture models or whatever."" Yeah. Uh Suppose you don't use any of those things. You say you have looked for acoustic change. Well  what does that mean? Yeah. That - that means you set some thresholds somewhere or something  right? Yeah. and - and so where do you get your thresholds from? From something that you looked at. So you always have this problem  you're going to new data um H- how are you going to adapt whatever you can very quickly learn about the new data? Uh  if it's gonna be different from old data that you have? And I think that's a problem Well  also what I'm doing right now is not intended to be an acoustic change detector for far-field mikes. What I'm doing with this. is trying to use the close-talking mike Yeah! and just use - Actually - You have candidates. Can- and just generate candidate and just try to get a first pass at something that sort of works. actually - actually - the candidate. I - to make marking easier. Or - Yeah. and I haven't spent a lot of time on it and I'm not intending to spend a lot of time on it. O_K. I - So. um  I  unfortunately  have to run  but  um I can imagine uh building a um model of speaker change detection that takes into account both the far-field and the Yep. uh actually  not just the close-talking mike for that speaker  but actually for all of th- Everyone else. for all of the speakers. Yeah. um If you model the - the effect that me speaking has on your microphone and everybody else's microphone  as well as on that  and you build  um - basically I think you'd - you would build a - an H_M_M that has as a state space all of the possible speaker combinations All the - Yep. Yeah. and  um you can control - It's a little big. It's not that big actually  um Two to the N_ . Two to the number of people in the meeting. But - Anyway. Actually  Andreas may- maybe - maybe just something simpler but - but Yeah. along the lines of what you're saying  I was just realizing  I used to know this guy who used to build  uh Mmm. um  mike mixers - automatic mike mixers where  you know  t- in order to able to turn up the gain  Yeah Mmm. Yeah. you know  uh Mm-hmm. as much as you can  you - you - you lower the gain on - on the mikes of people who aren't talking  right? Mm-hmm. And then he had some sort of reasonable way of doing that  but uh  what if you were just looking at very simple measures like energy measures but you don't just compare it to some threshold overall but you compare it to the energy in the other microphones. I was thinking about doing that originally to find out who's the loudest  and that person is certainly talking. Yeah. But I also wanted to find threshold - uh  excuse me  mol- overlap. Yeah. So  not just - just the loudest. Mm-hmm. But  eh I - I Sorry. I - I have found that when - when I Sorry  I have to go. O_K. I analyzed the - the speech files from the  eh mike  eh from the eh close eh microphone  eh I found zones Could you fill that out anyway? Just  put your name in. Are y- you want me to do it? I'll do it. But he's not gonna even read that. I know. Oh. with a - a different level of energy. including overlap zone. including. because  eh eh depend on the position of the - of the microph- of the each speaker to  eh  to get more o- or less energy i- in the mixed sign- in the signal. and then  if you consider energy to - to detect overlapping in - in  uh  and you process the - the - in - the - the - the speech file from the - the - the mixed signals. The mixed signals  eh. I - I think it's - it's difficult  um only to en- with energy to - to consider that in that zone We have eh  eh  overlapping zone Eh  if you process only the the energy of the  of each frame. Well  it's probably harder  but I - I think what I was s- nnn noting just when he - when Andreas raised that  was that there's other information to be gained from looking at all of the microphones and you may not need to look at very sophisticated things  Yeah. Yeah. because if there's - if most of the overlaps - you know  this doesn't cover  say  three  but if most of the overlaps  say  are two  Yeah. if the distribution looks like there's a couple high ones and - and the rest of them are low  you know  what I mean  there's some information there about their distribution even with very simple measures. Yeah. And everyone else is low  yeah. Yeah. Yeah. Yeah. Yeah. Uh  by the way  I had an idea with - while I was watching Chuck nodding at a lot of these things  is that we can all wear little bells on our heads  so that Yeah. Ding  ding  ding  ding. ""Ding"". That's cute! then you'd know that - Yeah. I think that'd be really interesting too  with blindfolds. Nodding with blindfolds  ""what are you nodding about?"" Then - Yeah. The question is  like whether - Well  trying with and - ""Sorry  I'm just - I'm just going to sleep."" with and without  yeah. But then there's just one @@   like. Yeah. Actually  I saw a uh - a woman at the bus stop the other day who  um  was talking on her cell phone speaking Japanese  and was bowing. Yeah Oh  yeah  that's really common. you know  profusely. Yeah. Yeah. Ah. Just  kept - Yeah. Wow. It's very difficult if you try - while you're trying  say  to convince somebody on the phone it's difficult not to move your hands. Not - Mm-hmm? You know  if you watch people they'll actually do these things. So. I still think we should try a - a meeting or two with the blindfolds  at least of this meeting that we have lots of recordings of Mm-hmm. Um  maybe for part of the meeting  we don't have to do it the whole meeting. Yeah  I think th- I think it's a great idea. That could be fun. It'll be too hard to make barriers  I was thinking because they have to go all the way W- Yeah. you know  I can see Chuck even if you put a barrier here. Well  we could just turn out the lights. Actually well also - I - I can say I made barr- barriers for - so that - the stuff I was doing with Collin wha- Y- Yeah? which just used  um this kind of foam board. R- really inexpensive. You can - you can masking tape it together  these are Yeah. you know  pretty l- large partitions. But then we also have these mikes  is the other thing I was thinking  so we need a barrier that doesn't disturb the sound  The acoustics. It's true  it would disturb the  um the - um the long-range - it would - Blindfolds would be good. I think  blindfolds. Probably we should wait until after Adam's set up the mikes  I mean  it sounds weird but - but - you know it's - But. it's cheap and  uh O_K. Be interesting to have the camera going. I think we're going to have to work on the  uh - I'll be peeking. on the human subjects form. Yeah  that's right  we didn't tell them we would be blindfolding. That's - ""Do you mind being blindfolded while you're interviewed?"" that's - that's - that's the one that we videotape. So. Um  I - I wanna move this along. Uh I did have this other agenda item which is  uh @@ - it's uh a list which I sent to uh - a couple folks  but um I wanted to get broader input on it  So this is the things that I think we did in the last three months obviously not everything we did but - but sort of highlights that I can - can tell s- some outside person  you know  what - what were you actually working on. Um in no particular order uh  one  uh  ten more hours of meeting r- meetings recorded  something like that  you know from - from  uh three months ago. Uh X_M_L formats and other transcription aspects sorted out and uh sent to I_B_M. Um  pilot data put together and sent to I_B_M for transcription  uh next batch of recorded data put together on the C_D-ROMs for shipment to I_B_M  Hasn't been sent yet  but - But yeah  that's why I phrased it that way  It's getting ready. yeah O_K. Um human subjects approval on campus  uh and release forms worked out so the meeting participants have a chance to request audio pixelization of selected parts of the spee- their speech. Um audio pixelization software written and tested. Um preliminary analysis of overlaps in the pilot data we have transcribed  and exploratory analysis of long-distance inferences for topic coherence  that was - I was - wasn't sure if those were the right way - that was the right way to describe that because of that little exercise that - that you and - and Lokendra did. What was that called? The  uh say again? I - well  I- I'm probably saying this wrong  but what I said was exploratory analysis of long-distance inferences for topic coherence. Something like that. Um so  uh I - a lot of that was from  you know  what - what - what you two were doing so I - I sent it to you  and you know  please mail me  you know  the corrections or suggestions for changing I - I don't want to make this twice it's length but - Mm-hmm. but you know  just im- improve it. Um Is there anything anybody - I - I did a bunch of stuff for supporting of digits. ""Bunch of stuff for s-"" O_K  maybe - maybe send me a sentence that's a little thought through about that. So  O_K  I'll send you a sentence that doesn't just say ""a bunch of""? ""Bunch of stuff""  yeah  Yep. "" stuff "" is probably bad too  ""Stuff"" is not very technical. Yeah  well. I'll try to phrase it in passive voice. Yeah. Yeah  yeah  Technical stuff. "" range of things ""  yeah. Um and - and you know  I sort of threw in what you did with what Jane did on - in - under the  uh uh preliminary analysis of overlaps. Uh Yeah. uh Thilo  can you tell us about all the work you've done on this project in the last  uh last three months? That's - So - what is - what - Um. Not really. Um  I didn't get it. Wh- what is ""audio pixelization""? It's too complicated. Uh  audio pix- wh- he did it  so why don't you explain it quickly? It's just  uh beeping out parts that you don't want included in the meeting so  you know you can say things like  ""Well  this should probably not be on the record  but beep"" O_K  O_K. I got that. Yeah. We - we - we spent a - a - a fair amount of time early on just talk- dealing with this issue about op w- e- e- @@ we realized  ""well  people are speaking in an impromptu way and they might say something that would embarrass them or others later""  and  O_K. how do you get around that so in the consent form it says  well you - we will look at the transcripts later and if there's something that you're O_K  and you can say - unhappy with  yeah. O_K. But you don't want to just totally excise it because um uh  well you have to be careful about excising it  how - how you excise it keeping the timing right and so forth so that at the moment tho- th- the idea we're running with is - is h- putting the beep over it. Yeah  you can either beep or it can be silence. I - I couldn't decide. O_K. Ah  yeah. which was the right way to do it. Beep is good auditorily  if someone is listening to it  there's no mistake that it's been beeped out  Yeah. Yeah. but for software it's probably better for it to be silence. No  no. You can - you know  you could make a m- Hmm. as long as you keep using the same beep  people could make a model of that beep  and - Yep. I like that idea. And I use - it's - it's  uh I think the beep is a really good idea. it's an A_ below middle C_ beep  so It's very clear. Then you don't think it's a long pause. Yeah. Also - Yeah  it's more obvious that there was something there than if there's just silence. Yeah  that - I mean  he's - he's removing the old thing and - and - and - Yeah. Yeah. Yeah Yep. Yea- right. Right. Yeah  it's not - But I mean if you just replaced it with silence  Yeah. it's not clear whether that's really silence or - Yeah. Yeah  I agree. Yep. @@ Yeah. One - one question. Do you do it on all channels? Yeah. Of course. Interesting. I like that. Yeah  I like that. Very clear. Yeah. Yeah you have to do it on all channels because it's  uh audible. Uh  it's - it's potentially audible  you could potentially recover it. Very clear. Ke- keep a back door. Well  the other thing that - you know  I mean the - the alternative might be to s- Yeah. Well  I - I haven't thrown away any of the meetings that I beeped. Actually yours is the only one that I beeped and then  uh the ar- DARPA meeting. Notice how quiet I am. Sorry  and then the DARPA meeting I just excised completely  so it's in a private directory. Yeah. You have some people who only have beeps as their speech in these meetings. That's great. Yeah. O_K. They're easy to find  then. Alright  so  uh I think we should  uh uh  go on to the digits? O_K. I have one concept a- t- I - I want to say  which is that I think it's nice that you're preserving the time relations  s- so you're - you're not just cutting - you're not doing scissor snips. You're - you're keeping the  uh Right. the time duration of a - de - deleted - deleted part. Yeah  definitely. Yeah. O_K  good  digits. Yeah  since we wanna possibly synchronize these things as well. Oh  I should have done that. Shoot. Oh well. It's great. Yeah. Oh- So I guess if there's an overlap  like  if I'm saying something that's bleepable and somebody else overlaps during it they also get bleeped  too? You'll lose it. There's no way around that. Yeah. Um I d- I did - before we do the digits  I did also wanna remind people  uh please do send me  you know  uh thoughts for an agenda  yeah that - that would be Agenda? Mm-hmm. Good. that'd be good. Eh So that  uh  people's ideas don't get yeah  well Thursday crept up on me this week. it does creep up  doesn't it? O_K. And  I wanted to say  I think this is really interesting It's cool stuff  definitely. Thank you. Thank you. analysis. I meant to say that before I started off on the Switchboard stuff. It's neat. I was gonna say ""can you do that for the other meetings  can you do it for them?"" And  no actually  you can't. Yeah. Does it take - Thank you. Actually - actually I - I thought that's what you were giving us was another meeting and I was like  ""Oh  O_K!"" ""Ooo  cool!"" Yeah. Aw  thanks. How long does it take  just briefly  like No. I have the script now  so  I mean  it can work off the  uh t- to - O_K. It's - to label the  O_K. other thing   As soon as we get labels  yep. But it has to be hand-labeled first? but - Uh  well  yeah. Because  uh well  I mean once his - his algorithm is up and running If it works well enough. then we can do it that way. Right now it's not. But I - I just worked off of my O_K. O_K  go ahead It's really neat. Thanks. Appreciate that. Not quite to the point where it works. I think - what I - what this has  uh  caused me - so this discussion caused me to wanna subdivide these further. I'm gonna take a look at the  uh backchannels  how much we have anal- I hope to have that for next time. Yeah  my - my algorithm worked great actually on these  That'd be interesting. but when you wear it like that or with the uh  lapel or if you have it very far from your face  that's when it starts failing. Mm-hmm. Well  I can wear it  Oh. I mean if you - It doesn't matter. O_K. I mean  we want it to work  right? I - I don't want to change the way we do the meeting. It's too late now. I feel like this troublemaker. It's uh - so  it was just a comment on the software  not a comment on prescriptions on how you wear microphones. O_K. Get the bolts  ""whh whh"" O_K  that's - let's - let's - let's do digits. Let's do it. O_K. O_K. Transcript one six one one  one six three O_. five three O_ six one eight five seven five O_ u- Strike that. seven five one O_ eight eight seven seven O_ nine eight O_ O_ zero zero zero nine one three nine two one three three four four two eight nine zero five nine seven eight zero nine eight six nine O_ two three eight five seven eight O_ five O_ nine four six five zero two zero seven one one seven five two nine two six zero zero zero seven two three four Transcript one eight five one dash one eight seven zero four six zero five five five nine eight seven six eight one four seven eight zero five four zero six four O_ one six O_ O_ zero seven zero zero four six five one two two two seven one six six three four five zero seven four seven two five four four eight three three one nine three four two one O_ zero nine nine one two four one Uh  transcript one eight three one  one eight five zero three six O_ eight four five six eight three seven nine two nine four eight one five O_ eight eight O_ three zero one nine O_ five six two three five four five three three five two seven O_ nine six five two O_ seven eight eight one two nine eight eight nine three six two three three nine O_ O_ six O_ three nine nine zero two four I'm sorry. two two four four three four zero Transcript one seven nine one dash one eight one zero one two zero zero five eight four three zero five one six six seven five five four zero eight nine O_ O_ three five eight four O_ one four three six nine two two four three seven two four seven five six seven zero five six nine one O_ five seven eight seven zero three eight one one seven two nine one O_ two O_K  transcript one eight one one dash one eight three zero two three zero six five four O_ six six one five four seven O_ six seven three seven eight nine O_ O_ eight nine five two nine seven O_ nine six four nine zero one zero three two four four five six O_ six eight six four seven seven four eight O_ nine zero zero one nine one two two eight eight two six zero four Transcript one seven seven one dash one seven nine zero zero zero five seven eight two zero two one three four eight three five eight six four five two O_ five five O_ six seven eight O_ four O_ three nine O_ two three O_ six four nine two zero one five four three two seven nine three four O_ O_ five O_ O_ four O_ three two seven one three four five three O_ eight two five eight one four six nine five five seven O_ eight nine zero nine Transcript one seven five one dash one seven seven zero zero one one four nine two eight eight seven eight nine seven three five four five O_ O_ six zero four zero one eight one two zero two seven six nine two three one five one eight O_ three three O_ two O_ five zero one two three O_ O_ three one eight four five two four six four seven five two two eight five one eight nine O_ O_K  thank you. Do you want us to put a mark on the bottom of these when they've actually been read  or do you just i- the only one that wasn't read is - is known  so we don't do it. O_K. ","A detailed diagram of the belief-net had already been disseminated. Its structure was discussed during the meeting. There are several endpoints (User  Ontology  Discourse etc) with separate EVA (Enter/View/Approach) values. Details of how different inputs feed into them were discussed at length. Ideas mentioned included grouping features of buildings like ""selling""  ""fixing"" and ""exhibiting""  as well as creating a User-compatibility node that would take different values depending on the situation and the user status. Similarly  a Go-there (towards a building) node can be influenced by things like the user's budget and discourse parameters amongst other things. The latter are still ill-defined at this stage. The study of the linguistic constructions that people use in this kind of navigational domain is expected to be prove useful in that respect. As each node in the tree is the decision point of the combination of its parent nodes  which rules govern this combination is an important issue. There are several approaches ranging from simply averaging the inputs to using a hidden variable in order to weight them differently depending on context. If the latter architecture is used  the net could -to an extent- be trained with the data that is currently being collected. Although this was mainly a brainstorming meeting  some minor tasks were allocated for the near future. Since the net architecture and possible decision algorithms were discussed  it is necessary to examine how much of this JavaBayes can accommodate and  if not  what modifications would be necessary. Additionally  the german partners visiting the institute will need to see some results of the new system design. Finally  the analysis of the linguistic constructions for the current research domain can begin even with limited data  as  at this stage  they need not be very detailed. The is only a diagrammatic view of how the decision tree for the EVA task looks like. A lot of the details have been glossed over: the user model can potentially comprise a huge number of factors; a planning ""go-there"" node needs input from several other areas of the net; there are intricate interactions between discourse and the situation model. Similarly  what discourse properties are of importance and how they influence EVA probabilities is still a mystery. On a more general note  there is also the question of whether the net should be updated continuously or only when it is needed. No final decision was taken as to the rules of computation applying in the belief-net. The more interesting solutions would ideally require training data  and it is still debatable whether the current collection would be appropriate for this particular task. In any case  how different architectures can be implemented in JavaBayes and what modifications would be necessary for the purposes of this project also need to be investigated. A simulator of the set of influence links forming the belief-net was created and put up for discussion. Different sections of the analysis  such as the user model  the ontology and the discourse are represented as a layer of nodes each with its own EVA probabilities. They form endpoints to which other nodes like Go-there  User_Budget  User_Thrift and Prosody feed into. The second presentation concerned the set of computational rules that are to be used with the net. The simple way to decide on the final output is the majority vote (which E  V or A form the majority of the parent nodes' outputs). This assumes that all inputs are of equal importance. Alternatively  each input can be weighted in a fixed way. A third option is to create a hidden variable that makes the decision of which of the inputs is more trusted in a particular situation. The same variable can potentially also change the weighting of each input. "
"Yeah  we had a long discussion about how much w- how easy we want to make it for people to bleep things out. So - Right. O_K. So this is - Morgan wants to make it hard. The  uh  counter is not moving again. It - It - it doesn't - It doesn't like me  but - Did - did - did it - ? I didn't even check yesterday whether it was moving. It didn't move yesterday either when I started it. Oh  it didn't? O_K. So. So I don't know if it doesn't like both of us - It didn't move on me on one other meeting and it did on one and both recorded the same so . Channel three? Channel three? You know  I discovered something yesterday on these  um  wireless ones. Channel two. Mm-hmm? You can tell if it's picking up breath noise and stuff. Yeah  it has a little indicator on it - on the A_F. Mm-hmm. Oh  really? Oh that's - the gains I've left here as the sort of normal ones  but - So if you - yeah  if you breathe under - breathe and then you see A_F go off  then you know it's p- picking up your mouth noise. Oh  that's good. Cuz we have a lot of breath noises. Yep. Test. In fact  if you listen to just the channels of people not talking  it's like "" @@ "". It's very disgust- Sorry. What? Did you see Hannibal recently or something? Exactly. It's very disconcerting. O_K. So  um  I was gonna try to get out of here  like  in half an hour  um  cuz I @@ really appreciate people coming  and the main thing that I was gonna ask people to help with today is to give input on what kinds of database format we should use in starting to link up things like word transcripts and annotations of word transcripts  so anything that transcribers or discourse coders or whatever put in the signal  with time-marks for  like  words and phone boundaries and all the stuff we get out of the forced alignments and the recognizer. So  we have this  um - I think a starting point is clearly the - the channelized output of Dave Gelbart's program  which Don brought a copy of  Yeah. Yeah  I'm - I'm familiar with that. I mean  we - I sort of already have developed an X_M_L format um  which - Can I see it? for this sort of stuff. And so the only question - is it the sort of thing that you want to use or not? Have you looked at that? I mean  I had a web page up. Right. So  I actually mostly need to be able to link up  or - So - You mean  this - I- it's - it's a question both of what the representation is and - I guess I am gonna be standing up and drawing on the board. O_K  yeah. So you should  definitely. Um  so - so it definitely had that as a concept. So tha- it has a single time-line  Mm-hmm. and then you can have lots of different sections  each of which have I_Ds attached to it  and then you can refer from other sections to those I_Ds  if you want to. So that  um - so that you start with - with a time-line tag. ""Time-line"". And then you have a bunch of times. I don't e- I don't remember exactly what my notation was  but it - Oh  I remember seeing an example of this. Yeah. Right  right. Yeah  ""T_ equals one point three two""  uh - And then I - I also had optional things like accuracy  and then ""I_D equals T_ one  uh  one seven"". And then  I also wanted to - to be i- to be able to not specify specifically what the time was and just have a stamp. Right. Yeah  so these are arbitrary  assigned by a program  not - not by a user. So you have a whole bunch of those. And then somewhere la- further down you might have something like an utterance tag which has ""start equals T_seventeen  end equals T_eighteen"". So what that's saying is  we know it starts at this particular time. We don't know when it ends. O_K. Right? But it ends at this T_eighteen  which may be somewhere else. We say there's another utterance. We don't know what the t- time actually is but we know that it's the same time as this end time. Mmm. You know   thirty-eight  whatever you want. So you're essentially defining a lattice. O_K. Yes  exactly. Yeah. And then  uh - and then these also have I_Ds. Right? So you could - you could have some sort of other - other tag later in the file that would be something like  um  oh  I don't know  uh  ""noise-type equals door-slam"". You know? And then  uh  you could either say ""time equals a particular time-mark"" or you could do other sorts of references. So - or - or you might have a prosody - ""Prosody"" right? D_? T_? It's an O_ instead of an I_  but the D_ is good. You like the D_? That's a good D_. Yeah. Um  you know  so you could have some sort of type here  and then you could have  um - the utterance that it's referring to could be U_seventeen or something like that. O_K. So  I mean  that seems - that seems g- great for all of the encoding of things with time and  Oh  well. um - I - I guess my question is more  uh  what d- what do you do with  say  a forced alignment? I mean you've got all these phone labels  and what do you do if you - How- how- just conceptually  if you get  um  transcriptions where the words are staying but the time boundaries are changing  cuz you've got a new recognition output  or s- sort of - what's the  um  sequence of going from the waveforms that stay the same  the transcripts that may or may not change  and then the utterance which - where the time boundaries that may or may not change - ? Oh  that's - That's actually very nicely handled here because you could - you could - all you'd have to change is the  Um. um  time-stamps in the time-line without - without  uh  changing the I_Ds. And you'd be able to propagate all of the - Right. That's  the who- that's why you do that extra level of indirection. the information? So that you can just change the time-line. Except the time-line is gonna be huge. If you say - suppose you have a phone-level alignment. You'd have - you'd have - Yes. Yeah  yeah  especially at the phone-level. The - we - we have phone-level backtraces. Yeah  this - I don't think I would do this for phone-level. Um - I think for phone-level you want to use some sort of binary representation because it'll be too dense otherwise. O_K. So  if you were doing that and you had this sort of companion  uh  thing that gets called up for phone-level  uh  what would that look like? How would you - ? I would use just an existing - Why- Mmm. But - but why not use it for phone-level? It's just a matter of - it's just a matter of it being bigger. But if you have - H- h- an existing way of doing it. you know  barring memory limitations  or- uh - I- w- I mean this is still the m- It's parsing limitations. I don't want to have this text file that you have to read in the whole thing to do something very simple for. Oh  no. You would use it only for purposes where you actually want the phone-level information  I'd imagine. So you could have some file that configures how much information you want in your - in your X_M_L or something. Right. I mean  you'd - y- I - I am imagining you'd have multiple versions of this depending on the information that you want. You - Um  cuz th- it does get very bush- with - Right. Um  I'm just - what I'm wondering is whether - I think for word-level  this would be O_K. Yeah. Yeah. Definitely. For word-level  it's alright. Mm-hmm. For lower than word-level  you're talking about so much data that I just - I don't know. I don't know if that - I mean  we actually have - So  one thing that Don is doing  is we're - we're running - Lattices are big  too. For every frame  you get a pitch value  and not only one pitch value but different kinds of pitch values depending on - Yeah  I mean  for something like that I would use P_file or - or any frame-level stuff I would use P_file. Meaning - ? Uh  that's a - well  or something like it. It's ICS- uh  ICSI has a format for frame-level representation of features. O_K. That you could call - that you would tie into this representation with like an I_D. Um. Right. Right. Or - or there's a - there's a particular way in X_M_L to refer to external resources. And - O_K. So you would say ""refer to this external file"". Um  so that external file wouldn't be in - So that might - that might work. But what - what's the advantage of doing that versus just putting it into this format? @@ More compact  which I think is - is better. I mean  if you did it at this - Uh-huh. I mean these are long meetings and with - for every frame  um - You don't want to do it with that - Anything at frame-level you had better encode binary or it's gonna be really painful. Or you just compre- I mean  I like text formats. Um  b- you can always  uh  G_zip them  and  um  you know  c- decompress them on the fly if y- if space is really a concern. Yeah  I was thi- I was thinking the advantage is that we can share this with other people. Well  but if you're talking about one per frame  you're talking about gigabyte-size files. You're gonna actually run out of space in your filesystem for one file. These are big files. These are really - I mean - Right? Because you have a two-gigabyte limit on most O_Ss. Right  O_K. I would say - O_K  so frame-level is probably not a good idea. But for phone-level stuff it's perfectly - And th- it's - Like phones  or syllables  or anything like that. Phones are every five frames though  so. Or something like that. But - but - but most of the frames are actually not speech. So  you know  people don't - Yeah  but we actually - v- Look at it  words times the average - The average number of phones in an English word is  I don't know  five maybe? So  look at it  t- number of words times five. That's not - that not - Oh  so you mean pause phones take up a lot of the - long pause phones. Yep. Exactly. Yeah. Yeah. O_K. That's true. But you do have to keep them in there. Y- yeah. O_K. So I think it - it's debatable whether you want to do phone-level in the same thing. But I think  a- anything at frame-level  even P_file   O_K. So - is too verbose. I would use something tighter than P_files. Do you - Are you familiar with it? I haven't seen this particular format  but - So. I mean  I've - I've used them. I don't know what their structure is. I've forgot what the str- O_K. But  wait a minute  P_file for each frame is storing a vector of cepstral or P_L_P values  right? It's whatever you want  actually. So that - what's nice about the P_file - It - i- Right. Built into it is the concept of frames  utterances  sentences  that sort of thing  that structure. And then also attached to it is an arbitrary vector of values. Oh. And it can take different types. So it - th- they don't all have to be floats. You know  you can have integers and you can have doubles  and all that sort of stuff. So that - that sounds - that sounds about what I w- Um. Right? And it has a header - it has a header format that describes it to some extent. So  the only problem with it is it's actually storing the utterance numbers and the frame numbers in the file  even though they're always sequential. And so it does waste a lot of space. Hmm. But it's still a lot tighter than - than ASCII. And we have a lot of tools already to deal with it. You do? O_K. Is there some documentation on this somewhere? O_K  great. So  Yeah  there's a ton of it. Man-pages and  uh  source code  and I mean  that sounds good. I - I was just looking for something - I'm not a database person  but something sort of standard enough that  me. you know  if we start using this we can give it out  other people can work on it  or - Is it - ? Yeah  it's not standard. I mean  it's something that we developed at ICSI. But  uh - But it's been used here and people've - But it's been used here and - and  you know  we have a well-configured system that you can distribute for free  and - I mean  it must be the equivalent of whatever you guys used to store feat- your computed features in  right? O_K. Yeah  th- we have - Actually  we - we use a generalization of the - the Sphere format. Mmm. Um  but - Yeah  so there is something like that but it's  um  probably not as sophist- And I think there's - Well  what does H_T_ K do for features? Or does it even have a concept of features? They ha- it has its own - I mean  Entropic has their own feature format that's called  like  S_- S_D or some so- S_F or something like that. Yeah. Yeah. I'm just wondering  would it be worth while to use that instead? Hmm? Yeah. Th- this is exactly the kind of decision - It's just whatever - But  I mean  people don't typically share this kind of stuff  right? I mean - Right. They generate their own. Actually  I - Yeah. I just - you know  we - we've done this stuff on prosodics and three or four places have asked for those prosodic files  and we just have an ASCII  uh  output of frame-by-frame. Which is fine  Ah  right. but it gets unwieldy to go in and - and query these files with really huge files. Right. I mean  we could do it. I was just thinking if there's something that - where all the frame values are - Hmm? And a- and again  if you have a - if you have a two-hour-long meeting  that's gonna - They're - they're fair- they're quite large. And these are for ten-minute Switchboard conversations  and - Yeah  I mean  they'd be emo- enormous. Right. So it's doable  it's just that you can only store a feature vector at frame-by-frame and it doesn't have any kind of  um - Is - is the sharing part of this a pretty important consideration or does that just sort of  uh - a nice thing to have? I - I don't know enough about what we're gonna do with the data. But I thought it would be good to get something that we can - that other people can use or adopt for their own kinds of encoding. And just  I mean we have to use some- we have to make some decision about what to do. And especially for the prosody work  what - Yeah. what it ends up being is you get features from the signal  and of course those change every time your alignments change. So you re-run a recognizer  you want to recompute your features  um  and then keep the database up to date. Or you change a word  or you change a Right. utterance boundary segment  which is gonna happen a lot. And so I wanted something where all of this can be done in a elegant way and that if somebody wants to try something or compute something else  that it can be done flexibly. Um  it doesn't have to be pretty  it just has to be  you know  easy to use  and - Yeah  the other thing - We should look at ATLAS  the NIST thing  Oh. Mmm. Uh - and see if they have anything at that level. I mean  I'm not sure what to do about this with ATLAS  because they chose a different route. I chose something that - Th- there are sort of two choices. Your - your file format can know about - know that you're talking about language and speech  which is what I chose  and time  or your file format can just be a graph representation. And then the application has to impose the structure on top. So what it looked like ATLAS chose is  they chose the other way  which was their file format is just nodes and links  and you have to interpret what they mean yourself. And why did you not choose that type of approach? Uh  because I knew that we were doing speech  and I thought it was better O_K. if you're looking at a raw file to be - t- for the tags to say ""it's an utterance""  as opposed to the tag to say ""it's a link"". O_K. But other than that  are they compatible? I mean  you could sort of - So  but - Yeah  they're reasonably compatible. I mean  you - you could - You could probably translate between them. Yeah  that's w- So  O_K. Yep. So  well  the other thing is if we choose to use ATLAS  which maybe we should just do  we should just throw this out before we invest a lot of time in it. I don't - So this is what the meeting's about  just sort of how to - Um  cuz we need to come up with a database like this just to do our work. And I actually don't Yeah. care  as long as it's something useful to other people  what we choose. So maybe it's - Yeah. maybe oth- you know  if - if you have any idea of how to choose  cuz I don't. The only thing - Yeah. Do they already have tools? I mean  I - I chose this for a couple reasons. One of them is that it's easy to parse. You don't need a full X_M_L parser. It's very easy to just write a Perl script to parse it. As long as uh each tag is on one line. Exactly. Exactly. Which I always do. And you can have as much information in the tag as you want  right? Well  I have it structured. Right? So each type tag has only particular items that it can take. Can you - But you can add to those structures Sure. if you - If you have more information. So what - Yeah. So - What NIST would say is that instead of doing this  you would say something like ""link start equals  um  you know  some node I_D  end equals some other node I_D""  and then ""type"" would be ""utterance"". Hmm. You know  so it's very similar. So why would it be a - a waste to do it this way if it's similar enough that we can always translate it? It probably wouldn't be a waste. It would mean that at some point if we wanted to switch  we'd just have to Write a translator. But it se- translate everything. But it - but that sounds - Since they are developing a big - But that's - I don't think that's a big deal. As long as it is - they're developing a big infrastructure. And so it seems to me that if - if we want to use that  we might as well go directly to what they're doing  rather than - If we want to - Do they already have something that's - that would be useful for us in place? Yeah. See  that's the question. I mean  how stable is their - Are they ready to go  or - ? The - I looked at it - The last time I looked at it was a while ago  probably a year ago  Hmm. uh  when we first started talking about this. And at that time at least it was still not very complete. And so  specifically they didn't have any external format representation at that time. They just had the sort of conceptual node - uh  annotated transcription graph  which I really liked. And that's exactly what this stuff is based on. Since then  they've developed their own external file format  which is  uh  you know  this sort of s- this sort of thing. Um  and apparently they've also developed a lot of tools  but I haven't looked at them. Maybe I should. @@ We should - we should find out. I mean  would the tools - would the tools run on something like this  if you can translate them anyway? Um  th- what would - would - would - what would worry me is that maybe we might miss a little detail I mean  that - I guess it's a question that - uh  yeah . It's a hassle if - that would make it very difficult to translate from one to the other. O_K. I - I think if it's conceptually close  and they already have or will have tools that everybody else will be using  I mean  O_K. Yeah  we might as well. it would be crazy to do something s- you know  separate that - Yep. Yeah. So I'll - I'll take a closer look at it. Actually  so it's - that - that would really be the question  is just what you would feel is in the long run the best thing. Cuz And - Right. The - once we start  sort of  doing this I don't - we don't actually have enough time to probably have to rehash it out again and - Yep. The other thing - the other way that I sort of established this was as easy translation to and from the Transcriber format. s- Right. Right. Um  but - I mean  I like this. This is sort of intuitively easy to actually r- read  as easy it could - as it could be. But  Yep. I suppose that as long as they have a type here that specifies ""utt""  um  It's almost the same. The - the - the - the point is - with this  though  is that you can't really add any supplementary information. it's - yeah  close enough that - Right? So if you suddenly decide that you want - You have to make a different type. Yeah. You'd have to make a different type. So - Well  if you look at it and - Um  I guess in my mind I don't know enough - Jane would know better  about the types of annotations and - and - But I imagine that those are things that would - well  you guys mentioned this  that could span any - it could be in its own channel  it could span time boundaries of any type  it could be instantaneous  things like that. Um  Right. and then from the recognition side we have backtraces at the phone-level. If - if it can handle that  it could handle states or whatever. Right. And then at the prosody-level we have frame - sort of like cepstral Yep. feature files  uh  like these P_files or anything like that. And that's sort of the world of things that I - And then we have the aligned channels  of course  and - Right. It seems to me you want to keep the frame-level stuff separate. Yeah. I - I definitely agree and I wanted to find actually a f- a nicer format or a - maybe a more compact format than what we used before. Just cuz you've got And then - Right. ten channels or whatever and two hours of a meeting. It's - it's a lot of - Now - now how would you - Huge. how would you represent  um  multiple speakers in this framework? Were - You would just represent them as - Um  You would have like a speaker tag or something? there's a spea- speaker tag up at the top which identifies them and then each utt- the way I had it is each turn or each utterance  I don't even remember now  had a speaker I_D tag attached to it. Mm-hmm. O_K. And in this format you would have a different tag  which - which would  uh  be linked to the link. Yeah. So - so somewhere else you would have another thing that would be  um - Let's see  would it be a node or a link? Um - And so - so this one would have  um  an I_D is link - link seventy-four or something like that. And then somewhere up here you would have a link that - that  uh  Mm-hmm. you know  was referencing L_seventy-four and had speaker Adam. Is i- ? Actually  it's the channel  I think  that - You know  or something like that. I mean  w- yeah  channel is what the channelized output out- Well  channel or speaker or whatever. It doesn't - This isn't quite right. I have to look at it again. Right. But - but - so how in the NIST format do we express Yeah  but - a hierarchical relationship between  um  say  an utterance and the words within it? So how do you tell that these are the words that belong to that utterance? Um  you would have another structure lower down than this that would be saying they're all belonging to this I_D. Mm-hmm. So each thing refers to the utterance that it belongs to. Right. And then each utterance could refer to a turn  and each turn could refer to something higher up. So it's - it's not hi- it's sort of bottom-up. And what if you actually have - So right now what you have as utterance  um  the closest thing that comes out of the channelized is the stuff between the segment boundaries that the transcribers put in or that Thilo put in  which may or may not actually be  like  a s- it's usually not - um  the beginning and end of a sentence  say. Well  that's why I didn't call it ""sentence"". So  right. Um  so it's like a segment or something. So  Yeah. I mean  I assume this is possible  that if you have - someone annotates the punctuation or whatever when they transcribe  you can say  you know  from - for - from the c- beginning of the sentence to the end of the sentence  from the annotations  this is a unit  even though it never actually - i- It's only a unit by virtue of the annotations at the word-level. Sure. I mean  so you would - you would have yet another tag. And then that would get a tag somehow. O_K. You'd have another tag which says this is of type ""sentence"". O_K. And  what - But it's just not overtly in the - O_K. Um  cuz this is exactly the kind of - So - I think that should be possible as long as the - But  uh  what I don't understand is where the - where in this type of file that would be expressed. Right. You would have another tag somewhere. It's - well  there're two ways of doing it. S- so it would just be floating before the sentence or floating after the sentence without a time-mark. You could have some sort of link type - type equals ""sentence""  and I_D is ""S_whatever"". And then lower down you could have an utterance. So the type is ""utterance"" - equals ""utt"". And you could either say that - No. I don't know - I take that back. So here's the thing. Um - See  cuz it's - Can you - can you say that this is Hhh. it's - You would just have a r- part of this  or do you say this is part of this? I think - S- But they're - You would refer up to the sentence. Well  the thing - they're actually overlapping each other  sort of. So - the thing is that some- something may be a part of Right. one thing for one purpose and another thing of another purpose. You have to have another type then  I guess. So f- s- Um  well  s- let's - let's ta- so let's - Well  I think I'm - I think w- I had better look at it again because I - I'm - Yeah. O_K. O_K. so - y- So for instance @@ sup- There's one level - there's one more level of indirection that I'm forgetting. Suppose you have a word sequence and you have two different segmentations of that same word sequence. f- Say  one segmentation is in terms of  um  you know  uh  sentences. And another segmentation is in terms of  um  I don't know  prosodic phrases. And let's say that they don't nest. So  you know  a prosodic phrase may cross Right. two sentences or something. I don't know if that's true or not but let's as- Well  it's definitely true with the segment. That's what I - exactly what I meant by the utterances versus the sentence could be sort of - Right. Yeah. So  you want to be s- you want to say this - this word is part of that sentence and this prosodic phrase. Yeah. Yeah. But the phrase is not part of the sentence and neither is the sentence part of the phrase. Right. I- I'm pretty sure that you can do that  but I'm forgetting the exact level of nesting. So  you would have to have two different pointers from the word up - one level up  one to the sent- So - so what you would end up having is a tag saying ""here's a word  and it starts here and it ends here"". @@ Right. And then lower down you would say ""here's a prosodic boundary and it has these words in it"". And lower down you'd have ""here's a sentence  and it has these words in it"". An- Right. Right. So you would be able to go in and say  you know  ""give me all the words in the bound- in the prosodic phrase and give me all the words in the -"" Yeah. Yep. So I think that's - that would wor- Let me look at it again. Um  O_K. Mm-hmm. O_K. The - the o- the other issue that you had was  how do you actually efficiently extract  um - So. That's good. find and extract information in a structure of this type? So you gave some examples like - Well  uh  and  I mean  you guys might - I don't know if this is premature because I suppose once you get the representation you can do this  but the kinds of things I was worried about is  No  that's not clear. I mean  yeah  you c- sure you can do it  but can you do it uh - Well  O_K. So i- if it - I- I mean  I can't do it  but I can - um  sort of l- l- you know  it - y- y- you gotta - you gotta do this - you - you're gonna want to do this very quickly Well - or else you'll spend all your time sort of searching through very complex data structures - Right. You'd need a p- sort of a paradigm for how to do it. But an example would be ""find all the cases in which Adam started to talk while Andreas was talking and his pitch was rising  Andreas's pitch"". That kind of thing. Right. I mean  that's gonna be - Is the rising pitch a feature  or is it gonna be in the same file? Well  the rising pitch will never be hand-annotated. So the - all the prosodic features are going to be automatically - So they're gonna be in those - But the - I mean  that's gonna be hard regardless  right? Because you're gonna have to write a program that goes through your feature file and looks for rising pitches. Yeah. So - Right. So normally what we would do is we would say ""what do we wanna assign rising pitch to?"" Are we gonna assign it to words? Are we gonna just assign it to sort of - when it's rising we have a begin-end rise representation? But suppose we dump out this file and we say  uh  for every word we just classify it as  w- you know  rise or fall or neither? O_K. Well  in that case you would add that to this format r- O_K. So we would basically be sort of  um  taking the format and enriching it with things that we wanna query in relation to the words that are already in the file  and then querying it. O_K. Right. You want sort of a grep that's - that works at the structural - You have that. on the structural representation. There's a standard again in X_M_L  specifically for searching X_M_L documents - structured X_- X_M_L documents  where you can specify both the content and the structural position. Yeah  but it's - it's not clear that that's - If - That's relative to the structure of the X_M_L document  not to the structure of what you're representing in the document. You use it as a tool. You use it as a tool  not an end-user. It's not an end-user thing. It's - it's - you would use that to build your tool to do that sort of search. Right. Right. Be- Uh - Because here you're specifying a lattice. So the underlying - that's the underlying data structure. And you want to be able to search in that lattice. But as long as the - It's a graph  but - That's different from searching through the text. But it seems like as long as the features that - Well  no  no  no. The whole point is that the text and the lattice are isomorphic. They represent each other completely. Um - So that - That's true if the features from your acoustics or whatever that are not explicitly in this are at the level of these types. I mean th- Hhh. That - that if you can Yeah  but that's gonna be the trouble no matter what. do that - Right? No matter what format you choose  you're gonna have the trou- you're gonna have the difficulty of relating That's right. That's true. That's why I was trying to figure out what's the best format for this representation. And it's still gonna be - the - the frame-level features - Yep. it's still gonna be  uh  not direct. You know  it - Hmm. Right. Or another example was  you know  uh  where in the language - where in the word sequence are people interrupting? So  I guess that one's actually easier. What about - what about  um  the idea of using a relational database to  uh  store the information from the X_M_L? So you would have - X_M_L basically would - Uh  you - you could use the X_M_L to put the data in  and then when you get data out  you put it back in X_M_L. So use X_M_L as sort of the - Transfer. the transfer format  uh  but then you store the data in the database  which The  uh - allows you to do all kinds of good search things in there. One of the things that ATLAS is doing is they're trying to define an A_P_I which is independent of the back store  Huh. so that  uh  you could define a single A_P_I and the - the storage could be flat X_M_L files or a database. Mm-hmm. My opinion on that is for the s- sort of stuff that we're doing  I suspect it's overkill to do a full relational database  that  um  But - just a flat file and  uh  search tools I bet will be enough. But that's the advantage of ATLAS  is that if we actually take - decide to go that route completely and we program to their A_P_I  then if we wanted to add a database later it would be pretty easy. Mm-hmm. Mm-hmm. It seems like the kind of thing you'd do if - I don't know  if people start adding all kinds of s- bells and whistles to the data. And so that might be - I mean  it'd be good for us to know - to use a format where we know we can easily  um  input that to some database if other people are using it. Yep. Something like that. I guess I'm just a little hesitant to try to go whole hog on sort of the - So - the whole framework that - that NIST is talking about  with ATLAS and a database and all that sort of stuff  cuz it's a big learning curve  Hmm. just to get going. Whereas if we just do a flat file format  Hmm. sure  it may not be as efficient but everyone can program in Perl and - and use it. @@ O_K. Right? So  as opposed to - But this is - I - I'm still  um  not convinced that you can do much at all on the text - on the flat file that - that - you know  the text representation. e- Because the text representation is gonna be  uh  not reflecting the structure of - of your words and annotations. It's just - it's - Well  if it's not representing it  then how do you recover it? Of course it's representing it. That's the whole point. No. You - you have to - what you have to do is you have to basically - Y- yeah. You can use Perl to read it in and construct a internal representation that is essentially a lattice. But  @@ Yeah. the - and then - O_K. Well  that was a different point. Right? So what I was saying Right. is that - But that's what you'll have to do. For Perl - if you want to just do Perl. If you wanted to use the structured X_M_L query language  that's a different thing. And it's a set of tools that let you specify given the D_- D_D_T - D_T_D of the document  Bec- be- um  what sorts of structural searches you want to do. So you want to say that  you know  you're looking for  um  a tag within a tag within a particular tag that has this particular text in it  um  and  uh  refers to a particular value. And so the point isn't that an end-user  who is looking for a query like you specified  wouldn't program it in this language. What you would do is  someone would build a tool that used that as a library. So that they - so that you wouldn't have to construct the internal representations yourself. Is a - See  I think the kinds of questions  at least in the next - to the end of this year  are - there may be a lot of different ones  but they'll all have a similar nature. They'll be looking at either a word-level prosodic  uh  an - a value  like a continuous value  like the slope of something. But Mm-hmm. you know  we'll do something where we - some kind of data reduction where the prosodic features are sort o- uh  either at the word-level or at the segment-level  or - or something like that. They're not gonna be at the phone-level and they're no- not gonna be at the frame-level when we get done with Right. sort of giving them simpler shapes and things. And so the main thing is just being able - Well  I guess  the two goals. Um  one that Chuck mentioned is starting out with something that we don't have to start over  that we don't have to throw away if other people want to extend it for other kinds of questions  Right. and being able to at least get enough  uh  information out on - where we condition the location of features on information that's in the kind of file that you put up there. And that would - that would do it  I mean  for me. Yeah. I think that there are quick and dirty solutions  and then there are long-term  big-infrastructure solutions. And so we want to try to pick something that lets us do a little bit of both. In the between  right. And especially that the representation doesn't have to be thrown away  even if your tools change. Um - Right. And so it seems to me that - I mean  I have to look at it again to see whether it can really do what we want  but if we use the ATLAS external file representation  um  it seems like it's rich enough that you could do quick tools just as I said in Perl  Yeah. and then later on if we choose to go up the learning curve  we can use the whole ATLAS inter- infrastructure  I mean  that sounds good to me. I - I don't - which has all that built in. So if - if you would l- look at that and let us know what you think. I mean  Sure. I think we're sort of guinea pigs  cuz I - I want to get the prosody work done but I don't want to waste time  you know  Oh  maybe - getting the - Yeah? um - Well  I wouldn't wait for the formats  because anything you pick we'll be able to translate Well - to another form. Ma- well  maybe you should actually look at it yourself too to get a sense of O_K. what it is you'll - you'll be dealing with  because  um  you know  Adam might have one opinion but you might have another  so Yeah. Yeah  definitely. I think the more eyes look at this the better. Especially if there's  e- um - you know  if someone can help with at least the - the setup of the right - Oh  hi. Hi  Jane. Mmm. the right representation  then  i- you know  I hope it won't - We don't actually need the whole full-blown thing to be ready  so. Can you - Oh  well. Um  so maybe if you guys can look at it and sort of see what  Sure. Yeah. um - I think we're - we're - we're actually just - yeah  wrapping up  but  um - Hmm. We're about done. Oh you are? Oh. Yeah  sorry  it's a uh short meeting  but  um - Well  I don't know. Is there anything else  like - I mean that helps me a lot  but - Well  I think the other thing we might want to look at is alternatives to P_file. I mean  th- the reason I like P_file is I'm already familiar with it  we have expertise here  and so if we pick something else  there's the learning-curve problem. But  I mean  it is just something we developed at ICSI. And so - Is there an - is there an I_P- A_P_I? Yeah. There's an A_P_I for it. And  uh  O_K. There used to be a problem that they get too large  a bunch of libraries  P_file utilities. and so basically the - uh the filesystem Well  that's gonna be a problem no matter what. You have the two-gigabyte limit on the filesystem size. wouldn't - And we definitely hit that with Broadcast News. Maybe you could extend the A_P_I to  uh  support  uh  like splitting up  you know  conceptually one file into smaller files on disk so that you can essentially  Yep. you know  have arbitrarily long f- Most of the tools can handle that. So that Yeah. we didn't do it at the A_P_I-level. We did it at the t- tool-level. That - that - most - many of them can s- you can specify several P_files and they'll just be done sequentially. O_K. So. So  I guess  yeah  if - if you and Don can - if you can show him the P_file stuff and see. So this would be like for the F_zero - Sure. I mean  if you do ""man P_file"" or ""apropos P_file""  you'll see a lot. True. I've used the P_file  I think. I've looked at it at least  briefly  I think when we were doing What does the P_ stand for anyway? s- something. I have no idea. Oh  in there. I didn't de- I didn't develop it. You know  it was - I think it was Dave Johnson. So it's all part of the Quicknet library. It has all the utilities for it. No  P_files were around way before Quicknet. Oh  were they? P_files were - were around when - w- with  um  RAP. Mm-hmm. It's like the history of ICSI. Like - Right? You worked with P_files. I worked with P_files. No. Mm-hmm. Yeah? I don't remember what the "" P_ "" is  though. Is it related to P_make? No. But there are ni- they're - The Quicknet library has a bunch of things in it to handle P_files  so it works pretty well. Yeah. And that isn't really  I guess  as important as the - the main - I don't know what you call it  the - @@ the main sort of word-level - Neither do I. Probably stands for ""Phil"". Phil Kohn. It's a Phil file? Yeah. That's my guess. Huh. O_K. Well  that's really useful. I mean  this is exactly the kind of thing that I wanted to settle. Um  so - Yeah  I've been meaning to look at the ATLAS stuff again anyway. So  just keep - Great. Yeah. I guess it's also sort of a political deci- I mean  if - if you feel like that's a community that would be good to tie into anyway  then it's - sounds like it's worth doing. Yeah  I think it - it w- j- I think there's - And  w- uh  as I said  I - what I did with this stuff - I based it on theirs. It's just they hadn't actually come up with an external format yet. So now that they have come up with a format  Mmm. it doesn't - it seems pretty reasonable to use it. But let me look at it again. O_K  great. As I said  that - Cuz we actually can start - There's one level - there's one more level of indirection and I'm just blanking on exactly how it works. I gotta look at it again. I mean  we can start with  um  I guess  this input from Dave's  which you had printed out  the channelized input. Cuz he has all of the channels  you know  with the channels in the tag and stuff like that. So that would be i- directly  Yeah  I've seen it. Yep. um - Easy - easy to map. Yeah. And so then it would just be a matter of getting - making sure to handle the annotations that are  you know  not at the word-level and  um  Where are those annotations coming from? t- to import the- Well  right now  I g- Jane would - would - Mm-hmm. Yeah. Are you talking about the overlap a- annotations? Yeah  any kind of annotation that  like  isn't already there. Uh  you know  anything you can envision. Yeah. So what I was imagining was - um  so Dave says we can have unlimited numbers of green ribbons. And so put  uh  a - a green ribbon on for an overlap code. And since we w- we - I - I think it's important to remain flexible regarding the time bins for now. And so it's nice to have - However  you know  you want to have it  uh  time- time- uh  located in the discourse. So  um  if we - if we tie the overlap code to the first word in the overlap  then you'll have a time-marking. It won't - it'll be independent of the time bins  however these e- evolve  shrink  or whatever  increase  or - Also  you could have different time bins for different purposes. And having it tied to the first word in an overlap segment is unique  uh  you know  anchored  clear. And it would just end up on a separate ribbon. So the overlap coding is gonna be easy with respect to that. Right. You look puzzled. I - I just - I don't quite understand what these things are. Uh. O_K. What  the codes themselves? Or the - ? Well  th- overlap codes. I'm not sure what that @@ - Well  I mean  is that - Well  we don't have to go into the codes. We don't have to go into the codes. But let me just - It probably doesn't matter. No  I d- I mean  it doesn't. I mean  that - not for the topic of this meeting. No. W- the idea is just to have a separate green ribbon  you know  and - and - and let's say that this is a time bin. There's a word here. This is the first word of an overlapping segment of any length  overlapping with any other  uh  word - uh  i- segment of any length. And  um  then you can indicate that this here was perhaps a ch- a backchannel  or you can say that it was  um  a usurping of the turn  or you can - you know  any - any number of categories. But the fact is  you have it time-tagged in a way that's independent of the  uh  sp- particular time bin that the word ends up in. If it's a large unit or a small unit  or we sh- change the boundaries of the units  Mm-hmm. Right. it's still unique and - and  uh  fits with the format  flexible  all that. Um  it would be nice - um  eh  gr- this is sort of r- regarding - uh  uh it's related but not directly germane to the topic of discussion  but  when it comes to annotations  um  you often find yourself in the situation where you have different annotations of the same  say  word sequence. Yeah. O_K? And sometimes the word sequences even differ slightly because they were edited s- at one place but not the other. Yeah. So  once this data gets out there  some people might start annotating this for  I don't know  dialogue acts or  um  you know  topics or what the heck. You know  there's a zillion things that people might annotate this for. And the only thing that is really sort of common among all the versi- the various versions of this data is the word sequence  Yep. or approximately. Or the time. Or the times. But  see  if you'd annotate dialogue acts  you don't necessarily want to - or topics - you don't really want to be dealing with time-marks. I guess. You'd - it's much more efficient for them to just see the word sequence  right? Mm-hmm. I mean  most people aren't as sophisticated as - as we are here with  you know  uh  time alignments and stuff. So - So the - the - the point is - Should - should we mention some names on the people who are n- ? Right. So  um  the p- my point is that you're gonna end up with  uh  word sequences that are differently annotated. And you want some tool  uh  that is able to sort of merge these different annotations back into a single  uh  version. O_K? Um  and we had this problem very massively  uh  at S_R_I when we worked  uh  a while back on  uh - well  on dialogue acts as well as  uh  you know  um  what was it? uh  Well  all the Switchboard in it. Yeah. utterance types. There's  uh  automatic  uh  punctuation and stuff like that. Because we had one set of annotations that were based on  uh  one version of the transcripts with a particular segmentation  and then we had another version that was based on  uh  a different s- slightly edited version of the transcripts with a different segmentation. So  we had these two different versions which were - you know  you could tell they were from the same source but they weren't identical. So it was extremely hard to reliably merge these two back together to correlate the information from the different annotations. Yep. I - I don't see any way that file formats are gonna help us with that. It's - it's all a question of semantic. No. No. But once you have a file format  I can imagine writing - Mm-hmm. not personally  but someone writing a tool that is essentially an alignment tool  Yeah. um  that mediates between various versions  and - uh  sort of like th- uh  you know  you have this thing in UNIX where you have  uh  Diff. W_diff or diff. Yeah. diff. There's the  uh  diff that actually tries to reconcile different - Is it S_diff ? two diffs f- based on the same original. Yep. Mmm. Something like that  um  but operating on these lattices that are really what's behind this - uh  this annotation format. Yep. So - You could definitely do that with the - There's actually a diff library you can use to do things like that that - So somewhere in the A_P_I you would like to have like a merge or some - some function that merges two - so you have different formats. Yeah  I think it's gonna be very hard. Any sort of structured two versions. Right. anything when you try to merge is really  really hard because you ha- i- The hard part isn't the file format. The hard part is specifying what you mean by ""merge"". Is - Exactly. But the one thing that would work here actually for i- that is more reliable than the utterances is the - And that's very difficult. the speaker ons and offs. So if you have a good  um - But this is exactly what I mean  is that - that the problem i- Yeah. You just have to know wha- what to tie it to. Yeah  exactly. The problem is saying ""what are the semantics  what do you mean by ""merge""?"" So. And - Right  right. Right. So - so just to let you know what we - where we kluged it by  uh  doing - uh  by doing - Hhh. Both were based on words  so  bo- we have two versions of the same words intersp- you know  sprinkled with - with different tags for annotations. And then you did diff. Yeah  that's just what I thought. And we did diff. Exactly! And that's how - Yeah. That's just wh- how I would have done it. But  you know  it had lots of errors and things would end up in the wrong order  and so forth. Uh  so  Yep. um  if you had a more - Uh  it - it was a kluge because it was basically reducing everything to - uh  to - uh  uh  to textual alignment. A textual - Um  so - But  d- isn't that something where whoever - if - if the people who are making changes  say in the transcripts  cuz this all happened when the transcripts were different - ye- um  if they tie it to something  like if they tied it to the acoustic segment - if they - You know what I mean? Then - Or if they tied it to an acoustic segment and we had the time-marks  that would help. But the problem is exactly as Adam said  that you get  Yep. you know  y- you don't have that information or it's lost in the merge somehow  so - Well  can I ask one question? It - it seems to me that  um  we will have o- an official version of the corpus  which will be only one - one version in terms of the words - where the words are concerned. We'd still have the - the merging issue maybe if coding were done independently of the - And you're gonna get that because if the data gets out  people will do all kinds of things to it. But - but - And  uh  s- you know  several years from now you might want to look into  um  the prosody of referring expressions. And someone at the university of who knows where has annotated the referring expressions. So you want to get that annotation and bring it back in line with your data. Right. O_K  then - But unfortunately they've also hand-edited it. O_K? But they've also - Exactly. And so that's exactly what we should - somehow when you distribute the data  say that - you know  Yeah. Yep. Right. Well  then the - that - have some way of knowing how to merge it back in and asking people to try to do that. What's - what's wrong with doing times? I - I agree. That was what I was wondering. Uh  yeah  time is the - Time is passing! Time is unique. You were saying that you didn't think we should - Andreas was saying - Yeah. Well  Time - time - times are what if they haven't notated with them  times? ephemeral. Yeah. He - he's a language modeling person  though. Um - So - so imagine - I think his - his example is a good one. Imagine that this person who developed the corpus of the referring expressions didn't include time. Mm-hmm. Yeah. Ach! He included references to words. He said that at this word is when - when it happened. Or she. Well  then - Yeah. Or she. But then couldn't you just indirectly figure out the time tied to the word? But still they - Exactly. Yeah. Sure. But what if - what if they change the words? Not - Well  but you'd have some anchoring point. He couldn't have changed all the words. But can they change the words without changing the time of the word? Sure. But they could have changed it a little. The - the point is  that - that they may have annotated it off a word transcript that isn't the same as our word transcript  so how do you merge it back in? I understand what you're saying. Mmm. Mm-hmm. And I - I guess the answer is  um  it's gonna be different every time. It's j- it's just gonna be - Yeah. Yeah. You only know the boundaries of the - I- it's exactly what I said before  which is that ""what do you mean by ""merge""?"" So in this case where you have the words and you don't have the times  well  what do you mean by ""merge""? If you tell me what you mean  I can write a program to do it. Right. Right. You can merge at the level of the representation that the other person preserved and that's it. Right. And that's about all you can do. And beyond that  all you know is - is relative ordering and sometimes even that is wrong. So - so in - so in this one you would have to do a best match between the word sequences  So. Mm-hmm. extract the times f- from the best match of theirs to yours  and use that. And then infer that their time-marks are somewhere in between. Yeah  exactly. Right. But it could be that they just - uh  I mean  it could be that they chunked - they - they lost certain utterances and all that stuff  or - Right  exactly. So it could get very  very ugly. Definitely. Definitely. Yeah. Alright. Well  I guess  w- I - I didn't want to keep people too long and Adam wanted t- people - I'll read the digits. If anyone else offers to  that'd be great. And That's interesting. Ah  well. For th- for the - for the benefit of science we'll read the digits. Yeah. if not  I guess - More digits  the better. O_K  this is Thanks - thanks a lot. It's really helpful. I mean  Adam and Don will sort of meet and I think that's great. Very useful. transcript two seven three one  two seven five O_. eight five O_ five one nine five zero six one O_ seven eight zero Go next. one two zero two four four three four four two seven five five six six six O_ O_ nine eight five seven six five eight eight four nine O_ O_ one nine one one five six one four two seven four eight five six zero three four four five O_ five six seven. Transcript one five one one dash one five three zero. Let me - zero one O_ six six seven three nine three one zero five eight eight one four five three eight five nine four three four six six O_ one three four eight seven eight zero one one two four three eight O_ seven two nine. Scratch that. nine O_ seven two nine zero one one three nine four two two five three eight seven O_ four five seven two two eight one zero nine five one O_ nine eight zero. Transcript one four five one  one four seven O_. nine five two O_ O_ three O_ four seven s- zero seven one three six one nine O_ two two three O_ six O_ seven three two Oh  right. four zero one zero four six two O_ one seven five one eight five one two four five five. nine five seven one O_ O_ four nine zero one zero eight one seven nine zero three three six one three four four five eight zero five two five one six six zero seven seven four three seven eight. Transcript number three three three one  three three five zero. one three seven seven eight one zero two five nine five five three eight four eight nine two five six seven zero five one nine one O_ three three zero eight five eight three one four O_ one two three O_ three six four zero six one three zero seven six five eight three seven nine nine three O_ zero. Transcript s- thirty-six eleven  thirty-six thirty. two two five two five nine O_ three four two four five six seven O_ eight nine four eight two six O_ three zero eight eight zero one two three four six one seven five seven eight four five three seven nine one nine five seven zero. O_ O_ two five zero one O_. Transcript two nine seven one dash two nine nine zero. seven seven nine eight nine nine O_ two O_ one one zero one two four five two three seven four four five six seven O_ nine one four six zero O_ three nine four zero one eight one nine nine five eight two eight three seven five nine O_ three four O_ seven three six two six two six seven three  zero one. ",The Berkeley Meeting Recorder Group discussed the progress of several of their members. The progress being made on the group's main project  a speech recogniser for the cellular industry was reported. The group also touched upon matters that had broader implications for the work  such as the work of other groups on the same project. There were also some progress reports from group members working on other projects. No one from the group attended a recent video conference about their main project  but they need to find out what was discussed in it. Until they do  they will continue on  assuming nothing major has been changed. Need to discuss any new investigation with partners to make sure work is not repeated. There was a recent video conference meeting discussing the cellular project  but no one from the group attended and so do not know if it has any implications for their work  if any important decisions were made. This includes decisions on the desired latency for the system  since the group is currently at the limit. Spectral subtraction  which the group is currently investigating as a method of dealing with noise  may add to the delay time  but also it is hard to do with non-linear noise. Speakers mn007 and fn002 have been working on the groups main project  looking for bugs in the system  and trying to improve latency. The group's work currently has the highest latency on the project  and they are looking for ways to cut the delays. These include replacing FIR filters with IIR  and investigating spectral subtraction methods which do not require taking the future into account. Speaker me006 has put together a proposal to extend work on a multiband system using low-level detectors  and applying it to recognition. Speaker me026 has been looking at method for recognition using far mics  trying to deal with reverberation and echo-cancellation. 
"Alright. So. So are you - Are we going? It is uh  must be February fifteenth. Yeah. Yu- I think the date's written in there  yep. And actually if everyone could cross out the R_nine next to ""Session""  and write M_R eleven. Yeah. Yeah. We didn't have a front-end meeting today. And let's remember also to make sure that one's gets marked as unread  unused. O_K. M_R eleven. M_R eleven. That sounds like a spy code. Mmm. O_K. So. There's lots of clicking I'm sure as I'm trying to get this to work correctly. Agenda. Any agenda items today? I wanna talk a little bit about getting - how we're gonna to get people to edit bleeps  parts of the meeting that they don't want to include. What I've done so far  and I wanna get some opinions on  how to - how to finish it up. O_K. I wanna ask about um  some aud- audio monitoring on some of the um well some of the equipment. In particular  the - well uh  that's just what I wanna ask. O_K audio monitoring  Jane. Ba- based on some of the tran- uh - i- In listening to some of these meetings that have already been recorded there are sometimes big spikes on particular things  and in pact - in fact this one I'm talking on is one of - of the ones that showed up in one of the meetings  so I - Mm-hmm. Yeah. Oh really. ""Spikes""  you mean like uh  instantaneous click type spikes  or - ? Spikes? Clicks. Yeah. Hmm. Yeah. Huh. And I don't know what the e- electronics is but. Yeah. Yeah. Well  I think it's Touching. uh  it - it could be a number of things. It could be touching and fiddling  and the other thing is that it could - the fact that it's on a wired mike is suspicious. It might be a connector. Yeah. Oh  O_K. Well maybe - Then we don't really have to talk about that as an - I - I take that off the agenda. You could try an experiment and say ""O_K  I'm about to test for spikes""  and then wiggle the thing there  and then Yeah. Right. go and when they go to transcribe it  it could  ask them to come and get you. ""Come get me when you transcribe this and see if there's spikes."" Oh that - Well  Um. No I'm just - @@ O_K. I mean  were this a professional audio recording  what we would do - what you would do is - in testing it is  you would actually do all this wiggling and make sure that - that - that things are not giving that kind of performance. And if they are  then they can't be used. Right. So. Um. Let's see. I guess I would like to have a discussion about you know where we are on uh  recording  transcription you know  basically you know where we are on the corpus. Good. And then um  the other thing which I would like to talk about which is a real meta-quest  I think  deal is  uh  agendas. So maybe I'll - I'll start with that actually. Uh  um. Andreas brought up the fact that he would kinda like to know  if possible  what we were gonna be talking about because he's sort of peripherally involved to this point  and if there's gonna be a topic about - discussion about something that he uh strongly cares about then he would come and - And I think part of - part of his motivation with this is that he's trying to help us out  in the - because of uh the fact that the meetings are - are tending to become reasonably large now on days when everybody shows up and so  he figures he could help that out by not showing - and - and I'm sure help Mmm. out his own time. by not showing up if it's a meeting that he's - he's - So  uh in order - I'd - I think that this is a wish on his part. Uh. It's actually gonna be hard because it seems like a lot of times uh things come up that are unanticipated and - and - But um  Right. we could try anyway  uh  do another try at coming up with the agenda uh  at some point before the meeting  uh  say the day before. Well maybe it would be a good idea for one of us to like on Wednesday  or Tuesday send out a reminder for people to send in agenda items. Yeah. O_K. You - you wanna volunteer to do that? Sure. O_K. Alright so we'll send out agenda request. Uh. Let me That'll be - I think that'll help - I'll put that on my spare brain or it will not get done. That'll help a lot  actually. Yeah  I have to tell you for the uh - for the admin meeting that we have  Lila does that um every time before an admin meeting. And uh  she ends up getting the agenda requests uh  uh ten minutes before the meeting. But - but - But. Uh. But we can try. Maybe it'll work. Mmm. Yeah. Maybe. Weirder things have happened. Yeah. I'm wondering if he were to just  uh  specify particular topics  I mean. Maybe we'd be able to meet that request of his a little more. I would - I would also guess that as we get more into processing the data and things like that there'll be more things of interest to him. Well then - Yeah. Actually it - This - this maybe brings up another topic which is um - So we're done with that topic. The other topic I was thinking of was the sta- status on microphones and channels  and all that. Yeah  actually I - I was going to say we need to talk about that too. Yeah. Why - why don't we do that. O_K. Um  the new microphones  the two new ones are in. Um. And they are being assembled as we speak  I hope. And I didn't bring my car today so I'm gonna pick them up tomorrow. Um  and then the other question I was thinking about is - well  a couple things. First of all  if the other headsets are a lot more comfortable  we should probably just go ahead and get them. So we'll have to evaluate that when they come in  and get people's opinions on - on what they think of them. Uh- Um  then the other question I had is maybe we should get another wireless. Another wireless setup. I mean it's expensive  but it does seem to be better than the wired. So how many channels do you get to have in a wireless setup? Um  well  I'm pretty sure that you can daisy-chain them together so what we would do is replace the wired mikes with wireless. So we currently have one base station with six wireless mike  possibility of six wireless receivers  and apparently you can chain those together. And so we could replace our wired mikes with wireless if we bought another base station and more wireless mikes. So  um. And - So let's see we - So  you know it's still  it's fifteen minus six. Right? So we could have up to nine. And right now we can have up to six. Right. And we have five  we're getting one more. Yeah. And it's um  about nine hundred dollars for the base station  and then eight hundred per channel. Oh. So yeah so the only - Beyond the mike - the cost of the mikes the only thing is the base station that's nine hundred dollars. Right. Oh  we should do it. O_K. Yeah. O_K  so I'll look into how you daisy-chain them and - and then just go ahead and order them. Yeah. I don't quite understand how that - how that works . If - So we're not increasing the number of channels. No  we're just replacing the wired - the two wired that are still working  O_K. O_K. I see. along with a couple of the wired that aren't working  one of the wired that's not working  with a wireless. Yeah. Basically we found - Three wireds work  right? I - I guess three wireds work  yeah. Yeah. Yeah. Yeah. But we've had more problems with that. Yep. And that sort of bypasses the whole - the whole Jimbox thing and all that. Right. And so um  we - we seem to have uh  a reliable way of getting the data in  which is through the ra- Sony radio mikes  as long as we're conscious about the batteries. That seems to be the key Right. Everyone's battery O_K? issue. I checked them this morning  they should be. O_K. Yeah. Um  That's the only thing with them. But the quality seems really good and - Um I heard from U_W that they're - they're uh very close to getting their  uh setup purchased. They're - they're - they're buying something that you can just sort of buy off the shelf. Well we should talk to them about it because I know that S_R_I is also in the process of looking at stuff  and so  you know  what we should try to keep everyone - on the same page with that. Yeah. S_R_I  really? Yeah. Oh. They got sa- apparent- Well  Maybe this needs to be bleeped out? I have no clue. Uh  I don't know. Probably we shouldn't - probably we shouldn't talk about funding stuff. I don't know how much of it's public. Right. Yeah. But anyway there's - there's - there's uh  uh other activities that are going on there and - and uh - and NIST and U_W. So. Um. But - but yeah I thin- I think that at least the message we can tell other people is that our experience is - is quite positive with the Sony  uh  radio-mikes . Right. Now the one thing that you have said that actually concerns me a little is you're talking about changing the headsets meaning changing the connector  which means some hand-soldering or something  right? Uh  no  we're having the - them do it. So it's so- hand-soldering it  but I'm not doing it. No? Oh. O_K. So  they - they charge right. Nothing against you and your hand-soldering but - You've never seen my hand-soldering. But uh  Uh  O_K  so that's being done professionally and - Yeah. a- as I said they're coming in. I - I mean - Yeah. I mean. Yeah. As professionally as I guess you can get it done. Well  it could - if they do a lot of it  it's - I mean i- it's just their repair shop. Right? Their maintenance people. Well  we'll see what it - it's like. That - tha- that can be quite good. Yep. Th- this - Yeah  O_K. Good. Yeah. So let's go with that. Uh  And  I mean we'll see  tomorrow  you know  what it looks like. Yeah. So  um  uh  Dave isn't here but he was going to start working on some things with the digits. Uh  so he'll be interested in what's going on with that. I guess - Was - the decision last time was that the - the uh transcribers were going to be doing stuff with the digits as well? Mm-hmm. Yeah. Has that started  or is that - ? Uh  it would be to use his interface and I was going to meet with him today about that. Right  so  the decision was that Jane did not want the transcribers to be doing any of the paperwork. So I did the - all that last week. So all the - all the forms are now on the computer. And uh  then I have a bunch of scripts that we'll read those and let the uh transcribers use different tools. And I just want to talk to Jane about how we transition to using those. Mm-hmm. So he has a nice set up that they - it w- it will be efficient for them to do that. O_K. So anyway - I - I don't think it'll take too long. So  you know  just uh  a matter of a few days I suspect. So anyway I think we - we have at least one uh  user for the digits once they get done  which will be Dave. Right. O_K. I've already done five or six sets. So if he wanted to  you know  just have a few to start with  he could. Yeah  he might - he might be asking - You know  and I also have a bunch of scripts that will  like  generate P_files and run recognition on them also. Right. O_K. Uh  is Dave - I don't know if Dave is on the list  if he's invited to these meetings  uh if he knows. I don't tend to get an invitation myself for them even. No  no. Uh  we don't have a active one but I'll make sure he's on the list. Yeah. Should we call him? I mean is he - d- is he definitely not available today? I don't know. Should I call his office and see? Uh  well i- it's uh - He was in. Yeah. I mean  he's still taking classes  so uh  he may well have conflicts. Yeah. Yeah  he was in s- He wasn't there at cof- Yeah  so this might be a conflict for him. Yeah. O_K. Yeah. O_K. Uh  so. Yeah didn't he say his signal-processing class was like I think he has a class. Yeah. Tuesdays and Thursdays? Yeah. He might have. Oh  O_K. You talking about David Gelbart? Oh well  whatever. Yeah. Yes. Yeah. Yeah. Yeah. Yeah  I think he's taking two twenty-five A_ which is now. O_K. So. Yeah. O_K. Yeah. So  that's why we're not seeing him. O_K. Uh  transcriptions  uh  beyond the digits  where we are  and so on. O_K. Um - And the - and the recordings also  just where we are. Yeah. Well  so um  should we - we don't wan- wanna do the recording status first  or - ? Well  we have about thirty-two hours uh as of  I guess a week and a half ago  so we probably now have about thirty- five hours. And - and that's - that's uh - How much of that is digits? It's uh - that's including digits  right? That's including digits. I haven't separated it out so I have no clue how much of that is digits. So - Yeah. So anyway there's at least probably thirty hours  or something of - There's got to be more than thirty hour - i- it couldn't - of - Of - of non-digits? Mmm. Of non-digits. Yeah  absolutely. I mean  the digits don't take up that much time. Yeah  yeah. O_K. O_K  and the transcribers h- I  uh  don't have the exact numbers  but I think it would come to about eleven hours that are finished uh  transcribing from them right now. The next step is to - that I'm working on is to insure that the data are clean first  and then channelized. What I mean by clean is that they're spell-checked  that the mark-up is consistent all the way throughout  and also that we now incorporate these additional conventions that uh  Liz requested in terms of um  um in terms of having a s- a systematic handling of numbers  and acronyms which I hadn't been specific about. Um  for example  i- they'll say uh ""ninety-two"". And you know  so how - you could - Nine two  right. e- Exactly. So if you just say ""nine two""  the - there are many s- ways that could have been expressed. An- and I just had them - I - I mean  a certain number of them did put the words down  but now we have a convention which also involves having it followed by  um  a gloss th- and things. You know  Jane? Mm-hmm. Um  one suggestion and you may already be doing this  but I've noticed in the past that when I've gone through transcriptions and you know in - in order to build lexicons and things  if you um  just take all the transcriptions and separate them into words and then alphabetize them  a lot of times just scanning down that list you'll find a lot of inconsistencies and mis- Misspelled. Yeah. You're talking about the type token frequency listings  and I use those too. Y- you mean just uh on each - on each line there's a one word right? It's one token from the - from the corpus. Mm-hmm. Mm-hmm. Yeah  those are e- extremely efficient and I- and I - I agree that's a very good use of it. Oh so you already have that  O_K. Well that's - that's a way - that's - You know  the spell-check basically does that but - but in addition - yes  that's - that's exactly the strategy I wanna do in terms of locating these things which are you know colloquial spoken forms which aren't in the lexicon. Mm-hmm. Cuz a lot of times they'll appear next to each other  Exactly. And then you ca- then you can do a s- Yeah. and uh  i- in alphabetized lists  they'll appear next to each other Absolutely. I agree. and - and so it makes it easier. That's a very good - that's a very good uh  suggestion. And that was - that's my strategy for handling a lot of these things  in terms of things that need to be glossed. I didn't get to that point but - So there are numbers  then there are acronyms  and then um  there's a - he- she wants the uh  actually a - an explicit marker of what type of comment this is  so i- curly b- inside the curly brackets I'm gonna put either ""VOC"" for vocalized  like cough or like laugh or whatever  ""NONVOC"" for door-slam  and ""GLOSS"" for things that have to do with - if they said a s- a spoken form with this - m- this pronunciation error. I already had that convention but I - Right. Oh that's great. I haven't been asking these people to do it systematically cuz I think it most - ha- most efficiently handled by uh - by a - a filter. That was what I was always planing on. So that  you know you get a whole long list - exactly what you're saying  you get a whole list of things that say ""curly bracket laugh curly bracket""  Mm-hmm. then y- you know it's - it's - You - you risk less error if you handle it by a filter  than if you have this transcriber ch- laboriously typing in sort of a VOC space  so man- Yeah. So many ways that error prone. Right. Right. So  um  um I'm - I'm going to convert that via a filter  into these tagged uh  subcategorized comments  and same thing with you know  we see you get a subset when you do what you're saying  you end up with a s- with uh  you're collapsing across a frequency you just have the tokens and you can um  have a filter which more efficiently makes those changes. But the numbers and acronyms have to be handled by hand  because  you know I mean  jus- Mm-hmm. Mm-hmm. You don't know what they could be. Yeah now TIMIT's clear um and P_L_P is clear but uh there are things that are not so well known  in - or - or have variant - u- u- uses like the numbers you can say ""nine two"" or you can say ""ninety-two""  and uh Yeah. So how are you doing the - I'd handle the numbers individually. How are you doing the uh  acronyms so if I say P_Z_M what would it appear on the transcript? It would be separate - The letters would be separated in space and potentially they'll have a curly bracket thing afterwards e- but I'm not sure if that's necessary  clarifying what it is  so gloss of whatever. O_K. Mm-hmm. Right. I don't know if that's really necessary to do that. Maybe it's a nice thing to do because of it then indicating this is uh  a step away from i- indicating that it really is intentional that those spaces are there  and indicating why they're there to indicate that it's uh the Mm-hmm. you know  uh enumerated  or i- it's not a good way of saying - but it's - it's the specific uh way of stating these - these letters. Right. So it sounds good. And so anyway  the clean - those are those things and then channelized is to then um  get it into this multichannel format. And at that point then it's ready for use by Liz and Don. But that's been my top priority - beyond getting it tanel- channelized  the next step is to work on tightening up the boundaries of the time bins. Yeah. Right. And uh  Thilo had a - e- e- a breakthrough with this - this last week in terms of getting the channel-based um uh s- s- speech-nonspeech segmentation um  up and running and I haven't - I haven't been able to use that yet cuz I'm working s- re- this is my top priority - get the data clean  and channelized. I actually gave Have you also been doing spot checks  Jane? Oh yes. Well you see that's part of the cleaning process. I spent Okay  good. um actually um I have a segment of ten minutes that was transcribed by two of our transcribers  Oh good. Good. and I went through it last night  it's - it's almost spooky how similar these are  word for word. And there are some differences in commas cuz commas I - I left them discretion at commas. Right. Uh - and so because it's not part of our st- of our ne- needed conventions. Mm-hmm. And um  and - so they'll be a difference in commas  but it's word-by-word the same  in - in huge patches of the data. And I have t- ten minute stretch where I can - where I can show that. And - and sometimes it turns out that one of these transcribers has a better ear for technical jargon  and the other one has a better ear for colloquial speech. So um  the one i- i- the colloquial speech person picked up ""gobbledy-gook"". And the other one didn't. And on this side  this one's picking up things like ""neural nets"" and the one that's good on the sp- o- on th- the vocabulary on the uh colloquial didn't. Hmm. Right. When - for the person who missed ""gobbledy-gook"" what did they put? It was an interesting approximation  put in parentheses  cuz I have this convention that  i- if they're not sure what it was  they put it in parentheses. Oh. So they tried to approximate it  but it was - Oh good. it was spelled G_A_B_B_L - Sort of how it sounds. Yes. More of an attempt to - I mean apparently it was very clear to her that these - the- a- this - this was a sound - these are the sounds  but - Yeah. It was a technical term that she didn't recognize  Yeah. Yeah. But she knew that she didn't know it. Maybe it was a technical ter- exactly. But she - even though her technical perception is just really - uh you know I've - I'm tempted to ask her if she's taken any courses in this area or if she's taken cognitive science courses then cuz ""neural nets"" and - oh she has some things that are - oh ""downsampled""  she got that right. Right. Hmm. And some of these are rather uh unexpected. Obscure  yeah. But ch- ten solid uh - m- ch- s- chunk of ten solid minutes where they both coded the same data. And um - And - and again the main track that you're working with is elev- eleven hours? Is that right? Yes exactly. Yeah  O_K. And that's part of this - Eleven hours. Is that - is that - that including digits? Yes it is. Mm-hmm. Yeah. So let's say roughly ten hours or so of - I mean it's probably more than that but - but with - of - of non-digits. It'd be more than that because I - my recollection is the minutes - that da- digits don't take more than half a minute. Per person. Yeah. Oh  O_K. But um the - the total set that I gave them is twelve hours of tape  Oh  I see. But they haven't gotten to the end of that yet. So they're still working - some of them are - Two of them are still working on completing that. Oh  I see. Yeah. Boy  they're moving right along. Yeah. They are. Mm-hmm. They're very efficient. Yeah. There're some who have more hours that they devote to it than others. Yeah. Mm-hmm. Mm-hmm. So what - what - what's the deal with - with your - The channel u- thing? Yeah. Oh  it's just uh  I ran the recognizer - uh  the speech-nonspeech detector on different channels and  it's just in uh - in this new multi-channel format and output  and Oh  I see. I just gave one - one meeting to - to Liz who wanted to - to try it for - for the recognizer as uh  apparently the recognizer had problems with those long chunks of speech  which took too much memory or whatever  Right. Yeah. and so she - she will try that I think and - I'm - I'm working on it. So  I hope - Is this anything different than the H_M_M system you were using before? Yeah. No. Uh  I mmm  use some - some different features but not - not - The basic thing is this H_M_M base. Mm-hmm. So there's still no - no knowledge using different channels at the same time. You know what I mean? There is some  uh as the energy is normalized across channels yeah. Across all of them. O_K. So. But basically that's one of the main changes. Mm-hmm. Mm-hmm. What are some of the other features? Besides the energy? You said you're trying some different features  or something. Oh I just uh - Mmm  I just use um our loudness-based things now as they - before there were - they were some in - in the log domain and I - I changed this to the - Cu- Cube root? to the - Yeah. To - No  I changed this to the - to the - to the loudness thingy with the - with the how do you call it? I'm not sure. Hmm. Ah. With the  uh - Fletcher Munson? No. I'm not sure about the term. Oh  O_K. Uh  I'll look it up. And say it to you. Yeah  alright. Uh  O_K  and - Yeah. That's - that's basically the - the - the thing. O_K. Yeah  and I - and I tried t- to normalize uh - uh the features  there's loudness and modified loudness  um  within one channel  because they're  yeah to - to be able to distinguish between foreground and background speech. And it works quite well. But  not always. Uh-huh. Uh-huh. So. O_K. Good. Um  let's see. I think the uh - Were - were you basically done with the transcription part? So I guess the next thing is this uh - bleep editing. Right. So the - The idea is that we need to have - We need to provide the transcripts to every participant of every meeting to give them an opportunity to bleep out sections they don't want. So I've written a bunch of tools that will generate web pages  uh with the transcription in it so that they can click on them and piece - pieces and they can scroll through and read them  and then they can check on each one if they want it excluded. And then  it's a form  H_T_M_L form  so they can submit it and it will end up sending me email with the times that they want excluded. And so  uh  some of the questions on this is what do we do about the privacy issue. Yeah. And so I thought about this a little bit and I think the best way to do it is every participant will have a password  a single password. Each person will have a single password  user name and password. And then each meeting  we'll only allow the participants who were at that meeting to look at it. And that way each person only has to remember one password. I - I can't help but wonder if this is maybe a little more elaborate than is needed. I mean if people have - Uh  I mean  for me I would actually want to have some pieces of paper that had the transcription and I would sort of flip through it. And then um if I thought it was O_K  I'd say ""it's O_K"". Mm-hmm. And  I - uh - I mean it depends how this really ends up working out  but I guess my thought was that the occasion of somebody wondering whether something was O_K or not and needing to listen to it was gonna be extremely rare. Right  I mean so th- th- th- the fact that you could listen to it over the web is a minor thing that I had already done for other reasons. O_K. And so that - that's a minor part of it  I just wanted some web interface so that people - you didn't actually have to send everyone the text. So m- what my intention to do is that as the transcripts become ready  um I would take them  and generate the web pages and send email to every participant or contact them using the contact method they wanted  and just uh  tell them  ""here's the web page""  um  ""you need a password"". So th- th- question number one is how do we distribute the passwords  and question number two is how else do we wanna provide this information if they want it. That's - I think what I was sort of saying is that if you just say ""here is a - here is -"" I mean this maybe it sounds paleolithic but - but I just thought if you handed them some sheets of paper  that said  uh  ""here's what was said in this transcription is it O_K with you? and if it is  here's this other sheet of paper that you sign that says that it's O_K"". And then they'd hand it back to you. I think that um there are a subset of people who will want printouts that we can certainly provide. But certainly I wouldn't want a printout. These are big  and I would much rather be ha- be able to just sit and leaf through it. You find it easier to go through a large - I mean how do you read books? Well I certainly read books by hand. But for something like this  I think it's easier to do it on the web. Really? I mean  it - Cuz you're gonna get  you know  if I - I'm - I'm in a bunch of meetings and I don't wanna get a stack of these. I wanna just be able to go to - go to the web site and visit it as I want. Going to a web site is easy  but flipping through a hundred pounds - a hundred pages of stuff is not easy on the web. Well  I don't think it's that much harder than  paper. I have one question. So are you thinking that um the person would have a transcript and go strictly from the transcript? Because I - I do think that there's a benefit to being able to hear the Really? So. tone of voice and the - So here's the way I was imagining it  and maybe I'm wrong  but the way I imagined it was that Yeah. um  the largest set of people is gonna go ""oh yeah  I didn't say anything funny in that meeting just go ahead  where's the - where's the release?"" And then there'll be a subset of people  right? - O_K there's - I mean think of who it is we've been recording mostly. O_K there'll be a subset of people  Yeah. who um  will say uh ""well  yeah  I really would like to see that."" And for them  the easiest way to flip through  if it's a really large document  I mean unless you're searching. Searching  of course  should be electronic  but if you're not - so if you provide some search mechanism you go to every place they said something or something like that  but see then we're getting more elaborate with this thing. Yeah. Um if - if uh you don't have search mechanisms you just sort of have this really  really long document  I mean whenever I've had a really  really long document that it was sitting on the web  I've always ended up printing it out. I mean  so it's - it's - I mean  you - you're - you're not necessarily gonna be sitting at the desk all the time  you wanna figure you have a train ride  and there's all these situations where - where I - I mean  this is how I was imagining it  anyway. And then I figured  that out of that group  there would be a subset who would go ""hmm you know I'm really not sure about this section here "" and then that group would need it - S- It seems like i- if I'm right in that  it seems like you're setting it up for the most infrequent case  rather than for the most frequent case. So that uh  now we have to worry about privacy  we have to worry about all these passwords  for different people Well  no fre- for the most - For the most frequent case they just say ""it's O_K"" and then they're done. And I think almost everyone would rather do that by email than any other method. Mm-hmm. The other thing too is it seems like - Go ahead. Um  yeah  that's true. I mean  cuz you don't have to visit the web page if you don't want to. Yeah. I guess - Yeah  I guess we don't need their signature. I guess an email O_K is alright. Oh that was another thing I - I had assumed that we didn't need their signature  that it - that an email approval was sufficient. But I don't actually know. Are - are people going to be allowed to bleep out sections of a meeting where they weren't speaking? Yes. I also - mm-hmm. If someone feels strongly enough about it  then I - I - I think they should be allowed to do that. Uh So that means other people are editing what you say? Yeah. I don't know about that. I don't know if I like that. Well  the only other choice is that the person would say ""no  don't distribute this meeting at all ""  and I would rather they were able to edit out other people then just say ""don't distribute it at all"". But th- what they signed in the consent form  was something that said you can use my voice. Well  but if - Right? if someone is having a conversation  and you only bleep out one side of it  that's not sufficient. Yeah. Yeah  but that's our decision then. Right? Um  I don't think so. I mean  because if I object to the conversation. If I say I think it is. ""we were having a conversation  and I consider that conversation private "" and I consider that your side of it is enough for other people to infer  I wanna be able to bleep out your side. The - I agree that the consent forms were - uh  I cons- agree with what Adam's saying  that um  the consent form did leave open this possibility that they could edit things which they found offensive whe- whether they said them or didn't say them. I see. And the other thing is from the standpoint of the l- of the l- I'm not a law- lawyer  but it strikes me that uh  we wouldn't want someone to say ""oh yes  I was a little concerned about it but it was too hard to access"". O_K  well  if that's what it said. So I think it's kind of nice to have this facility to listen to it. Now - in terms of like editing it by hand  I mean I think it's - i- some people would find that easier to specify the bleep part by having a document they edited. But - but it seems to me that sometimes um  you know i- if a person had a bad day  and they had a tone in their voice that they didn't really like  you know it's nice - it's nice to be able to listen to it and be sure that that was O_K. I mean I can certainly provide a printable version if people want it. Um I mean it's also a mixture of people  I mean some people are r- Um. do their work primarily by sitting at the computer  flipping around the web  and others do not. Others would consider it - this uh - a - Yep. a set of skills that they would have to gain. You know? It depends on what meetings. Well I think most of the people in the meetings are the former. That's true. So far. So. In the meetings so far  yeah. Yep. But we're trying to expand this  right? So I - I - I actually think that paper is the more universal thing. Right. And that - Mm-hmm. Yeah. Well  but if they want to print it out that's alright. I think everyone in the meeting can access the web. No  I think we have to be able to print it out. It's not just if they want to print it out. I - I think - O_K  so does that mean that I can't use email? Or what? Cuz you could send it through email you're thinking. I - I th- well - we - there was this - Well  I don't think I - well I don't think we can send the text through email because of the privacy issues. Good. For security? Yeah  O_K good. Good point. No. Yeah. Right. Um. @@ So giving them  you think a web site to say  ""if you wanna print it out here it is""  is not sufficient? Yeah. I- Certainly for everybody who's been in the meetings so far it would be sufficient. I'm just wondering about - Yeah  I'm just thinking for people that that's not sufficient for  what - the only sufficient thing would be for me to walk up to them and hand it to them. You could mail it to them. Get an- a mailing address. Yeah. Equivalent. But - But I think it's easier to drop in the box. Just put the button on - on the web page which say ""please send me the - the scripts"". Oh that's interesting. That's right. Yeah. What um - When you display it on the web page  what are - what are you showing them? Utterances  or - ? Mm-hmm. And so can they bleep within an utterance? No. Whole utterances only. Whole utterances. And that was just convenience for my sake  that it's uh  uh it would end up being fairly difficult to edit the transcripts if we would do it at the sub-utterance level. Because this way I can just delete an entire line out of a transcript file rather than have to do it by hand. There's another aspect to this which maybe - is part of why this is bothering me. Um  I think you're really trying very hard to make this as convenient as possible for people to do this. I mean that's why I did the web form  because for me that would be my most convenient. Mmm. I - I - I understand. I think that's the bad idea. I know where you're going. Oh. See because you're gon- you're - uh - Really. You're gonna end up with all these little patchy things  whereas really what we want to do is have the - the - the bias towards letting it go. Because nob- you know it - There was a - one or twi- once or twice  in the re- in the meetings we've heard  where somebody said something that they might be embarrassed by  but overall people are talking about technical topics. Nobody's gonna get hurt. Nobody's being l- libeled. You know  this is - this - we're - we're covering - We're playing the lawyer's game  and we're playing- we're - we're - we're looking for the extreme case. If we really orient it towards that extreme case  make it really easy  we're gonna end up encouraging a headache. That - I think that's - I'm sort of psyching myself out here  I - I'm trying to - uh - but I - I think that's - I guess I don't see having a few phrases here and there in a meeting being that mu- much of a headache  bleeped out. So. Well  it's - but i- I think what Morgan's saying is the easier it is  the more is gonna be bleeped. And - and it really depends on what kind of research you're doing. I think some researchers who are gonna be working with this corpus years from now are really gonna be cursing the fact that there's a bunch of stuff in there that's missing from the dialogue. Mm-hmm. You know  it depends on the kind of research they're doing  but it might be  uh it might be really a - a pain. Yeah. And  you know where it's really gonna hurt somebody  in some way - the one who said it or someone who is being spoken about  we definitely want to allow the option of it being bleeped out. But I really think we wanna make it the rare incidence. And - and uh  I am just a little worried about making it so easy for people to do  and so much fun! that they're gonna go through and bleep out stuff. So much fun. and they can bleep out stuff they don't like too  right from somebody else  as you say  you know  so ""well I didn't like what he said."" Well I don't see any way of avoiding that. I mean  we have to provi- we have promised that we would provide them the transcript and that they can remove parts that they don't like. Yeah. So that the - No  no  I - I - I don't - The only question is - You- you've talked me into that  but I - I just think that we should make it harder to do. The problem is if it's harder for them it's also harder for me. Whereas this web interface  I just get email  it's all formatted  it's all ready to go and I can just insert it. So maybe you don't give them access to the web interface unless they really need it. Well I guess - So - so - so I'm sorry - so - Yeah. Hmm. so - So maybe this is a s- a way out of it. You've provided something that's useful for you to do - handle  and useful for someone else if they need it. But Well - I think the issue of privacy and ease and so forth should be that uh  they get access to this if they really need it. So you're saying the - the sequence would be more like first Adam goes to the contact lists  contacts them via whatever their preferred method is  to see if they want to review the meeting. Right. And then if they don't  you're done. If they do  then he provides them access to the - the web site. Well  to some extent I have to do that anyway because as I said we have to distribute passwords. W- w- Or - a printed-out form. There's - there - y- So  but you don't necessarily have to distribute passwords is what I'm saying. Well  but - So - what I'm saying is that I can't just email them the password because that's not secure. Only if they want it. No  no  no. But you aren't necessarily giving them - So they have to call me and ask. Right. But - we don't even necessarily need to end up distributing passwords at all. Well  we do because of privacy. We can't just make it openly available on the web. Mm-hmm. No  no. You're missing the point. We're - We're trying i- We're trying to make it less of an obvious just l- l- l- l- uh fall off a log  to do this. Not everyone gets a password  unless they ask for it. Yeah. Right? So th- so what I would see  is that first you contact them and ask them if they would like to review it for to check for the - not just for fun  O_K? but to - to check this for uh things that they're worried about having said or if they're willing to just send an approval of it  at - from their memory. Um - and  uh  and we should think carefully actually we should review - go through how that's worded  O_K? Then  if someone uh - wants to review it  uh  and I know you don't like this  but I'm offering this as a suggestion  is that - is that we then give them a print out. And then if they say that ""I have a potential problem with these things "" then  you - you say ""O_K well you might wanna hear this in context to s- think if you need that "" you issue them a password  i- in the - But the - the problem with what you're suggesting is it's not just inconvenient for them  it's inconvenient for me. Because that means multiple contacts every time - for every single meeting every time anyone wants anything. I would much prefer to have all be automatic  they visit the web site if they want to. Obviously they don't have to. I know you'd prefer it  but the proble- we have - there's a problem with it. Yeah. So I think you're thinking people are going to arbitrarily start bleeping and I just don't think that's gonna happen. I'm also concerned about the spirit of the - of the informed consent thing. Cuz I think if they feel that uh  it's - I th- I th- You know  if it turns out that something gets published in this corpus that someone really should have eliminated and didn't detect  then it could have been because of their own negligence that they didn't pursue that next level and get the password and do that  um  but - but they might be able to argue ""oh well it was cumbersome  and I was busy and it was gonna take me too much time to trace it down"". So it could that the burden would come back onto us. So I'm a little bit worried about uh  making it harder for them  from the legal standpoint. Well you can go too far in that direction  and you need to find somewhere between I think  because - Yeah. It seems to me that sending them email  Uh-huh. saying ""if you have an O_- O_K reply to this email and say O_K  If you have a problem with it contact me and I'll give you a password""  seems like is a perfectly  reasonable compromise. And if they want a printout they can print it out themselves. Or we could print it up for them  I mean we could offer that - but - but there's uh  another aspect to that and that is that in the informed consent form  Yeah. um  my impression is that they - that we offered them at the very least that they definitely would have access to the transcript. And - and I ha- I don't know that there's a chance of really skipping that stage. Yeah. I mean I - I thought that you were - Maybe I misinterpreted what you said but it's - Having access to it doesn't necessarily mean  that - having it right? It just means they have the right to have it. Having it. Well the in - the consent form is right in there if anyone wants to look at it  so. Giving it to them. O_K. Alright. Fine. O_K. Fair enough. Yeah. Sh- sh- well I could - I'm closer. I could - D- you want me to grab one? Yeah  but you're wired aren't you? Yeah. That is true. Um. Yeah  I mean I don't wanna fool them  I don't know - I just meant that e- every - ev- any time you say anything to anyone there is in fact a - a bias that is presented  right? of - and - Oh yeah yeah - oh I know. Yeah that's true. Yeah. ""If you agree to participate you'll have the opportunity to have anything ex- anything excised  which you would prefer not to have included in the data set."" Yeah. ""Once a transcript is available we will ask your permission to include the data in the corpus for the r- larger research community. There again you will be allowed to indicate any sections that you'd prefer to have excised from the database  and they will m- be removed both from the transcript and the recording."" Hmm. Well that's more open than I realized. Well  I mean it - I - The one question is definitely clear with anything as opposed to just what you said. Yeah. Tha- that's true. That's more severe  Yeah  uh no that - it - tha- that's right. but the next one says the transcript will be around. And it doesn't really say we'll send it to you  or wi- it'll be available for you on the web  or anything. I think it probably leaves it open how we get it to them. I- I - At least it more often. Yeah. It means also we don't have to g- To give it to them. I mean like - like Morgan was saying they - they - They just have to make sure that it is available to them. It's available to them if they ask for it. Yeah  O_K  so. wh- um - I think I have an idea that may be sat- may satisfy both you and me in this which is  um  it's a - it - we just go over carefully how these notes to people are worded. So I - I just want it to be worded in such a way where it gives the strong impre- it gives very  I mean nothing hidden  v- very strongly the bias that we would really like to use all of these data. Right. That - that we really would rather it wasn't a patchwork of things tossed out  that it would be better for  um  our  uh  field Good. if that is the case. But if you really think something is gonna - And I don't think there's anything in the legal aspects that - that is hurt by our expressing that bias. Great. Great  great. Yeah. I agree. And then - then my concern about - which - you know you might be right  it may be it was just paranoia on my part  uh but people just - See I'm @@ worried about this interface so much fun that people start bleeping stuff out just as - just because they can. Yeah. It's just a check box next to the text  it's not any fun at all. Well I don't know. I kind of had fun when you played me something that was bleeped out. You know. I - Well  but they won't get that feedback. All - no because it doesn't automatically bleep it at the time. It just sends me - Oh they won't? Oh good. So you haven't made it so much fun. Right. Oh good. O_K  It just sends me the time intervals. And then at some point I'll incorporate them all and put bleeps. I mean I don't wanna have t- ha- do that yet until we actually release the data because um  then we have to have two copies of every meeting and we're already short on disk space. Yeah. Yeah. So I - I wanna - I - just keep the times until we actually wanna release the data and then we bleep it. O_K. Alright  so I think - Yeah so if we have if - i- Again let's you know  sort of circulate the - the wording on each of these things and get it right  but - Well since you seem to feel heart- uh  strongest about it  would you like to do the first pass? but - O_K. Uh  fair enough. Al- Turn about is fair play  Sorry. Also it- ther- there is this other question  the legal question that - that Adam's raised  uh about whether we need a concrete signature  or email c- i- suffices or whatever and I don't know how that works. i- There's something down there about ""if you agree to -"" Yeah. I'm - I'm - I'm - I thought - I - I thought about it with one of my background processes and I - uh it's - uh it's uh  it's fine to do the email. I don't think so. Ah. Fine. Good. O_K. O_K. Yeah because thi- th- they're signing here that they're agreeing to the paragraph which says ""you'll be given an opportunity."" Yeah. And - And so I don't think they need another signature. Well and furthermore I - it's now fairly routine in a lot of arrangements that I do with people on contracts and so forth that - that uh if it's - if it's that sort of thing where you're you're saying uh ""O_K I agree  we want eighty hours of this person at such-and-such amount  and I agree that's O_K "" uh if it's a follow up to some other agreement where there was a signature it's often done in email now so it's - it's O_K. Right. Great. Um. So I guess I probably should at the minimum  think about how to present it in a printed form. I'm not really sure what's best with that. The problem is a lot of them are really short  Well - and so I don't necessarily wanna do one per line. But I don't know how else to do it. Well I s- I also have this - I - I think it's nice you have it uh  viewab- her- hearable on the - on the web for those who might wonder about um  the non- nonverbal side  I mean I - I agree that our bias should be as - as expressed here  and - but I - I think it's nice that a person could check. Cuz sometimes you know you - the words on a - on the page  come out soun- sounding different in terms of the social dynamics if they hear it. And I realize we shouldn't emphasize that people you know  shouldn't borrow trouble. What it comes down to but - Hmm. Mm-hmm. Mm-hmm. Yeah I think actually - my opinion probably is that the only time someone will need to listen to it is if the transcript is uh not good. You know  if - if there are lots of mumbles and parentheses and things like that. Oh  you know  or what if there was an error in the transcript that didn't get detected and there was a whole uh - i- segment a- against some - personal i- th- Right. That was all mumbled? I think Microsoft is @@ Yeah. Yeah exactly @@ Oh  Sorry transcribers. Or - or even - or even there was a - a line you know about how Yeah. ""hmm-mmm-mmm Bill Gates duh-duh-duh-duh."" but - but it was all - the words were all visible  but they didn't end up i- some- there was a slip in the transcript. @@ Oh  God. They're gonna hate this meeting. Yeah. Yeah that's true. Actually Liz will like it. You know  but. Liz will like it. We had a pretty strong disagreement going there. Yeah. Yep  yep  that's right. Yeah. So I don't know. I mean  I - I guess we're assuming that the transcript is a close enough approximation and that - that my double checking will be so close to absolutely perfect that it - that nothing will slip by. Mm-hmm. But it - the - some- something might sometime  and they - uh - if - if it's something that they said  they might - i- i- I mean  you might be very accurate in putting down what they actually said  Mm-hmm. but  when they hear it  themselves  they may hear something different because they know what they meant. I don't know how to notate that. Yeah  that's right. Yeah that's right. Sarcasm  how do you - how do you indicate sarcasm? No  I'm serious. So - the - so - i- the - so we might - we might get some feedback from people that such-and-such was  you know  not - not really what I said. Yeah. Well that would be good to get  definitely. Yeah  but  Yeah  sure. Just for corrections. Yeah. So um  in terms of password distribution  I think phone is really the only way to do it  phone and in person. Or mail  physical mail. Yeah. Or if for- leave it on their voice mail. Any sub-word level thing. @@ Any sub-wor- Yeah  O_K. I mean you could do it with P_G_P or things like that but it's too complex. You know I just realized something  which is of - e- th- this question about the - uh- the possible mismatch of - I mean i- well  and actually also the lawyer saying that um  we shouldn't really have them - have the people believing that they will be cleared by our checks. You know? I mean. So it's like i- in a way it's - it's nice to have the responsibility still on them to Mm-hmm. listen to the tape and - and hear the transcript  to have that be the - Well yeah  but you can't dep- I mean  most people will not wanna take the time to do that  though. Yeah  O_K  fair enough. And they're s- they're absorbing the responsibility themselves. So it's not - it's not um - Yeah  good. And they - they have to - But I mean if you were at a meeting  and - and you - you don't think  at least  that you said anything funny and the meeting was about  you know  some - some funny thing about semantics or something  or uh - You probably won't listen to it. Yeah. It is true that tec- that the content is technical  I - and so i- Yeah. and we're not having these discussions which - I - I mean  when I listen to these things  I don't find things that are questionable  in other people's speech or in my own. Yeah. You would think it would be rare  I mean we're not talking about the energy crisis or something  people have - Just - It should be very rare. Yeah. Yeah  O_K. How about them energy crises. Yeah. I think we're uh - Done? Kind of done. Actually  I was gonna - Di- Did you have anything n- that's going on  or - Not really. No. Um  my project is going along but um  I'm really just here to um fill the project uh - the overall progress. Yeah. I don't really have anything specific to - to talk about. That's fine. I just didn't wanna go by you  if you had something. Oh  O_K. Hmm. You don't have anything to say. j- I don't know. @@ Nah. No. Transcribers  he was rattling the b- marbles in his brain back and forth just then this - this - Shall we do digits? Oh yeah. It um Um  oh by the way I did find a bunch - Uh  we should count out how many more digits to forms do we have back there? There were quite a few. That's what I thought. I f- I was going through them all and I found actually a lot filed in with them  Uh. that were blanks  that no one had actually read. And so we still have more than I thought we did. So  we have a few more digits before we're done. Mmm. Oh good. So transcript three zero one one three zero three zero. nine eight five eight O_ zero one two zero seven four two four one uh eight six six five four six two two six three seven eight one one six eight eight eight nine eight six nine O_ O_ nine O_ one one nine four two five four three four six eight nine two seven four nine O_ one uh  correction four nine one O_ five seven zero eight four six seven O_ seven five seven nine three Transcript three five three one dash three five five zero. nine zero zero one one nine one four seven O_ O_ two two six three four five one eight three nine five five six seven nine two five three O_ five five six four zero five four nine zero one four two three four six two eight seven two five six eight seven zero nine eight seven three four five zero Transcript two eight five one dash two eight seven zero. two three nine O_ three nine two three seven seven four eight five six eight two eight six four nine five zero O_ seven one two zero nine seven two one nine two three zero five five one six five seven seven six nine eight eight four zero nine O_ O_ zero two four two two one Transcript two six three one two six five zero. three zero zero five one four O_ six six two two five six seven seven seven three eight nine one eight one nine O_ zero two two seven zero zero three two four five O_ two four two six five nine nine six seven That was one. six seven seven eight O_ two five O_ one three five six five five zero four three one zero one seven five six two nine four nine five seven eight three nine nine O_ four six seven Transcript two four one one  dash two four three zero. five seven eight four four six nine six two one seven eight nine zero one one two eight six two three five two O_ two seven three nine five one four five six eight one three nine six two zero four O_ eight O_ one four O_ seven zero six two six six eight seven one two three five two six three Transcript three two one one dash three two  three zero. six zero eight one one nine five O_ three O_ six five one three zero five zero two three one two three O_ O_ eight two two eight five two one five six two seven four three two four eight nine eight O_ O_ one zero zero zero two two five two O_ three six four eight three three zero zero one five seven zero seven six Oops. You know having this headset reminds me of like working at Burger King or something. No I never did. I'd like a burger with that  do you want fries with that? Burger King Oh  did you do that? Wow. But I feel like I could now. And ",The Berkeley Meeting Recorder Group met to discuss their recent progress. This included a recap of a meeting with one of the members of their research partner OGI. There were progress reports from group members working on echo cancellation  acoustic feature detection  and HTK optimization  along with discussion of many issues arising from this topics. The group must try to communicate more with research partners OGI. There have been communication problems between the group and their partners at OGI  with both groups waiting for the other to be the one to make contact. This has made it hard to coordinate efforts. When looking at articulatory features  the data will be continuous  so may need a mapping to binary decisions. This could end up as a huge research project by itself  so must be careful. Mn002 and fe002 are continuing with try to reduce delays and improve the system of the groups main digit recogniser project. This includes look at filers  and trying to remove noise. In the coming weeks they have much to try. Speaker Me018 has also been looking to improve results on the Aurora digit task. More specifically he is looking at the number of states in the HMMs representing the digits  and is interested in shortening some of them  making them more word specific and hence more accurate. Speaker Me026 reports his minimal progress on echo cancellation work  in that he is awaiting code along with a better understanding of the original process he is studying. Me006 is looking to combine previous sonorant detection work with other articulatory feature work. 
"Hmm. Testing channel two. Two  two. Two. Two. Oh. Hello? Hmm? Yeah Thank You. O_K O_K. Well  so Ralf and Tilman are here. Great. Great. Made it safely. So the - what w- we h- have been doing i- they would like us all to read these digits. But we don't all read them but a couple people read them. O_K. Uh  wanna give them all with German accents today or - ? Sure. O_K. O_K and the way you do it is you just read the numbers not as uh each single  so just like I do it. Mm-hmm. O_K. First you read the transcript number. My transcript number is L_ ninety-five. three two three  O_ nine  seven seven three five two two one  four O_ eight  two one one five one six zero zero  eight  seven O_ nine six four four O_  four  five six two eight five O_  five nine three  five two six one three five eight  one O_ five  six two four four six eight four  eight five five one  four two two nine O_ five seven  five six  O_ seven three two. O_K  uh - What's - Turn. Uh  my transcript number is  um  L_ n- ninety-one one four one six  two  eight six one three w- one five seven  O_ three nine five  eight  w- uh  O_  six six two two  four two  O_ three  one six  one four three four one  six one seven  one three four O_ six five two  one O_ five  nine eight two one four O_  u- eight four  nine two  u- one two  s- six five eight six  six six  one three  one one  one O_ two O_  six six  six five  four one  five nine. O_K  my transcript number is L_ ninety-two. eight five O_  three four two  one three seven six. O_ six three two  four nine seven O_  six O_  six eight. six three six  five six four  nine three O_ six. six four one  eight seven  six O_ O_ six. eight O_ seven three  one seven seven zero  seven five five six. four four  five six  six O_  four five  one O_. eight four five  eight four one  one O_ three seven. one O_ seven two  six seven seven four  one seven O_ O_. O_K. Let's be done with this. O_K. O_K. this is Ami  who - And this is Tilman and Ralf. Hi. Uh-huh . Hi. Hi. Nice to meet you. O_K. So we're gonna try to finish by five so people who want to can go hear Nancy Chang's talk  uh downstairs. Hmm. And you guys are g- giving talks on tomorrow and Wednesday lunch times  right? That's great. Yes. Mmm. O_K so  do y- do you know what we're gonna do? I thought two things uh we'll introduce ourselves and what we do. And um we already talked with Andreas  Thilo and David and some lines of code were already written today and almost tested and just gonna say we have um again the recognizer to parser thing where we're working on and that should be no problem and then that can be sort of developed uh as needed when we get - enter the tourism domain. em we have talked this morning with the - with Tilman about the generator. S- and um There one of our diligent workers has to sort of volunteer to look over Tilman's shoulder while he is changing the grammars to English Mm-hmm. because w- we have - we face two ways. Either we do a syllable concatenating um grammar for the English generation which is sort of starting from scratch and doing it the easy way  or we simply adopt the ah um more in-depth um style that is implemented in the German system and um are then able not only to produce strings but also the syntactic parse uh not parse not the syntactic tree that is underneath in the syntactic structure which is the way we decided we were gonna go because A_  it's easier in the beginning Mm-hmm. and um it does require some - some knowledge of - of those grammars and - and - and some ling- linguistic background. But um it shouldn't be a problem for anyone. O_K So That sounds good. Johno  are you gonna have some time t- to do that uh w- with these guys? Sure. cuz y- you're the grammar maven. O_K. I mean it makes sense  doesn't it? Yeah. Yeah Good. O_K. So  I think that's probably the - the right way to do that. And an- Yeah  so I - I actually wanna f- to find out about it too  but I may not have time to get in. the - the ultimate goal is that before they leave we - we can run through the entire system input through output on at least one or two sample things. And um and by virtue of doing that then in this case Johno will have acquired the knowledge of how to extend it. Ad infinitum. When needed  if needed  when wanted and so forth. O_K that sounds great. And um also um Ralf has hooked up with David and you're gonna continue either all through tonight or tomorrow on whatever Mmm. to get the er parser interface working. They are thinning out and thickening out lattices and doing this kind of stuff to see what works best. Mmm  yep. Great. So  you guys enjoy your weekend? Yes  very much so. Yeah  very much O_K  before - before you got put to work? Yeah Great. O_K  so that's - Sort of one branch is to get us caught up on what's going on. Also of course it would be really nice to know what the plans are  in addition to what's sort of already in code. Yes. and we can d- I dunno w- w- was there uh a time when we were set up to do that? It probably will work better if we do it later in the week  after we actually understand Yes. Hmm. Yeah. uh better what's going on. So when do you guys leave? Um we're here through Sunday  so Oh Oh  O_K  so - All through Friday would be fine. O_K  So - so anyt- we'll find a time later in the week to uh get together and talk about your understanding of what SmartKom plans are. Mm-hmm. and how we can change them. Yes. Sure. Uh  Should we already set a date for that? Might be beneficial while we're all here. O_K? um What - what does not work for me is Thursday afternoon. I can do earlier in the day on Thursday  or um most of the time on Friday  not all. Wha- but  Johno  Thursday morning sounds fine? Mm-hmm. what are your constraints? um Thursday afternoon doesn't work for me  but - Neither does Thursday morning  no? Uh Thursday morning should be fine. Eleven? O_K. Eleven on Thursday? @@ I was just thinking I w- I will have leavened by Right. eleven. Right. This is then out of deference to our non-morning people. Mm-hmm. O_K. So at eleven? Hmm. Thursday around eleven? Yeah. O_K. And actually we can invite um Andreas as well. Uh he will be in Washington  though. Oh that's true. He's off - off on his trip already. but um David is here and Thilo. he's actually knows everything about the SmartKom recognizer. O_K well yeah maybe we'll see if David could make it. That would be good. O_K so facing to - to what we've sort of been doing here um well for one thing we're also using this room to collect data. Yeah obviously. um um Not this type of data  no not meeting data but sort of - sort ah our version of a wizard experiment such Oh  O_K. not like the ones in Munich but pretty close to it. Mm-hmm. The major difference to the Munich ones is that we do it via the telephone O_K. even though all the recording is done here and so it's a - sort of a computer call system Mm-hmm. that gives you tourist information tells you how to get places. And it breaks halfway through the experiment and a human operator comes on. and part of that is sort of trying to find out whether people change their linguistic verbal behavior when first thinking they speak to a machine and then to a human. Yeah. and we're setting it up so that we can - we hope to implant certain intentions in people. For example um we have first looked at a simple sentence that ""How do I get to the Powder-Tower?"" O_K so you have the - castle of Heidelberg O_K. and there is a tower and it's called Powder-Tower. Oh  O_K. Yeah. and um so What will you parse out of that sentence? Probably something that we specified in M_-three-L_  that is Mmm. @@ ""action go to whatever domain  object whatever Powder-Tower"". And maybe some model will tell us  some G_P_S module  in the mobile scenario where the person is at the moment. And um we've sort of gone through that once before in the Deep Mail project and we noticed that first of all what are - I should've brought some slides  but what our - So here's the tower. Think of this as a two-dimensional representation of the tower. And our system led people here  to a point where they were facing a wall in front of the tower. There is no entrance there  but it just happens to be the closest point of the road network to the geometric center Because that's how the algorithm works. So we took out that part of the road network as a hack and then it found actually the way to the entrance. which was now the closest point of the road network to Yeah. O_K  geometric center. But what we actually observed in Heidelberg is that most people when they want to go there they actually don't want to enter  because it's not really interesting. They wanna go to a completely different point where they can look at it and take a picture. Oh  O_K. Yeah. Hmm. And so what uh uh a s- you s- let's say a simple parse from a s- from an utterance won't really give us is what the person actually wants. Does he wanna go there to see it? Does he wanna go there now? Later? How does the person wanna go there? Is that person more likely to want to walk there? Walk a scenic route? and so forth. There are all kinds of decisions that we have identified in terms of getting to places and in terms of finding information about things. And we are constructing - and then we've identified more or less the extra-linguistic parameters that may f- play a role. Information related to the user and information related to the situation. And we also want to look closely on the linguistic information that what we can get from the utterance. That's part of why we implant these intentions in the data collection to see whether people actually phrase things differently whether they want to enter in order to buy something or whether they just wanna go there to look at it. And um so the idea is to construct uh um suitable interfaces and a belief-net for a module that actually tries to guess what the underlying intention was. And then enrich or augment the M_-three-L_ structures with what it thought what more it sort of got out of that utterance. So if it can make a good suggestion  ""Hey!"" you know  ""that person doesn't wanna enter. That person just wants to take a picture "" cuz he just bought film  or ""that person wants to enter because he discussed the admission fee before"". Or ""that person wants to enter because he wants to buy something and that you usually do inside of buildings"" and so forth. These ah these types of uh these bits of additional information are going to be embedded into the M_-three-L_ structure in an - sort of subfield that we have reserved. And if the action planner does something with it  great. If not you know  then that's also something um that we can't really - at least we want to offer the extra information. We don't really - Mm-hmm. Hmm. um we're not too worried. I mean - t- s- Ultimately if you have - if you can offer that information  somebody's gonna s- do something with it sooner or later. That's sort of part of our belief. What was he saying? Um  for example  right now I know the G_I_S from email is not able to calculate these viewpoints. So that's a functionality that doesn't exist yet Mm-hmm. to do that dynamically  but if we can offer it that distinction  maybe somebody will go ahead and implement it. Surely nobody's gonna go ahead and implement it if it's never gonna be used  so. What have I forgotten about? Oh yeah  how we do it  yeah that's the Well th- uh- No no. It's a good time to pause. I s- I see questions on peoples' faces  so why don't - let's - let's - Oh- Let's hear - Well the obvious one would be if - if you envision this as a module within SmartKom  where exactly would that Sit? um - so far I've thought of it as sort of adding it onto the modeler knowledge module. So this is one that already adds That's the d- Hmm. O_K  yeah. Makes perfect sense. Yes. Hmm  ah. additional information to the but it could sit anywhere in the attention-recognition I mean basically this is what attention-recognition literally sort of can - Well it's supposed to do. Mmm. Yeah That's what it should do. Right  yeah. Yeah. Yeah. Huh. Yeah. Well f- from my understanding of what the people at Phillips were originally trying to do doesn't seem to quite fit into SmartKom currently so what they're really doing right now is only selecting among the alternatives  the hypotheses that they're given enriched by the domain knowledge and Yeah. the um discourse modeler and so on. Yeah. So if - if this is additional information that could be merged in by them. And then it would be available to action planning and - and others. Yeah. the - let's - let's That w- O_K that was one question. Is there other - other things that cuz we wanna not Pa- pass over any you know  questions or concerns that you have. Well there're - there're Mmm. two levels of - of giving an answer and I guess on both levels I don't have any um Mmm. further questions. uh the - the two levels will be Mmm. as far as I'm concerned as uh standing here for the generation module and the other is - is my understanding of what SmartKom uh is supposed to be Right. and I - I think that fits in perfectly So - well  let me - Hmm. Let me s- expand on that a little bit from the point of view of the generation. Yeah. So the idea is that we've actually got this all laid out an- and we could show it to you ig- um Robert didn't bring it today but there's a - a belief-net which is - There's a first cut at a belief-net that - that doesn't - it - isn't fully uh instantiated  and in particular some of the - the combination rules and ways of getting the - the conditional probabilities aren't there. But we believe that we have laid out the fundamental decisions in this little space and the things that influence them. Mm-hmm. So one of the decisions is what we call this A_V_E thing. Do you want to um access  view or enter a thing. Hmm. So that's a- a discrete decision. There are only three possibilities and the uh - what one would like is for this uh  knowledge modeling module to Mm-hmm. add which of those it is and give it to the planner. Mm-hmm. But  uh th- the current design suggests that if it seems to be an important decision and if the belief-net is equivocal so that it doesn't say that one of these is much more probable than the other  then an option is to go back and ask Mm-hmm. for the information you want. Alright? Now there are two ways one can go - a- imagine doing that. For the debugging we'll probably just have a - a drop-down menu and the - while you're debugging you will just - O_K. But for a full system  then one might very well formulate a query  give it to the dialogue planner and say this  you know Mm-hmm. ar- are you know you - are you planning to enter? Or whatever it - whatever that might be. So that's - under that model then  There would be a - uh - um a loop in which this thing would formulate a query  presumably give it to you. Yes. That would get expressed and then hopefully you know  you'd get an answer back. Yep. And that would of course - the answer would have to be parsed. Mmm. right and - Yep. O_K so  th- that Yes. uh  We probably won't do this early on  because the current focus is more on the decision making and stuff like that. Yep. But While we're on the subject I just wanted to give you a sort of head's up Mm-hmm. that it could be that some months from now we said ""O_K we're now ready to try to close that loop "" Mm-hmm. Hmm. in terms of querying about some of these decisions. Yep. So - my suggestion then is that you um look into the currently ongoing discussion about how the action plans are supposed to look like. And they're currently um Agreeing or - or in the process of agreeing on an X_M_L_ification of um something like a state-transition network of how dialogues would proceed. and - The - these um transition networks uh will be what the action planner interprets in a sense. Hmm. D- did you know this Robert? uh Michael is doing that  right? Well uh Marcus Lerkult is actually implementing that stuff and Marcus and Michael together are um leading the discussion there  yeah. O_K. So we ha- we have to get in on that. Mm-hmm. Yep. because um Mmm. Definitely. partly those are like X_schemas. the transition diagrams. Hmm. And it may be that - that um we should early on make sure that they have the flexibility that we need. Hmm. But they uh Have I understood this right? They - they govern more or less the - the dialogue behavior or the action - Mm-hmm. It's not really what you do with the content of the dialogue but it's So  I mean there is this - this - this nice interf- i- uh  No  it's - it's also a quantrant uh uh - Is it - So there's ac- so there - th- the word ""action""  O_K  is - is what's ambiguous here. I think. Hmm. Yes. So  um one thing is there's an actual planner that tells the person in the tourist domain now  per- tells the person how to go  ""First go here  first go there O_K. Mm-hmm. uh  you know  take a bus ""  whatever it is. So that's that form of planning  and action  and a route planner and G_I_S  all sort of stuff. uh But I think that isn't what you mean. No. No  in SmartKom terminology that's um called a function that's modeled by a function modeler. And it's th- that's completely um encapsulated from th- the dialogue system. That's simply a functionality that you give data as in a query and then you get back from that mmm  a functioning model um which might be a planner or a V_C_R or whatever. um some result and that's then - then used. Well  O_K  so that's what I thought. So action he- action here means dia- uh speech ac- uh you know Yeah  yeah. Mmm. Yeah  in that - in that sense yes  dialogue act  yeah. dialogue act. Yeah. Um  I think tha- I think it's not going to - I think that's not going to be good enough. I- I don- what uh - what I meant by that. So I think the idea of having a  you know  transition diagram for the grammar of conversations is a good idea. Mm-hmm. O_K? And I think that we do hav- definitely have to get in on it and find out - O_K . But I think that um when - so  when you get to the tourist domain Mm-hmm. it's not just an information retrieval system. Right? So this i- this is where Clearly. Yes. I think this - people are gonna have to think this through a bit more carefully. Mm-hmm. So  if it's only like in - in the - in the film and T_ V thing  O_K  you can do this. And you just get information and give it to people. But what happens when you actually get them moving and so forth and so on Yep. Uh  y- y- your - I d- I think the notion of this as a self contained uh module you know th- the functional module that - that interacts with - with where the tourism g- stuff is going Yep. probably is too restrictive. Now I dunno how much people have thought ahead to the tourist domain in this Probably not enough  I mean an - another uh more basic point there is that the current um tasks and therefore th- the concepts in this ac- what's called the action plan and what's really the dialogue manager. Yeah um is based on slots that have to be filled and the um Mm-hmm. kind of values in these slots would be fixed things like the a time or a movie title or something like Right. this whereas in the a um tourist domain it might be an entire route. Set-based  or even Indeed. very complex structured information in these slots and Right. I'm not sure if - if complex slots of that type are really um being taken into consideration. O_K. Could you - could you put a message into the right place to see if we can at least ask that question? So that's - that's really something we Yep. Mm-hmm. rea- I mean nothing's being yep completely settled there so this is really an ongoing discussion and that's Mm-hmm yeah and um it might actually O_K ah also - because um again in- in Deep Map we have faced and implemented those problems once already maybe we can even shuffle some know how from there to Mm-hmm. Yes. to Markus and Michael. Mmm. Yep. And um mmm You don't know - O_K th- I'll - I'll talk to Michael it's what I do anyway. Who - How far is the uh the - the M_-three-L_ specification for - for the la- natural language input gone on the - the uh I haven't seen anything for the uh tourist path domain. Yeah  it's - it's not defined yet . And um you are probably also involved in that  right? Um - Yeah. uh together with the usual gang  um Petra and Jan Mmm. Yeah  there's a meeting next next week I think O_K because That's - Those are the - I think the - the true key issues is how does the whatever comes out of the language input pipeline look like and then what the action planner does with it - and how that is uh specified. I didn't think of the internal working of the uh the action planner and the language - uh the function model as sort of relevant. Because what - what they take is sort of this - this fixed representation of a - of an intention. And that can be as detailed or as crude as you want it to be. Mm-hmm. But um the internal workings of of the - whether you know there're dialogue - action planners that work with belief-nets that are action planners that work with you know state automata . So that shouldn't really matter too much. I mean it does matter because it does have to keep track of you - we are on part six of r- a route that consists of eight steps and so forth Yeah  th- there - there - I think there are a lot of reasons why it matters. Right. O_K  so that uh  for example  the i- it's the action planner is going to take some spec and s- make some suggestions about what the user should do. What the user says after that is going to be very much caught up with what Yes. the action planner told it. If the - If the parser and the language end doesn't know what the person's been told O_K th- it's you're making your life much more difficult than it has to be. Yeah. Right? So if someone says the best t- to uh go there is by taxi  let's say. Now the planner comes out and says you wanna get there fast  take a taxi. O_K. And the language end doesn't know that. O_K  there's all sorts of dialogues that won't make any sense which would be hmm just fine. Yeah. uh That would b- but that - I think that - that uh point has been realized and it's - it's not really um been defined yet but there's gonna be some kind of feedback and input from uh the action planner into all the analysis modules  telling them what to expect and what the current state of the discourse is. Mmm. Beyond what's currently being implemented which is just word lists. @@ Yeah  but this is not the st- this is not just the state of the discourse. Mm-hmm. Of - of special interest. This is actually the state of the plan. That's why Mm-hmm. Yes  Yes  Mm-hmm yeah. O_K so it - z- and s- uh  It's great if people are already taking that into account. But One would have t- have to see - see the details. The specifics aren't really there yet. Yes. Yeah. So  there's work to do there. So anyway  Robert  that's why I was thinking that Mm-hmm. um I think you're gonna need - We talked about this several times that - that - the - the input end is gonna need a fair amount of feedback hmm from the planning end. In - in one of these things which are - are much more continuous than the - just the dialogue over movies and stuff. Yeah. Mmm. And even on - on a more basic level the - the action planner actually needs to be able to have um an expressive power that can deal with these structures. Hmm? And not just um say um - um the dialogue um will consist of ten possible states and th- these states really are fixed in - in a certain sense. Would there be any chance You have to - of getting the terminology changed so that the dialogue planner was called a ""dialogue planner""? Because there's this other thing- That'd be nice. The o- There's this other thing in - in the tourist domain which is gonna be a route planner or - It's really gonna be an action planner. And It oughta be called a - a dialogue manager. i- it - cuz that's what everybody else calls it. Yeah. I would think  yeah. Mmm. Huh? So  s- So what would happen if we sent a note saying ""Gee we've talked about this and couldn't we change this uh th- the whole word?"" I have no idea how complicated these things are. Probably close to impossible. Depends on who you talk to how. We'll see. I'll go check  cause I completely agree. Mmm. Yeah  and I think this is just for historical reasons within uh  the preparation phase of the project and not because somebody actually believes it ought to be action planner. So if there is resistance against changing it  that's just because ""Oh  We don't want to change things."" That - that not deep reason O_K  anyway. I- if - if that c- in- persists then we're gonna need another term. for the thing that actually does the planning of the uh routes and whatever we are doing for the tourist. That's external services. Yeah  but that's not g- eh tha- That ha- has all the wrong connotations. it's - it sounds like it's you know stand alone. It doesn't interact  it doesn't That's why I'm saying. I think you can't - it's fine for looking up when T- you know when the show's on T_V. You go to th- but I - I - I - I think it's really - really wrong headed for something that you - that has a lot of state  it's gonna interact co- in a complicated way with the uh understanding parts. Yeah. Yeah I think just the - the spatial planner and the route planner I showed you once the interac- action between them among them in the deep map system so - Right. a printout of the communication between those two fills up I don't know how many pages and that's just part of how do I get to one place. It's really insane. and uh Hmm but um so this is um definitely a good point to get uh Michael into the discussion. Or to enter his discussion  actually. That's the way around. Yeah  Marcus. Markus Is he new in the - in the? Wh- where's? Yeah  he's - he started um Yeah. I think January. And he's gonna be responsible for the implementation of this action planner. Dialogue manager. Is he gonna continue with the old - uh - thing? No  no he's completely gonna rewrite everything. O_K. In Java. O_K so that's interesting. Yes I was just - that's my next question whether we're - we're gonna stick to Prolog or not. hmm No. No  that's gonna be phased out. Yeah . O_K But I do think the - the function modeling concept has a certain - makes sense in a - in a certain light because Yeah. the action planner should not be - or the dialogue manager in that case should not um w- have to worry about whether it's interfacing with um something that does route planning in this way or that way huh  it j- Mm-hmm. I- I totally agree. Sure. Yeah I - I agree. There is - there's a logic to dialogue which - which is - is separable. I- Yeah. and it - cant - sort of formulate its- what it wants in a - in a rather a- abstract uh way  you know f- Mm-hmm. ""Find me a good route for this."" It doesn't really have to worry ab- how route planner A_ or how route planner B_ actually wants it. So this is - seemed like a good idea. In the beginning. It's tricky. It's tricky because one could well imagine - I think it will turn out to be the case that uh  this thing we're talking about  th- the extended n- uh knowledge modeler will fill in some parameters about what the person wants. One could well imagine that the next thing that's trying to fill out the detailed uh  route planning  let's say  will also have questions that it would like to ask the user. You could well imagine you get to a point where it's got a - a choice to make and it just doesn't know something. And so y- you would like it t- also be able to Mm-hmm. uh formulate a query. And to run that back through uh. the dialogue manager and to the output module and back around. hmm And a- I- a- a good design would - would a lot of  yeah allow that to happen. Mmm. If - if you know if - if you can't make it happen then you - you do your best. Yeah but that doesn't necessarily contradict um an architecture where there really is a pers- a def- well-defined interface. I totally agree. But - but what it nee- but th- what the point is the- in that case the dialogue manager is sort of event driven. and - and So the dialogue manager may think it's in a dialogue state of one sort  and this - Mm-hmm. one of these planning modules comes along and says ""hey  right now we need to ask a question"". So that forces the dialogue manager to change state. Yes O_K. It could be y- Sure  ye- yeah I - I think that's - that's the um Yeah  yeah it - it - concept that people have  yep. O_K. And - and the - the underlying idea of course is that there is something like kernel modules with kernel functionality that you can plug uh certain applications like tourist information or um the home scenario with uh controlling a V_C_R and so on. And then extend it to an arbitrary number of applications eventually. So - wouldn't That's an additional reason to have this well-defined interface and keep these things like uh tourist information external. Oh  yeah  yeah. And then call it external services. Hmm. But of course the - the more complex - yep. Yeah  there is another philosophical issue that I think you know you can - evade but  at- at least it makes sense to me that sooner or later uh - a service is gonna come and describe itself to you. and that's sort of what Srini is working on in - in - in the DAML Yeah. uh project where um you - you find a G_I_S about - that gives you information on Berkeley  and it's - it's gonna be there and tell you what it can do and how it wants to do things. and so you can actually interface to such a system without ever having met it before and Hmm. the function modeler and a self-description of the um external service haggle it out and you can use the same language core  understanding core to interface with planner-A_  planner-B_  planner-C_ and so forth. Hmm. Mmm. Which is  you know  uh - uh - utopian - completely utopian at the moment  but slowly  you know  getting into the realm of the uh contingent. Hmm. But we are facing of course much more um realistic problems. And language input for example  is of course uh crucial you know also when you do the sort of deep understanding analysis that we envision. um Then of course  the uh um  you know what is it - poverty of the stimulus  yet the m- uh the less we get of that the better. and um so we - we're thinking  for example how much syntactic analysis actually happens already in the parser. Hmm. and whether one could interface to that potentially Yeah  are there currently is uh no syntactic analysis but in the next release there will be some. Hmm. How's it - unless and it's um uh you can access this S- so uh y- we - we looked at the e- current pattern matching thing. Hmm. And as you say it's just a surface pattern matcher. Uh  So what are - what are the plans roughly? um it's to - to integrate and syntactic analysis. and um add some more features like segmentation. So then an utter- more than one utterance is - There um there's often uh pause between it and a segmentation occurs. So  the um - um So the idea is to uh - have a pa- y- y- a particular - Do you have a particular parser in mind? Is it yeah uh - partic- d- I mean have you thought through - ? Is it an H_P_S_G parser? Is it a whatever? No - no it's - uh I think it's it's totally complicated for O_K. it's just one - one person and so I have to keep the - Oh  you have to do it. You have to do it  yeah. Yeah  ah and so things must be simpler I see  so but uh  Miel syntactic analysis with um finite state transducers. But the people at D_F_- Yeah. People at D_F_K_I have written a fair number of parsers. Other - you know  people over the years. uh have written various parsers at D_F_K_I. None of them are suitable? I - I - I d- I'm asking. I don't know. Yeah  uh the problem is th- that it has to be very fast because um if you want to for more than one path anywhere O_K. what's in the latches from the Mm-hmm. speech recognizer so it's speed is crucial. uh And they are not fast enough. Mm-hmm. And they also have to be very robust. cuz of um speech recognition errors and O_K. So  um - So there was a chunk parser @@ in Verbmobil  that was one of the uh branchers. You know they - d- th- I c- There were these various uh  competing uh syntax modules. And I know one of them was a chunk parser and I don't remember who did that. I think it's A- Alan? that might  at Tuebingen I thought. Yeah I d- I don't remember. was - Do you know something about that? Tuebingen was at least involved in putting the chunks together I - In Tub- at - can't quite recall whether they actually produced the chunks in the first place. oh Uh. I see. Yeah  that's right. Or wh- There w- That's right. They w- They had - There were - This was done with a two phase thing  where Oh from - from Stuttgart  yeah  also the chunk parser itself was pretty stupid and then there was a kind of trying to fit them together that Right. h- used more context. Right? Yeah Well you s- and - and especially you did some - some um  l- um was a learning-based approach which learned from a big corpus of - of trees. Right. Right. Mm-hmm. And yes the - it - the chunk parser was a finite-state machine that um Mark Light originally w- worked on in - while he was in Tuebingen and then somebody else in Tuebingen picked that up. So it was done in Tuebingen  yeah. Definitely. But is that the kind of thing y- It sounds like the kind of thing that you were thinking of. yeah. yeah that's Yeah I guess it's similar. In this direction  yes What? Yeah  it's in - in this direction. Hmm . The - From Michael Strube  I've heard very good stuff about the chunk parser that is done by FORWISS  uh  which is in embassy doing the parsing. Mm-hmm. So this is sort of - came as a surprise to me that you know  embassy s- is featuring a nice parser but it's what I hear. One could also look at that and see Mm-hmm  yeah  it would be very interesting  Mm-hmm. whether there is some synergy Mmm  yeah. possible. And they're doing chunk parsing and it's uh - I - I can give you the names of the people who do it there. But um. Then there is of course more ways of parsing things. Of course. But - But uh given th- the constraints  that you want it to be small and fast and so forth  my guess is you're probably into some kind of chunk parsing. And uh I'm not a big believer in this um statistical you know  cleaning up uh It - That seems to me kind of a last resort if uh you can't do it any other way. uh but I dunno. It may - i- i- may be that's what you guys finally decide do. Hmm. Uh. And have you looked - uh just - again for context - Mm-hmm. There is this - this one that they did at S_R_I some years ago - Fastus ? a - um yeah  I've - I've looked at it but - but it's no - not much uh information available. ah! I found  but it's also finite-state transducers  I thought. It is. Yeah. I mean - it's - it was pretty ambitious. and And of course it was English oriented  um Yeah  and - and w- Purely finite-state transducers are not so good for German since there's um Right. Yeah  I guess that's the point is - is all the morphology and stuff. The word order is - is uh not fixed And English is all th- all word order. And it makes a lot more sense. And - Yeah. e- Yeah  O_K. Good point. So in - in - in German you've got uh most of this done with Mm-hmm. Also it's uh - it's um - Yes  uh the um choice between uh this processing and that processing and my template matcher. Right. Right. So what about @@ Um Did y- like Morfix? a- a- e- y- you've got stemmers? Or is that something that - Um  yeah but it's all in the - in the lexicon. But did you have that? So it's - Yeah th- the information is available. O_K. I see. So  but - So y- you just So - connect to the lexicon Yeah and uh at least for German you have all - all of the - uh the stemming information. Yeah  we can  oh yeah. We have knowledge bases from - from Verbmobil Yep. system we can use and so. Right. But it - it - it doesn't look like i- you're using it. I didn't n- see it being used in the current template uh parser. I - I didn't see any Uh - of course we l- actually only looked at the English. It - Did we look at the German? um I don't remember. So w- wha- Yeah  but - but it's used for - for stem forms. i- n- Well I think - I think there's some misunderstanding here it's - @@ Oh  O_K. Morphix is not used on-line. s- so the lexicon might be derived What? by Morphix but Right. What - what's happening on-line is just um um a - a retrieval from the lexicon which would Right. give all the stemming information so it would be a full foreign lexicon. Hmm. And that's what you have. Yeah Yep. O_K. What - uh I didn't reme- We threw out all the forms. Huh? We threw out all the forms because  you know  English  well - Oh O_K  so it - yeah  s- s- I thought I'd - Mm-hmm. So in German then you actually do case matching and things like in the - in the pattern matcher or not? um Not yet but it's planned to do that. O_K. Cuz I r- I didn't reme- I didn't think I saw it. Have we looked at the German? Yeah Oh  I haven- yeah that's - getting it from the lexicon is just fine. Yeah  yeah  yeah. No problem with that. Sure  right. Oh yes . um Yeah and here's the case where the English and the German might really be significantly different. In terms of if you're trying to build some fast parser and so forth and - You really might wanna do it in a significantly different way. I don't know. So you've - you guys have looked at this? also? in terms of You know  w- if you're doing this for English as well as German Um Do you think now that it would be this - doing it similarly? um Yeah  it's um I think it's um yes  it's - it's um possible to - to do list processing. and Maybe this is um more adequate for English and in German um Set. set processing is used. Maybe yeah. Some extensions uh have to be made. For - for a English version Mmm. O_K. Interesting. Not easy. Well there's m- I'm sure there's gonna be more discussion on that after your talk. Mm-hmm  yeah. Right. We're just gonna foreshadow what we saw that Right. and um Now actually  um Are you guys free at five? Or - Do you have to go somewhere at five o'clock tonight? W- in ten minutes? Ah - mmm. No. uh - uh - I think we're expect - Oder there was an - talk? uh Yeah  there - there's the um practice talk. Great. So you're going to that. @@ Yeah  that - that's what we were planning to do. Yeah. That's good  because that will Mmm  yeah. uh tell you a fair amount about The form of semantic construction grammar that we're using. so - Mm-hmm. Ah. So I th- I think that probably as good an introduction as you'll get. Uh to the form of - of uh - conceptual grammar that - that w- we have in mind for this. Mmm  ah. It won't talk particularly about how that relates to what uh Robert was saying at the beginning. But let me give you a very short version of this. So we talked about the fact that There're going to be a certain number of decisions That you want the knowledge modeler to make  that will be then fed to the function module  that does uh  route planning. It's called the ""route planner"" or something. Mm-hmm. So there are these decisions. And then one half of this we talked about at little bit is how if you had the right information  if you knew something about what was said and about th- the something about was the agent a tourist or a native or a business person or uh young or old  whatever. That information  and also about the Uh  what we're calling ""the entity""  Is it a castle  is it a bank? Is it a s- town square  is it a statue? Whatever. So all that kind of information could be combined into decision networks and give you decisions. But the other half of the problem is How would you get that kind of information from the parsed input? So  um So what you might try to do is just build more templates  saying uh we're trying to build a templ- you know build a template that w- uh somehow would capture the fact that Mmm. he wants to take a picture. O_K? And - and we could - you could do this. And it's a small enough domain that probably you  you know - Mmm. O_K . You could do this. But uh from our point of view this is also a research project and there are a couple of people not here for various reasons who are doing doctoral dissertations on this  Mm-hmm. and the idea that we're really after is a very deep semantics based on cognitive linguistics and the notion that there are a relatively small number of primitive conceptual schemas that characterize a lot of activity. So a typical one in this formulation is a container. So this is a static thing. And the notion is that all sorts of physical situations are characterized in terms of containers. Going in and out the portals and con- O_K. Mmm. But also  importantly for Lakoff and these guys is all sorts of metaphorical things are also characterized this way. You get in trouble and you know et cetera and so - s- Mmm. So  what we're really trying to do is to map from the discourse to the conceptual semantics level. And from there to the appropriate decisions. So another one of these primitive  Mm-hmm. what are called "" image schemas""  is uh goal seeking. So this a notion of a source  Mm-hmm. path  goal  trajector  possibly obstacles. And the idea is this is another conceptual primitive. And that all sorts of things  particularly in the tourist domain  can be represented in terms of uh source  path and goal. So the idea would be could we build an analyzer that would take an utterance Mm-hmm. and say ""Aha! th- this utterance is talking about an attempt to reach a goal. The goal is this  the pers- the  uh traveler is that  uh the sor- w- where we are at now is is this  they've mentioned possible obstacles  et cetera."" So th- the - and this is an - again attempt to get very wide coverage. So if you can do this  then the notion would be that across a very large range of domains  you could use this deep conceptual basis as the interface. Mm-hmm. Mm-hmm. And then  uh The processing of that  both on the input end  recognizing that certain words in a language talk about containers or goals  et cetera  and on the output end  given this kind of information  you can then uh make decisions about what actions to take. Provides  they claim  a very powerful  general notion of deep semantics. So that's what we're really doing. Mm-hmm. And Nancy is going to - Her talk is going to be not about using this in applications  but about modeling how children might learn Mm-hmm. this kind of uh deep semantic grammar. Yep  yep. And how do you envision um the - the um this deep semantic to be worked with. Would it be highly ambiguous if and then there would be another module that takes that um highly underspecified deep semantic construction and map it onto the current context to find out what the person really was talking about in that context. or - Well that's - that's - that's where the belief-net comes in. So th- the idea is  or a - let's take this business about going to the Powder-Tower. Mm-hmm. So part of what you'll get out of this will be the fact tha- w- if it works right  O_K  that this is an agent that wants to go to this place and that's their goal and there will be additional situational information. Mm-hmm. Oh  O_K. Uh  O_K  part of it comes from the ontology. The tower is this kind of object. Part of it comes from the user model. th- Mm-hmm. Yeah  O_K. Mm-hmm. And the idea of the belief-net is it combines the information from the dialogue which comes across in this general way  Mm-hmm. you know this is a - this is a goal seeking behavior  along with specific information from the ontology about the kinds of objects involved and about the situation Yeah O_K  Yeah  yep yep yep yep about ""Is it raining?"" I don't know. Whatever it is. And so that's the belief-net that we've laid out. Mm-hmm. And so th- the coupling to the situation comes in this model from  at th- at th- at the belief-net  combining evidence from the dialogue with the ontology with the situation. Yeah. Hmm. But Nancy isn't gonna talk about that  just about the Yeah  oh yeah  I see  yeah yeah  really. um First steps. Right. The - the construction grammar. And she's gonna start in a minute. In a minute. Ah  O_K. O_K. Is it i- in  then  your place  in five - five-A_? Alright. ","The meeting was taken up by discussion about a thesis proposal and a talk about to take place at EML. The latter will present the work that is currently being done at ICSI including examples of inference of user intentions and of the recordings of the on-going data collection. The talk will also outline the theoretical (X-schemas  image schemas  Bayes-nets) and neural background. The thesis proposal  on the other hand  presents the idea of ""construal"" and makes claims as to how inferences are drawn in a probabilistic relational model by using information from the ontology  situation  user and discourse models. It was advised that more emphasis should be put on the role of construal in the understanding of metaphor and metonymy. Base constructions deal with the norm  while further general domain mechanisms determine how the constructions are invoked depending on the context. Several potential examples of polysemy were discussed in detail: ""walk/run into""  ""on the bus""  ""out of film""  ""where is X?"". However  none of them was an example of lexical polysemy resolved by construal straightforward enough to include in the proposal; the tourist domain is not metaphor rich. As the talk at EML will also refer to a theoretical framework  it was suggested that along with presenting NTL and the piece on mirror neurons  it also alludes to relevant fMRI work. The neural side of the research could be of interest to various groups. The language analysis itself will be introduced in terms of image schemas. On the other hand  it was arranged for more feedback on the thesis proposal to be sent by email. The latest version of the construction formalism will also be needed to complete the presentation. It was noted that the thesis proposal did not put any emphasis on metaphor or on the scalability achieved by combining constructions with general construal mechanisms: constructions cover base cases  while metaphoric and metonymic cases are resolved with the extra help provided by construal. During this discussion  however  the suggested examples of polysemy  which is tackled by the proposed framework  were not straightforward enough. It was agreed that metaphors are not abundant in the spatial domain. Some of the candidates discussed were phrases like ""on the bus""  ""out of film""  non-spatial uses of ""where"" or other fixed expressions. The thesis proposal focuses on how construal works in the tourist domain. One can build a probabilistic relational model with domain general rules that define how ontology  situation  user and discourse models combine to infer intentions. The talk for EML includes a presentation of the motivation behind the project  as well as the work already done on SmartKom (parser  generator  synthesis). Furthermore  the data collection currently taking place and some preliminary observations are going to be outlined. All this is going to be given from the perspective of the general theoretical framework  NTL  with further explanations on X-schemas and also the embodied simulation approach. "
"@@ I think for two years we were two months  uh  away from being done. And what was that  Morgan? What project? Uh  the  uh  TORRENT chip. Oh. Yeah. We were two - we were - Yeah. Uh  uh  we went through it - Jim and I went through old emails at one point and - and for two years there was this thing saying  yeah  we're - we're two months away from being done. It was very - very believable schedules  too. I mean  we went through and - with the schedules - and we - Yeah. Oh  yeah. It was very true. It was true for two years. So  should we just do the same kind of deal where we go around and do  uh  status report kind of things? O_K. And I guess when Sunil gets here he can do his last or something. Mm-hmm. So. Yeah. So we probably should wait for him to come before we do his. O_K. That's a good idea. O_K. Yeah. Yeah. Any objection? Do y- O_K  M- All in favor Do you want to start  Morgan? Do you have anything  or - ? Uh  I don't do anything. I - No  I mean  I - I'm involved in discussions with - with people about what they're doing  but I think they're - since they're here  they can talk about it themselves. O_K. So should I go so that  uh  Yeah. Why don't you go ahead  Barry? you're gonna talk about Aurora stuff  per se? O_K. Um. O_K. Well  this past week I've just been  uh  getting down and dirty into writing my - my proposal. So  um - Mmm. I just finished a section on  uh - on talking about these intermediate categories that I want to classify  um  as a - as a middle step. And  um  I hope to - hope to get this  um - a full rough draft done by  uh  Monday so I can give it to Morgan. When is your  uh  meeting? Um  my meeting with  uh - ? Oh  oh  you mean the - the quals. Uh  the quals are happening in July twenty-fifth. Yeah. The quals. Yeah. Oh. Soon. Yeah. D_-Day. Uh-huh. Yeah. Uh-huh. So  is the idea you're going to do this paper and then you pass it out to everybody ahead of time and - ? Right  right. So  y- you write up a proposal  and give it to people ahead of time  and you have a short presentation. And  um  and then  um - then everybody asks you questions. Hmm. Yeah. I remember now. Yep. So  um. Y- s- Have you d- ? I was just gonna ask  do you want to say any - a little bit about it  or - ? Oh. Uh  a little bit about - ? Mmm. Wh- what you're - what you're gonna - You said - you were talking about the  uh  particular features that you were looking at  or - Oh  the - the - Right. Well  I was  um  I think one of the perplexing problems is  um  for a while I was thinking that I had to come up with a complete set of intermediate features - in- intermediate categories to - to classify right away. But what I'm thinking now is  I would start with - with a reasonable set. Something - something like  um  um - like  uh  re- regular phonetic features  just to - just to start off that way. And do some phone recognition. Um  build a system that  uh  classifies these  um - these feat- uh  these intermediate categories using  uh  multi-band techniques. Combine them and do phon- phoneme recognition. Look at - then I would look at the errors produced in the phoneme recognition and say  O_K  well  I could probably reduce the errors if I included this extra feature or this extra intermediate category. That would - that would reduce certain confusions over other confusions. And then - and then reiterate. Um  build the intermediate classifiers. Uh  do phoneme recognition. Look at the errors. And then postulate new - or remove  um  intermediate categories. And then do it again. So you're gonna use TIMIT? Um  for that - for that part of the - the process  yeah  I would use TIMIT. Mm-hmm. And  um  then - after - after  uh  um  doing TIMIT. Right? Um  that's - Mm-hmm. that's  um - that's just the ph- the phone recognition task. Yeah. Uh  I wanted to take a look at  um  things that I could model within word. So  I would mov- I would then shift the focus to  um  something like Schw- Switchboard  uh  where I'd - I would be able to  um - to model  um  intermediate categories that span across phonemes  not just within the phonemes  themselves  Mm-hmm. um  and then do the same process there  um  on - on a large vocabulary task like Switchboard. Uh  and for that - for that part I would - I'd use the S_R_I recognizer since it's already set up for - for Switchboard. And I'd run some - some sort of tandem-style processing with  uh  my intermediate classifiers. Oh. So that's why you were interested in getting your own features into the S_R_I files. Yeah. That's why I - I was asking about that. Yeah. Yeah. Yeah. Um  and I guess that's - that's it. Any - any questions? Sounds good. So you just have a few more weeks  huh? Um  yeah. A few more. It's about a month from now? It's a - it's a month and - and a week. Yeah. Yeah. So  uh  you want to go next  Dave? And we'll do - Oh. O_K  sure. So  um  last week I finally got results from the S_R_I system about this mean subtraction approach. And  um  we - we got an improvement  uh  in word error rate  training on the T_I-digits data set and testing on Meeting Recorder digits of  um  six percent to four point five percent  um  on the n- on the far-mike data using P_Z_M F_  but  um  the near-mike performance worsened  um  from one point two percent to two point four percent. And  um  wh- why would that be  um  considering that we actually got an improvement in near-mike performance using H_T_K? And so  uh  with some input from  uh  Andreas  I have a theory in two parts. Um  first of all H_T_K - sorry  S_R- the S_R_I system is doing channel adaptation  and so H_T_K wasn't. Um  so this  um - This mean subtraction approach will do a kind of channel normalization and so that might have given the H_T_K use of it a boost that wouldn't have been applied in the S_R_I case. And also  um  the - Andreas pointed out the S_R_I system is using more parameters. It's got finer-grained acoustic models. So those finer-grained acoustic models could be more sensitive to the artifacts in the re-synthesized audio. Um. And me and Barry were listening to the re-synthesized audio and sometimes it seems like you get of a bit of an echo of speech in the background. And so that seems like it could be difficult for training  cuz you could have different phones lined up with a different foreground phone  um  depending on the timing of the echo. So  um  I'm gonna try training on a larger data set  and then  eh  the system will have seen more examples o- of these artifacts and hopefully will be more robust to them. So I'm planning to use the Macrophone set of  um  read speech  and  um - Hmm. I had another thought just now  which is  uh  remember we were talking before about - we were talking in our meeting about  uh  this stuff that - some of the other stuff that Avendano did  where they were  um  getting rid of low-energy sections? Um  uh  if you - if you did a high-pass filtering  as Hirsch did in late eighties to reduce some of the effects of reverberation  uh  uh  Avendano and Hermansky were arguing that  uh  perhaps one of the reasons for that working was ma- may not have even been the filtering so much but the fact that when you filter a - an all-positive power spectrum you get some negative values  and you gotta figure out what to do with them if you're gonna continue treating this as a power spectrum. So  what - what Hirsch did was  uh  set them to zero - set the negative values to zero. So if you imagine a - a waveform that's all positive  which is the time trajectory of energy  um  and  uh  shifting it downwards  and then getting rid of the negative parts  that's essentially throwing away the low-energy things. And it's the low-energy parts of the speech where the reverberation is most audible. You know  you have the reverberation from higher-energy things showing up in - So in this case you have some artificially imposed reverberation-like thing. I mean  you're getting rid of some of the other effects of reverberation  but because you have these non-causal windows  you're getting these funny things coming in  uh  at n- And  um  what if you did - ? I mean  there's nothing to say that the - the processing for this re-synthesis has to be restricted to trying to get it back to the original  according to some equation. I mean  you also Uh-huh. could  uh  just try to make it nicer. Mm-hmm. And one of the things you could do is  you could do some sort of V_A_D-like thing and you actually could take very low-energy sections and set them to some - some  uh  very low or - or near zero value. Uh-huh. I mean  uh  I'm just saying if in fact it turns out that - that these echoes that you're hearing are  uh - or pre-echoes  whichever they are - are - are  uh  part of what's causing the problem  you actually could get rid of them. Uh-huh. Be pretty simple. O_K. I mean  you do it in a pretty conservative way so that if you made a mistake you were more likely to keep in an echo than to throw out speech. Hmm. Um  what is the reverberation time like there ? In thi- in this room? Uh - On  uh  the - the one what - the s- in the speech that you are - you are using like? Y- Yeah. I - I - I - I don't know. So  it's this room. It's  uh - Oh  this room? O_K. It's - it's this room. So - so it's - these are just microphone - this micro- close microphone and a distant microphone  he's doing these different tests on. Oh. Uh  we should do a measurement in here. I g- think we never have. I think it's - I would guess  uh  Hmm! point seven  point eight seconds f- uh  R_T_sixty - something like that? Mm-hmm. But it's - you know  it's this room. So. O_K. Mm-hmm. Uh. But the other thing is  he's putting in - w- I was using the word ""reverberation"" in two ways. He's also putting in  uh  a - he's taking out some reverberation  but he's putting in something  because he has averages over multiple windows stretching out to twelve seconds  which are then being subtracted from the speech. And since  you know  what you subtract  sometimes you'll be - you'll be subtracting from some larger number and sometimes you won't. And - Mm-hmm. Mm-hmm. So you can end up with some components in it that are affected by things that are seconds away. Uh  and if it's a low energy compo- portion  you might actually hear some funny things. Yeah. O- o- one thing  um  I noticed is that  um  the mean subtraction seems to make the P_Z_M signals louder after they've been re-synthesized. So I was wondering  is it possible that one reason it helped with the Aurora baseline system is just as a kind of gain control? Cuz some of the P_Z_M signals sound pretty quiet if you don't amplify them. Mm-hmm. I don't see why - why your signal is louder after processing  because yo- Yeah. I don't know why -y  uh  either. Yeah. I don't think just multiplying the signal by two would have any effect. Mm-hmm. Oh  O_K. Yeah. I mean  I think if you really have louder signals  Well  well - what you mean is that you have better signal-to-noise ratio. So if what you're doing is improving the signal-to-noise ratio  then it would be better. Mm-hmm. But just it being bigger if - with the same signal-to-noise ratio - It w- i- i- it wouldn't affect things. O_K. Yeah. No. Well  the system is - use the absolute energy  so it's a little bit dependent on - on the signal level. But  not so much  I guess. Well  yeah. But it's trained and tested on the same thing. So if the - if the - if you change Mmm. Mm-hmm. in both training and test  the absolute level by a factor of two  it will n- have no effect. Yeah. Did you add this data to the training set  for the Aurora? Uh - Or you just tested on this? Um. Did I w- what? Sorry? Well  Morgan was just saying that  uh  as long as you do it in both training and testing  it shouldn't have any effect. But I - Yeah. I was sort of under the impression that you just tested with this data. You didn't train it also. I - I b- I - Right. I trained on clean T_I-digits. I - I did the mean subtraction on clean T_I-digits. But I didn't - I'm not sure if it made the clean ti- T_I-digits any louder. Oh  I see. I only remember noticing it made the  um  P_Z_M signal louder. O_K. Well  I don't understand then. Yeah. Huh. I don't know. If it's - if it's - like  if it's trying to find a - a reverberation filter  it could be that this reverberation filter is making things quieter. And then if you take it out - that taking it out makes things louder. I mean. Uh  no. I mean  uh  there's - there's nothing inherent about Nuh-huh. removing - if you're really removing  uh  r- uh  then I don't see how that would make it louder. So it might be just some - The mean. O_K. Yeah  I see. Yeah. O_K. So I should maybe listen to that stuff again. Yeah. It might just be some artifact of the processing that - that  uh  if you're - Uh  yeah. I don't know. Oh. O_K. I wonder if there could be something like  uh - Eh- for s- for the P_Z_M data  uh  you know  if occasionally  uh  somebody hits the table or something  you could get a spike. Uh. I'm just wondering if there's something about the  um - you know  doing the mean normalization where  uh  it - it could cause you to have better signal-to-noise ratio. Um. Well  you know  there is this. Wait a minute. It - it - i- maybe - i- If  um - Subtracting the - the mean log spectrum is - is - is like dividing by the spectrum. So  depending what you divide by  if your - if s- your estimate is off and sometimes you're - you're - you're getting a small number  Mm-hmm. you could make it bigger. Mm-hmm. So  it's - it's just a - a question of - there's - It - it could be that there's some normalization that's missing  or something to make it - Mm-hmm. Uh  y- you'd think it shouldn't be larger  but maybe in practice it is. Hmm. That's something to think about. I don't know. I had a question about the system - the S_R_I system. So  you trained it on T_I-digits? But except this  it's exactly the same system as the one that was tested before and that was trained on Macrophone. Right? So on T_I-digits it gives you one point two percent error rate and on Macrophone it's still O_ point eight. Uh  but is it exactly the same system? Uh. I think so. If you're talking about the Macrophone results that Andreas had about  um  a week and a half ago  I think it's the same system. Hmm. Mm-hmm. So you use V_T_L- uh  vocal tract length normalization and  um  Mm-hmm. like M_L_L_R transformations also  and - I'm sorry  was his point eight percent  er  a - a result on testing on Macrophone or - or training? That's - all that stuff. It was training on Macrophone and testing - yeah  on - on meeting digits. Oh. So that was done already. So we were - Uh  and it's point eight? O_K. Mm-hmm. O_K. Yeah. I - I've just been text- testing the new Aurora front-end with - well  Aurora system actually - so front-end and H_T_K  um  acoustic models on the meeting digits and it's a little bit better than the previous system. We have - I have two point seven percent error rate. And before with the system that was proposed  it's what? It was three point nine. So. We are getting better. Oh  that's a lot better. So  what - w- ? With the - with the H_T_K back-end? What we have for Aurora? And - Yeah. Two point seven. I know in the meeting  like - Right. On the meeting we have two point seven. Oh. That's with the new I_I_R filters? Uh. Yeah  yeah. So  yeah  we have the new L_D_A filters  and - O_K. I think  maybe - I didn't look  but one thing that makes a difference is this D_C offset compensation. Uh  eh - Do y- did you have a look at - at the meet- uh  meeting digits  if they have a D_C component  or - ? I - I didn't. No. Oh. Hmm. No. The D_C component could be negligible. I mean  if you are recording it through a mike. I mean  any - all of the mikes have the D_C removal - some capacitor sitting right in that bias it . Yeah. But this - uh  uh  uh  no. Because  uh  there's a sample and hold in the A_to_D. And these period- these typically do have a D_C offset. Oh  O_K. And - and they can be surprisingly large. It depends on the electronics. Oh  so it is the digital - O_K. It's the A_to_D that introduces the D_C in. Yeah. The microphone isn't gonna pass any D_C. But - but  Yeah. Yeah. Yeah. O_K. typi- you know  unless - Actually  there are instrumentation mikes that - that do pass - go down to D_C. But - but  Mm-hmm. uh  no  it's the electronics. And they - and - Mm-hmm. then there's amplification afterwards. And you can get  I think it was - I think it was in the Wall Street Journal data that - that - I can't remember  one of the DARPA things. There was this big D_C- D_C offset we didn't - we didn't know about for a while  while we were Mm-hmm. messing with it. And we were getting these terrible results. And then we were talking to somebody and they said  ""Oh  yeah. Didn't you know? Everybody knows that. There's all this D_C offset in th-"" So  yes. You can have D_C offset in the data. Yeah. Oh  O_K. O_K. So was that - was that everything  Dave? Oh. And I also  um  did some experiments about normalizing the phase. Um. So I c- I came up with a web page that people can take a look at. And  um  the interesting thing that I tried was  um  Adam and Morgan had this idea  um  since my original attempts to  um  take the mean of the phase spectra over time and normalize using that  by subtracting that off  didn't work. Um  so  well  that we thought that might be due to  um  problems with  um  the arithmetic of phases. They - they add in this modulo two pi way and  um  there's reason to believe that that approach of taking the mean of the phase spectrum wasn't really mathematically correct. So  what I did instead is I took the mean of the F_F_T spectrum without taking the log or anything  and then I took the phase of that  and I subtracted that phase off to normalize. But that  um  didn't work either. See  we have a different interpretation of this. He says it doesn't work. I said  I think it works magnificently  but just not for the task we intended. Uh  it gets rid of the speech. What does it leave? Uh  gets rid of the speech. Uh  it leaves - you know  it leaves the junk. I mean  I - I think it's - it's tremendous. Oh  wow. You see  all he has to do is go back and reverse what he did before  and he's really got something. Well  could you take what was left over and then subtract that? Ex- exactly. Yeah  you got it. Yeah. Yeah. Oh  it's - So  it's - it's a general rule. Just listen very carefully to what I say and do the opposite. Including what I just said. And  yeah  that's everything. All set? Do you want to go  Stephane? Um. Yeah. Maybe  concerning these d- still  these meeting digits. I'm more interested in trying to figure out what's still the difference between the S_R_I system and the Aurora system. And - Um. Yeah. So  I think I will maybe train  like  gender-dependent models  because this is also one big difference between the two systems. Um  the other differences were the fact that maybe the acoustic models of the S_R_I are more - S_R_I system are more complex. But  uh  Chuck  you did some experiments with this and It didn't seem to help in the H_T_K system. it was hard t- to - to have some exper- some improvement with this. Um. Well  it sounds like they also have - he - he's saying they have all these  uh  Mm-hmm. uh  different kinds of adaptation. You know  they have channel adaptation. They have speaker adaptation. Yeah. Right. Yeah. Yeah. Yeah. Yeah. Well  there's also the normalization. Like they do  um - I'm not sure how they would do it when they're working with the digits  but  like  in the Switchboard data  there's  um - The vocal tr- conversation-side normalization for the non-C_zero components  and then utterance normalization for the C_zero components. Yeah. Yeah. This is another difference. Their normalization works like on - on the utterance levels. Mm-hmm. But we have to do it - We have a system that does it on-line. So  it might be - Right. it might be better with - it might be worse if Yeah. the channel is constant  or - And the acoustic models are like -k triphone models or - or is it the whole word? Nnn. S_R_I - it's - it's tr- Yeah. I guess it's triphones. S_R_I. Yeah. It's triphone. I think it's probably more than that. I mean  so they - they have - I - I thin- think they use these  uh  Huh. uh  genone things. So there's - there's these kind of  uh  uh  pooled models and - and they can go out to all sorts of dependencies. Oh. It's like the tied state. So. Mm-hmm. They have tied states and I think - I - I - I don't real- I'm talk- I'm just guessing here. But I think - I think they - they don't just have triphones. I think they have a range of - of  uh  dependencies. O_K. Mm-hmm. Mm-hmm. Mm-hmm. Hmm. And - Yeah. Well. Um. Well  the first thing I - that I want to do is just maybe these gender things. Uh. And maybe see with Andreas if - Well  I - I don't know how much it helps  what's the model. So - so the n- stuff on the numbers you got  the two point seven  is that using the same training data that the S_R_I system used and got one point two? That's right. So it's the clean T_I-digits training set. So exact same training data? Right. Mm-hmm. O_K. I guess you used the clean training set. Right. For - with the S_R_I system - Mm-hmm. Well. You know  the - the Aurora baseline is set up with these  um - this version of the clean training set that's been filtered with this G_-seven-one-two filter  and  um  to train the S_R_I system on digits S_- Andreas used the original T_I-digits  um  under U_ doctor-speech data T_I-digits  which don't have this filter. But I don't think there's any other difference. Mm-hmm. Mm-hmm. Yeah. So is that - ? Uh  are - are these results comparable? So you - you were getting with the  uh  Aurora baseline something like two point four percent on clean T_I-digits  when  uh  training the S_R_I system with clean T_R digits - T_I-digits. Right? And - Yeah. Um. Uh-huh. And  so  is your two point seven comparable  where you're  uh  uh  using  uh  the submitted system? Yeah. I think so. Yeah. O_K. So it's about the same  maybe a little worse. Mm-hmm. W- w- it was one - one point two Ye- with the S_R_I system  I - I'm sorry. Yeah. The complete S_R_I system is one point two. Yeah. You - you were H_T_K. Right? O_K. That's right. So - O_K  so the comparable number then  uh Mm-hmm. for what you were talking about then  since it was H_T_K  would be the um  two point f- It was four point something. Right? D- d- The H_T_K system with  uh  b- Oh  right  right  right  right. M_F_C_C features - Do you mean the b- ? The baseline Aurora-two system  trained on T_I-digits  tested on Meeting Recorder near  I think we saw in it today  and it was about six point six percent. Oh. Right. Right  right  right. O_K. Alright. So - So - Yeah. The only difference is the features  right now  between this and - He's doing some different things. Yes. O_K  good. So they are helping. That's good to hear. Yeah. Mm-hmm. They are helping. Yeah. Um. Yeah. And another thing I - I maybe would like to do is to just test the S_R_I system that's trained on Macrophone - test it on  Yeah. uh  the noisy T_I-digits  cuz I'm still wondering where this improvement comes from. When you train on Macrophone  it seems better on meeting digits. But I wonder if it's just because maybe Macrophone is acoustically closer to the meeting digits than - than T_I-digit is  which is - T_I-digits are very clean recorded digits and - Mm-hmm. Uh  f- s- You know  it would also be interesting to see  uh - to do the regular Aurora test  um  but use the S_R_I system instead of H_T_K. That's - Yeah. That's what I wanted  just  uh - Yeah. So  just using the S_R_I system  test it on - and test it on Aurora T_I-digits. Right. Why not the full Aurora  uh  test? Um. Yeah. There is this problem of multilinguality yet. So we don't - Mm-hmm. You'd have to train the S_R_I system with - with all the different languages. i- i- We would have to train on - Right. Yeah. Yeah. That's what I mean. So  like  comple- It'd be a lot of work. That's the only thing. Yeah. It's - Mmm. Well  I mean  uh  Mmm. uh  I guess the work would be into getting the - the files in the right formats  or something. Right? I mean - Mm-hmm. Because when you train up the Aurora system  you're  uh - you're also training on all the That's right. Yeah. data. I mean  it's - Yeah. I see. Oh  so  O_K. Right. I see what you mean. That's true  but I think that also when we've had these meetings week after week  oftentimes people have not done the full arrange of things because - on - on whatever it is they're trying  because it's a lot of work  even just with the H_T_K. Mm-hmm. Mm-hmm. So  it's - it's a good idea  but it seems like it makes sense to do some pruning Mm-hmm. first with a - a test or two that makes sense for you  and then take the likely candidates and go further. Yeah. Yeah. Mm-hmm. Yeah. But  just testing on T_I-digits would already give us some information about what's going on. And - mm-hmm. Uh  yeah. O_K. Uh  the next thing is this - this V_A_D problem that  um  um - So  I'm just talking about the - the curves that I - I sent - I sent you - so  whi- that shows that when the S_N_R decrease  uh  the current V_A_D approach doesn't drop much frames for some particular noises  uh  which might be then noises that are closer to speech  uh  acoustically. I- i- Just to clarify something for me. I- Mm-hmm. They were supp- Supposedly  in the next evaluation  they're going to be supplying us with boundaries. So does any of this matter? I mean  other than our interest in it. Uh - Uh - Well. First of all  the boundaries might be  uh - like we would have t- two hundred milliseconds or - before and after speech. Uh. So removing more than that might still make a difference in the results. And - Do we - ? I mean  is there some reason that we think that's the case? No. Because we don't - didn't Yeah. looked that much at that. But  still  I think it's an interesting problem. And - Oh  yeah. Um. Yeah. But maybe we'll get some insight on that when - when  uh  the gang gets back from Crete. Because there's lots of interesting problems  of course. And then the thing is if - if they really are going to have some means of giving us fairly tight  Mm-hmm. Yeah  yeah. Mm-hmm. uh  boundaries  then that won't be so much the issue. Mm-hmm. Um But I don't know . Because w- we were wondering whether that V_A_D is going to be  like  a realistic one or is it going to be some manual segmentation. And then  like  if - if that V_A_D is going to be a realistic one  then we can actually use their markers to shift the point around  I mean  the way we want to find a - I mean  rather than keeping the twenty frames  we can actually move the marker to a point which we find more suitable for us. Mm-hmm. But if that is going to be something like a manual  uh  segmenter  then we can't use that information anymore  because that's not going to be the one that is used in the final evaluation. Right. Mm-hmm. Right. So. We don't know what is the type of V_A_D which they're going to provide. Yeah. Yeah. And actually there's - Yeah. There's an - uh  I think it's still for - even for the evaluation  uh  it might still be interesting to work on this because the boundaries apparently that they would provide is just  um  starting of speech and end of speech uh  at the utterance level. And - Um. With some - some gap. I mean  with some pauses in the center  So - provided they meet that - whatever the hang-over time which they are talking. Yeah. But when you have like  uh  five or six frames  both - Yeah. Then the- they will just fill - fill it up. I mean  th - Yeah. it - it - with - Yeah. So - So if you could get at some of that  uh - although that'd be hard. But - but - Yeah. Yeah. It might be useful Yeah. for  like  noise estimation  and a lot of other things that we want to work on. But - Mmm. Right. O_K. Yeah. So I did - I just started to test putting together two V_A_D which was - was not much work actually. Um  I im- re-implemented a V_A_D that's very close to the  um  energy-based V_A_D that  uh  the other Aurora guys use. Um. So  which is just putting a threshold on the noise energy  Mm-hmm. and  detect- detecting the first group of four frames that have a energy that's above this threshold  and  uh  from this point  uh  tagging the frames there as speech. So it removes the first silent portion - portion of each utterance. And it really removes it  um  still o- on the noises where our M_L_P V_A_D doesn't work a lot. Mmm. Uh  and - Cuz I would have thought that having some kind of spectral information  uh - uh  you know  in the old days people would use energy and zero crossings  for instance - uh  would give you some better performance. Right? Cuz you might have low-energy fricatives or - or  uh stop consonants  or something like that. Mm-hmm. Uh. Yeah. So  your point is - will be to u- use whatever - Oh  that if you d- if you use purely energy and don't look at anything spectral  then you don't have a good way of distinguishing between low-energy speech components and nonspeech. Mm-hmm. And  um  just as a gross generalization  most nonsp- many nonspeech noises have a low-pass kind of characteristic  some sort of slope. And - and most  um  low-energy speech components that are unvoiced have a - Mm-hmm. a high-pass kind of characteristic - an upward slope. Yeah. So having some kind of a - uh  you know  at the beginning of a - of a - of an S_ sound for instance  just starting in  it might be pretty low-energy  but it will tend to have this high-frequency component. Whereas  Mm-hmm. a - a lot of rumble  and background noises  and so forth will be predominantly low-frequency. Uh  you know  by itself it's not enough to tell you  but it plus energy is sort of - Yeah. it plus energy plus timing information is sort of - Mm-hmm. I mean  if you look up in Rabiner and Schafer from like twenty-five years ago or something  that's sort of what they were using then. So it's - it's not a - Mm-hmm. Mm-hmm. Hmm. So  yeah. It - it might be that what I did is - so  removes like low  um  uh - low-energy  uh  speech frames. Because the way I do it is I just - I just combine the two decisions - so  the one from the M_L_P and the one from the energy-based - with the - with the and operator. So  I only keep the frames where the two agree that it's speech. So if the energy-based dropped - dropped low-energy speech  mmm  they - they are - they are lost. Mmm. But s- still  the way it's done right now it - it helps on - on the noises where - it seems to help on the noises where Mm-hmm. our V_A_D was not very good. Well  I guess - I mean  one could imagine combining them in different ways. But - but  I guess what you're saying is that the - the M_L_P-based one has the spectral information. Yeah. So. But - Yeah. But the way it's combined wi- is maybe done - Well  yeah . Well  you can imagine - The way I use a an- a ""AND"" operator is - So  it - I  uh - Is - ? The frames that are dropped by the energy-based system are - are  uh  dropped  even if the  um  M_L_P decides to keep them. Right. Right. And that might not be optimal  but - but - I mean  I guess in principle what you'd want to do is have a - But  yeah. Mm-hmm. No- uh  a probability estimated by each one and - and put them together. Yeah. Mmm. M- Yeah. Something that - that I've used in the past is  um - when just looking at the energy  is to look at the derivative. And you make your decision when the derivative is increasing for so many frames. Then you say that's beginning of speech. Uh-huh. But  I'm - I'm trying to remember if that requires that you keep some amount of speech in a buffer. I guess it depends on how you do it. But I mean  that's - that's been a useful thing. Yeah. Mm-hmm. Mm-hmm. Yeah. Well  every- everywhere has a delay associated with it. I mean  you still have to k- always keep a buffer  Mm-hmm. then only make a decision because you still need to smooth the decision further. Right. Right. So that's always there. Yeah. O_K. Well  actually if I don't - maybe don't want to work too much of - on it right now. I just wanted to - to see if it's - what I observed was the re- was caused by this - this V_A_D problem. And it seems to be the case. Mm-hmm. Um. Uh  the second thing is the - this spectral subtraction. Um. Um  which I've just started yesterday to launch a bunch of  uh  twenty-five experiments  uh  with different  uh  values for the parameters that are used. So  it's the Makhoul-type spectral subtraction which use an over-estimation factor. So  we substr- I subtract more  um  noise than the noise spectra that is estimated on the noise portion of the s- uh  the utterances. So I tried several  uh  over-estimation factors. And after subtraction  I also add a constant noise  and I also try different  uh  noise  uh  values and we'll see what happen. Hmm. O_K. Mm-hmm. Mm-hmm. But st- still when we look at the  um - Well  it depends on the parameters that you use  but for moderate over-estimation factors and moderate noise level that you add  you st- have a lot of musical noise. Um. On the other hand  when you subtract more and when you add more noise  you get rid of this musical noise but maybe you distort a lot of speech. So. Well. Mmm. Well  it - until now  it doesn't seem to help. But We'll see. So the next thing  maybe I - what I will try to - to do is just to try to smooth mmm  the  um - to smooth the d- the result of the subtraction  to get rid of the musical noise  using some kind of filter  or - Can smooth the S_N_R estimate  also. Yeah. Right. Mmm. Your filter is a function of S_N_R. Hmm? Yeah. So  to get something that's - would be closer to what you tried to do with Wiener filtering. And - Yeah. Mm-hmm. Yeah. Actually  it's  uh - Uh. I don't know  it's - go ahead. And it's - go ahead. It - Maybe you can - I think it's - That's it for me. O_K. So  uh - u- th- I've been playing with this Wiener filter  like. And there are - there were some bugs in the program  so I was p- initially trying to clear them up. Because one of the bug was - I was assuming that always the VAD - uh  the initial frames were silence. It always started in the silence state  but it wasn't for some utterances. So the - it wasn't estimating the noise initially  and then it never estimated  because I assumed that it was always Mm-hmm. silence. So this is on SpeechDat-Car Italian? Yeah. SpeechDat-Car Italian. So  in some cases s- there are also - Yeah. There're a few cases  actually  which I found later  that there are. o- Uh-huh. So that was one of the bugs that was there in estimating the noise. And  uh  so once it was cleared  uh  I ran a few experiments with different ways of smoothing the estimated clean speech and how t- estimated the noise and  eh  smoothing the S_N_R also. And so the - the trend seems to be like  uh  smoothing the current estimate of the clean speech for deriving the S_N_R  which is like deriving the Wiener filter  seems to be helping. Then updating it quite fast using a very small time constant. So we'll have  like  a few results where the - estimating the - the - More smoothing is helping. But still it's like - it's still comparable to the baseline. I haven't got anything beyond the baseline. But that's  like  not using any Wiener filter. And  uh  so I'm - I'm trying a few more experiments with different time constants for smoothing the noise spectrum  and smoothing the clean speech  and smoothing S_N_R. So there are three time constants that I have. So  I'm just playing around. So  one is fixed in the line  like Smoothing the clean speech is - is helping  so I'm not going to change it that much. But  the way I'm estimating the noise and the way I'm estimating the S_N_R  I'm just trying - trying a little bit. So  that h- And the other thing is  like  putting a floor on the  uh  S_N_R  because that - if some - In some cases the clean speech is  like - when it's estimated  it goes to very low values  so the S_N_R is  like  very low. And so that actually creates a lot of variance in the low-energy region of the speech. So  I'm thinking of  like  putting a floor also for the S_N_R so that it doesn't vary a lot in the low-energy regions. And  uh. So. The results are  like - So far I've been testing only with the baseline  which is - which doesn't have any L_D_A filtering and on-line normalization. I just want to separate the - the contributions out. So it's just VAD  plus the Wiener filter  plus the baseline system  which is  uh  just the spectral - I mean  the mel sp- mel  uh  frequency coefficients. Um. And the other thing that I tried was - but I just took of those  uh  Carlos filters  which Hynek had  to see whether it really h- helps or not. I mean  it was just a - a run to see whether it really degrades or it helps. And it's - it seems to be like it's not hurting a lot by just blindly picking up one filter which is nothing but a four hertz - a band-pass m- m- filter on the cubic root of the power spectrum. So  that was the filter that Hy- uh  Carlos had. And so - Yeah. Just - just to see whether it really - it's - it's - is it worth trying or not. So  it doesn't seems to be degrading a lot on that. So there must be something that I can - that can be done with that type of noise compensation also  which - I guess I would ask Carlos about that. I mean  how - how he derived those filters and - and where d- if he has any filters which are derived on O_G_I stories  added with some type of noise which - what we are using currently  or something like that. So maybe I'll - This is cubic root of power spectra? Yeah. Cubic root of power spectrum. So  if you have this band-pass filter  you probably get n- you get negative values. Right? Yeah. And I'm  like  floating it to z- zeros right now. O_K. So it has  like - the spectrogram has  like - Uh  it actually  uh  enhances the onset and offset of - I mean  the - the begin and the end of the speech. So it's - there seems to be  like  deep valleys in the begin and the end of  like  high-energy regions  because the filter has  like  a sort of Mexican-hat type structure. Mm-hmm. So  those are the regions where there are  like - when I look at the spectrogram  there are those deep valleys on the begin and the end of the speech. Mm-hmm. But the rest of it seems to be  like  pretty nice. Mm-hmm. So. That's something I observe using that filter. And - Yeah. There are a few - very - not a lot of - because the filter doesn't have a - really a deep negative portion  so that it's not really creating a lot of negative values in the cubic root. So  I'll - I'll s- may- continue with that for some w- I'll - I'll - Maybe I'll ask Carlos a little more about how to play with those filters  and - but while making this Wiener filter better. So. Yeah. That - that's it  Morgan. Uh  last week you were also talking about building up the subspace stuff? Yeah. I - I - I would actually m- m- didn't get enough time to work on the subspace last week. It was mostly about finding those bugs and th- you know  things  and I didn't work much on that. O_K. How about you  Carmen? Well  I am still working with  eh  V_T_S. And  one of the things that last week  eh  say here is that maybe the problem was with the diff because the signal have different level of energy. Hmm? And  maybe  talking with Stephane and with Sunil  we decide that maybe it was interesting to - to apply on-line normalization before applying V_T_S. But then we decided that that's - it doesn't work absolutely  because we modified also the noise. And - Well  thinking about that  we - we then - we decide that maybe is a good idea. We don't know. I don't hav- I don't - this is - I didn't do the experiment yet - to apply V_T_S in cepstral domain. The other thing is - So - so  in - i- i- and - Not - and C_zero would be a different - So you could do a different normalization for C_zero than for other things anyway. I mean  the other thing I was gonna suggest is that you could have two kinds of normalization with - with  uh  different time constants. So  uh  you could do some normalization s- uh  before the V_T_S  and then do some other normalization after. I don't know. But - but C_zero certainly acts differently than the others do  so that's - Uh. Mm-hmm. Well  we s- decide to m- to - to obtain the new expression if we work in the cepstral domain. And - Well. I am working in that now  but I'm not sure if that will be usefu- useful. I don't know. It's k- it's k- Uh-huh. It's quite a lot - It's a lot of work. Well  it's not too much  but this - it's work. And I want to know if - Uh-huh. Yeah. if we have some feeling that the result - I - I would like to know if - I don't have any feeling if this will work better than apply V_T_S aft- in cepstral domain will work better than apply in m- mel - in filter bank domain. I r- I'm not sure. I don't - I don't know absolutely nothing. Mm-hmm. Yeah. Well  you're - I think you're the first one here to work with V_T_S  so  uh  maybe we could call someone else up who has  ask them their opinion. Uh  I don't - I don't have a good feeling for it. Mm-hmm. Um. Pratibha. Actually  the V_T_S that you tested before was in the log domain and so the codebook is Yeah? e- e- kind of dependent on the level of the speech signal. And - So I expect it - If - if you have something that's independent of this  I expect it to - it - to  uh  be a better To have better - model of speech. And. Well. You - you wouldn't even need to switch to cepstra. Right? I mean  you can just sort of normalize the - No. We could normali- norm- I mean  remove the median. Yeah. Yeah. Mm-hmm. And then you have one number which is very dependent on the level cuz it is the level  and the other which isn't. Mm-hmm. Yeah. But here also we would have to be careful about removing the mean of speech and Ye- not of noise. Because it's like first doing general normalization and then Yea- noise removal  which is - Yeah. We - I was thinking to - to - to estimate the noise with the first frames and then apply the V_A_D  Mm-hmm. Mm-hmm. before the on-line normalization. We - we see - Mm-hmm. Well  I am thinking about that and working about that  but I don't have result this week. Yeah. Sure. I mean  one of the things we've talked about - maybe it might be star- time to start thinking about pretty soon  is as we look at the pros and cons of these different methods  how do they fit in with one another? Because we've talked about potentially doing some combination of a couple of them. Maybe - maybe pretty soon we'll have some sense of what their characteristics are  so we can see what Mm-hmm. should be combined. Mm-hmm. Is that it? O_K. Why don't we read some digits? O_K? Yep. Want to go ahead  Morgan? Sure. Transcript L_ dash two one two. Two  four eight eight  two nine  one zero zero  nine. Seven zero  six one  seven eight  two five  nine six. Eight five eight  seven seven nine  six one nine. Four eight four four  one  seven five two. Two four  six two  seven two  one eight  two one. Eight  five five two  three five  one two five  one. Five zero three eight  one  one seven seven. Six  nine seven four  nine six  three one one  five. Transcript L_ dash two one three. Two four  five four  four three  seven six  five seven. Three one eight  O_ two O_  nine nine seven. One  three one seven  three eight  eight three one  three. Zero seven one seven  two four one five  three nine eight five. Zero  nine eight nine  zero six  two one seven  three. Two nine  nine five  six nine  six six  nine nine. Eight five nine  nine four six  four six five one. Two four five  three seven five  nine nine three five. Transcript L_ dash two one four. Seven five three  nine five nine  one three five two. Five three five  one O_ one  O_ six seven. One seven zero  four three seven  one five five. Nine  two four six  eight one  O_ seven one  zero. One four seven  seven six  seven nine one five. Two one nine two  two zero nine five  six zero nine seven. Zero  two five zero  eight five  eight four zero  five. Three four t- two three  seven  four nine one. Transcript L_ dash two one five. Three five seven one  six five four five  nine nine one nine. Five five two  zero eight five  two two five. One three seven  nine eight eight  nine five eight. Zero five six  six five four  zero five eight nine. Three four five six  five  two six eight. Three zero one eight  two  eight six eight. Eight three  nine four  four zero  seven six  three three. Five zero seven  nine three two  zero three zero. Transcript L_ dash two one six. Zero two zero  one three  two nine five zero. Zero one three  eight zero  nine six two six. Zero two  seven four  four zero  seven five  nine one. Two four  seven two  two nine  three two  three three. Seven four five  one one  four eight one seven. Six  nine one eight  nine five  three nine six  four. Six six seven six  three  seven nine nine. Five one  seven zero  nine zero  seven eight  one one. Transcript L_ dash two one seven. Seven two three  nine five  four two one one. Nine  two three eight  five two  three six three  nine. Zero four seven  two one seven  one one nine. Three one five five  nine two six six  zero five eight four. Five nine eight eight  four eight three eight  six nine three four. Eight five one  one four nine  zero one seven. Zero three zero zero  six  five eight seven. Five three two  four two  five seven eight eight. Transcript L_ dash two one eight. Eight  eight nine six  O_ two  three six six  seven. Seven eight  O_ seven  one seven  two O_  three nine. O_ six four  three two two  eight five one. Five one five  five nine  nine nine six one. Four three seven  eight seven six  six eight five seven. O_ three six  nine two six  O_ two nine. Three eight one  five five  eight four three four. Four one five  five O_ one  five two eight. O_ K. ",The Berkley Meeting Recorder Group discussed the most recent progress with their current project  a digit recognition system for use in cell phones. This included some discussion of results  comparing various other groups' systems  issues involving the set up  and plans for future work. Results are required for an upcoming meeting  but since some group members will be away  results need to be in sooner. Although error rates have been greatly reduced  current rates are still unusable in a practical situation. There is a problem replicating some results found by partner OGI  but it is unclear why. mn007 and fn002 have been working with the new Danish and German databases  making improvements  though more so with results on the Danish. The results are reasonable  but still not good enough. However  it has not been possible to compare results to the best  as still development system. There are a number of things that the group wishes to consider for looking for further improvements. There are various techniques that various groups have tried  and they should all be considered for possible combinations of systems. One suitable candidate for combination is the group's own filtering which reduces bandwidth to half bit transmission rate. 
"O_K. Oh  I don't - I think I'm zero. Wow! Ah- Hello  hello  hello  hello. Wh- what causes the crash? Unprecedented. Did you fix something? Hello. Five  five. Oh  maybe it's the turning - turning off and turning on of the mike  right? Hello  hello. Uh  you think that's you? Yeah  O_K  mine's working. Aaa-aaa-aaa. O_K. That's me. Oh. O_K. O_K. So  um I guess we are um gonna do the digits at the end. Uh Channel - channel three  yeah. O_K. Mmm  channel five? Channel two. Doesn't work? Two. Yeah  that's the mike number there  uh No? Is it written on her sheet  I believe. Mike four. Watch this. Yep  that's me. Uh  mike number five  Ah  era el cuatro. Yeah. and But  channel channel - channel four. Yeah yeah yeah. This is you. O_K. I saw that. Ah - yeah  it's O_K. Yeah. And I'm channel uh two I think  or channel - Ooo. I think I'm channel two. Oh  I'm channel - must be channel one. Channel one? Yes  O_K. Channel - O_K. So uh I also copied uh the results that we all got in the mail I think from uh - from O_G_I and we'll go - go through them also. So where are we on - on uh our runs? Uh so. uh - We - So As I was already said  we - we mainly focused on uh four kind of features. The P_L_P  the P_L_P with J_RASTA  the M_S_G  and the M_F_C_C from the baseline Aurora. Excuse me. I decided to talk about that. Mm-hmm. Uh  and we focused for the - the test part on the English and the Italian. Um. We've trained uh several neural networks on - so - on the T_I-digits English and on the Italian data and also on the broad uh English uh French and uh Spanish databases. Mmm  so there's our result tables here  for the tandem approach  and um  actually what we - we @@ observed is that if the network is trained on the task data it works pretty well. I can't get back far enough. Chicken on the grill. O_K. Our - our uh - There's a - We're pausing for a photo - Sorry  guys. Try that corner. How about over th- from the front of the room? Yeah  it's longer. We're pausing for a photo opportunity here. Uh. Uh. So. Oh wait wait wait wait wait. Wait. Hold on. Hold on. Let me give you a black screen. Get out of the - Yeah. O_K. One more. He's facing this way. Because we said we were gonna do this and I just remembered. What? O_K  this - this would be a good section for our silence detection. O_K. Um Mm-hmm. Musical chairs everybody! Oh. O_K. So um  you were saying about the training data - Yeah  so if the network is trained on the task data um tandem works pretty well. And uh actually we have uh  results are similar Yeah. Do you mean if it's trained only on - Only on  yeah. On data from just that task  that language? Just that task. But actually we didn't train network on uh both types of data I mean uh phonetically ba- phonetically balanced uh data and task data. We only did either task - task data or uh broad data. Mmm. Mm-hmm. Um Yeah. So  So how - I mean - So what's th- clearly it's gonna be good then but the question is how much worse is it if you have broad data? I mean  my assump- From what I saw from the earlier results  uh I guess last week  was that um  if you trained on one language and tested on another  say  that the results were - were relatively poor. Mmm. Yeah. But - but the question is if you train on one language but you have a broad coverage and then test in another  does that - is that improve things i- c- in comparison? If we use the same language? No  no  no. Different lang- So um If you train on T_I-digits and test on Italian digits  you do poorly  Mm-hmm. let's say. I don't have the numbers in front of me  so I'm just imagining. But - Yeah but I did not uh do that. We - E- So  you didn't train on TIMIT and test on - on Italian digits  say? No  we did four - four kind of - of testing  actually. The first testing is with task data - So  with nets trained on task data. So for Italian on the Italian speech @@ . The second test is trained on a single language um with broad database  but the same language as the t- task data. O_K. But for Italian we choose Spanish which we assume is close to Italian. The third test is by using  um the three language database and the fourth is W- which in - It has three languages. That's including the w- the - the - This includes - the one that it's - Yeah. But not In- digits. I mean it's - Right. The three languages is not digits  it's the broad data. O_K. Yeah And the fourth test is uh excluding from these three languages the language that is the task language. Oh  O_K  yeah  so  that is what I wanted to know. Yeah. I just wasn't saying it very well  I guess. Uh  yeah. So um for uh T_I-digits for ins- example uh when we go from T_I-digits training to TIMIT training uh we lose uh around ten percent  uh. Relative. The error rate increase u- of - of - Right. of ten percent  relative. So this is not so bad. And then when we jump to the multilingual data it's uh it become worse and  well Ab- about how much? Around uh  let's say  twenty perc- twenty percent further. So. Yeah. Twenty percent further? Twenty to - to thirty percent further. Yeah. And so  remind me  the multilingual stuff is just the broad data. Right? Yeah. It's not the digits. So it's the combination of two things there. It's removing the task specific training and it's adding other languages. Yeah. Yeah. O_K. But the first step is al- already removing the task s- specific from - from - Already  right right right. So. And we lose - So they were sort of building here? O_K? Yeah. Uh So  basically when it's trained on the - the multilingual broad data um or number - so  the - the ratio of our error rates uh with the baseline error rate is around uh one point one. So. Yes. And it's something like one point three of - of the uh - I- i- if you compare everything to the first case at the baseline  you get something like one point one for the - for the using the same language but a different task  and something like one point three for three - three languages No no no. broad stuff. Uh same language we are at uh - for at English at O_ point eight. So it improves  compared to the baseline. But - So. Le- let me. I - I - I'm sorry. I - I - I meant something different by baseline Tas- task data we are u- Yeah. So let me - let me - Um  so  um - Mmm. O_K  fine. Let's - let's use the conventional meaning of baseline. I - I - Hmm. By baseline here I meant uh using the task specific data. Oh yeah  the f- Yeah  O_K. Yeah. But uh - uh  because that's what you were just doing with this ten percent. So I was just - I just trying to understand that. So if we call Yeah. Sure. a factor of w- just one  just normalized to one  the word error rate Mmm. that you have for using T_I-digits as - as training and T_I-digits as test  uh different words  I'm sure  but - Mm-hmm. but uh  uh the same task and so on. If we call that ""one""  then what you're saying is Mm-hmm. that the word error rate for the same language but using uh different training data than you're testing on  say TIMIT and so forth  Mm-hmm. it's one point one. Yeah  it's around one point one. Yeah. Right. And if it's - you do go to three languages including the English  it's something like one point three. Ye- That's what you were just saying  I think. Uh  more actually. If I - One point four? Yeah. So  it's an additional thirty percent. What would you say? Around one point four yeah. O_K. And if you exclude English  from this combination  what's that? If we exclude English  um there is not much difference with the data with English. Aha! So. Yeah. That's interesting. That's interesting. Do you see? Because - Uh. Uh  so - No  that - that's important. So what - what it's saying here is just that ""yes  there is a reduction in performance  when you don't um have the s- when you don't have um Task data. Wait a minute  th- th- the - Hmm. No  actually it's interesting. So it's - So when you go to a different task  there's actually not so different. It's when you went to these - So what's the difference between two and three? Between the one point one case and the one point four case? I'm confused. It's multilingual. Yeah. The only difference it's - is that it's multilingual - Um Yeah. Cuz in both - in both - both of those cases  you don't have the same task. Yeah sure. So is - is the training data for the - for this one point four case - does it include the training data for the one point one case? Uh yeah. Yeah  a fraction of it. A part of it  yeah. How m- how much bigger is it? Um Yeah  um. It's two times  actually? Yeah. Um. The English data - No  the multilingual databases are two times the broad English data. We just wanted to keep this  w- well  not too huge. So. So it's two times  but it includes the - but it includes the broad English data. I think so. Do you - Uh  Yeah. And the broad English data is what you got this one point one with. So that's TIMIT basically right? Mm-hmm. Yeah. So it's band-limited TIMIT. Mm-hmm. Mm-hmm. This is all Yeah. Downs- eight kilohertz sampling. Right. So you have band-limited TIMIT  gave you uh almost as good as a result as using T_I- digits on a T_I-digits test. O_K? Hmm? Um and um But  when you add in more training data but keep the neural net the same size  it um performs worse on the T_I-digits. O_K  now all of this is - This is noisy T_I-digits  I assume? Yep. Both training and test? Yeah. O_K. Um O_K. Well. We - we - we may just need to uh - So I mean it's interesting that h- going to a different - different task didn't seem to hurt us that much  and going to a different language um It doesn't seem to matter - The difference between three and four is not particularly great  so that means that whether you have the language in or not is not such a big deal. Mmm. It sounds like um uh we may need to have more of uh things that are similar to a target language or - I mean. You have the same number of parameters in the neural net  you haven't increased the size of the neural net  and maybe there's just - just not enough complexity to it to represent the variab- increased variability in the - in the training set. That - that could be. Um So  what about - So these are results with uh th- that you're describing now  that they are pretty similar for the different features or - or uh - Uh  let me check. Uh. So. This was for the P_L_P  Yeah. Um. The - Yeah. For the P_L_P with J_RASTA the - Yeah. the - we - This is quite the same tendency  with a slight increase of the error rate  uh if we go to - to TIMIT. And then it's - it gets worse with the multilingual. Um. Yeah. There - there is a difference actually with - b- between P_L_P and J_RASTA is that J_RASTA seems to perform better with the highly mismatched condition but slightly - slightly worse for the well matched condition. Mmm. I have a suggestion  actually  even though it'll delay us slightly  would - would you mind running into the other room and making copies of this? Cuz we're all sort of - Yeah  yeah. If we c- if we could look at it  while we're talking  I think it 'd be uh - O_K. Uh  I'll - I'll sing a song or dance or something while you do it  too. Alright. So um - Go ahead. Ah  while you're gone I'll ask s- some of my questions. Yeah. Um. Yeah. Uh  this way and just slightly to the left  yeah. The um - What was - Was this number forty or - It was roughly the same as this one  he said? When you had the two language versus the three language? Um. That's what he was saying. That's where he removed English  right? Yeah. Right. It sometimes  actually  depends on what features you're using. Yeah. But - but i- it sounds like - Um  but - I mean. That's interesting because it - it seems like what it's saying is not so much that you got hurt uh because you uh didn't have so much representation of English  because in the other case you don't get hurt any more  at least when it seemed like uh it - it might simply be a case that you have something that is just much more diverse  Mm-hmm. but you have the same number of parameters representing it. Mm-hmm. I wonder - were um all three of these nets using the same output? This multi-language uh labeling? He - Mm-hmm. He was using uh sixty-four phonemes from SAMPA. O_K  O_K. Yeah. So this would - From this you would say  ""well  it doesn't really matter if we put Finnish into the training of the neural net  if there's gonna be  you know  Finnish in the test data."" Right? Well  it's - it sounds - I mean  we have to be careful  cuz we haven't gotten a good result yet. Yeah. And comparing different bad results can be tricky. Hmm. But I - I - I - I think it does suggest that it's not so much uh uh cross language as cross type of speech. Mm-hmm. It's - it's um - But we did - Oh yeah  the other thing I was asking him  though  is that I think that in the case - Yeah  you - you do have to be careful because of com- compounded results. I think we got some earlier results in which you trained on one language and tested on another and you didn't have three  but you just had one language. So you trained on one type of digits and tested on another. Didn- Wasn't there something of that? Where you  say  trained on Spanish and tested on - on T_I-digits  or the other way around? Something like that? No. I thought there was something like that  that he showed me last week. We'll have to wait till we get - Yeah  that would be interesting. Um  This may have been what I was asking before  Stephane  but - but  um  wasn't there something that you did  where you trained on one language and tested on another? I mean no - no mixture but just - I'll get it for you. Uh  no  no. We've never just trained on one lang- Training on a single language  you mean  and testing on the other one? Yeah. Not yet. Uh  no. So the only task that's similar to this is the training on two languages  and But we've done a bunch of things where we just trained on one language. Right? that - I mean  you haven't - you haven't done all your tests on multiple languages. Uh  No. Either thi- this is test with uh the same language but from the broad data  or it's test with uh different languages also from the broad data  excluding the - The early experiment that - So  it's - it's three or - three and four. Did you do different languages from digits? Uh. No. You mean training digits on one language and using the net to recognize on the other? Digits on another language? No. See  I thought you showed me something like that last week. You had a - you had a little - Uh  No  I don't think so. Um What - These numbers are uh ratio to baseline? So  I mean wha- what's the - This - this chart - this table that we're looking at is um  So. show- is all testing for T_I -digits  or - ? Bigger is worse. This is error rate  I think. So you have uh basically two uh parts. The upper part is for T_I-digits Ratio. No. No. Yeah  yeah  yeah. and it's divided in three rows of four - four rows each. Mm-hmm. Yeah. And the first four rows is well-matched  then the s- the second group of four rows is mismatched  and finally highly mismatched. And then the lower part is for Italian and it's the same - the same thing. So  so the upper part is training T_I-digits? So. It's - it's the H_T_K results  I mean. So it's H_T_K training testings Ah. with different kind of features and what appears in the uh left column is the networks that are used for doing this. Hmm. So. Uh Yeah. Well  What was is that i- What was it that you had done last week when you showed - Do you remember? Wh- when you showed me the - your table last week? It- It was part of these results. Mmm. Mmm. So where is the baseline for the T_I-digits located in here? You mean the H_T_K Aurora baseline? Yeah. It's uh the one hundred number. It's  well  all these numbers are the ratio Ah! with respect to the baseline. Ah  O_K  O_K. So this is word - word error rate  so a high number is bad. Yeah  this is a word error rate ratio. Yeah. Yeah. O_K  I see. So  seventy point two means that we reduced the error rate uh by thirty - thirty percent. So. O_K  O_K  gotcha. O_K  Hmm. so if we take uh um let's see P_L_P uh with on-line normalization and delta-del- so that's this thing you have circled here in the second column  Yeah. um and ""multi-English"" refers to what? To TIMIT. Mmm. Then you have uh M_F  M_S and M_E which are for French  Spanish and English. And  yeah. Actually I - I uh forgot to say that the multilingual net are trained on uh features without the s- derivatives uh but with increased frame numbers. Mmm. And we can - we can see on the first line of the table that it - it - it's slightly - slightly worse when we don't use delta but it's not - not that much. Right. So w- w- So  I'm sorry. I missed that. What's M_F  M_S and M_E? Multi-French  Multi-Spanish So. Multi-French  Multi-Spanish  and Multi-English. Uh O_K. So  it's uh broader vocabulary. Yeah. Then - And - O_K so I think what I'm - what I saw in your smaller chart that I was thinking of was - was there were some numbers I saw  I think  that included these multiple languages and it - and I was seeing that it got worse. I - I think that was all it was. You had some very limited results that - at that point Yeah. which showed having in these - these other languages. In fact it might have been just this last category  having two languages broad that were - where - where English was removed. So that was cross language and the - and the result was quite poor. What I - we hadn't seen yet was that if you added in the English  it's still poor. Yeah. Still poor. Uh Um now  what's the noise condition um of the training data - Well  I think this is what you were explaining. The noise condition is the same - It's the same uh Aurora noises Yeah. uh  in all these cases Yeah. for the training. So there's not a statistical - sta- a strong st- statistically different noise characteristic between No these are the s- s- s- same noises  yeah. uh the training and test and yet we're seeing some kind of effect - At least - at least for the first - for the well-matched  yeah. Well matched condition. Right. So there's some kind of a - a - an effect from having these - uh this broader coverage um Now I guess what we should try doing with this is try testing these on u- this same sort of thing on - you probably must have this lined up to do. To try the same t- with the exact same training  do testing on the other languages. Mmm. On - on um - So. Um  oh I well  wait a minute. You have this here  for the Italian. That's right. O_K  so  Yeah. Yeah  so for the Italian the results are uh stranger um So. Mmm. So what appears is that perhaps Spanish is not very close to Italian because uh  well  when using the - the network trained only on Spanish it's - the error rate is almost uh twice the baseline error rate. Mm-hmm. Mmm. Uh. Well  I mean  let's see. Is there any difference in - So it's in the uh - So you're saying that when you train on English and uh and - and test on - Yeah. No  you don't have training on English testing - There - there is - another difference  is that the noise - the noises are different. Well  In - in what? For - for the Italian part I mean the uh the um networks are trained with noise from Aurora - T_I- digits  mmm. Aurora-two. Yeah. And the noise is different in th- And perhaps the noise are quite different from the noises in the speech that Italian . And - Do we have any um test sets uh in any other language that um have the same noise as in the Aurora? Mmm  no. No. Can I ask something real quick? In - in the upper part - in the English stuff  it looks like the very best number is sixty point nine? and that's in the uh - the third section in the upper part under P_L_P J_RASTA  sort of the middle column? Yeah. I- is that a noisy condition? Yeah. So that's matched training? Is that what that is? It's - no  the third part  so it's uh highly mismatched. So. Training and test noise are different. So - why do you get your best number in - Wouldn't you get your best number in the clean case? Well  it's relative to the um baseline mismatching Yeah. Ah  O_K so these are not - Yeah. O_K  alright  I see. Yeah. Yeah. O_K. And then - so  in the - in the um - in the non-mismatched clean case  your best one was under M_F_C_C? That sixty-one point four? Yeah. But it's not a clean case. It's a noisy case but uh training and test noises are the same. Oh! So this upper third? So - Yeah. Uh that's still noisy? Yeah. Ah  O_K. So it's always noisy basically  and  well  the - Mm-hmm. I see. Mmm. O_K? Um So uh  I think this will take some looking at  thinking about. But  what is uh - what is currently running  that's - uh  i- that - just filling in the holes here or - or - ? Uh  no we don't plan to fill the holes but pretty much? O_K. actually there is something important  is that um we made a lot of assumption concerning the on-line normalization and we just noticed uh recently that uh the approach that we were using was not uh leading to very good results when we used the straight features to H_T_K. Um Mmm. So basically d- if you look at the - at the left of the table  the first uh row  with eighty-six  one hundred  and forty-three and seventy-five  these are the results we obtained for Italian uh with straight mmm  P_L_P features using on-line normalization. Mm-hmm. Mmm. And the  mmm - what's in the table  just at the left of the P_L_P twelve on-line normalization column  so  the numbers seventy-nine  fifty-four and uh forty-two are the results obtained by uh Pratibha with uh his on-line normalization - uh her on-line normalization approach. Where is that? seventy-nine  fifty Fifty-one? This - Uh  it's just sort of sitting right on the uh - the column line. So. Uh. Oh I see  O_K. Just - uh Yeah. Yeah. So these are the results of O_G_I with on-line normalization and straight features to H_T_K. And the previous result  eighty-six and so on  Yes. are with our features straight to H_T_K. So Yes. what we see that - is - there is that um uh the way we were doing this was not correct  but still the networks are very good. When we use the networks our number are better that We improve. uh Pratibha results. So  do you know what was wrong with the on-line normalization  or - ? Yeah. There were diff- there were different things and basically  the first thing is the mmm  alpha uh value. So  the recursion uh part. um  I used point five percent  which was the default value in the - in the programs here. And Pratibha used five percent. So it adapts more quickly Uh-huh. Yes. Yeah. Um  but  yeah. I assume that this was not important because uh previous results from - from Dan and - show that basically the both - both values g- give the same - same uh results. It was true on uh T_I-digits but it's not true on Italian. Mm-hmm. Uh  second thing is the initialization of the stuff. Actually  uh what we were doing is to start the recursion from the beginning of the utterance. And using initial values that are the global mean and variances measured across the whole database. Right. Right. And Pratibha did something different is that he - uh she initialed the um values of the mean and variance by computing this on the twenty-five first frames of each utterance. Mmm. There were other minor differences  the fact that she used fifteen dissities instead s- instead of thirteen  and that she used C_zero instead of log energy. Uh  but the main differences concerns the recursion. So. Uh  I changed the code uh and now we have a baseline that's similar to the O_G_I baseline. O_K. We - It - it's slightly uh different because I don't exactly initialize the same way she does. Actually I start  mmm  I don't wait to a fifteen - twenty-five - twenty-five frames before computing a mean and the variance to e- to - to start the recursion. Mm-hmm. Yeah. I - I use the on-line scheme and only start the re- recursion after the twenty-five - twenty-fifth frame. But  well it's similar. So uh I retrained the networks with these - well  the - the - the networks are retaining with these new features. Mm-hmm. And  yeah. O_K. So basically what I expect is that these numbers will a little bit go down but perhaps not - not so much because I think the neural networks learn perhaps Right. to - even if the features are not normalized. It - it will learn how to normalize and - Right. O_K  but I think that Mmm. given the pressure of time we probably want to draw - because of that especially  we wanna draw some conclusions from this  do some reductions in what we're looking at  Yeah. and make some strong decisions for what we're gonna do testing on before next week. So do you - are you - w- Yeah I'd - did you have something going on  on the side  with uh multi-band or - on - on this  or - ? No  I - we plan to start this uh so  act- actually we have discussed uh @@ um  these - what we could do more as a - as a research and - and we were thinking perhaps that uh the way we use the tandem is not - Uh  well  there is basically perhaps a flaw in the - in the - the stuff because we trained the networks - If we trained the networks on the - on a language and a t- or a specific task  Mm-hmm. um  what we ask is - to the network - is to put the bound- the decision boundaries somewhere in the space. And uh mmm and ask the network to put one  Mmm. at one side of the - for - for a particular phoneme at one side of the boundary - decision boundary and one for another phoneme at the other side. And so there is kind of reduction of the information there that's not correct because if we change task and if the phonemes are not in the same context in the new task  obviously the decision boundaries are not - should not be at the same place. But the way the feature gives - I di- The - the way the network gives the features is that it reduce completely the - it removes completely the information - a lot of information from the - the features by uh uh placing the decision boundaries at optimal places for one kind of data but this is not the case for another kind of data. It's a trade-off  right? Any- anyway go ahead. So - Yeah. So uh what we were thinking about is perhaps um one way to solve this problem is increase the number of outputs of the neural networks. Doing something like  um um phonemes within context and  well  basically context dependent phonemes. Maybe. I mean  I - I think you could make the same argument  it'd be just as legitimate  for hybrid systems as well. Yeah but  we know that - Right. And in fact  th- things get better with context dependent versions. Right? Ye- yeah but here it's something different. We want to have features uh well  Yeah. um. Yeah  but it's still true that what you're doing is you're ignoring - you're - you're coming up with something to represent  whether it's a distribution  probability distribution or features  you're coming up with a set of variables that are representing uh  things that vary w- over context. Mm-hmm. Uh  and you're putting it all together  ignoring the differences in context. That - that's true for the hybrid system  it's true for a tandem system. So  for that reason  when you - in - in - in a hybrid system  when you incorporate context one way or another  you do get better scores. Yeah. O_K? But I - it's - it's a big deal to get that. I - I'm - I'm sort of - And once you - the other thing is that once you represent - start representing more and more context it is uh much more um specific to a particular task in language. So um Uh  the - the acoustics associated with uh a particular context  for instance you may have some kinds of contexts that will never occur in one language and will occur frequently in the other  so the qu- the issue of getting enough training for a particular kind of context becomes harder. We already actually don't have a huge amount of training data um Yeah  but - mmm  I mean  the - the way we - we do it now is that we have a neural network and basically the net- network is trained almost to give binary decisions. Right. And uh - binary decisions about phonemes. Nnn - Uh Almost. But I mean it - it - it does give a distribution. It's - Yeah. It's - and - and it is true that if there's two phones that are very similar  that uh the - i- it may prefer one but it will give a reasonably high value to the other  too. Yeah. Yeah  sure but uh So basically it's almost binary decisions and um the idea of using more classes is to get something that's less binary decisions. Oh no  but it would still be even more of a binary decision. It - it'd be even more of one. Because then you would say But - yeah  but - that in - that this phone in this context is a one  but the same phone in a slightly different context is a zero. That would be even - even more distinct of a binary decision. I actually would have thought you'd wanna go the other way and have fewer classes. Yeah  but if - Uh  I mean for instance  the - the thing I was arguing for before  but Mmm. again which I don't think we have time to try  is something in which you would modify the code so you could train to have several outputs on and use articulatory features Mm-hmm. cuz then that would - that would go - that would be much broader and cover many different situations. But if you go to very very fine categories  it's very binary. Mmm. Yeah  but I think - Yeah  perhaps you're right  but you have more classes so you - you have more information in your features. So  Mm-hmm. Um You have more information in the True. uh posteriors vector um which means that - But still the information is relevant because it's - it's information that helps to discriminate  Mm-hmm. if it's possible to be able to discriminate Mm-hmm. among the phonemes in context. Well it's - it's - But the - it's an interesting thought. I mean we - we could disagree about it at length Mmm. Mmm. but the - the real thing is if you're interested in it you'll probably try it and - and we'll see. But - but what I'm more concerned with now  as an operational level  is Mmm. uh  you know  what do we do in four or five days? Uh  and - so we have to be concerned with Are we gonna look at any combinations of things  you know once the nets get retrained so you have this problem out of it. Mmm. Um  are we going to look at multi-band? Are we gonna look at combinations of things? Uh  what questions are we gonna ask  uh now that  I mean  we should probably turn shortly to this O_G_ I note. Um  how are we going to combine with what they've been focusing on? Uh  Uh we haven't been doing any of the L_D_ A RASTA sort of thing. Mm-hmm. And they  although they don't talk about it in this note  um  there's um  the issue of the um Mu law business uh versus the logarithm  um  so. Mm-hmm. So what i- what is going on right now? What's right - you've got nets retraining  Are there - is there - are there any H_T_ K trainings - testings going on? N- I - I - I'm trying the H_T_K with eh  P_L_P twelve on-line delta-delta and M_S_G filter together. The combination  I see. The combination  yeah. But I haven't result at this moment. M_S_G and - and P_L_P. Yeah. And is this with the revised on-line normalization? Ye- Uh  with the old older   yeah. Yeah. Old one. So it's using all the nets for that but again we have the hope that it - Yeah. But We can We have the hope that it - know soon. maybe it's not making too much difference  Maybe. but - but yeah. I don't know. Yeah. Uh  O_K. Uh so there is this combination  yeah. Working on combination obviously. Um  I will start work on multi-band. Mm-hmm. And we plan to work also on the idea of using both features and net outputs. Yep. Um. And we think that with this approach perhaps we could reduce the number of outputs of the neural network. Um  So  get simpler networks  because we still have the features. So we have um come up with um different kind of broad phonetic categories. And we have - Basically we have three types of broad phonetic classes. Well  something using place of articulation which - which leads to nine  I think  broad classes. Uh  another which is based on manner  which is - is also something like nine classes. And then  something that combine both  and we have twenty f- twenty-five? Twenty-seven. Twenty-seven broad classes. So like  uh  oh  I don't know  like back vowels  front vowels. So what you do - Um um I just wanna understand so You have two net or three nets? Was this? How many - how many nets do you have? For the moments we do not - don't have nets  I mean  No nets. It's just - Begin to work in this. Were we just changing the labels to retrain nets with fewer out- outputs. We are @@ . Right. But - but I didn't understand - And then - Mm-hmm. Uh. the software currently just has - uh a - allows for I think  the one - one hot output. So you're having multiple nets and combining them  or - ? Uh  how are you - how are you coming up with - If you say uh If you have a place characteristic and a manner characteristic  how do you - It- I think they have one output. It's the single net  yeah. Oh  it's just one net. mm-hmm Yeah. It's one net with um twenty-seven outputs if we have twenty-seven classes  yeah. I see. I see  O_K. So it's - Well  it's basically a standard net with fewer classes. So you're sort of going the other way of what you were saying a bit ago instead of - yeah. Yeah  but I think - Yeah. But including the features. B- b- Yeah. including the features  yeah. I don't think this will work alone. I think it will get worse because Uh-huh. Well  I believe the effect that - of - of too reducing too much the information is basically - basically what happens and - but - But you think if you include that plus the other features  Yeah  because there is perhaps one important thing that the net brings  and O_G_I show- showed that  is the distinction between sp- speech and silence Because these nets are trained on well-controlled condition. I mean the labels are obtained on clean speech  and we add noise after. So this is one thing And But perhaps  something intermediary using also some broad classes could - could bring so much more information. Uh. So - so again then we have these broad classes and - well  somewhat broad. I mean  it's twenty-seven instead of sixty-four  basically. Yeah. And you have the original features. Yeah. Which are P_L_P  or something. Mm-hmm. And then uh  just to remind me  all of that goes into - uh  that all of that is transformed by uh  uh  K_- K_L or something  or - ? There will probably be  yeah  one single K_L to transform everything or Mu. Right. uh  per- No transform the P_L_P and only transform the other I'm not sure. This is still something that yeah  we don't know - Well no  I think - I see. Two e- So there's a question of whether you would - Yeah. Right. Whether you would transform together or just one. @@ it's one. Yeah. Might wanna try it both ways. But that's interesting. So that's something that you're - you haven't trained yet but are preparing to train  and - Yeah. Yeah. Um Mmm. Yeah  so I think Hynek will be here Monday. Monday or Tuesday. So Uh  yeah. So I think  you know  we need to choose the - choose the experiments carefully  so we can get uh key - key questions answered Mm-hmm. uh before then and leave other ones aside even if it leaves incomplete tables someplace  uh uh  it's - it's really time to - time to choose. Mm-hmm. Um  let me pass this out  by the way. Um These are - Did - did - did I interrupt you? Were there other things that you wanted to - Yeah  I have one. Uh  no. I don't think so. @@ Yeah  I have one. Oh  thanks. Ah! O_K. We have one. @@ O_K  we have lots of them. O_K  so um  Something I asked - So they're - they're doing the - the V_A_D I guess they mean voice activity detection So again  it's the silence - So they've just trained up a net which has two outputs  I believe. Um I asked uh Hynek whether - I haven't talked to Sunil - I asked Hynek whether they compared that to just taking the nets we already had and summing up the probabilities. Mm-hmm. Uh. To get the speech - voice activity detection  or else just using the silence  if there's only one silence output. Um And  he didn't think they had  um. But on the other hand  maybe they can get by with a smaller net and maybe sometimes you don't run the other  maybe there's a computational advantage to having a separate net  anyway. Mm-hmm. So um Their uh - the results look pretty good. Yeah. Um  I mean  not uniformly. I mean  there's a - an example or two that you can find  where it made it slightly worse  but uh in - in all but a couple Mmm. examples. Uh. But they have a question of the result. Um how are trained the - the L_D_A filter? How obtained the L_D_A filter? Mmm. I- I'm sorry. I don't understand your question. Yes  um the L_D_A filter needs some training set to obtain the filter. Maybe I don't know exactly how they are obtained. It's on training. Training  with the training test of each - You understand me? No. Yeah  uh for example  L_D_A filter need a set of - a set of training to obtain the filter. And maybe for the Italian  for the T_D T_E on for Finnish  these filter are - are obtained with their own training set. Yes. Yes  I don't know. That's - that's - so that's a - that's a very good question  then - now that it - I understand it. It's ""yeah  where does the L_D_A come from?"" In the - In earlier experiments  they had taken L_D_A from a completely different database  right? Yeah. Yeah  because maybe it the same situation that the neural network training with their own Mmm. set. So that's a good question. Where does it come from? Yeah  I don't know. Um  but uh to tell you the truth  I wasn't actually looking at the L_D_A so much when I - I was looking at it I was mostly thinking about the - the V_A_D. And um  it ap- it ap- Oh what does - what does A_S_P? Oh that's - The features  yeah. Yeah. I don't understand also what is - It says ""baseline A_S_P"". what is the difference between A_S_P and uh baseline over ? Yeah  I don't know. A_S_P. This is - Oh. Anybody know any - There it is. Um Cuz there's ""baseline Aurora "" above it. Mm-hmm. And it's - This is mostly better than baseline  although in some cases it's a little worse  in a couple cases. Well  it says baseline A_S_P is twenty-three mill Yeah. minus thirteen. Yeah  it says what it is. But I don't how that's different from - From the baseline. O_K. I think this was - Yeah. I think this is the same point we were at I think - when - when we were up in Oregon. I think it's the C_zero - using C_zero instead of log energy. Yeah  it's this. Ah  O_K  mm-hmm. yeah. Oh. O_K. It should be that  yeah. Because - Shouldn't it be - They s- they say in here that the V_A_D is not used as an additional feature. Does - does anybody know how they're using it? Yeah. So - so what they're doing here is  Yeah. i- if you look down at the block diagram  um  they estimate - they get a - But that - they get an estimate of whether it's speech or silence  and then they have a median filter of it. Mm-hmm. And so um  basically they're trying to find stretches. The median filter is enforcing a - i- it having some continuity. Mm-hmm. You find stretches where the combination of the frame wise V_A_D and the - the median filter say that there's a stretch of silence. And then it's going through and just throwing the data away. Hmm. Right? So um - So it's - it's - I don't understand. You mean it's throwing out frames? It's throwing out chunks of frames  yeah. Before - There's - the - the median filter is enforcing that it's not gonna be single cases of frames  or isolated frames. Yeah. So it's throwing out frames and the thing is um  what I don't understand is how they're doing this with H_T_ K. This is - Yeah  that's what I was just gonna ask. How can you just throw out frames? Yeah. Well  i- you - you can  right? I mean y- you - you - it stretches again. For single frames I think it would be pretty hard. But if you say speech starts here  speech ends there. Right? Yeah. Yeah. Mm-hmm. Huh. Yeah. Yeah  you can basically remove the - the frames from the feature - feature files. And. Yeah. Yeah  so I mean in the - i- i- in the - in the decoding  you're saying that we're gonna decode from here to here. I t- Mm-hmm. I think they're - they're - they're treating it  you know  like uh - well  it's not isolated word  but - but connected  you know  the - the - In the text they say that this - this is a tentative block diagram of a possible configuration we could think of. So that sort of sounds like they're not doing that yet. Well. No they - they have numbers though  right? So I think they're - they're doing something like that. I think that they're - they're - I think what I mean by tha- that is they're trying to come up with a block diagram that's plausible for the standard. In other words  it's - uh - I mean from the point of view of - of uh reducing the number of bits you have to transmit it's not a bad idea to detect silence anyway. Yeah. Yeah. I'm just wondering what exactly did they do up in this table if it wasn't this. Um. But it's - the thing is it's that - that - that's - that's I - I - Certainly it would be tricky about it intrans- in transmitting voice  uh uh for listening to  is that these kinds of things uh cut speech off a lot. Right? And so um Mm-hmm. Plus it's gonna introduce delays. It does introduce delays but they're claiming that it's - it's within the - the boundaries of it. Mmm. And the L_D_A introduces delays  and b- what he's suggesting this here is a parallel path so that it doesn't introduce uh  any more delay. I- it introduces two hundred milliseconds of delay but at the same time the L_D_A down here - I don't know - Wh- what's the difference between T_L_D_A and S_L_D_A ? Temporal and spectral. Ah  thank you. Temporal L_D_A. Yeah  you would know that. Yeah So um. The temporal L_D_A does in fact include the same - so that - I think he - well  by - by saying this is a b- a tentative block di- diagram I think means if you construct it this way  this - this delay would work in that way and then it'd be O_K. Ah. They - they clearly did actually remove silent sections in order - because they got these word error rate results. So um I think that it's - it's nice to do that in this because in fact  it's gonna give a better word error result and therefore will help within an evaluation. Whereas to whether this would actually be in a final standard  I don't know. Um. Uh  as you know  part of the problem with evaluation right now is that the word models are pretty bad and nobody wants - has - has approached improving them. So it's possible that a lot of the problems with so many insertions and so forth would go away if they were better word models to begin with. So this might just be a temporary thing. But - But  on the other hand  and maybe - maybe it's a decent idea. So um The question we're gonna wanna go through next week when Hynek shows up I guess is given that we've been - if you look at what we've been trying  we're uh looking at uh  by then I guess  combinations of features and multi-band Uh  and we've been looking at cross-language  cross task issues. And they've been not so much looking at the cross task uh multiple language issues. But they've been looking at uh - at these issues. At the on-line normalization and the uh voice activity detection. And I guess when he comes here we're gonna have to start deciding about um what do we choose from what we've looked at to um blend with some group of things in what they've looked at And once we choose that  how do we split up the effort? Uh  because we still have - even once we choose  we've still got uh another month or so  I mean there's holidays in the way  but - but uh I think the evaluation data comes January thirty- first so there's still a fair amount of time to do things together it's just that they probably should be somewhat more coherent between the two sites in that - that amount of time. When they removed the silence frames  did they insert some kind of a marker so that the recognizer knows it's - knows when it's time to back trace or something? Well  see they  I - I think they're Um. I don't know the - the specifics of how they're doing it. They're - they're getting around the way the recognizer works because they're not allowed to um  change the scripts Oh  right. for the recognizer  I believe. So. Uh. Maybe they're just inserting some nummy frames or something? Uh  you know that's what I had thought. But I don't - I don't think they are. I mean that's - sort of what - the way I had imagined would happen is that on the other side  yeah you Hmm. p- put some low level noise or something. Probably don't want all zeros. Most recognizers don't like zeros but Hmm. but you know  Yeah. put some epsilon in or some rand- sorry epsilon random variable in or something. Some constant vector. Maybe not a constant but it doesn't  uh - don't like to divide by the variance of that  I mean i- w- Or something - but I mean it's That's right. But something that - what I mean is something that is very distinguishable from speech. Mm-hmm. So that the - the silence model in H_T_K will always pick it up. Yeah. So I - I - that's what I thought they would do. or else  uh uh maybe there is some indicator to tell it to start and stop  I don't know. Hmm. But whatever they did  I mean they have to play within the rules of this specific evaluation. Yeah. We c- we can find out. Cuz you gotta do something. Otherwise  if it's just a bunch of speech  stuck together - No they're - Yeah. It would do badly and it didn't so badly  right? So they did something. Yeah  right. Yeah  yeah. Yeah. Uh. So  O_K  So I think this brings me up to date a bit. It hopefully brings other people up to date a bit. And um Um I think - Uh  I wanna look at these numbers off-line a little bit and think about it and - and talk with everybody uh  outside of this meeting. Um  but uh No I mean it sounds like - I mean there - there - there are the usual number of - of little - little problems and bugs and so forth but it sounds like they're getting ironed out. And now we're seem to be kind of in a position to actually uh  look at stuff and - and - and compare things. So I think that's - that's pretty good. Um I don't know what the - One of the things I wonder about  coming back to the first results you talked about  is - is how much  uh things could be helped by more parameters. And uh - And uh how many more parameters we can afford to have  in terms of the uh computational limits. Because anyway when we go to twice as much data and have the same number of parameters  particularly when it's twice as much data and it's quite diverse  um  I wonder if having twice as many parameters would help. Uh  just have a bigger hidden layer. Mm-hmm. Uh But - I doubt it would help by forty per cent. But Yeah. but uh Just curious. How are we doing on the resources? Disk  and - I think we're alright  um  not much problems with that. O_K. Computation? It's O_K. Well this table took uh more than five days to get back . We - Yeah. Yeah  well. But - Yeah. Are - were you folks using Gin? That's a - that just died  you know? Mmm  no. You were using Gin perhaps  yeah? No. No. It just died. No? Oh  that's good. O_K. Yeah  we're gonna get a replacement Yes. server that'll be a faster server  actually. That'll be - It's a Hmm. seven hundred fifty megahertz uh SUN Tonic. uh But it won't be installed for a little while. Mm-hmm. U- Do we - Go ahead. Do we have that big new I_B_M machine the  I think in th- We have the little tiny I_B_M machine that might someday grow up to be a big I_B_M machine. It's got s- slots for eight  uh I_B_M was donating five  I think we only got two so far  processors. We had originally hoped we were getting eight hundred megahertz processors. They ended up being five fifty. So instead of having eight processors that were eight hundred megahertz  we ended up with two that are five hundred and fifty megahertz. And more are supposed to come soon and there's only a moderate amount of dat- of memory. So I don't think anybody has been sufficiently excited by it to spend much time uh with it  but uh Hopefully  they'll get us some more parts  soon and - Uh  yeah  I think that'll be - once we get it populated  that'll be a nice machine. I mean we will ultimately get eight processors in there. And uh - and uh a nice amount of memory. Uh so it'll be a pr- pretty fast Linux machine. And if we can do things on Linux  some of the machines we have going already  like Swede? Mm-hmm. Um It seems pretty fast. Mm-hmm. But - I think Fudge is pretty fast too. Yeah  I mean you can check with uh Dave Johnson. I mean  it - it's - I think the machine is just sitting there. And it does have two processors  you know and - Somebody could do - you know  uh  check out uh the multi-threading libraries. And I mean i- it's possible that the - I mean  I guess the prudent thing to do would be for somebody to do the work on - on getting our code running on that machine with two processors even though there aren't five or eight. There's - there's - there's gonna be debugging hassles and then we'd be set for when we did have five or eight  to have it really be useful. But. Notice how I said somebody and turned my head your direction. That's one thing you don't get in these recordings. You don't get the - don't get the visuals but - I- is it um mostly um the neural network trainings that are um slowing us down or the H_T_K runs that are slowing us down? Uh  I think yes. Uh  Isn't that right? I mean I think you're - you're sort of held up by both  right? If the - if the neural net trainings were a hundred times faster you still wouldn't be anything - running through these a hundred times faster because you'd be stuck by the H_T_K trainings  right? Mmm. Yeah. But if the H_T_K - I mean I think they're both - It sounded like they were roughly equal? Is that about right? Yeah. Yeah. Because  um I think that'll be running Linux  and Sw- Swede and Fudge are already running Linux so  um I could try to get um the train- the neural network trainings or the H_T_K stuff running under Linux  and to start with I'm wondering which one I should pick first. Uh  probably the neural net cuz it's probably - it - it's - it's um - Well  I - I don't know. They both - H_T_K we use for um this Aurora stuff Um Um  I think It's not clear yet what we're gonna use for trainings uh - Well  there's the trainings uh - is it the training that takes the time  or the decoding? Uh  is it about equal between the two? For - for Aurora? For H_T_K? For - Yeah. For the Aurora? Uh Training is longer. O_K. Yeah. O_K. Well  I don't know how we can - I don't know how to - Do we have H_T_K source? Is that - Mmm. Yeah. You would think that would fairly trivially - the training would  anyway  th- the testing uh I don't - I don't think would parallelize all that well. But I think that you could certainly do d- um  distributed  sort of - Ah  no  it's the - each individual sentence is pretty tricky to parallelize. But you could split up the sentences in a test set. They have a - they have a thing for doing that and th- they have for awhile  in H_T_ K. Yeah? And you can parallelize the training. And run it on several machines and it just basically keeps counts. Aha! And there's something - a final thing that you run and it accumulates all the counts together. I see. Mmm. I don't what their scripts are set up to do for the Aurora stuff  but - Yeah. Something that we haven't really settled on yet is other than this Aurora stuff  uh what do we do  large vocabulary training slash testing for uh tandem systems. Cuz we hadn't really done much with tandem systems for larger stuff. Cuz we had this one collaboration with C_M_U and we used SPHINX. Uh  we're also gonna be collaborating with S_R_I and we have their - have theirs. Um So I don't know Um. So I - I think the - the advantage of going with the neural net thing is that we're gonna use the neural net trainings  no matter what  O_K. for a lot of the things we're doing  whereas  w- exactly which H_M_M - Gaussian-mixture-based H_M_M thing we use is gonna depend uh So with that  maybe we should uh go to our digit recitation task. And  it's about eleven fifty. Canned. Uh  I can - I can start over here. Two zero one one dash two zero three zero O_ six nine zero six zero one four two three zero five one zero eight one four zero zero four seven two two six one seven four two eight seven eight nine nine seven five nine. O_. zero. one zero three two two four three zero zero one five five six nine two four zero six three seven eight nine zero three Uh  transcript number one nine nine one two O_ one zero. O_ nine zero eight two seven six one nine three three four two O_ five five. three zero five one six three two seven four eight nine one O_ nine O_ zero zero zero seven three one two three seven seven four three two five three six one five four eight zero six six zero zero five six seven nine one nine six nine five one Transcript two O_ seven one dash two O_ nine O_ one two zero eight four six four one. five three six six zero three seven eight nine two nine O_ O_ five eight one one one five three five six four two seven five six O_ three six five four zero four five five six seven nine one nine four O_ eight six zero three one zero nine one two one eight one zero zero Transcript two zero five one dash two zero seven zero zero zero two two zero four three two two one three four six nine five six seven eight O_ eight eight three seven seven O_ two eight six one O_ five zero two one nine two seven five O_ two two six three nine nine eight three four O_ five seven two five six one O_ eight eight two eight four nine six four seven four O_ zero Transcript one nine seven one dash one nine nine zero nine O_ eight zero one O_ three one one three zero six four four three five seven two six seven one three eight seven eight nine zero one one two four two two three five zero seven four five six zero three three eight one six zero three two five mine one eight seven Transcript two zero three one dash two zero five zero zero two one two O_ two six six three two seven three four nine seven nine O_ five six O_ O_ two eight one two nine four seven nine one six five O_ eight three four zero five three one two O_ six O_ seven three zero five two six four eight eight one seven eight six seven four eight six one nine four nine O_ Transcript one nine three one dash one nine five zero seven eight O_ one zero two six three five three one one four five seven two seven zero one three four five seven one nine O_ O_ eight one O_ nine eight zero seven six zero eight O_ five one zero one two four one five two nine zero four six seven eight three six Great  uh  could you give Adam a call. Tell him to Oh. He's at two nine seven seven. O_K. I think we can @@ You know Herve's coming tomorrow  right? Herve will be giving a talk  yeah  talk at eleven. Hello  is Adam there? Hey Adam  this is Barry. Yeah we're all done. O_K  thanks. Bye bye. Did uh  did everybody sign these consent Er everybody Has everyone signed a consent form before  on previous meetings? You don't have to do it again each time Yes. microphones off ",ICSI's Meeting Recorder Group met to discuss their progress in various aspects of the Aurora Project  but also to hear more about other developments relevant to the group. On the Aurora Project  there were reports on a project conference call  the status of the tandem neural networks  and progress with the Mississippi State recognizer. The latency limit has been set  and the group's system is performing very well  but is a little over. On the larger vocabulary task  there are still a few issues to resolve before work can really get started. The group heard of the plan of one of it's member's work into intermediate classifiers  and also of how a visiting research student's work into auditory models can be applied to their work. Speaker me013 wants to know how much memory the tandem network takes up. It is only a minor problem that the latency limit has been set below the current systems level  and also keeping the number of features within limits only drops performance a little. A more significant problem is that the tandem approach may not fit in the memory space allowed  and removing it drops performance more. Some of the group had issues with mn007's approach to human performance testing  but this was considered more of a side issue. Speaker mn007 has been working on the tandem network approach  and the current results are good. He has found a good way of calculating the silence probabilities  that does not increase insertions. He also attempted to transcribe data himself  to establish a human performance level. Speaker me018 is working on Mississippi Sate recognizer for dealing with the Wall Street Journal Data. 
"@@ Now can you give me the uh remote T_? seven nine seven seven  five  eight one five three two five  four one  six zero zero three nine six seven eight  five nine nine five  four eight six nine two eight  three nine  six two  nine one  two three six four seven  eight three two  seven three three one seven  nine three nine  five seven  six two one  two one six four  seven zero  nine three four three. six one  four nine  zero one  six zero  six four. O_K  so Eva  co- uh - could you read your numbers? Yeah. Go ahead and read. O_K. zero  one nine four  five zero  five eight five  eight nine four two eight  eight  seven three seven four seven six five  one  seven eight eight three nine four one  three zero two nine  two eight one three four five six five  six six seven eight  six eight eighty-seven five  six eight nine  five four  zero eight eight  seven seven nine  nine four  nine three  nine four  two seven zero one seven eight  zero  one four three Alright. Yeah  let's get started. Um - Hopefully Nancy will come  if not  she won't. Uh  Robert  do you uh have any way to turn off your uh screensaver on there so that it's not going off every - uh  it seems to have about at two minute - Yeah  I've - I - uh - it's not that I didn't try. O_K. and um I - I told it to stay on forever and ever  but if it's not plugged in it just doesn't obey my commands. O_K. It has a mind. Got it. But I- Wants to conserve. I just - You know  sort of keep on wiggling. Yeah  O_K. But uh - we'll just be m- m- working on it at intensity so it doesn't happen. We'll see. Should we plunge right into it? Yeah. So  would you like to - I think so. So what I've tried to do here is list all the decision nodes that we have identified on this side. Commented and - what they're about and sort of - the properties we may um give them. And here are the uh - tasks to be implemented via our data collection. So all of these tasks - The reading is out of these tasks more or less imply that the user wants to go there  sometime or the other. And analogously for example  here we have our EVA um - intention. And these are the data tasks where w- we can assume the person would like to enter  view or just approach the thing. Analogously the same on the object information we can see that  you know  we have sort of created these tasks before we came up with our decision nodes so there's a lot of things where we have no analogous tasks  and that may or may not be a problem. We can change the tasks slightly if we feel that we should have data for e- sort of for every decision node so - trying to im- um - implant the intention of going to a place now  going to a place later on the same tour  or trying to plant the intention of going sometime on the next tour  or the next day or whenever. Right  right. But I think that might be overdoing it a little. So - Yeah. So let me pop up a level. And uh s- s- make sure that we're all oriented the same. So What we're gonna do today is two related things. Uh one of them is to work on the semantics of the belief-net which is going to be the main inference engine for thi- the system uh making decisions. And decisions are going to turn out to be parameter choices for calls on other modules. so f- the natural language understanding thing is uh  we think gonna only have to choose parameters  but You know  a fairly large set of parameters. So to do that  we need to do two things. One of which is figure out what all the choices are  which we've done a fair amount. Then we need to figure out what influences its choices and finally we have to do some technical work on the actual belief relations and presumably estimates of the probabilities and stuff. But we aren't gonna do the probability stuff today. Technical stuff we'll do - uh - another day. Probably next week. But we are gonna worry about all the decisions and the things that pert- that contribute to them. And we're also  sort of uh in the same process  going to work with Fey on what there should be in the dialogues. So One of the s- steps that's coming up real soon is to actually get subjects uh - in here  and have them actually record like this. Uh record dialogues more or less. And - depending on what Fey sort of provokes them to say  we'll get information on different things. So - Well how people phrase different intentions more or less  huh? Fo- v- yeah- people with the - phrase them and so - Uh for  you know  Keith and people worrying about what constructions people use  uh - we have some i- we have some ways to affect that by the way the dialogues go. So what Robert kindly did  is to lay out a table of the kinds of uh things that - that might come up  and  the kinds of decisions. So the uh - uh - on the left are decision nodes  and discreet values. So if - if we're right  you can get by with um just this middle column worth of decisions  and it's not all that many  and it's perfectly feasible technically to build belief-nets that will do that. And he has a handout. Yeah. Maybe it was too fast plunging in there  because Yeah. j- we have two updates. Um you can look at this if you want  these are what our subject's going to have to fill out. Any comments I can - can still be made and the changes will be put in correspondingly. m- Let me summarize in two sentences  mainly for Eva's benefit  who probably has not heard about the data collection  at all. Or have you heard about it? O_K. No. Not that much you didn't . O_K. We were gonna put this in front of people. They give us some information on themselves. Then - then they will read uh - a task where lots of German words are sort of thrown in between. O_K. And um - and they have to read isolated proper names And these change - S- I don't see a release No  this is not the release form. This is the speaker Got it. O_K  fine. O_K. information form. The release form is over there in that box. Alright  fair enough. And um - And then they gonna have to f- um um choose from one of these tasks  which are listed here. They - they pick a couple  say three - uh - uh six as a matter of fact. Six different things they sort of think they would do if they were in Heidelberg or traveling someplace - and um - and they have a map. Hmm. Like this. Very sketchy  simplified map. And they can take notes on that map. And then they call this computer system that works perfectly  and understands everything. O_K. And um - This is a fictional system obviously  huh. The comp- Yeah  the computer system sits right in front of you  that's Fey. Yes. I've - I understand everything. Yes I do. And she does know everything. And she has a way of making this machine talk. So she can copy sentences into a window  or type really fast and this machine will use speech synthesis to produce that. So if you ask ""How do I get to the castle"" then a m- s- several seconds later it'll come out of here ""In order to get to the castle you do -"" O_K? Yeah. And um - And then after three tasks the system breaks down. And Fey comes on the phone as a human operator. And says ""Sorry the system broke down but let's continue."" And we sort of get the idea what people do when they s- think they speak to a machine and what people say when they think they speak to a human  or know  or assume they speak to a human. O_K. Huh. Mm-hmm. Mm-hmm. That's the data collection. And um - And Fey has some thirty subjects lined up? Something? Yeah. And more and more every day. And um - And they're - r- ready uh - to roll. And we're gonna start tomorrow at three? four? one? Tomorrow  well we don't know for sure. Because we don't know whether that person is coming or not  but - O_K. Around four-ish. And um we're still l- looking for a room on the sixth floor because they stole away that conference room. Um - behind our backs. But - Well  there are these - uh - uh - oh  I see  we have to - Yeah  it's tricky. We'll - let's - let - we'll do that off-line  O_K. Yeah  but I - i- i- it's happening. David and - and Jane and - and Lila are working on that as we speak. O_K. O_K. That was the uh - the data collection in a nutshell. And um - I can report a - so I did this but I also tried to do this - so if I click on here  Isn't this wonderful? we get to the uh - uh belief-net just focusing on - on the g- Go-there node. uh - Analogously this would be sort of the reason node and the timing node and so forth. And what w- Mm-hmm. what happened is that um design-wise I'd sort of n- noticed that we can - we still get a lot of errors from a lot of points to one of these sub Go-there User Go-there Situation nodes. So I came up with a couple of additional nodes here where um whether the user is thrifty or not  and what his budget is currently like  is going to result in some financial state of the user. How much will he - is he willing to spend? Or can spend. Being the same at this - just the money available  which may influence us  whether he wants to go there if it is - you know - charging tons of dollars for admission or its gonna g- cost a lot of t- e- whatever. Twenty-two million to fly to International Space Station  you know. Right. just - Not all people can do that. So  and this actually turned out to be pretty key  because having specified sort of these - uh - this - this - intermediate level Um and sort of noticing that everything that happens here - let's go to our favorite endpoint one is again more or less - we have - um - then the situation nodes contributing to the - the endpoint situation node  which contributes to the endpoint and so forth. um - I can now sort of draw straight lines from these to here  meaning it g- of course goes where the sub-S_ - everything that comes from situation  everything that comes from user goes with the sub-U_  and whatever we specify for the so-called ""Keith node""  or the discourse  what comes from the - um - parser  construction parser  um will contribute to the D_ and the ontology to the sub-O_ node. And um one just s- sort of has to watch which - also final decision node so it doesn't make sense - t- to figure out whether he wants to enter  view or approach an object if he never wants to go there in the first place. But this makes the design thing fairly simple. And um now all w- that's left to do then is the C_P_G's  the conditional probabilities  for the likelihood of a person having enough money  actually wanting to go a place if it costs  you know this or that. And um - O_K. and once um Bhaskara has finished his classwork that's where we're gonna end up doing. You get involved in that process too. And um - And for now uh- the - the question is ""How much of these decisions do we want to build in explicitly into our data collection?"" So - Um  one could - sort of - think of - you know we could call the z- see or - you know  people who visit the zoo we could s- call it ""Visit the zoo tomorrow""  so we have an intention of seeing something  but not now - but later. Right. Yeah. Yeah  so - let's s- uh- s- see I th- I think that from one point of view  Uh  um  all these places are the same  so that d- d- That  um - in terms of the linguistics and stuff  there may be a few different kinds of places  so I th- i- it seems to me that We ought to decide you know  what things are k- are actually going to matter to us. And um  so the zoo  and the university and the castle  et cetera. Um are all big-ish things that um - you know - have different parts to them  and one of them might be fine. Hmm. Hmm  hmm. Yeah - The - the reason why we did it that way  as a - as a reminder  is uh - And - no person is gonna do all of them. They're just gonna select u- um  according to their preferences. ""Ah  yeah  I usually visit zoos  or I usually visit castles  or I usually -"" And then you pick that one. Yeah  yeah. Right  no no  but - but s- th- point is They're redundant. to - to y- to - build a system that's got everything in it that might happen you do one thing. T- to build a system that um - had the most data on a relatively confined set of things  you do something else. And the speech people  for example  are gonna do better if they - if - things come up uh - repeatedly. Now  of course  if everybody says exactly the same thing then it's not interesting. So  all I'm saying is i- th- there's - there's a kind of question of what we're trying t- to accomplish. and - I think my temptation for the data gathering would be to uh  you know - And each person is only gonna do it once  so you don't have to worry about them being bored  so if - if it's one service  one luxury item  you know  one big-ish place  and so forth and so on  um - then my guess is that - that the data is going to be easier to handle. Now of course you have this I guess possible danger that somehow there're certain constructions that people use uh when talking about a museum that they wouldn't talk about with a university and stuff  um - but I guess I'm - I- uh- m- my temptation is to go for simpler. You know  less variation. But I don't know what other people think about this in terms of - uh - So I don't exactly understand - like I- I - I guess we're trying to - limit the detail of our ontology or types of places that someone could go  right? But who is it that has to care about this  or what component of the system? Oh  well  uh - th- I think there are two places where it comes up. One is uh - in the - th- these people who are gonna take this and - and try to do speech with it. Mm-hmm. uh - Lots of pronunciations of th- of the same thing are going to give you better data than l- you know  a few pronunciations of lots more things. O_K. That's one. So we would rather just ask - uh have a bunch of people talk about the zoo  uh and assume that that will - that the constructions that they use there will give us everything we need to know about these sort of Bigger - zoo  castle  whatever type things  these bigger places. Y- yeah thi- well this is a question for - And that way you get the speech data of people saying ""zoo"" over and over again or whatever too. O_K. Yeah. Yeah. Yeah. So this is a question for you  and  you know  if we - if we do  and we probably will  actually try to uh build a prototype  Mm-hmm. uh probably we could get by with the prototype only handling a few of them anyway. So  Um - Yeah  the- this was sort of - these are all different sort of activities. Um But I think y- I - I got the point and I think I like it. We can do - put them in a more hierarchical fashion. So  ""Go to place"" and then give them a choice  you know either they're the symphony type or opera type or the tourist site guide type or the nightclub disco type person and they say ""yeah this is - on that ""go to big-ish place""  this is what I would do."" Mm-hmm. And then we have the ""Fix"" thing  and then maybe ""Do something the other day"" thing  so. My question is - I guess  to some extent  we should - y- we just have to try it out and see if it works. It would be challenging  in - in a sense  to try to make it so - so complex that they even really should schedule  or to plan it  uh  a more complex thing in terms of O_K  you know  they should get the feeling that there are these s- six things they have to do Well - yeah. and they sh- can be done maybe in two days. Well I think th- th- yeah. So they make these decisions  ""Can I go there tomorrow?"" or - you know - influences Yeah. Mm-hmm. Well  I think it's easy enough to set that up if that's your expectation. So  the uh system could say  ""Well  uh we'd like to - to set up your program for two days in Heidelberg  you know  let's first think about all the things you might like to do. So there - th- i- i- in - I mean - in - I th- I - I'm sure that if that's what you did then they would start telling you about that  and then you could get into um various things about ordering  if you wanted. Mm-hmm. Yeah. Yeah  but I think this is part of the instructor's job. And that can be done  sort of to say  ""O_K now we've picked these six tasks."" ""Now you have- you can call the system and you have two days."" I'm sorry. No  we have to help - we have to decide. Fey will p- carry out whatever we decide. And th- w- But we have to decide you know  what is the appropriate scenario. That's what we're gonna talk about t- yeah. Yep  yep. But these are two different scenarios entirely. I mean  one is a planner - The other  it kind of give you instructions on the spot Yeah  but th- the - I don't - I'm not really interested in sort of ""Phase planning"" capabilities. But it's more the - how do people phrase these planning requests? So are we gonna masquerade the system as this - as you said simple response system  ""I have one question I get one response""  or should we allow for a certain level of complexity. And a- I w- think the data would be nicer if we get temporal references. Well  so Keith  what do you think? Well  um it seems that - Yeah  I mean  off the top of my head it kinda seems like you would probably just want  you know  richer data  more complex stuff going on  people trying to do more complex sets of things. I mean you know  if our goal is to really sort of be able to handle a whole bunch of different stuff  then throwing harder situations at people will get them to do more linguistic - more interesting linguistic stuff. But I mean - I'm - I'm not really sure Uh  because I don't fully understand like what our choices are of ways to do this here yet. I mean w- we have tested this and a- y- have you heard - listen to the f- first two or th- as a matter of fact the second person is uh - is - was faced with exactly this kind of setup. And - I started to listen to one and it was just like  um  uh  sort of depressing. I thought I'd just sort of listen to the beginning part and the person was just sort of reading off her script or something. And. Oh  O_K. That was the first subject. Yeah. Yeah. First one wasn't very good. Yeah. Yeah. So um  I - Although - Um  it is - already with this it got pretty - with this setup and that particular subject Mm-hmm. it got pretty complex. Maybe - I suggest we make some fine tuning of these  get - sort of - run through ten or so subjects and then take a breather  and see whether we wanna make it more complex or not  depending on what - what sort of results we're getting. Mm-hmm. Right. Yeah. It - In fact  um  I am just you know - today  next couple days gonna start really diving into this data. I've basically looked at one of the files - you know one of these - l- y- y- y- you gave me those dozens of files and I looked at one of them which was about ten sentences  found fifteen  twenty different construction types that we would have to look for and so on and like  ""alright  well  let's start here."" Um. So I haven't really gone into the  you know - looked at all of the stuff that's going on. So I don't really - Right  I mean  once I start doing that I'll have more to say about this kind of thing. O_K. And y- and always - But well th- but you did say something important  which is that um you can probably keep yourself fairly well occupied uh - with the simple cases for quite a while. Yeah. Although  obviously th- so - so that sa- s- does suggest that - Uh  now  I have looked at all the data  and it's pre- it's actually at least to an amateur  quite redundant. That - that it was - it was very stylized  Yeah  Yeah. and quite a lot of people said more or less the same thing. I um - I did sort of scan it at first and noticed that  and then looked in detail at one of them. But yeah  yeah I noticed that  too. Yeah. So  we - we - we wanna do more than that. And with this we're getting more. O_K. No question. Right. So - uh w- do we wanna get going beyond more  which is sort of the - Well  O_K  so let's - let's take - let's I - I think your suggestion is good  which is we'll do a b- uh - a batch. O_K. And  uh  Fey  How long is it gonna be till you have ten subjects? Couple days? Or thr- f- a- A week? Or - I don't - I don't have a feel for th- Um - I can - Yeah  I mean I s- I think can probably schedule ten people  uh  whenever. Well  it's - it's up to you  I mean I j- I - uh- e- We don't have any huge time pressure. It's just - when you have t- Yeah. How long will it be? Um - I - I would say maybe two weeks. Oh  O_K. So let's do this. Let's plan next Monday  O_K  to have a review of what we have so far. and - This means audio  but - Huh? no transcriptions of course  yeah. No  we won't have the transcriptions  but what we should be able to do and I don't know if  Fey  if you will have time to do this  but it would be great if you could  um  not transcribe it all  but pick out uh  some stuff. I mean we could lis- uh - just sit here and listen to it all. Are you gonna have the audio on the web site? O_K. Until we reach the gigabyte thing and David Johnson s- ki- kills me. And we're gonna put it on the web site. Yeah. Oh  we could get - I mean  you can buy another disk for two hundred dollars  right? I mean it's - it's not like - O_K. So  we'll take care of David Johnson. O_K. No  he - Take - care of him. uh  he - he has been solving all our problems or - is wonderful  so s- O_K. Alright. So we'll buy a disk. But anyway  so  um  If you - if you can think of a way - to uh  point us to th- to interesting things  sort of as you're doing this or - or something uh  make your - make notes or something that - that this is  you know  something worth looking at. And other than that  yeah I guess we'll just have to uh  listen - although I guess it's only ten minutes each  right? Roughly. Well  I guess. I'm not sure how long it's actually going to take. The reading task is a lot shorter. That was cut by fifty percent. And the reading  nobody's interested in that except for the speech people. Right. No  we don't care about that at all. So. It's actually like five minutes dialogue. I- b- My guess is it's gonna be ten. People - Ten minutes is long. I understand  but people - people - you know uh - It feels like a long time but. Yeah. It feels like forever when you're doing it  but then it turns out to be three minutes and forty five seconds. Yeah. Yeah. Yeah. Could be. O_K. I was thinking people would  you know  hesitate and - Whatever. Whatever it is we'll - we'll deal with it. Yeah  it's not - O_K  so that'll be - that'll be - And it's fun. um - on - on the web page. O_K. That's great. Um But anyway - yeah  so I think - it's a good idea to start with the sort of relatively straight forward res- just response system. And then if we want to uh - get them to start doing - uh - multiple step planning with a whole bunch of things and then organize them an- um tell them which things are near each other and - you know  any of that stuff. uh - You know  ""Which things would you like to do Tuesday morning?"" Yeah. So yeah I - th- that seems - pretty straight forward. @@ But were you saying that - I need those back by the way. O_K. O_K. Yeah. That's for - I'm sorry  Fey  what? That w- maybe one thing we should do is go through this list and sort of select things that are categories and then o- offer only one member of that category? That's what I was suggesting for the first round  yeah. O_K. So rather than having zoo and castle. And then  I mean  they could be alternate versions of the same - They could  but i- but i- uh- tha- If you wanted data on different eh- they- c- yeah  but - uh - constructions. but - Like one person gets the version with the zoo as a choice  and the other person gets the - You could  but i- but I - I - I think in the short run  - And no  th- the per- the person don't get it. I mean  this is why we did it  because when we gave them just three tasks for w- part-A_ and three tasks for part-B_ a- Right. Yeah. Well no  they could still choose. They just wouldn't be able to choose both zoo and say  touring the castle. Exactly. This is limiting the choices  but yeah. Right. O_K  sorry. But um I - I think this approach will very well work  but the person was able to look at it and say ""O_K  This is what I would actually do."" Yeah. O_K. Yeah. @@ O_K. @@ He was vicious. O_K  we gotta - we gotta disallow uh - traveling to zoos and uh castles at the same time  sort of - I mean there - they are significantly different  but. But no  they're - I mean they're sort of - this is where tour becomes - you know tourists maybe a bit different and  um  these are just places where you - you enter um  much like here. Yeah  I guess so. Yeah. But we can uh - Yeah  in fact if y- if y- if you use the right verb for each in common  like at- you know  ""attend a theater  symphony or opera"" is - is a group  and ""tour the university  castle or zoo""  mm-hmm all of these d- do have this kind of ""tour"" um - aspect about the way you would go to them. Yeah. And uh  the movie theater is probably also uh - e- is a- ""attend"" Attend  yeah. et cetera. So it may turn out to be not so many different kinds of things  and then  what one would expect is that - that the sentence types would - Hmm  mm-hmm. uh their responses would tend to be grouped according to the kind of activity  you would expect. Mm-hmm. But I mean i- it seem that um - there is a difference between going - to see something  and things like ""exchange money"" or ""dine out"" uh - Oh  absolutely. Yeah. Yeah  this is where - yeah - th- the function stuff is definitely different and the @@ function  yeah. getting information or g- stuff - yeah. O_K. But this is open. So since people gonna still pick something  we- we're not gonna get any significant amount of redundancy. And for reasons  we don't want it  really  in that sense. And um we would be ultimately more interested in getting all the possible ways of people asking  oh  for different things with - or with a computer. And so if you can think of any other sort of high level tasks a tourist may do just always - just m- mail them to us and we'll sneak them into the collection. We're not gonna do much statistical stuff with it. We don't have enough. No. But it seems like since we - since we are getting towards uh subject - uh fifty subjects and if we can keep it up um to a - uh - sort of five four-ish per week rate  we may even reach the one hundred before Fey t- takes off to Chicago. That means that one hundred people have to be interested. Good luck. Yeah. Well  um  these are all f- people off campus s- from campus so far  right? Yeah. Yeah. So we - yeah we don't know how many we can get next door at the - uh shelter for example. Hmm. Uh for ten bucks  probably quite a few. Yeah. That's right. Yeah. So  alright  so let's go - let's go back then  Yep. to the - the chart with all the decisions and stuff  and see how we're doing. Do - do people think that  you know this is - is gonna - um cover what we need  or should we be thinking about more? Okay  in terms of decision nodes? Yep. I mean  Go-there is - is a yes or no. Right? Yep. Mm-hmm. I'm also interested in th- in this ""property"" uh line here  so if you look at - sorry  look at that um   timing was um - I have these three. Do we need a final differentiation there? Now  later on the same tour  sometimes on the next tour. What's this idea of ""next tour""? I mean - It's sort of next day  so you're doing something now and you have planned to do these three four things  and you can do something immediately  Mm-hmm. Mm-hmm. you could sort of tag it on to that tour Or - O_K. or you can say this is something I would do s- I wanna do sometime l- O_K. in my life  O_K. basically. So - so this tour is sort of just like th- the idea of current s- round of - of touristness or whatever  O_K. Right. Yeah. Yeah  probably between stops back at the hotel. O_K. Got it. I mean if you - if - if you wanted precise about it  uh you know  uh - and I think that's the way tourists do organize their lives. Got it. Sure  sure  sure. You know  ""O_K  we'll go back to the hotel and then we'll go off and -"" O_K. So all tours - b- a tour happens only within one day? Yes. O_K. So the next tour will be tomorrow? It - Right. For this. O_K. Just to be totally clear. O_K. Well  my visit to Prague there were some nights where I never went back to the hotel  so whether that counts as a two-day tour or not we'll have to think. You just spend the whole time at U_ Fleku or something  ri- Yeah. I - w- we will - we will not ask you more. Right. That's enough. Right. I don't know. What is the uh - the - the English co- uh um cognate if you want  for ""Sankt Nimmerlandstag""? Keine Ahnung Yeah. Sort of ""We'll do it on - when you say on that d- day it means it'll never happen. Right. O_K. Do you have an expression? Probably you sh- Not that I know of actually. Yeah  when hell - Yep  we'll do it when hell freezes over. So maybe that should be another Yeah. Never. Yeah. property in there. Right. Yeah. No. O_K. Um  the reason why - why do we go there in the first place I_E uh - it's either uh - for sightseeing  for meeting people  for running errands  or doing business. Is this the theater thing? Like entertainment or something like that? Entertainment is a good one in there  I think. I agree. So  business is supposed to uh  be sort of - it - like professional type stuff  right  or something like that? Yep. O_K. Um. I mean - this w- this is uh an old uh Johno thing. He sort of had it in there. ""Who is the - the tour- is the person?"" So it might be a tourist  it might be a business man who's using the system  who wants to sort of go to some - Mm-hmm. Yeah. Yeah  or - or both. Yeah. Yeah  I mean like for example my - my father is about to travel to Prague. He'll be there for two weeks. He is going to uh - Yep. He's there to teach a course at the business school but he also is touring around and so he may have some mixture of these things. Yep. Yep. Mmm. Sure. Right. He would - What ab- What do you have in mind in terms of um - socializing? What kind of activities? Eh  just meeting people  basically. Oh - ""I want to meet someone somewhere""  which be- puts a very heavy constraint on the ""EVA"" Yeah. you know  because then if you're meeting somebody at the town hall  you're not entering it usually  you're just - want to approach it. So - I mean  does this capture  like  where do you put - ""Exchange money"" is an errand  right? But what about uh - So  like ""Go to a movie"" is now entertainment  ""Dine out"" is - Mm-hmm Yep. Socializing  I guess. No  I- I- well  I dunno. Let - Let - well  we'll put it somewhere  but - but - um - So I mean - Right. I would say that if ""Dine out"" is a special c- uh - if you're doing it for that purpose then it's entertainment. Yeah. And - we'll also as y- as you'll s- further along we'll get into business about ""Well  you're - you know - this is going over a meal time  do you wanna stop for a meal or pick up food or something?"" Mm-hmm. And that's different. That's - that's sort of part of th- that's not a destination Right. reason  that's sort of ""en passant "" right. That goes with the ""energy depletion"" function  blech. Yeah. Right  yeah. O_K  ""endpoint"". ""Tourist needs food  badly"" Right. ""Endpoint"" is pretty clear. Um  ""mode""  uh  I have found three  ""drive there""  ""walk there"" uh - or ""be driven""  which means bus  taxi  BART. O_K. The public transport in general is ""be driven""? Yeah. Yep. Obviously taxis are very different than buses  but on the other hand the system doesn't have Yeah. any public transport - This - the planner system doesn't have any public transport in it yet. O_K. So this granularity would suffice  I think w- if we say the person probably  based on the utterance we- on the situation we can conclude wants to drive there  walk there  or use some other form of transportation. H- How much of Heidelberg can you get around by public transport? I mean in terms of the interesting bits. There's lots of bits where you don't really I've only ev- was there ten years ago  for a day  so I don't remember  but. Mm- I mean  like the - sort of the tourist-y bits - is it like - Everywhere. Well  you can't get to the Philosophers' Way very well  but  I mean there are hikes that you can't get to  but - Yeah. Yeah. O_K. but I think other things you can  if I remember right. So is like ""biking there"" - part of like ""driving there""  or - ? Yeah  um we actually - biking should be - should be a separate point because we have a very strong bicycle planning component. Oh! So. Mmm g- that's good. Put it in. Um. Bicycles c- should be in there  but  will we have bic- I mean is this realistic? I mean - Yeah. O_K  we can leave it out  I guess. Yeah. We can - we can sort of uh  drive - I would - I would lump it with ""walk"" because hills matter. Right? You know. Things like that. Yeah. Yeah. O_K. Skateboards @@ right  anyway. Right. Scooters  right? Yep. Alright. O_K  ""Length"" is um  you wanna get this over with as fast as possible  you wanna use some part of what - of the time you have. They can specify ""long"". Um  they can. But we should just make a decision whether we feel that they want to use some substantial or some fraction of their time. Ye- You know  they wanna do it so badly that they are willing to spend uh - you know the necessary and plus time. Hmm. And um - And y- you know  if we feel that they wanna do nothing but that thing then  you know  we should point out that - to the planner  that they probably want to use all the time they have. So  stretch out that visit for Mm-hmm. Wow - It seems like this would be really hard to guess. that. I mean  on the part of the system. It seems like it - I mean you're - you're talking about rather than having the user decide this you're supposed t- we're supposed to figure it out? w- Th- the user can always s- say it  but it's just sort of we - we hand over these parameters if we make - if we have a feeling that they are important. well- Overrider @@ Yeah. Mm-hmm. And that we can actually infer them to a significant de- degree  or we ask. And - O_K. And par- yeah  and part of the system design is that if it looks to be important and you can't figure it out  then you ask. Yeah. O_K. But hopefully you don't ask you know  a- all these things all the time. Or - Yeah. Yeah. Right. Yeah. eh- so  y- but there's th- but definitely a back-off position to asking. And if no - no part of the system ever comes up with the idea that this could be important  no planner is ever gonna ask for it. y- so - And I like the idea that  you know  sort of - Jerry pushed this idea from the very beginning  that it's part of the understanding business to sort of make a good question of what's s- sort of important in this general picture  what you need t- Yeah. Mm-hmm. If you wanna simulate it  for example  what parameters would you need for the simulation? And  Timing  uh  uh  Length would definitely be part of it  ""Costs""  ""Little money  some money  lots of money""? Mm-hmm. Actually  maybe uh F- uh so  F- Yeah  O_K. Some @@ . Hmm? You could say ""some"" in there. I must say that thi- this one looks a bit strange to me. Um - maybe - It seems like appropriate if I go to Las Vegas. Well - but I decide k- kind of how much money uh I'm willing to lose. But a- I as a tourist  I'll just paying what's - what's more or less is required. Well  no. I think there are - there're different things where you have a ch- choice  for example  Mmm. Yeah. uh this t- interacts with ""do- am I do- oh- are you willing to take a taxi?"" Dinner. Restaurant. Or uh  you know  if - if you're going to the opera are you gonna l- look for The best seat or - or - the best seats or the peanut gallery or  you know  whatever? Right. O_K. So - S- so I think there are a variety of things in which um - Tour- tourists really do have different styles eating. Another one  you know. Yeah. Right  that's true. Right. The - what - what my sort of sentiment is they're - Well  I - I once had to write a - a - a - a charter  a carter for a - a student organization. And they had - wanted me to define what the quorum is going to be. And I looked at the other ones and they always said ten percent of the student body has to be present at their general meeting otherwise it's not a - And I wrote in there ""En- Enough"" people have to be there. And it was hotly debated  but people agreed with me that everybody probably has a good feeling whether it was a farce  a joke  or whether there were enough people. And if you go to Turkey  you will find Yeah. when people go shopping  they will say ""How much cheese do you want?"" and they say ""Ah  enough."" And the - and the - this used all over the place. Because the person selling the cheese knows  you know  that person has two kids and you know  a husband that dislikes cheese  so this is enough. Mm-hmm. And um so the middle part is always sort of the - the golden way  right? So you can s- you can be really - make it as cheap as possible  or you can say ""I want  er  you know  I don't care"" Money is no object. Mm-hmm. Yeah. Money is no object  or you say ""I just want to spend enough"". Mm-hmm. Or the sufficient  or the- the appropriate amount. Yeah. But  Then again  this may turn out to be insufficient for our purposes. But well  this is my first guess  in much the same way as how - how d- I mean y- Yeah. you know - should the route be? Should it be the easiest route  even if it's a b- little bit longer? Mm-hmm. No steep inclinations? Go the normal way? Whatever that again means  er - or do you - does the person wanna rough it? Mm-hmm. I mean - th- so there's a couple of different ways you can interpret these things right? You know - ""I want to go there and I don't care if it's really hard."" Or if you're an extreme sport person  you know. ""I wanna go there and I insist on it being the hard way."" Right? you know  so I assume we're going for the first interpretation  right? Something like - Right. Right. I'll go th- I mean - I'd li- I dunno. It's different from thing to - No  I think he was going for the second one ar- actually. Yeah? Anyway  we'll sort th- yeah  we'll sort that out. I - I - O_K. Right. Yeah. Absolutely. Well  this is all sort of um  top of my head. No - no research behind that. Yeah. Um - ""Object information""  ""Do I - do I wanna know anything about that object?"" is either true or false. And. if I care about it being open  accessible or not  I don't think there's any middle ground there. Um  either I wanna know where it is or not  I wanna know about it's history or not  or  um I wanna know about what it's good for or not. Maybe one could put scales in there  too. So I wanna know a l- Yeah  now ob- O_K  I'm sorry  go ahead  what were you gonna say? lot about it. One could put scales in there. So I wanna know a lot about the history  just a bit. Yeah  right well y- i- w- if we - w- right. So ""object"" becomes ""entity""  right? Yep  that's true. Yeah  but we don't have to do it now. Yep. That was the wrong shortcut anyhow. And we think that's it  interestingly enough  that um  you know  th- or - or - or something very close to it is going to be uh - going to be enough. And - Still wrong. Yeah. O_K. Alright  so um - So I think the order of things is that um  Robert will clean this up a little bit  although it looks pretty good. And - What  well this is the part that - this is the part that needs the work. Huh? Right. Yeah  so - right  so - So  Yeah. um In parallel  uh - three things are going to happen. Uh Robert and Eva and Bhaskara are gonna actually - build a belief-net that - that  um  has C_P_T's and  you know  tries to infer this from various kinds of information. And Fey is going to start collecting data  and we're gonna start thinking a- about - uh - what constructions we want to elicit. And then w- go- it may iterate on uh  further data collection to elicit - D- Do you mean - Do you mean eliciting particular constructions? Or do you mean like what kinds of things we want to get people talking about? Well  yes. Semantically speaking  eh? Both. O_K. Uh  and - Though for us  constructions are primarily semantic  right? Right. And - Sure. And so - uh - I mean from my point of view I'm - I'm trying to care about the syntax  so you know - Well that too  but um - O_K. You know if th- if we in - if we you know  make sure that we get them talking about temporal order. Yeah. O_K  that would be great and if th- if they use prepositional phrases or subordinate clauses or whatever  um - Mm-hmm. Right. O_K. W- You know  whatever form they use is fine. O_K. But I - I think that probably we're gonna try to look at it as you know  s- what semantic constructions d- do we - do we want them to uh do direc- you know  um  O_K. ""Caused motion""  I don't know  something like that. O_K. Uh Got it. But  Eh- uh- this is actually a conversation you and I have to have about Yeah. your thesis fantasies  and how all this fits into that. Uh Yeah. O_K. But uh - Well  I will tell you the German tourist data. O_K. Because I have not been able to dig out all the stuff out of the m- ta- thirty D_V_Ds. O_K. Um - Is that roughly the equivalent of - of what I've seen in English or is it - If you - No  not at all. Dialogues. O_K. O_K. SmartKom - Human. Wizard of Oz. O_K. Same - O_K  that. Got it. Like what - What have I got now? I mean I have uh what - what I'm loo- what I - Those files that you sent me are the user side of some interaction with Fey? Is that what it is? Or - ? A little bit of data  I - With nothing. No  no. Just talking into a box and not hearing anything back. O_K. Yep. Yep. Some data I collected in a couple weeks for training recognizers and email way back when. O_K. O_K. Nothing to write home about. O_K. And um - the - see this - this - this - uh - ontology node is probably something that I will try to expand. Once we have the full ontology A_P_I  what can we expect to get from the ontology? And hopefully you can sort of also try to find out  you know  sooner or later in the course of the summer what we can expect to get from the discourse that might  you know - or the - not the discourse  the utterance Mm-hmm. mm-hmm. Mm-hmm. Right. as it were  uh  in terms of uh - Right  but we're not expecting Keith to actually build a parser. No  no  no  no  no. Uh  this is - Right  Right. O_K. We are expecting Johno to build a parser  but that's a - Yes. By the end of the summer  too. No. No. No. Uh - But it's sort of - it's - He's g- he's hoping to do this for his masters' thesis s- by a year from now. Right. Hmm. Still  pretty formidable actually. Eh- absolutely. Uh - limited. I mean  you know  the idea is - is  Well  the hope is that the parser itself is  uh  pretty robust. Yeah. But it's not popular - it's only p- only - Right  Right. Existence proof  you know. Set up the Right. It's only popula- infrastructure  yeah. Right. Um sometime  I have to talk to some subset of the people in this group  at least about um what sort of constructions I'm looking for. I mean  you know obviously like just again  looking at this one uh thing  you know  I saw y- things from - sort of as general as argument structure constructions. Oh  you know  I have to do Verb Phrase. I have to do uh - uh - unbounded dependencies  you know  which have a variety of constructions in - uh - uh - instantiate that. On the other hand I have to have  you know  there's particular uh  fixed expressions  or semi-fixed expressions like ""Get"" plus path expression for  you know  ""how d- ho- how do I get there?""  ""How do I get in?""  ""How do I get away?"" and all that kind of stuff. Mm-hmm. Right. Um  so there's a variety of sort of different Absolutely. sorts of constructions and it - you know it's - it's sort of like anything goes. Like - O_K  so this is - I think we're gonna mainly work on with George. O_K. O_K  and hi- let me f- th - say what I think is - is - so the idea is - uh - first of all I misspoke when I said we thought you should do the constructions. Cause apparently for a linguist that means to do completely and perfectly. So what I - yeah  O_K  - So what - what I meant was ""Do a first cut at"". er - that's what Yeah  yeah. O_K  Because uh - we do wanna get them r- u- perfectly - but I think we're gonna have to do a first cut at a lot of them to see how they interact. Of course. Right  exactly. Now it - w- we talked about this before  right. And I - I me- it would- it would be completely out of the question to really do more than  say  like  oh I don't know  ten  over the summer  but uh  but you know obviously we need to get Yeah. sort of a general view of what things look like  so yeah. Right. So the idea is going to be to do - sort of like Nancy did in some of the- er these papers where you do enough of them so you can go from top to bottom - so you can do f- you know  f- f- uh - have a complete story ov- of s- of some piece of dialogue. Mm-hmm. And that's gonna be much more useful than having all of the clausal constructions and nothing else  or - or - or something like that. So that the - the trick is going to be Yeah. Sure. Yeah. t- to take this and pick a - some sort of lattice Mm-hmm. of constructions  so- some lexical and some phrasal  and - and  you know  whatever you need in order to Mm-hmm. uh  be able to then  uh  by hand  you know  explain  Mm-hmm. Yeah. some fraction of the utterances. And so  exactly which ones will partly depend on your research interests and a bunch of other things. Mm-hmm. Sure. O_K. But I mean in terms of the s- th- sort of level of uh - of analysis  you know  these don't necessarily have to be more complex than like the ""Out of"" construction in the B_C_P paper where it's just like  you know  half a page on each one or something. Correct. Oh yeah - yeah. V- a half a page is - is what we'd like. Yeah. And if - if there's something that really requires a lot more than that then it does and we have to do it  but - Yeah. For the first cut  that should be fine  yeah. Yeah. We could sit down and think of sort of the - the ideal speaker utterances  and I mean Mm-hmm. two or three that follow each other  so  where we can also sort of  once we have everything up and running  show the tremendous  insane inferencing capabilities of our system. Mm-hmm. So  you know  as - as the SmartKom people have. This is sort of their standard demo dialogue  which is  you know  what the system survives and nothing but Mm-hmm. that. Um  we could also sor- sort of have the analogen of Mm-hmm. o- our sample sentences  the ideal sentences where we have complete construction coverage Mm-hmm. and  sort of  they match nicely. So the - the ""How do I get to X_?""  you know  that's definitely gonna be Yeah. uh  a major one. Yeah. That's about six times in this little one here  so uh  yeah. Right. Yep. ""Where is X_?"" might be another one which is not too complicated. Yeah. Mm-hmm. And um ""Tell me something about X_."" Yeah. And hey  that's - that's already covering eighty percent of the system's functionality. Ye- Right  but it's not covering eighty percent of the intellectual interest. Yeah. No  we can w- throw in an ""Out of Film"" construction if you want to  but - No  no  no. Well the - th- the thing is there's a lot that needs to be done to get this right. O_K  I th- O_K. We done? I have one bit of news. Um  Good. Good. the action planner guy has wrote - has written a - a p- lengthy - proposal on how he wants to do the action planning. And I responded to him  also rather lengthy  how he should do the action planning. And - ""Action planning"" meaning ""Discourse Modeling""? Yes. And I tacked on a little paragraph about the fact that the whole world calls that Right. module a dis- disc- dialogue manager  and Right. wouldn't it make sense to do this here too? And also Rainer M- Malaka is going to be visiting us shortly  most likely in the beginning of June. Uh-huh  I'll be gone. Yeah. He- he's just in a conference somewhere and he is just swinging through town. Sure  O_K. And um - m- making me incapable of going to N_A_A_C_L  for which I had funding. But. No  no Pittsburg this year. Hmm. S- When is the uh- Santa Barbara? Who is going to? uh- should a lot of people. That's something I will - would - sort of enjoy. Probably should go. That was - that's one you should probably go to. Yep. How much does it cost? I haven't planned to go. There's Uh  probably we can uh - pay for it. O_K. Um a student rate shouldn't be very high. So  if we all decide it's a good idea for you to go then you'll - we'll pay for it. Right. Sure. Then you can go. I mean I - I don't have a feeling one way or the other at the moment  but it probably is. O_K. O_K  great. Thanks. ","The data collection script has been slightly modified  so that it encourages more natural dialogue between the subjects and the ""wizard"". Another trial run will take place  while a call to recruit subjects is being emailed to students. Meanwhile  the translation of the TV and cinema information system to english is almost complete. This was the basic requirement of the project. On the other hand  there was a presentation of the model that offers more elaborate action planning for SmartKom  of which Enter/View/Approach (EVA) modes are a part. These modes will form categories of complete XML schemas with information filled in from the language understanding in a more elaborate way than the current Object-""Go Action""-Object model. These categories will  in turn  be linked with action schemas  one of which is Source-Path-Goal (SPG). Categories and action schemas can have -in theory- any number of blocks depending on the expansion of the domain. The notation provides for linking and referencing between different schemas. The model also allows for multiple action schemas to be triggered in parallel. However  the structure of the model is open for discussion  since its use was to elicit discussion and highlight issues. As the data collection is about to start  a call for the recruitment of subjects is going to be sent out. The main pool of subjects is going to be the student community in the institute. Along with the ""wizard""  who is going to be an integral part of the experiments  another person needs to be hired as the instructor for the tasks involved in them. Meetings were rescheduled and are now going to take place on Fridays. For the next meeting  there is going to be a presentation of the modifications in the parser module of the basic system. Additionally  the proposed XML model will be put to the test with concrete data. Similarly  such examples will clarify issues relating to the binding and redundancy of features with common characteristics amongst the shcemas (eg ""Container"" for Enter and ""Goal"" for SPG). Subjects in the trial runs of the experiment were given detailed descriptions of the tasks  which led to the subsequent dialogue being a re-iteration or re-phrasing of the instructions. Using pictures instead would be one way to deal with the problem  however  it was deemed too laborious and it would divert the focus of the experiment. As the original action planner of the SmartKom system only included a generic SPG schema  a new module was presented that allows for variety in the user intentions to be included. This being only a model  there are several issues that will need to be clarified in the future. How the model deals with redundancy of information among categories and action schemas  and whether a flat or a hierarchical model would be preferable are two of them. What is also clear is that as the domain of research broadens beyond the study of EVA modes  the complexity of the model will also increase. Another trial run of the data collection experiment is to take place  while subjects are being recruited. There have been some adjustments in the script. The prior description of tasks the subjects are going to be given is now going to be more schematic  although the intentions are still going to be clear. The lack of detailed  written explanation will hopefully encourage more natural and varied dialogue between subjects and ""wizard"". On the other hand  the generator module of the system has been translated from german. Eventually  a user is going to be able to request and receive TV- and cinema-related information in english. This will satisfy the basic project requirements. The model of a new module for SmartKom was presented. It is an interface between the language understanding and the action planning modules. One layer of XML schemas creates a richer representation of the linguistic analysis  which is subsequently used to trigger one or more action schemas. The model keeps the concept of XML messages being sent between the modules of the system  although it is open-ended as to the number of schemas involved. "
"Is it starting now? Yep. So what - what - from - what - Whatever we say from now on  Hello? it can be held against us  right? That's right. and uh It's your right to remain silent. Yeah. So I - I - the - the problem is that I actually don't know how th- these held meetings are held  if they are very informal and sort of just people are say what's going on and O_K. Yeah. Yeah  that's usually what we do. We just sorta go around and people say what's going on  what's the latest uh - Yeah. O_K. So I guess that what may be a - reasonable is if I uh first make a report on what's happening in Aurora in general  at least what from my perspective. Yeah. That would be great. Uh o- And - and uh so  I - I think that Carmen and Stephane reported on uh Amsterdam meeting  which was kind of interesting because it was for the first time we realized we are not friends really  but we are competitors. Cuz until then it was sort of like everything was like wonderful and - Yeah. It seemed like there were still some issues  right? that they were trying to decide? Yeah. There is a plenty of - there're plenty of issues. Like the voice activity detector  and - Well and what happened was that they realized that if two leading proposals  which was French Telecom Alcatel  and us both had uh voice activity detector. Right. And I said ""well big surprise  I mean we could have told you that n- n- n- four months ago  except we didn't because nobody else was bringing it up"". Obviously French Telecom didn't volunteer this information either  Right. cuz we were working on - mainly on voice activity detector for past uh several months because that's buying us the most uh thing. And everybody said ""Well but this is not fair. We didn't know that."" And of course uh the - it's not working on features really. And be- I agreed. I said ""well yeah  you are absolutely right  I mean if I wish that you provided better end point at speech because uh - Right. or at least that if we could modify the recognizer  uh to account for these long silences  because otherwise uh that - that - th- that wasn't a correct thing."" And so then ev- ev- everybody else says ""well we should - we need to do a new eval- evaluation without voice activity detector  or we have to do something about it"". And in principle I - uh I - we agreed. Right. Mm-hmm. We said uh ""yeah"". Because uh - but in that case  uh we would like to change the uh - the algorithm because uh if we are working on different data  we probably will use a different set of tricks. Right. But unfortunately nobody ever officially can somehow acknowledge that this can be done  because French Telecom was saying ""no  no  no  now everybody has access to our code  so everybody is going to copy what we did."" Yeah well our argument was everybody ha- has access to our code  and everybody always had access to our code. We never uh - uh denied that. We thought that people are honest  that if you copy something and if it is protected - protected by patent Yeah. then you negotiate  or something  right? I mean  if you find our technique useful  we are very happy. Right. Right. Mm-hmm. But - And French Telecom was saying ""no  no  no  there is a lot of little tricks which uh sort of uh cannot be protected and you guys will take them "" which probably is also true. I mean  you know  it might be that people will take uh uh th- the algorithms apart and use the blocks from that. But I somehow think that it wouldn't be so bad  as long as people are happy abou- uh uh uh honest about it. And I think they have to be honest in the long run  because winning proposal again - Yeah. uh what will be available th- is - will be a code. So the uh - the people can go to code and say ""well listen this is what you stole from me"" you know? Mm-hmm. Right. Right. ""so let's deal with that"". So I don't see the problem. The biggest problem of course is that f- that Alcatel French Telecom cl- claims ""well we fulfilled the conditions. We are the best. Uh. We are the standard."" And e- and other people don't feel that  because they - so they now decided that - that - is - the whole thing will be done on well-endpointed data  essentially that somebody will endpoint the data based on clean speech  because most of this the SpeechDat-Car has the also close speaking mike and endpoints will be provided. Mm-hmm. Ah. And uh we will run again - still not clear if we are going to run the - if we are allowed to run uh uh new algorithms  but I assume so. Because uh we would fight for that  really. uh but - since uh u- u- n- u- - at least our experience is that only endpointing a - a mel cepstrum gets uh - gets you twenty-one percent improvement overall and twenty-seven improvement on SpeechDat-Car Hmm. then obvious the database - uh I mean the - the - the - uh the baseline will go up. And nobody can then achieve fifty percent improvement. Right. So they agreed that uh there will be a twenty-five percent improvement required on - on uh h- u- m- bad mis- badly mismatched - But wait a minute  I thought the endpointing really only helped in the noisy cases. It uh - Oh  but you still have that with the M_F_C_C. O_K. Yeah. Y- yeah. Yeah but you have the same prob- I mean M_F_C_C basically has an enormous number of uh insertions. Right. Yeah. Yeah. Yeah. And so  so now they want to say ""we - we will require fifty percent improvement only for well matched condition  and only twenty-five percent for the serial cases."" Hmm. And uh - and they almost agreed on that except that it wasn't a hundred percent agreed. And so last time uh during the meeting  I just uh brought up the issue  I said ""well you know uh quite frankly I'm surprised how lightly you are making these decisions because this is a major decision. For two years we are fighting for fifty percent improvement and suddenly you are saying ""oh no we - we will do something less""  but maybe we should discuss that. And everybody said ""oh we discussed that and you were not a mee- there"" and I said ""well a lot of other people were not there because not everybody participates at these teleconferencing c- things."" Then they said ""oh no no no because uh everybody is invited. "" However  there is only ten or fifteen lines  so people can't even con- you know participate. So eh they agreed  and so they said ""O_K  we will discuss that."" Immediately Nokia uh raised the question and they said ""oh yeah we agree this is not good to to uh dissolve the uh uh - the uh - the criterion."" So now officially  Nokia is uh uh complaining and said they - they are looking for support  Mm-hmm. uh I think QualComm is uh saying  too ""we shouldn't abandon the fifty percent yet. We should at least try once again  one more round."" Mm-hmm. Mm-hmm. So this is where we are. I hope that - I hope that this is going to be a- adopted. Next Wednesday we are going to have uh Hmm. another uh teleconferencing call  so we'll see what uh - where it goes. So what about the issue of um the weights on the - for the different systems  the well-matched  and medium-mismatched and - Yeah  that's what - that's a g- very good uh point  because David says ""well you know we ca- we can manipulate this number by choosing the right weights anyways."" So while you are right but - uh you know but Mm-hmm. Uh yeah  if- of course if you put a zero - uh weight zero on a mismatched condition  or highly mismatched then - then you are done. Mm-hmm. But weights were also deter- already decided uh half a year ago. And they're the - staying the same? So - Well  of course people will not like it. Mm-hmm. Now - What is happening now is that I th- I think that people try to match the criterion to solution. They have solution. Now they Right. want to make sure their criterion is - And I think that this is not the right way. Yeah. Uh it may be that - that - Eventually it may ha- may ha- it may have to happen. Mm-hmm. But it's should happen at a point where everybody feels comfortable that we did all what we could. Mm-hmm. And I don't think we did. Basically  I think that - that this test was a little bit bogus because of the data and uh essentially there were these arbitrary decisions made  and - and everything. So  so - so this is - so this is where it is. So what we are doing at O_G_I now is uh uh uh working basically on our parts which we I think a little bit neglected  like noise separation. Uh so we are looking in ways is - in uh which - uh with which we can provide better initial estimate of the mel spectrum basically  which would be a l- uh  f- more robust to noise  and so far not much uh success. Hmm. We tried uh things which uh a long time ago Bill Byrne suggested  instead of using Fourier spectrum  from Fourier transform  use the spectrum from L_P_C model. Their argument there was the L_P_C model fits the peaks of the spectrum  so it may be m- naturally more robust in noise. And I thought ""well  that makes sense "" but so far we can't get much - much out of it. Hmm. uh we may try some standard techniques like spectral subtraction and - You haven't tried that yet? not - not - not much. Or even I was thinking about uh looking back into these totally ad- hoc techniques like for instance uh Hmm. Dennis Klatt was suggesting uh the one way to uh deal with noisy speech is to add noise to everything. Hmm! So. I mean  uh uh add moderate amount of noise Oh! to all data. I see. So that makes uh th- any additive noise less addi- less a- a- effective  right? Because you already uh had the noise uh in a - Right. And it was working at the time. It was kind of like one of these things  you know  but if you think about it  it's actually pretty ingenious. So well  you know  just take a - take a spectrum and - and - and add of the constant  C_  to every - every value. Well you're - you're basically y- Yeah. So you're making all your training data more uniform. Exactly. And if - if then - if this data becomes noisy  it b- it becomes eff- effectively becomes less noisy basically. Hmm. But of course you cannot add too much noise because then you'll s- then you're clean recognition goes down  but I mean it's yet to be seen how much  it's a very simple technique. Mm-hmm. Yes indeed it's a very simple technique  you just take your spectrum and - and use whatever is coming from F_F_T  add constant  Hmm. you know? on - onto power spectrum. That - that - Or the other thing is of course if you have a spectrum  what you can s- start doing  you can leave - start leaving out the p- the parts which are uh uh low in energy and then perhaps uh one could try to find a - a all-pole model to such a spectrum. Because a all-pole model will still try to - to - to put the - the continuation basically of the - of the model into these parts where the issue set to zero. That's what we want to try. I have a visitor from Brno. He's a - kind of like young faculty. pretty hard-working so he - so he's - so he's looking into that. Hmm. And then most of the effort is uh now also aimed at this e- e- TRAP recognition. This uh - this is this recognition from temporal patterns. Hmm! What is that? Ah  you don't know about TRAPS! Hmm. The TRAPS sound familiar  I - but I don't - Yeah I mean tha- This is familiar like sort of because we gave you the name  but  what it is  is that Mm-hmm. normally what you do is that you recognize uh speech based on a shortened spectrum. Mm-hmm. Essentially L_P_- L_P_C  mel cepstrum  uh  everything starts with a spectral slice. Uh so if you s- So  given the spectrogram you essentially are sliding - sliding the spectrogram along the uh f- frequency axis and you keep shifting this thing  Mm-hmm. Mm-hmm. and you have a spectrogram. So you can say ""well you can also take the time trajectory of the energy at a given frequency ""  Mm-hmm. and what you get is then  that you get a p- vector. And this vector can be a - a - s- assigned to s- some phoneme. Namely you can say i- it - I will - I will say that this vector will eh - will - will describe the phoneme which is in the center of the vector. And you can try to classify based on that. Hmm. And you - so you classi- so it's a very different vector  very different properties  we don't know much about it  Hmm. but the truth is - But you have many of those vectors per phoneme  right? Well  so you get many decisions. Uh-huh. And then you can start dec- thinking about how to combine these decisions. Exactly  that's what - yeah  that's what it is. Hmm. Hmm. Because if you run this uh recognition  you get - you still get about twenty percent error - uh twenty percent correct. You know  on - on like for the frame by frame basis  so uh - Hmm. uh so it's much better than chance. How wide are the uh frequency bands? That's another thing. Well c- currently we start - I mean we start always with critical band spectrum. For various reasons. But uh the latest uh observation uh is that you - you - you are - you can get quite a big advantage of using two critical bands at the same time. Are they adjacent  or are they s- O_K. Adjacent  adjacent. And the reasons - there are some reasons for that. Because there are some reasons I can - I could talk about  will have to tell you about things like masking experiments which uh uh uh uh yield critical bands  and also experiments with release of masking  which actually tell you that something is happening across critical bands  across bands. And - Well how do you - how do you uh convert this uh energy over time in a particular frequency band into a vector of numbers? It's uh uh uh I mean time T_zero is one number  time t- Yeah but what's the number? Is it just the - It's a spectral energy  logarithmic spectral energy  yeah. it's just the amount of energy in that band from f- in that time interval. Yes  yes. Yes  yes. O_K. And that's what - that's what I'm saying then  so this is a - this is a starting vector. It's just like shortened f- spectrum  or something. But now we are trying to understand what this vector actually represents  for instance a question is like ""how correlated are the elements of this vector?"" Mm-hmm. Turns out they are quite correlated  because I mean  especially the neighboring ones  right? They - they represent the same - almost the same configuration of the vocal tract. Yeah. Yeah. Mm-hmm. So there's a very high correlation. So the classifiers which use the diagonal covariance matrix don't like it. So we're thinking about de-correlating them. Hmm. Then the question is uh ""can you describe elements of this vector by Gaussian distributions""  or to what extent? Because uh - And - and - and so on and so on. So we are learning quite a lot about that. And then another issue is how many vectors we should be using  I mean the - so the minimum is one. Hmm. Mm-hmm. But I mean is the - is the critical band the right uh uh dimension? So we somehow made arbitrary decision  ""yes"". Then - but then now we are thinking a lot how to - uh how to use at least the neighboring band because that seems to be happening - This I somehow start to believe that's what's happening in recognition. Cuz a lot of experiments point to the fact that people can split the signal into critical bands  but then oh uh uh so you can - you are quite capable of processing a signal in- uh uh independently in individual critical bands. That's what masking experiments tell you. But at the same time you most likely pay attention to at least neighboring bands when you are making any decisions  you compare what's happening in - in this band to what's happening to the band - to - to - to the - to the neighboring bands. And that's how you make uh decisions. That's why the articulatory events  which uh F- F- Fletcher talks about  they are about two critical bands. You need at least two  basically. You need some relative  relative relation. Hmm. Hmm. Absolute number doesn't tell you the right thing. You need to - you need to compare it to something else  what's happening but it's what's happening in the - in the close neighborhood. So if you are making decision what's happening at one kilohertz  you want to know what's happening at nine hundred hertz and it - and maybe at eleven hundred hertz  but you don't much care what's happening at three kilohertz. So it's really w- It's sort of like saying that what's happening at one kilohertz depends on what's happening around it. It's sort of relative to it. To some extent  it - that is also true. Yeah. But it's - but for - but for instance  Mm-hmm. th- uh uh what - what uh humans are very much capable of doing is that if th- if they are exactly the same thing happening in two neighboring critical bands  recognition can discard it. Hmm. Is what's happening - Hey! Hey! O_K  we need us another - another voice here. Hey Stephane. Yeah  I think so. Yep. Sure. Go ahead. Yeah? And so so - so for instance if you d- if you a- if you add the noise that normally masks - masks the uh - the - the signal Mm-hmm. right? and you can show that in - that if the - if you add the noise outside the critical band  that doesn't affect the - the decisions you're making about a signal within a critical band. Unless this noise is modulated. If the noise is modulated  with the same modulation frequency Hmm. as the noise in a critical band  the amount of masking is less. Mmm. The moment you - moment you provide the noise in n- neighboring critical bands. So the s- m- masking curve  normally it looks like sort of - I start from - from here  so you - you have uh no noise then you - you - you are expanding the critical band  so the amount of maching is increasing. And when you e- hit a certain point  which is a critical band  then the amount of masking is the same. Mmm. So that's the famous experiment of Fletcher  a long time ago. Like that's where people started thinking ""wow this is interesting!"" Yeah. So. But  if you - if you - if you modulate the noise  the masking goes up and the moment you start hitting the - another critical band  the masking goes down. So essentially - essentially that's a very clear indication that - that - that cognition can take uh uh into consideration what's happening in the neighboring bands. But if you go too far in a - in a - if you - if the noise is very broad  you are not increasing much more  so - so if you - if you are far away from the signal - Mm-hmm. uh from the signal f- uh the frequency at which the signal is  Yeah. then the m- even the - when the noise is co-modulated it - it's not helping you much. Mm-hmm. So. Hmm. So things like this we are kind of playing with - with - with the hope that perhaps we could eventually u- use this in a - in a real recognizer. Mm-hmm. Like uh partially of course we promised to do this under the - the - the Aurora uh program. But you probably won't have anything before the next time we have to evaluate  right? Yeah. Probably not. Well  maybe  most likely we will not have anything which c- would comply with the rules. Ah. like because uh uh Latency and things. latency currently chops the require uh significant uh latency Mm-hmm. amount of processing  because uh we don't know any better  yet  than to use the neural net classifiers  Yeah. uh and uh - and uh TRAPS. Though the - the work which uh everybody is looking at now aims Mm-hmm. Hmm. at s- trying to find out what to do with these vectors  so that a g- simple Gaussian classifier would be happier with it. Mm-hmm. or to what extent a Gaussian classifier should be unhappy uh that  and how to Gaussian-ize the vectors  and - Hmm. So this is uh what's happening. Then Sunil is uh uh uh asked me f- for one month's vacation and since he did not take any vacation for two years  I had no - I didn't have heart to tell him no. So he's in India. Wow. And uh - Is he getting married or something? Uh well  he may be looking for a girl  for - for I don't - I don't - I don't ask. I know that Naran- when last time Narayanan did that he came back engaged. Right. Well  I mean  I've known other friends who - they - they go to Ind- they go back home to India for a month  they come back married  Yeah. I know. I know  I know  and then of course then what happened with Narayanan was that he start pushing me that he needs to get a P_H_D because they wouldn't give him his wife. you know  huh. And she's very pretty and he loves her and so - so we had to really - So he finally had some incentive to finish  huh? Oh yeah. We had - well I had a incentive because he - he always had this plan except he never told me. Oh. Sort of figured that - That was a uh that he uh he told me the day when we did very well at our NIST evaluations of speaker recognition  the technology  and he was involved there. We were - after presentation we were driving home and he told me. When he knew you were happy  huh? Yeah. So I - I said ""well  yeah  O_K"" so he took another - another three quarter of the year but uh he was out. So I - wouldn't surprise me if he has a plan like that  though - though uh Pratibha still needs to get out first. Hmm. Cuz Pratibha is there a - a year earlier. Hmm. And S- and Satya needs to get out very first because he's - he already has uh four years served  though one year he was getting masters. So. Hmm. So. So have the um - when is the next uh evaluation? June or something? Which? Speaker recognition? No  for uh Aurora? Uh there  we don't know about evaluation  next meeting is in June. And uh uh but like getting - get together. Hmm. Oh  O_K. Are people supposed to rerun their systems  or - ? Nobody said that yet. I assume so. Hmm. Uh yes  uh  but nobody even set up yet the date for uh delivering uh endpointed data. Wow. And this uh - that - that sort of stuff. But I uh  yeah  what I think would be of course extremely useful  if we can come to our next meeting and say ""well you know we did get fifty percent improvement. If - if you are interested we eventually can tell you how""  Mm-hmm. but uh we can get fifty percent improvement. Because people will s- will be saying it's impossible. Hmm. Do you know what the new baseline is? Oh  I guess if you don't have - Twenty-two - t- twenty - twenty-two percent better than the old baseline. Using your uh voice activity detector? u- Yes. Yes. But I assume that it will be similar  I don't - I - I don't see the reason why it shouldn't be. I d- I don't see reason why it should be worse. Similar  yeah. Mm-hmm. Yeah. Cuz if it is worse  then we will raise the objection  we say ""well you know how come?"" Because eh if we just use our voice activity detector  which we don't claim even that it's wonderful  it's just like one of them. Mm-hmm. Yeah. We get this sort of improvement  how come that we don't see it on - on - on - on your endpointed data? Yeah. I guess it could be even better  because the voice activity detector that I choosed is something that cheating  it's using the alignment I think so. Yeah. of the speech recognition system  C- yeah uh and on clean speech data. and only the alignment on the clean channel  and then Oh  O_K. Yeah. Well David told me - David told me yesterday or Harry actually he told Harry from QualComm and Harry uh brought up the suggestion we should still go for fifty percent mapped this alignment to the noisy channel. he says are you aware that your system does only thirty percent uh comparing to - to endpointed baselines? So they must have run already something. Yeah. Hmm. So. And Harry said "" Yeah. But I mean we think that we - we didn't say the last word yet  that we have other - other things which we can try."" Hmm. So. So there's a lot of discussion now about this uh new criterion. Mm-hmm. Because Nokia was objecting  with uh QualComm's - we basically supported that  we said ""yes"". Mm-hmm. Now everybody else is saying ""well you guys might - must be out of your mind."" uh The - Guenter Hirsch who d- doesn't speak for Ericsson anymore because he is not with Ericsson and Ericsson may not - may withdraw from the whole Aurora activity because they have so many troubles now. Wow. Ericsson's laying off twenty percent of people. Wow. Where's uh Guenter going? Well Guenter is already - he got the job uh already was working on it for past two years or three years - Mm-hmm. he got a job uh at some - some Fachschule  the technical college not too far from Aachen. Hmm! So it's like professor - u- university professor Mm-hmm. you know  not quite a university  not quite a sort of - it's not Aachen University  but it's a good school and he - he's happy. Mm-hmm. Hmm! And he - well  he was hoping to work uh with Ericsson like on t- uh like consulting basis  but right now he says - says it doesn't look like that anybody is even thinking about speech recognition. Mm-hmm. Wow! They think about survival. Yeah. Hmm. So. So. But this is being now discussed right now  and it's possible that uh - that - that it may get through  that we will still stick to fifty percent. But that means that nobody will probably get this im- this improvement. Mm-hmm. yet  wi- with the current system. Which event- es- essentially I think that we should be happy with because that - that would mean that at least people may be forced to look into alternative solutions and - Mm-hmm. Mm-hmm. But maybe - I - I mean we are not too far from - from fifty percent  from the new baseline. Uh  but not - Which would mean like sixty percent Yeah. over the current baseline  which is - Yes. Yes. Well. We - we getting - we getting there  right. We are around fifty  fifty-five. So. Yeah. Yeah. Mm-hmm. Is it like sort of - is - How did you come up with this number? If you improve twenty - by twenty percent the c- the f- the all baselines  it's just a quick c- comp- co- Yeah. I don't know exactly if it's - computation? Uh-huh. I think it's about right. Yeah  because it de- it depends on the weightings and - Yeah  yeah. Yeah. But. Mm-hmm. Hmm. How's your documentation or whatever it w- what was it you guys were working on last week? Yeah  finally we - we've not finished with this. We stopped. More or less it's finished. Yeah. Ma- nec- to need a little more time to improve the English  and maybe s- to fill in something - some small detail  something like that  but it's more or less ready. Mm-hmm. Hmm. Yeah. Well  we have a document that explain Necessary to - to include the bi- the bibliography. Mm-hmm. a big part of the experiments  but it's not  yeah  finished yet. Mm-hmm. So have you been running some new experiments? I - I thought I saw some jobs of yours running on some of the machine - Yeah. Right. We've fff done some strange things like removing C_zero or C_one from the - the vector of parameters  and we noticed that C_one is almost not useful at all. Really?! You can remove it from the vector  it doesn't hurt. That has no effect? Um. Eh - Is this in the baseline? or in uh - In the - No  in the proposal. in - uh-huh  uh-huh. So we were just discussing  since you mentioned that  in - it w- driving in the car with Morgan this morning  Mm-hmm. Mm-hmm. we were discussing a good experiment for b- for beginning graduate student who wants to run a lot of - who wants to get a lot of numbers on something which is  like  ""imagine that you will - you will start putting every co- any coefficient  which you are using in your vector  in some general power. In some what? General pow- power. Like sort of you take a s- power of two  or take a square root  Mm-hmm. Mm-hmm. or something. Mm-hmm. So suppose that you are working with a s- C_zer- C_one. So if you put it in a s- square root  that effectively makes your model half as efficient. Because uh your uh Gaussian mixture model  right? computes the mean. Mm-hmm. And - and uh i- i- i- but it's - the mean is an exponent of the whatever  the - the - this You're compressing the range  right? of that - Gaussian function. So you're compressing the range of this coefficient  so it's becoming less efficient. Right? Mm-hmm. So. So. Morgan was @@ and he was - he was saying well this might be the alternative way how to play with a - with a fudge factor  you know  uh in the - you know  just compress the whole vector. Oh. Yeah. And I said ""well in that case why don't we just start compressing individual elements  like when - when - because in old days we were doing - when - when people still were doing template matching and Euclidean distances  we were doing this liftering of parameters  right? Uh-huh. because we observed that uh higher parameters were more important than lower for recognition. And basically the - the C_ze- C_one contributes mainly slope  and it's highly affected by Right. Mm-hmm. uh frequency response of the - of the recording equipment and that sort of thing  so - so we were coming with all these f- various lifters. Mm-hmm. Mm-hmm. uh Bell Labs had he - this uh uh r- raised cosine lifter which still I think is built into H_ - H_T_K for reasons n- unknown to anybody  but - but uh we had exponential lifter  or triangle lifter  basic number of lifters. Hmm. And. But so they may be a way to - to fiddle with the f- with the f- Insertions. Insertions  deletions  or the - the - giving a relative - uh basically modifying relative importance of the various parameters. Mm-hmm. The only of course problem is that there's an infinite number of combinations and if the - if you s- if y- Oh. Uh-huh. You need like a - some kind of a - Yeah  you need a lot of graduate students  and a lot of computing power. You need to have a genetic algorithm  that basically tries random I know. Exactly. Oh. permutations of these things. If you were at Bell Labs or - I d- d- I shouldn't be saying this in - on - on a mike  right? Or I - uh - I_B_M  that's what - maybe that's what somebody would be doing. Yeah. Hmm. Oh  I mean  I mean the places which have a lot of computing power  so because it is really it's a p- Mm-hmm. it's a - it's - it will be reasonable search Yeah. uh but I wonder if there isn't some way of doing this uh search like when we are searching say for best discriminants. You know actually  I don't know that this wouldn't be all that bad. I mean you - you compute the features once  right? Yeah. Yeah. And then Absolutely. these exponents are just applied to that - So. And hev- everything is fixed. Everything is fixed. Each - each - And is this something that you would adjust for training? or only recognition? For both  you would have to do. You would do it on both. So you'd actually - Yeah. You have to do bo- both. Because essentially you are saying ""uh this feature is not important"". Mm-hmm. Or less important  so that's - th- that's a - that's a painful one  yeah. So for each - uh set of exponents that you would try  it would require a training and a recognition? Yeah. But - but wait a minute. You may not need to re- uh uh retrain the m- model. You just may n- may need to c- uh give uh less weight to - to uh a mod- uh a component of the model which represents this particular feature. You don't have to retrain it. Oh. So if you - Instead of altering the feature vectors themselves  You just multiply. you - you modify the - the - the Gaussians in the models. Yeah. Yep. You modify the Gaussian in the model  but in the - in the test data you would have to put it in the power  but in a training what you c- in a training uh - in trained model  Uh-huh. all you would have to do is to multiply a model by appropriate constant. But why - if you're - if you're multi- if you're altering the model  why w- in the test data  why would you have to muck with the uh cepstral coefficients? Because in uh test - in uh test data you ca- don't have a model. You have uh only data. But in a - in a tr- No. But you're running your data through that same model. That is true  but w- I mean  so what you want to do - You want to say if uh obs- you - if you observe something like Stephane observes  that C_one is not important  you can do two things. Mm-hmm. Mm-hmm. If you have a trained - trained recognizer  in the model  you know the - the - the - the component which - I - I mean di- dimension Mm-hmm. All of the - all of the mean and variances that correspond to C_one  you put them to zero. Yeah. wh- To the s- you - you know it. But what I'm proposing now  if it is important but not as important  you multiply it by point one in a model. But - but - but - But what are you multiplying? Cuz those are means  right? I mean you're - You're multiplying the standard deviation? So it's - I think that you multiply the - I would - I would have to look in the - in the math  I mean how - how does the model uh - Yeah. I think you - Yeah  I think you'd have to modify the standard deviation or something  so that you make it Cuz - Yeah. Yeah. Yeah. Effectively  that's - that - that's - I - Exactly. That's what you do. That's what you do  you - you - you modify the standard deviation as it was trained. wider or narrower. Yeah. Effectively you  you know y- in f- in front of the - of the model  you put a constant. S- yeah effectively what you're doing is you - is you are modifying the - the - the deviation. Right? The spread  right. Oop. Sorry. Yeah  the spread. So. It's the same - same mean  right? And - and - and - So by making th- the standard deviation narrower  Yeah. uh your scores get worse for - unless it's exactly right on the mean. Your als- No. By making it narrower  Right? I mean there's - you're - you're allowing for less variance. uh y- your - Mm-hmm. Yes  so you making this particular dimension less important. Because see what you are fitting is the multidimensional Gaussian  right? Mm-hmm. It's a - it has - it has uh thirty-nine dimensions  or thirteen dimensions if you g- ignore deltas and double-deltas. Mm-hmm. Mm-hmm. So in order - if you - in order to make dimension which - which Stephane sees uh less important  uh uh I mean not - not useful  less important  what you do is that this particular component in the model you can multiply by w- you can - you can basically de- weight it in the model. But you can't do it in a - in a test data because you don't have a model for th- I mean uh when the test comes  but what you can do is that you put this particular component in - and - and you compress it. That becomes uh th- gets less variance  subsequently becomes less important. Couldn't you just do that to the test data and not do anything with your training data? That would be very bad  because uh your t- your model was trained uh expecting uh  that wouldn't work. Because your model was trained expecting a certain var- variance on C_one. Uh-huh. And because the model thinks C_one is important. After you train the model  you sort of - y- you could do - you could do still what I was proposing initially  that during the training you - you compress C_one Mm-hmm. that becomes - then it becomes less important in a training. But if you have - if you want to run e- ex- extensive experiment without retraining the model  you don't have to retrain the model. You train it on the original vector. But after  you - wh- when you are doing this parametric study of importance of C_one you will de-weight the C_one component in the model  and you will put in the - you will compress the - this component in a - in the test data. Could you also if you wanted to - s- by the same amount. if you wanted to try an experiment uh by leaving out say  C_one  couldn't you  in your test data  uh modify the - all of the C_one values to be um way outside of the normal range of the Gaussian for C_one that was trained in the model? So that effectively  Mm-hmm. the C_one never really contributes to the score? Do you know what I'm say- No  that would be a severe mismatch  right? what you are proposing? N- no you don't want that. Because that would - then your model would be unlikely. Yeah  someth- Your likelihood would be low  right? Mm-hmm. Because you would be providing severe mismatch. But what if you set if to the mean of the model  then? And it was a cons- you set all C_ones coming in through your test data  you - you change whatever value that was there to the mean that your model had. No that would be very good match  right? Yeah. That you would - Which - Well  yeah  but we have several means. So. I see what you are sa- saying  but Right? Saying. uh  no  no I don't think that it would be the same. I mean  no  the - If you set it to a mean  that would - No  you can't do that. Y- you ca- you ca- Ch- Chuck  you can't do that. Because that would be a really f- fiddling with the data  you can't do that. Oh  that's true  right  yeah  because you - you have - Wait. Which - Yeah. Mm-hmm. Mm-hmm. But what you can do  I'm confident you ca- well  I'm reasonably confident and I putting it on the record  right? I mean y- people will listen to it for - for centuries now  is what you can do  is you train the model uh with the - with the original data. Mm-hmm. Then you decide that you want to see how important C_ - C_one is. So what you will do is that a component in the model for C_one  you will divide it by - by two. And you will compress your test data by square root. Mm-hmm. Then you will still have a perfect m- match. Except that this component of C_one will be half as important in a - in a overall score. Mm-hmm. Mm-hmm. Then you divide it by four and you take a square  f- fourth root. Mm-hmm. Then if you think that some component is more - is more important then th- th- th- it then - then uh uh i- it is  based on training  then you uh multiply this particular component in the model by - You're talking about the standard deviation? by - by - yeah. Yeah  multiply this component uh i- it by number b- larger than one  Yeah. Mm-hmm. and you put your data in power higher than one. Then it becomes more important. In the overall score  I believe. Yeah  but  at the - But don't you have to do something to the mean  also? No. No. No. Yeah. But I think it's - uh the - The variance is on - on the denominator in the - in the Gaussian equation. So. I think it's maybe it's the contrary. If you want to decrease the importance of a c- parameter  Yes. you have to Right. increase it's variance. Multiply. Yes. Exactly. Yeah. So you - so you may want to do it other way around  yeah. Hmm. That's right. O_K. Mm-hmm. Right. But if your - If your um original data for C_one had a mean of two. Uh-huh. And now you're - you're - you're changing that by squaring it. Now your mean of your C_one original data has - is four. But your model still has a mean of two. So even though you've expended the range  your mean doesn't match anymore. Mm-hmm. Let's see. Do you see what I mean? I think - What I see - What could be done is you don't change your features  which are computed once for all  Uh-huh. but you just tune the model. So. You have your features. You train your - your model on these features. Mm-hmm. And then if you want to decrease the importance of C_one you just take the variance of the C_one component in the - in the model Yeah. and increase it if you want to decrease the importance of C_one or decrease it - Yeah. Right. Yeah. You would have to modify the mean in the model. I - you - I agree with you. Yeah. Yeah  but I mean  but it's - it's i- it's do-able  right? I mean  it's predictable. Yeah  so y- Well. It's predictable  yeah. Uh. Yeah. Yeah. Yeah  it's predictable. Mmm. Yeah. But as a simple thing  you could just - just muck with the variance. Just adjust the model  yeah. to get uh this - uh this - the effect I think that you're talking about  right? Mm-hmm. It might be. Could increase the variance to decrease the importance. Mm-hmm. Mm-hmm. Yeah  because if you had a huge variance  Yeah  it becomes more flat and - Doesn't matter - you're dividing by a large number  you get a very small Right. Yeah. contribution. Yeah. Yeah. Hmm. Yeah  the sharper the variance  the more - more important to get that one right. Mm-hmm. Yeah  you know actually  this reminds me of something that happened uh when I was at B_B_N. We were playing with putting um pitch Mm-hmm. into the Mandarin recognizer. And this particular pitch algorithm um when it didn't think there was any voicing  was spitting out zeros. So we were getting - uh when we did clustering  we were getting groups uh of features p- Pretty new outliers  interesting outliers  right? yeah  with - with a mean of zero and basically zero variance. Variance. So  when ener- when anytime any one of those vectors came in that had a zero in it  we got a great score. I mean it was just  you know  Mm-hmm. incredibly high score  and so that was throwing everything off. So if you have very small variance you get really good scores when you get something that matches. So. Yeah. Mm-hmm. So that's a way  yeah  yeah - That's a way to increase the - yeah  n- That's interesting. So in fact  that would be - That doesn't require any retraining. Yeah. No. No  that's right. No. So that means So it's just Yeah. it's just recognitions. tuning the models and testing  actually. Yeah. Yeah. You - you have a step where you It would be quick. you modify the models  make a d- copy of your models with whatever variance modifications you make  and rerun recognition. And then do a whole bunch of those. Mm-hmm. Yeah. Yeah. Yeah. Yeah. Yeah. Mm-hmm. That could be set up fairly easily I think  and you have a whole bunch of you know - Chuck is getting himself in trouble. That's an interesting idea  actually. For testing the - Yeah. Huh! Didn't you say you got these uh H_T_K's set up on the new Linux boxes? Yeah. That's right. In fact  and - and they're just t- right now they're installing uh - increasing the memory on that uh - Hey! And Chuck is sort of really fishing for how to keep his computer busy  right? the Linux box. Right. Yeah. Absinthe. Absinthe. We've got five processors on that. Well  you know  that's - that's - yeah  that's a good thing because then y- you just write the ""do""-loops and then you pretend that you are working while you are sort of - you c- you can go fishing. Oh yeah. That's right. And two gigs of memory. Yeah. Yeah. Exactly. Pretend  yeah. Yeah. See how many cycles we used? Go fishing. Yeah. Then you are sort of in this mode like all of those ARPA people are  right? Yeah. Uh  since it is on the record  I can't say uh which company it was  but it was reported to me that uh somebody visited a company and during a - d- during a discussion  there was this guy who was always hitting the carriage returns uh on a computer. Uh-huh. So after two hours uh the visitor said ""wh- why are you hitting this carriage return?"" And he said ""well you know  we are being paid by a computer ty- I mean we are - we have a government contract. And they pay us by - by amount of computer time we use."" It was in old days when there were uh - of P_D_P-eights and that sort of thing. Oh  my gosh! So he had to make it look like - Because so they had a - they literally had to c- monitor at the time - at the time on a computer how much Yeah. How - time is being spent Idle time. Yeah. I - i- i- or on - on this particular project. Yeah. Nobody was looking even at what was coming out. Have you ever seen those little um - It's - it's this thing that's the shape of a bird and it has a red ball and its beak dips into the water? Yeah  I know  right. So if you could hook that up so it hit the keyboard - Yeah. Yeah. Yeah. Yeah. That's an interesting experiment. It would be similar - similar to - I knew some people who were uh that was in old Communist uh Czechoslovakia  right? so we were watching for American airplanes  coming Mm-hmm. to spy on - on uh - on us at the time  Mm-hmm. so there were three guys uh uh stationed in the middle of the woods on one l- lonely uh watching tower  pretty much spending a year and a half there because there was this service right? Ugh! And so they - very quickly they made friends with local girls and local people in the village and - Yeah. and so but they - there was one plane flying over s- always uh uh above  and so that was the only work which they had. They - like four in the afternoon they had to report there was a plane from Prague to Brno Basically f- flying there  Yeah. so they f- very q- f- first thing was that they would always run back and - and at four o'clock and - and quickly make a call  ""this plane is uh uh passing"" then a second thing was that they - they took the line from this u- u- post to uh uh a local pub. And they were calling from the pub. And they - but third thing which they made  and when they screwed up  they - finally they had to p- the - the p- the pub owner to make these phone calls because they didn't even bother to be there anymore. And one day there was - there was no plane. At least they were sort of smart enough that they looked if the plane is flying there  right? Yeah. And the pub owner says ""oh my - four o'clock  O_K  quickly p- pick up the phone  call that there's a plane flying."" There was no plane for some reason  it was downed  or - And there wasn't? and - so they got in trouble. But. But uh. Huh! Well that's - that's a really i- So. So. Yeah. Yeah. Yeah. That wouldn't be too difficult to try. Maybe I could set that up. And we'll just - Well  at least go test the s- test the uh assumption about C_- C_one I mean to begin with. Mm-hmm. But then of course one can then think about some predictable result to change all of them. It's just like we used to do these uh - these uh - um the - the uh distance measures. It might be that uh - Yeah. Yeah  so the first set of uh variance weighting vectors would be just you know one - modifying one and leaving the others the same. Yeah. Yeah. Yeah. Yeah. Yeah. Maybe. And - and do that for each one. That would be one set of experiment - Because you see  I mean  what is happening here in a - in a - in a - in such a model is that it's - tells you yeah what has a low variance uh is uh - is uh - is more reliable  right? How do we - Yeah. Yeah. Yeah. Yeah. Yeah. Wh- yeah  when the data matches that  then you get really - Yeah. Right. How do we know  especially when it comes to noise? But there could just naturally be low variance. Yeah? Because I - Like  I've noticed in the higher cepstral coefficients  the numbers seem to get smaller  right? So d- They - t- Yeah. They have smaller means  also. I mean  just naturally. Yeah  th- that's - Uh. Yeah. Exactly. And so it seems like they're already sort of compressed. Uh-huh. The range of values. Yeah that's why uh people used these lifters were inverse variance weighting lifters basically that makes uh uh Mm-hmm. Euclidean distance more like uh Mahalanobis distance Mm-hmm. with a diagonal covariance when you knew what all the variances were over the old data. Hmm. What they would do is that they would weight each coefficient by inverse of the variance. Turns out that uh the variance decreases at least at fast  I believe  as the index of the cepstral coefficients. I think you can show that uh uh analytically. Mm-hmm. Hmm. So typically what happens is that you - you need to weight the - uh weight the higher coefficients more than uh the lower coefficients. Mm-hmm. Hmm. Mmm. So. Any - When - Yeah. When we talked about Aurora still I wanted to m- make a plea - uh encourage for uh more communication between - between uh uh different uh parts of the distributed uh uh center. Uh even when there is absolutely nothing to - to s- to say but the weather is good in Ore- in - in Berkeley. I'm sure that it's being appreciated in Oregon and maybe it will generate similar responses down here  like  uh - We can set up a webcam maybe. Yeah. Yeah. What - you know  nowadays  yeah. It's actually do-able  almost. Is the um - if we mail to ""Aurora- inhouse ""  does that go up to you guys also? I don't think so. No. No. O_K. So i- What is it - So we should do that. Yeah. We should definitely set up - Yeah we sh- Do we have a mailing list that includes uh the O_G_I people? Yeah. Uh no. We don't have. Oh! Uh-huh. Maybe we should set that up. That would make it much easier. Yeah. Yeah. Yeah  that would make it easier. So maybe just call it ""Aurora"" or something that would - Yeah. Yeah. And then we also can send the - the dis- to the same address right  and it goes to everybody Mm-hmm. Mm-hmm. Yeah. O_K. Maybe we can set that up. Because what's happening naturally in research  I know  is that people essentially start working on something and they don't want to be much bothered  right? but what the - the - then the danger is in a group like this  is that two people are working on the same thing Mm-hmm. and i- c- of course both of them come with the s- very good solution  but it could have been done somehow in half of the effort or something. Oh  there's another thing which I wanted to uh uh report. Lucash  I think  uh wrote the software for this Aurora-two system. reasonably uh good one  because he's doing it for Intel  but I trust that we have uh rights to uh use it uh or distribute it and everything. Cuz Intel's intentions originally was to distribute it free of charge anyways. Hmm! u- s- And so - so uh we - we will make sure that at least you can see the software and if - if - if - if it is of any use. Just uh - Mm-hmm. It might be a reasonable point for p- perhaps uh start converging. Mm-hmm. Because Morgan's point is that - He is an experienced guy. He says ""well you know it's very difficult to collaborate if you are working with supposedly the same thing  in quotes  except which is not s- is not the same. Mm-hmm. Which - which uh uh one is using that set of hurdles  another one set - is using another set of hurdles. So. And - And then it's difficult to c- compare. What about Harry? Uh. We received a mail last week and you are starting He got the - he got the software. Yeah. They sent the release. Yeah. Yeah. Yeah. to - to do some experiments. And use this Intel version. Yeah. Hmm. Yeah because Intel paid us uh should I say on a microphone? uh some amount of money  not much. Not much I can say on a microphone. Much less then we should have gotten for this amount of work. And they wanted uh to - to have software so that they can also play with it  which means that it has to be in a certain environment - they use actu- actually some Intel Hmm. libraries  but in the process  Lucash just rewrote the whole thing because he figured rather than trying to f- make sense uh of uh - including ICSI software Hmm. uh not for training on the nets but I think he rewrote the - the - the - or so- maybe somehow reused over the parts of the thing so that - so that - the whole thing  including M_L_P  trained M_L_P is Oh. Mm-hmm. one piece of uh software. Wow! Is it useful? Yeah? Ye- Yeah. I mean  I remember when we were trying to put together all the ICSI software for the submission. Or - That's what he was saying  right. He said that it was like - it was like just so many libraries and nobody knew what was used when  and - and so that's where he started and that's where he realized that it needs to be - needs to be uh uh at least cleaned up  and so I think it - this is available. Yeah. Mm-hmm. Hmm. Yeah. So - Well  the - the only thing I would check is if he - does he use Intel math libraries  because if it's the case  uh e- ev- n- not maybe - Maybe not in a first - maybe not in a first ap- approximation because I think he started first just with a plain C_ - C_ or C_-plus-plus or something it's maybe not so easy to use it on another architecture. Ah yeah. Mm-hmm. before - I - I can check on that. Yeah. Yeah. O_K. Hmm. And uh in - otherwise the Intel libraries  I think they are available free of f- freely. But they may be running only on - on uh - on uh Windows. Yeah. Or on - on the - On Intel architecture maybe. I'm - Yeah  on Intel architecture  may not run in SUN. Yeah. Yeah. Yeah. That is p- that is - that is possible. Hmm. That's why Intel of course is distributing it  right? Well. Or - Yeah. Well there are - at least there are optimized version for their architecture. That's - Yeah. I don't know. I never checked carefully these sorts of - I know there was some issues that initially of course we d- do all the development on Linux but we use - we don't have - we have only three uh uh uh uh s- SUNs and we have them only because they have a SPERT board in. Otherwise - otherwise we t- almost exclusively are working with uh P_C's now  with Intel. In that way Intel succeeded with us  because they gave us too many good machines Yeah. for very little money or nothing. So. Wow! So. So we run everything on Intel. Hmm. And - Does anybody have anything else? to - Shall we read some digits? Yeah. Yes. I have to take my glasses - So. Hynek  I don't know if you've ever done this. The way that it works is each person goes around in turn  No. Mm-hmm. and uh you say the transcript number and then you read the digits  the - the strings of numbers as individual digits. So you don't say ""eight hundred and fifty""  you say ""eight five oh""  and so forth. O_K. O_K. Um. So can - maybe - can I t- maybe start then? Sure. So this is scrap- transcript L_ one O_ one. eight five O_ seven two O_ five three eight one one eight five two eight seven five nine nine six one four five nine five zero zero eight eight two seven five eight seven two zero nine three four eight three six three nine six five four five eight two O_ eight four four one eight one O_ O_ O_ two five eight six one eight six seven five one four three seven four four O_ two eight nine zero Transcript L_ one zero two. two nine five six four five two six seven six eight four three eight four nine one three four eight nine zero two five six four five two two three two nine two nine nine zero one three one seven eight zero nine two nine three six six zero zero five six seven four two one three three seven six six eight three three three two one zero three two six six nine one three seven nine nine one Transcript L_ dash one O_ three. seven six three seven seven seven three two two one nine two three four six zero six three one four nine four O_ eight five three six eight six two zero zero nine one eight seven six eight one eight O_ nine O_ O_ nine four three four five five three two seven one one one five O_ three one nine zero eight three two three zero nine five five four nine two two nine eight six seven eight six Transcript L_ dash one zero four. six nine five zero zero seven four nine zero nine one five zero zero five one six nine nine two nine nine one five one two zero seven eight five three eight nine zero three two one five four one two i- zero four one one three eight five five three seven seven five three nine nine eight one seven one zero five zero eight two seven s- two two six nine one eight six nine three two five two Transcript L_ dash one zero zero. one four nine five three six four nine six four three three seven three one three eight eight one one four eight one two zero zero three six one one zero seven eight eight six two seven six five six two seven one one four nine nine one six two three six eight two seven six nine one four two three six four one five five eight two eight three zero nine two three one seven five zero ",The Berkely Meeting Recorder group discussed efforts by speaker mn005 to measure energy levels in cases of speaker overlap in which the time window analyzed was 200 milliseconds or greater. Preliminary results were presented showing that log domain analyses did not reveal a significant difference in mean energy levels for windows of overlapping versus non-overlapping speech. In contrast  raw energy analyses were successful in showing the two groups to be distinct. Participants discussed alternate strategies for examining energy and the importance of categorizing types of speaker overlap. Participants also reviewed the latest iteration of speaker forms  and discussed recent changes to the Transcriber tool. Continuing efforts by speaker mn005 to measure energy levels in cases of speaker overlap will not include additional log energy analyses  but rather an analysis of raw energy for normalized speaker dataacross windows of varying duration  followed by an examination of pitch- and harmonicity-related features. Speakers fe008 and fe016 discussed plans to categorize and produce a taxonomy of types of speaker overlap. Time marks for transcribed or force-aligned data are needed to analyze types of speaker overlap. With respect to speaker forms  the group discussed problems associated with categorizing regional dialects of American English. The new version of Transcriber does not feature the waveform  as re-drawing of this window is too slow using Snack. The latest iteration of speaker forms was presented. New mutitrans patches were added to the Transcriber tool to enable it to run with fewer delays  still allowing users to open multiple panes and listen to different channels of the mixed signal. 
"Eh  we should be going. So ne- next week we'll have  uh  both Birger and  uh  Mike - Michael - Michael Kleinschmidt and Birger Kollmeier will join us. Uh-huh. Um  and you're - you're probably gonna go up in a couple - three weeks or so? When d- when are you thinking of going up to  uh  O_G_I? Yeah  like  uh  not next week but maybe the week after. O_K. Good. So at least we'll have one meeting with yo- with you still around  and - Uh-huh. and - That's good. Um  Yeah. Well  maybe we can start with this. Mmm. All today  huh? Yeah. Oh. Um. Yeah. So there was this conference call this morning  um  and the only topic on the agenda was just to discuss a- and to come at - uh  to get a decision about this latency problem. No  this - I'm sorry  this is a conference call between different Aurora people or just - ? Uh  yeah. It's the conference call between the Aurora  uh  group. It's the main conference call. O_K. Uh  yeah. There were like two hours of discussions  and then suddenly  uh  people were tired  I guess  and they decided on a number  two hundred and twenty  um  included e- including everything. Uh  it means that it's like eighty milliseconds less than before. Um. And what are we sitting at currently? So  currently d- uh  we have system that has two hundred and thirty. So  Yeah . that's fine. Two thirty. Yeah. So that's the system that's described on the second point of this document. So it's - we have to reduce it by ten milliseconds somehow. Yeah. But that's - Yeah. That's not a problem  I - I guess. O_K. Um. W- It's - it's p- d- primary - primarily determined by the V_A_D at this point  right? Yeah. Yeah. At this point  yeah. S- so we can make the V_A_D a little shorter. That's - Yeah  uh-huh. Yeah. We probably should do that pretty soon so that we don't get used to it being a certain way. Uh-huh. Yeah. Um. Was Hari on the - on the phone? Yeah  sure. O_K. Well  it was mainly a discussion between Hari and Hmm. David  who was like - Yeah. Uh  O_K. mmm - Uh  yeah. So  the second thing is the system that we have currently. Oh  yes. We have  like  a system that gives sixty-two percent improvement  but if you want to stick to the - this latency - Well  it has a latency of two thirty  but if you want also to stick to the number of features that - limit it to sixty  then we go a little bit down but it's still sixty-one percent. Uh  and if we drop the tandem network  then we have fifty-seven percent. Uh  but th- the two th- two thirty includes the tandem network? Yeah. O_K. And i- is the tandem network  uh  small enough that it will fit on the terminal size in terms of - ? Uh  no  I don't think so. No. No. O_K. It's still - in terms of computation  if we use  like  their way of computing the - the maps - the - the MIPs  Mm-hmm. Mm-hmm. I think it fits  but it's  uh  m- mainly a problem of memory. Right. Um  and I don't know how much this can be discussed or not  because it's - it could be in ROM  so it's maybe not that expensive. But - Ho- how much memory d- ? H- how many - ? I d- I d- uh  I - I don't kn- remember exactly  but - Uh. Yeah  I c- I - I have to check that. Yeah. I'd like to see that  cuz maybe I could think a little bit about it  cuz we- maybe we could make it a little smaller or - I mean  it'd be - it'd be neat if we could fit it all. Uh-huh. Uh  I'd like to see how far off Mm-hmm. we are. But I guess it's still within their rules to have - have it on the  uh  t- uh  server side. Right? Yeah. Yeah. O_K. Mmm. And this is still - ? Uh  well  y- you're saying here. I c- I should just let you go on. Yeah  there were small tricks to make this tandem network work. Uh  mmm  and one of the trick was to  um  use some kind of hierarchical structure where the silence probability is not computed by the final tandem network but by the V_A_D network. Um  so apparently it looks better when  uh  we use the silence probability from the V_A_D network and we re-scale the other probabilities by one minus Huh. the silence probability. Um. So it's some kind of hierarchical thing  uh  that Sunil also tried  um  on SPINE and apparently it helps a little bit also. Mmm. And. Yeah  the reason w- why - why we did that with the silence probability was that  um - Could - ? Uh  uh  I'm - I'm really sorry. Can you repeat what you were saying about the silence probability? I only - Mm-hmm. Yeah. My mind was some - So there is the tandem network that e- e- e- estimates the phone probabilities Yeah. Yeah. and the silence probabilities also. Right. And things get better when  instead of using the silence probability computed by the tandem network  we use the silence probability  uh  given by the V_A_D network  Oh. um  The V_A_D network is - ? Which is smaller  but maybe  um - So we have a network for the V_A_D which has one hundred hidden units  and the tandem network has five hundred. Um. So it's smaller but th- the silence probability from this network seems  uh  better. O_K. Mmm. Uh. Well  it looks strange  but - Yeah. But - O_K. but it - Maybe it's - has something to do to the fact that we don't have infinite training data and - We don't? Well! And so - Well  things are not optimal and - Yeah. Mmm - Are you - you were going to say why - what made you - wh- what led you to do that. Yeah. Uh  there was a p- problem that we observed  um  that there was - there were  like  many insertions in the - in the system. Mm-hmm. Mmm. Hmm. Actually plugging in the tandem network was increasing  I - I - I think  the number of insertions. Mm-hmm. And  um - So it looked strange and then just using the - the other silence probability helps. Mmm. Um - Yeah. The next thing we will do is train this tandem on more data. So  you know  in a way what it might - i- it's - it's a little bit like Um - combining knowledge sources. Right? Because the fact that you have these two nets Mm-hmm. that are different sizes means they behave a little differently  they find different things. And  um  if you have  um - f- the distribution that you have from  Mm-hmm. uh  f- speech sounds is w- sort of one source of knowledge. And this is - and rather than just taking one minus that to get the other  which is essentially what's happening  you have this other source of knowledge that you're putting in there. So you make use of both of them in - in what you're ending up with. Maybe it's better. Yeah. Anyway  you can probably justify anything if what's use- Yeah. Yeah. And - and the features are different also. I mean  the V_A_D doesn't use the same features there are . Mm-hmm. Hmm. Oh! Um - That might be the key  Mm-hmm. actually. Cuz you were really thinking about speech versus nonspeech Mm-hmm. for that. That's a good point. Mmm. Uh. Well  there are other things that we should do but  um  it requires time and - We have ideas  like - so  these things are like hav- having a better V_A_D. Uh  we have some ideas about that. It would - probably implies working a little bit on Mm-hmm. features that are more suited to a voice activity detection. Working on the second stream. Of course we have ideas on this also  but - w- we need to try different things and - Uh  but their noise estimation  um - uh - I mean  back on the second stream  I mean  that's something we've talked about for a while. I mean  I think that's certainly a high hope. Yeah. Mmm. Um  so we have this - this default idea about just using some sort of purely spectral thing? Uh  yeah. But  for a second stream? um  we - we did a first try with this  and it - it clearly hurts. But  uh  how was the stream combined? Uh. It was c- it was just combined  um  by the acoustic model. So there was  no neural network for the moment. Mm-hmm. Right. So  I mean  if you just had a second stream that was just spectral and had another neural net and combined there  Yeah. Mm-hmm. that - that  uh  Mm-hmm. might be good. Mmm. Yeah. Um - Yeah  and the other thing  that noise estimation and th- um  maybe try to train - uh  the training data for the t- tandem network  right now  is like - i- is using the noises from the Aurora task and I think that people might  um  try to argue about that because then in some cases we have the same noises in - for training the network than the noises that are used for testing  and - Right. So we have t- n- uh  to try to get rid of these - Yeah. Maybe you just put in some other noise  something that's different. this problem. Mm-hmm. Yeah. I mean  it - it's probably helpful to have - have a little noise there. Uh-huh. But it may be something else th- at least you could say it was. Yeah. And then - if it doesn't hurt too much  though. Uh-huh. Yeah. That's a good idea. Um. Yeah. The last thing is that I think we are getting close to human performance. Well  that's something I would like to investigate further  but  um  I did  like  um - I did  uh  listen to the m- most noisy utterances of the SpeechDat-Car Italian and tried to transcribe them. And  um - So this is a particular human. This is - this i- this is Stephane. Yeah. Yeah. So that's - that's - St- Stephane. that's the - the flaw of the experiment. This is just - i- j- Yeah. Getting close. it's just one subject  but - but still  uh  what happens is - is that  uh  the digit error rate on this is around one percent  Yeah. while our system is currently at seven percent. Um  but what happens also is that if I listen to the  um - a re-synthesized version of the speech and I re-synthesized this using a white noise that's filtered by a L_P_C  Yeah. uh  filter - Um  well  you can argue  that  uh - that this is not speech  so the ear is not Yeah. trained to recognize this. But s- actually it sound like whispering  so we are - Well  I mean  it's - eh - There's two problems there. I mean - I mean  so - so the first is Uh-huh. that by doing L_P_C-twelve with synthesized speech w- like you're saying  uh  it's - i- i- you're - you're adding other degradation. Right? So it's not just the noise but you're adding in fact some degradation because it's only an approximation. Um  and the second thing is - which is m- maybe more interesting - is that  um  if you do it with whispered speech  you get this number. What if you had done analysis re-synthesis and taken the pitch as well? Alright? So now you put the pitch in. Uh-huh. What would the percentage be then? Um - See  that's the question. So  you see  if it's - if it's - if it's  uh - Let's say it's back down to one percent again. Uh-huh. That would say at least for people  having the pitch is really  really important  which would be interesting in itself. Uh  yeah. But - Um  if i- on the other hand  if it stayed up near five percent  then I'd say ""boy  L_P_C n- twelve is pretty crummy "". You know? Uh-huh. So I- I- I'm not sure - I'm not sure how we can conclude from this anything about - that our system is close to Ye- Yeah. Well  the point is that eh- l- ey- - the point is that  um  the human performance. what I - what I listened to when I re-synthesized the L_P- the L_P_C-twelve spectrum is in a way what the system  uh  is hearing  cuz @@ - all the - all the  um  excitation - all the - well  the excitation is - is not taken into account. That's what we do with our system. And Well  you're not doing the L_P_C - I mean  so - so what if you did a - in this case - Well  it's not L_P_C  sure  but - What if you did L_P_C-twenty? L_P_C - ? Twenty. Right? I mean  th- the thing is L_P_C is not a - a really great Mm-hmm. representation of speech. Mm-hmm. So  all I'm saying is that you have in addition to the w- the  uh  removal of pitch  Mm-hmm. you also are doing  uh  a particular parameterization  which  um  uh - Mmm. Uh  so  let's see  how would you do - ? So  fo- @@ But that's - that's what we do with our systems. And - No. Actually  we d- we - we don't  because we do - we do  uh  uh  mel filter bank  for instance. Right? Yeah  but is it that - is it that different  I mean? Um  I don't know what mel  uh  based synthesis would sound like  but certainly the spectra are quite different. I- Mm-hmm. Mm-hmm. Couldn't you t- couldn't you  um  test the human performance on just the original audio? This is the one percent number. Yeah  it's one percent. He's trying to remove the pitch information Mm-hmm. Oh  oh. O_K  I see. Mm-hmm. and make it closer to what - to what we're seeing as the feature vectors. O_K. So  y- @@ uh  your performance was one percent  Uh-huh. and then when you re-synthesize with L_P_C-twelve it went to five. Yeah. O_K. I mean - We were - we were j- It - it - it's a little bit still apples and oranges because we are choosing these features in order to be the best for recognition. Uh-huh. And  um  i- if you listen to them they still might not be very - Even if you made something closer to what we're gonna - i- it might not sound very good. Yeah. Uh  and i- the degradation from that might - might actually make it even harder  uh  to understand than the L_P_C-twelve. So all I'm saying is that the L_P_C-twelve puts in - synthesis puts in some degradation Uh-huh. that's not what we're used to hearing  and is  um - It's not - it's not just a question of how much information is there  as if you will always take maximum advantage of any information that's presented to you. In fact  you Mm-hmm. hear some things better than others. And so it - it isn't - But  But - I agree that it says that  uh  the kind of information that we're feeding it is probably  um  um  a little bit  um  minimal. There's definitely some things that we've thrown away. And that's why I was saying it might be interesting if you - an interesting test of this would be if you - if you actually put the pitch back in. So  you just extract it from the actual speech and put it back in  and see does that - is that - does that make the difference? Uh-huh. If that - if that takes it down to one percent again  then you'd say ""O_K  it's - it's in fact having  um  not just the spectral envelope but also the - also the - the pitch that  uh  @@ has the information that people can use  anyway."" Mmm. But from this it's pretty safe to say that the system is with- either two to seven percent away from the performance of a human. Right? So it's somewhere in that range. Well  or it's - it's - Yeah  so - Two - two to six percent. It's - it's one point four times  uh  to  uh  seven times the error  To f- seven times  yeah. for Stephane. Um. So  uh - @@ uh  but i- I don't know. I- do- don't wanna take you away from other things. But that's - But - but - that's what - that's the first thing that I would be curious about  is  you know  i- i- when you we- But the signal itself is like a mix of - um  of a - a periodic sound and  @@ uh  unvoiced sound  and the noise which is mostly  Mm-hmm. uh  noise. I mean not periodic. So  what - what do you mean exactly by putting back the pitch in? Because - In the L_P_C synthesis? @@ Yeah. You did L_P_C re-synthesis - L_ P_C re-synthesis. So  I think - I- Uh-huh. uh - and you did it with a noise source  Mm-hmm. rather than with - with a s- periodic source. Right? So if you actually did real re-synthesis like you do in an L_P_C synthesizer  where it's unvoiced you use noise  where it's voiced you use  uh  periodic pulses. Um. Yeah  but it's neither purely voiced or purely unvoiced. Right? Esp- especially because there is noise. Well  it might be hard to do it but it- but - but the thing is that if you - So - Oh. um  if you detect that there's periodic - s- strong periodic components  then you can use a voiced - Uh-huh. Yeah. voice thing. Yeah. I mean  it's probably not worth your time. It's - it's a side thing and - and - and there's a lot to do. But I'm - I'm just saying  at least as a thought experiment  Uh-huh  yeah. Mm-hmm. that's what I would wanna test. Uh  I wan- would wanna drive it with a - a - a two-source system rather than a - than a one-source system. Mm-hmm. Mm-hmm. And then that would tell you whether in fact it's - Cuz we've talked about  like  this harmonic tunneling or other things that people have done based on pitch  maybe that's really a key element. Maybe - maybe  uh  uh  without that  it's - it's not possible to do a whole lot better than we're doing. That - that could be. Yeah. That's what I was thinking by doing this es- experiment  like - Yeah. Mmm. Evi- But  I mean  other than that  I don't think it's - I mean  other than the pitch de- information  it's hard to imagine that there's a whole lot more in the signal that - that  uh - that we're throwing away that's important. Yeah  but - Yeah. Mm-hmm. Yeah  right. Right? I mean  we're using a fair number of filters in the filter bank and - Mm-hmm. uh - Uh  yeah. Hmm. Yeah. Um. Yeah  that's it. Yeah. That look- Yeah. That's - that's - I mean  one - one percent is sort of what I would - I would figure. If somebody was paying really close attention  you might get - I would actually think that if  you looked at people on various times of the day and different amounts of attention  you might actually get up to three or four percent error on digits. Uh  Mm-hmm. uh - Um. So it's - you know  we're not - @@ we're not incredibly far off. On the other hand  with any of these numbers except maybe the one percent  it's st- it's not actually usable in a commercial system with Uh-huh. a full telephone number or something. Yeah. At these noise levels. Yeah. Mm-hmm. Yeah. Right. Well  yeah. These numbers  I mean. Mmm. Good. Um  while we're still on Aurora stuff maybe you can talk a little about the status with the  uh  Wall Street Journal things for it. So I've  um  downloaded  uh  a couple of things from Mississippi State. Um  one is their software - their  uh  L_V_C_S_R system. Downloaded the latest version of that. Got it compiled and everything. Um  downloaded the scripts. They wrote some scripts that sort of make it easy to run the system on the Wall Street Journal  uh  data. Um  so I haven't run the scripts yet. Uh  I'm waiting - there was one problem with part of it and I wrote a note to Joe asking him about it. So I'm waiting to hear from him. But  um  I did print something out just to give you an idea about where the system is. Uh  they - on their web site they  uh  did this little table of where their system performs relative to other systems that have done this - this task. And  um  the Mississippi State system using a bigram grammar  uh  is at about eight point two percent. Other comparable systems from  uh - were getting from  uh  like six point nine  six point eight percent. So they're - This is on clean This is on clean - test set? on clean stuff. Yeah. They - they've started a table where they're showing their results on various different noise conditions but they - they don't have a whole lot of it filled in and - @@ and I didn't notice until after I'd printed it out that  um  they don't say here what these different testing conditions are. You actually have to click on it on the web site to see them. So I - I don't know what those numbers really mean . What kind of numbers are they getting on these - on the test conditions? Well  see  I was a little confused because on this table  I'm - the- they're showing word error rate. But on this one  I - I don't know if these are word error rates because they're really big. So  under condition one here it's ten percent. Then under three it goes to sixty- four point six percent. Yeah  that's probably Aurora. I mean - Yeah. So m- I guess maybe they're error rates but they're  uh - they're really high. I - I - I don't find that surpri- I mean  we - So - W- what's - what's some of the lower error rates on - on - on - uh  some of the higher error rates on  uh  some of these w- uh  uh  highly mismatched difficult conditions? What's a - ? Uh. Yeah  it's around fifteen to twenty percent. Correct? And the baseline  eh - Accuracy? Uh  error rate. Yeah. Twenty percent error rate  Yeah. So twenty percent error rate on digits. and - So if you're doing - so if you're doing  and - Oh  oh  on digits. Yeah. On digits. And this is so - so - still the baseline. Right? O_K. you know  @@ sixty-thousand - Yeah  and if you're saying sixty-thousand word recognition  getting sixty percent error on some of these noise condition- not at all surprising. Yeah. Yeah. The baseline is sixty percent also on digits  Oh  is it? on the m- more mismatched conditions. O_K. So. Yeah. So  yeah  that's probably what it is then. Yeah. So they have a lot of different conditions that they're gonna be filling out . It's a bad sign when you - looking at the numbers  you can't tell whether it's accuracy or error rate. Yeah. Yeah. It's - it's gonna be hard. Um  they're - I- I'm still waiting for them to release the  um  multi-C_P_U version of their scripts  cuz right now their script only handles processing on a single C_P_U  which will take a really long time to run. So. This is for the training? But their s- Uh - I beli- Yes  for the training also. O_K. And  um  they're supposed to be coming out with it any time  the multi-C_P_U one. O_K. So  as soon as they get that  then I'll - I'll grab those too and so w- Yeah. Cuz we have to get started  cuz it's - cuz  uh  Yeah. Yeah. I'll go ahead and try to run it though with just the single C_P_U one  and - I - they - they  if the - um  released like a smaller data set that you can use that only takes like sixteen hours to train and stuff. So I can - I can run it on that just to make sure that the - Oh! Good. Yeah. the thing works and everything. Hmm. Cuz we'll - I guess the actual evaluation will be in six weeks or something. So. Is that about right you think? Uh  we don't know yet  I - I think. Really  we don't know? Uh-huh. Um. Hmm. It wasn't on the conference call this morning? No. Hmm. Did they say anything on the conference call about  um  how the Wall Street Journal part of the test was going to be run? Because I - No. Mmm. I thought I remembered hearing that some sites were saying that they didn't have the compute to be able to run the Wall Street Journal stuff at their place  so there was some talk about having Mississippi State run the systems for them. And I - Did - did that come up at all? Uh  no. Well  this - first  this was not the point at all of this - Oh  O_K. the meeting today and  Some- uh  frankly  I don't know because I d- didn't read also the most recent mails about the large-vocabulary task. But  uh  did you - do you still  uh  get the mails? You're not on the mailing list or what? Hmm-mm. The only  um  mail I get is from Mississippi State - Uh-huh. so - Oh  yeah. So we should have a look at this. about their system. I - I don't get any mail about - I have to say  there's uh something funny-sounding about saying that one of these big companies doesn't have enough cup- compute power do that  so they're having to have it done by Mississippi State. Yeah. It just - just sounds funny. But  Yeah. It does. anyway. Yeah. I'm - I'm wondering about that because there's this whole issue about  you know  simple tuning parameters  like word insertion penalties. Mm-hmm. And whether or not those are going to be tuned or not  and - So. Mm-hmm. I mean  it makes a big difference. If you change your front-end  you know  the scale is completely - can be completely different  so. It seems reasonable that that at least should be tweaked to match the front-end. But - You didn't get any answer from Joe? I did  but Joe said  Uh-huh. you know  ""what you're saying makes sense and I don't know"". Uh-huh. So he doesn't know what the answer is. I mean  that's th- We had this back and forth a little bit about  you know  are sites gonna - are you gonna run this data for different sites? And  well  if - if Mississippi State runs it  then maybe they'll do a little optimization on that parameter  and  uh - But then he wasn't asked to run it for anybody. So i- it's - it's just not clear yet what's gonna happen. Mm-hmm. Uh  he's been putting this stuff out on their web site and - for people to grab but I haven't heard too much about what's happening. So it could be - I mean  Chuck and I had actually talked about this a couple times  and - and - over some lunches  I think  that  um  one thing that we might wanna do - The- there's this question about  you know  what do you wanna scale? Suppose y- you can't adjust these word insertion penalties and so forth  so you have to do everything at the level of the features. What could you do? And  uh  one thing I had suggested at an earlier time was maybe some sort of scaling  some sort of root or - or something of the  um  uh  features. But the problem with that is that isn't quite the same  it occurred to me later  because what you really want to do is scale the  uh  @@ the range of the likelihoods rather than - Nnn  the dist- Yeah. But  what might get at something similar  it just occurred to me  is kind of an intermediate thing - is because we do this strange thing that we do with the tandem system  at least in that system what you could do is take the  um  uh  values that come out of the net  which are something like log probabilities  and scale those. And then  uh  um - then at least those things would have the right values or the right - the right range. And then that goes into the rest of it and then that's used as observations. So it's - it's  Mm-hmm. um  Mm-hmm. another way to do it. But  these values are not directly used as probabilities anyway. So there are - there is - I know they're not. I know they're not. But - but  you know - Uh-huh. So because what we're doing is pretty strange and complicated  we don't really know what the effect is Mm-hmm. at the other end. So  um  my thought was maybe - I mean  they're not used as probabilities  but the log probabilities - we're taking advantage of the fact that something like log probabilities has more of a Gaussian shape than Gaus- than probabilities  and so we can model them better. So  in a way we're taking advantage of the fact that they're probabilities  because they're this quantity that looks kind of Gaussian when you take it's log. So  uh  maybe - Mm-hmm. maybe it would have a - a reasonable effect to do that. I d- I don't know. But  I mean  I guess we still haven't had a - a ruling back on this. And we may end up being in a situation where we just you know really can't change the word insertion penalty. But the other thing we could do is - also we could - I mean  this - this may not help us  uh  in the evaluation but it might help us in our understanding at least. We might  just run it with different insper- insertion penalties  and show that  uh  ""well  O_K  not changing it  playing the rules the way you wanted  we did this. But in fact if we did that  it made a - a big difference."" I wonder if it - it might be possible to  uh  simulate the back-end with some other system. So we - we get our f- front-end features  and then  uh  as part of the process of figuring out the scaling of these features  you know  if we're gonna take it to a root or to a power or something  Mm-hmm. we have some back-end that we attach onto our features that sort of simulates what would be happening. Um  And just adjust it until it's the best number? and just adjust it until that - our l- version of the back-end  uh  decides that - that - Well  we can probably use the real thing  can't we? And then jus- just  uh  Yeah. Oh  yeah. use it on a reduced test set or something. That's true. Yeah. And then we just use that to determine some scaling factor that we use. Yeah. So I mean  I- I think that that's a reasonable thing to do and the only question is what's the actual knob that we use? And the knob that we use should - Mm-hmm. uh  uh  unfortunately  like I say  I don't know the analytic solution to this cuz what we really want to do is change the scale of the likelihoods  not the cha- not the scale of the - Mm-hmm. the observations. But - but  uh - Mm-hmm. Yeah. Out of curiosity  what - what kind of recognizer is the one from Mississippi State? Uh  w- what do you mean when you say ""what kind""? Is it - ? Um  is it like a Gaussian mixture model? Yeah. Gaussian mixture model. O_K. It's the same system that they use when they participate in the Hub-five evals. It's a  um - sort of came out of  uh - uh  looking a lot like H_T_K. I mean  they started off with - um  when they were building their system they were always comparing to H_T_K to make sure they were getting similar results. And so  it's a Gaussian mixture system  uh - Do they have the same sort of mix-down sort of procedure  where they start off with a small number of some things and - ? Yeah. I don't know. Yeah. And then divide the mixtures in half. I don't know if they do that. I'm not really sure. Yeah. Hmm. D- Do you know what kind of tying they use? Are they - they sort of - some sort of - a bunch of Gaussians that they share across everything? Or - Yeah  th- I have - I - I - I don't have it up here but I have a - the whole system description  or if it's - ? that describes exactly what their system is and I - I'm not sure. O_K. But  um - O_K. It's some kind of a mixture of Gaussians and  uh  clustering and  uh - They're - they're trying to put in sort of all of the standard features that people use nowadays. Mm-hmm. So the other  uh  Aurora thing maybe is - I- I dunno if any of this is gonna come in in time to be relevant  but  uh  we had talked about  uh  Guenter playing around  uh  Mm-hmm. uh  over in Germany and - and  @@ uh  possibly coming up with something that would  uh  uh  fit in later. Uh  I saw that other mail where he said that he - uh  it wasn't going to work for him to do C_V_S. Yeah. Yeah. So now he has a version of the software. So he just has it all sitting there. Yeah. Yeah. Um - Mm-hmm. So if he'll - he might work on improving the noise estimate or on some histogram things  or - Yeah. Mm-hmm. Yeah. I just saw the Eurospeech - We - we didn't talk about it at our meeting but I just saw the - just read the paper. Someone  I forget the name  and - and Ney  uh  about histogram equalization? Did you see that one? Um  it was a poster. Or - Yeah. I mean  I just read the paper. I didn't see the poster. Yeah. Yeah. Um - It was something similar to n- on-line normalization finally - I mean  in the idea of - of normalizing - Yeah. But it's a little more - it - it's a little finer  right? So they had like ten quantiles and - Yeah. Right. and they adjust the distribution. So you - you have the distributions from the training set  N- and then  uh - So this is just a - a histogram of - of the amplitudes  I guess. Right? And then - Mm-hmm. Um  people do this in image processing some. You have this kind of - of histogram of - of levels of brightness or whatever. And - and - and then  Hmm. when you get a new - new thing that you - you want to adjust to be better in some way  you adjust it so that the histogram of the new data looks like the old data. You do this kind of piece-wise linear or  uh  some kind of piece-wise approximation. They did a - uh one version that was piece-wise linear and another that had a power law thing between them - between the points. And  uh  they said they s- they sort of see it in a way as s- for the speech case - as being kind of a generalization of spectral subtraction in a way  because  you know  in spectral subtraction you're trying to get rid of this excess energy. Uh  you know  it's not supposed to be there. Uh - and  uh  this is sort of adjusting it for - for a lot of different levels. And then they have s- they have some kind of  uh  a floor or something  so if it gets too low you don't - Hmm. Hmm. don't do it. And they - they claimed very nice results  and - Mm-hmm. So is this a histogram across different frequency bins? Or - ? Um  I think this i- You know  I don't remember that. Do you remember - ? I think they have  yeah  different histograms. I- uh - Something like one per frequency band  or - But I did - One - So  one histogram per frequency bin. One per critical - Yeah  I guess. But I should read the paper. I just went through the poster quickly  and I didn't - And that's - Yeah. And I don't remember whether it was filter bank things or whether it was F_F_T bins or - So th- Oh. Huh. And - and that - I don't remember that. that  um  histogram represents the different energy levels that have been seen at that frequency? And how often they - you've seen them. Yeah. Hmm. Uh-huh. Yeah. And they do - they said that they could do it for the test - So you don't have to change the training. You just do a measurement over the training. And then  uh  for testing  uh  you can do it for one per utterance. Even relatively short utterances. And they claim it - it works pretty well. So they  uh - Is the idea that you - you run a test utterance through some histogram generation thing and then you compare the histograms and that tells you I guess in pri- what to do to the utterance to Yeah. make it more like - ? In principle. I didn't read carefully how they actually implemented it  whether it was some  I see. Hmm. Yeah. uh  on-line thing  or whether it was a second pass  or what. But - but they - Hmm. That - that was sort of the idea. So that - that seemed  you know  different. We're sort of curious about  uh  what are some things that are  u- u- um  @@ conceptually quite different from what we've done. Cuz we - you know  one thing that w- that  Mm-hmm. uh  Stephane and Sunil seemed to find  uh  was  you know  they could actually make a unified piece of software that handled a range of different things that people were talking about  and it was really just sort of setting of different constants. And it would turn  you know  one thing into another. It'd turn Wiener filtering into spectral subtraction  or whatever. But there's other things that we're not doing. So  we're not making any use of pitch  uh  uh  which again  might - might be important  uh  because the stuff between the harmonics is probably a schmutz. And - and the  uh  transcribers will have fun with that. Uh - And  um  the  uh  stuff at the harmonics isn't so much. And - and  uh - And we- there's this overall idea of really sort of matching the - the hi- distributions somehow. Uh  not just  um  um - not just subtracting off your estimate of the noise. So. So I guess  uh  Guenter's gonna play around with some of these things now over this next period  or - ? Uh  I dunno. I don't have feedback from him  but Yeah. I guess he's gonna  maybe - Well  he's got it anyway  so he can. Yeah. Uh-huh. So potentially if he came up with something that was useful  like a diff- a better noise estimation module or something  he could ship it to you guys u- up there and Yeah. we could put it in. Mm-hmm. Mm-hmm. Yeah. Yeah. So  that's good. So  why don't we just  uh  um - I think starting - starting a w- couple weeks from now  especially if you're not gonna be around for a while  we'll - we'll be shifting more over to some other - other territory. But  uh  uh  uh  n- not - not so much in this meeting about Aurora  but - but  uh  uh  maybe just  uh  quickly today about - maybe you could just say a little bit about what you've been talking about with Michael. And - and then Barry can say something about what - what we're talking about. O_K. So Michael Kleinschmidt  who's a P_H_D student from Germany  showed up this week. He'll be here for about six months. And he's done some work using an auditory model of  um  human hearing  and using that f- uh  to generate speech recognition features. And he did work back in Germany with  um  a toy recognition system using  um  isolated digit recognition as the task. It was actually just a single-layer neural network that classified words - classified digits  in fact. Um  and he tried that on - I think on some Aurora data and got results that he thought seemed respectable. And he w- he's coming here to u- u- use it on a- uh  a real speech recognition system. So I'll be working with him on that. And  um  maybe I should say a little more about these features  although I don't understand them that well. The - I think it's a two-stage idea. And  um  the first stage of these features correspond to what's called the peripheral auditory system. And I guess that is like a filter bank with a compressive nonlinearity. And I'm- I'm not sure what we have @@ in there that isn't already modeled in something like  um  P_L_P. I should learn more about that. And then the second stage is  um  the most different thing  I think  from what we usually do. It's  um - it computes features which are  um  based on - sort of like based on diffe- different w- um  wavelet basis functions used to analyze the input. @@ So th- he uses analysis functions called Gabor functions  um  which have a certain extent  um  in time and in frequency. And the idea is these are used to sample  um  the signal in a- represented as a time-frequency representation. So you're sampling some piece of this time-frequency plane. And  um  that  um  is - is interesting  cuz  @@ for - for one thing  you could use it  um  in a - a multi-scale way. You could have these - instead of having everything - like we use a twenty-five millisecond or so analysis window  typically  um  and that's our time scale for features  but you could - using this  um  basis function idea  you could have some basis functions which have a lot longer time scale and  um  some which have a lot shorter  and so it would be like a set of multi-scale features. So he's interested in  um - Th- this is - because it's  um - there are these different parameters for the shape of these basis functions  um - there are a lot of different possible basis functions. And so he - he actually does an optimization procedure to choose an - an optimal set of basis functions out of all the possible ones. Hmm. H- What does he do to choose those? The method he uses is kind of funny - is  um  he starts with - he has a set of M_ of them. Um  he - and then he uses that to classify - I mean  he t- he tries  um  using just M_ minus one of them. So there are M_ possible subsets of this length-M_ vector. He tries classifying  using each of the M_ Hmm. possible sub-vectors. Whichever sub-vector  um  works the - the best  I guess  he says - Y- yeah. the - the fe- feature that didn't use Gets thrown out. was the most useless feature  Yeah. so we'll throw it out and we're gonna randomly select another feature from the set of possible basis functions. Hmm! Yeah. So i- so it's actuall- So it's a - it's a little bit like a genetic algorithm or something in a way. It's like a greedy - Well  it's - it's much simpler. But it's - but it's - uh  it's - there's a lot - number of things I like about it  let me just say. Greedy. So  first thing  well  you're absolutely right. I mean  i- i- in truth  both pieces of this are - have their analogies in stuff we already do. But it's a different take at how to approach it and potentially one that's m- maybe a bit more systematic than what we've done  uh  and a b- a bit more inspiration from - from auditory things. So it's - so I think it's a neat thing to try. The primary features  um  are in fact - Yeah  essentially  it's - it's  uh  you know  P_L_P or - or mel cepstrum  or something like that. You've - you've got some  uh  compression. We always have some compression. We always have some - you know  the - the - the kind of filter bank with a kind of quasi-log scaling. Um  if you put in - if you also include the RASTA in it - i- RASTA - the filtering being done in the log domain has an A_G_C-like  uh  characteristic  which  you know  people typi- typically put in these kind of  uh  um  uh  auditory front-ends. So it's very  very similar  uh  but it's not exactly the same. Um  I would agree that the second one is - is somewhat more different but  um  it's mainly different in that the things that we have been doing like that have been - um  had a different kind of motivation and have ended up with different kinds of constraints. So  for instance  if you look at the L_D_A RASTA stuff  you know  basically what they do is they - they look at the different eigenvectors out of the L_D_A and they form filters out of it. Right? And those filters have different  uh  kinds of temporal extents and temporal characteristics. And so in fact they're multi- scale. But  they're not sort of systematically multi-scale  like ""let's start here and go to there  and go to there  and go to there""  and so forth. It's more like  you run it on this  you do discriminant analysis  and you find out what's helpful. I- it's multi-scale because you use several of these in parallel  is that right? Of - @@ Yeah. They use several of them. Yeah. O_K. Uh  I mean  you don't have to but - but - but  uh  Hynek has. Um  but it's also  uh - @@ Hyn- when Hynek's had people do this kind of L_D_A analysis  they've done it on frequency direction and they've done it on the time direction. I think he may have had people sometimes doing it on both simultaneously - some two-D_ - and that would be the closest to these Gabor function kind of things. Uh  but I don't think they've done that much of that. And  uh  the other thing that's interesting - the - the  uh - the feature selection thing  it's a simple method  but I kinda like it. Um  there's a - a old  old method for feature selection. I mean  eh  uh  I remember people referring to it as old when I was playing with it twenty years ago  so I know it's pretty old  uh  called Stepwise Linear Discriminant Analysis in which you - which - I think it's used in social sciences a lot. So  you - you - you - you pick the best feature. And then you take - y- you find the next feature that's the best in combination with it. And then so on and so on. And what - what Michael's describing seems to me much  much better  because the problem with the stepwise discriminant analysis is that you don't know that - you know  if you've picked the right set of features. Just because something's a good feature doesn't mean that you should be adding it. So  um  uh  here at least you're starting off with all of them  and you're throwing out useless features. I think that's - that seems  uh - that seems like a lot better idea. Uh  you're always looking at things in combination with other features. Um  so the only thing is  of course  there's this - this artificial question of - of  uh  exactly how you - how you a- how you assess it and if - if your order had been different in throwing them out. I mean  it still isn't necessarily really optimal  but it seems like a pretty good heuristic. Hmm. So I th- I think it's - it's - I think it's kinda neat stuff. And - and - and  uh  the thing that I wanted to - to add to it also was to have us use this in a multi-stream way. Hmm. Um  so - so that  um  when you come up with these different things  and these different functions  you don't necessarily just put them all into one huge vector  but perhaps you have some of them in one stream and some of them in another stream  and so forth. And  um  um  um - And we've also talked a little bit about  uh  uh  Shihab Shamma's stuff  in which you - the way you look at it is that there's these different mappings and some of them emphasize  uh  upward moving  uh  energy and fre- and frequency. And some are emphasizing downward and fast things and slow things and - and so forth. So. So there's a bunch of stuff to look at. But  uh  I think we're sorta gonna start off with what he  uh  came here with and branch out - branch out from there. And his advisor is here  too  at the same time. So  he'll be another interesting source of wisdom. Hmm. So. As - as we were talking about this I was thinking  Yeah. um  whether there's a relationship between - um  between Michael's approach to  uh  some - some sort of optimal brain damage or optimal brain surgeon on the neural nets. So  like  if we have  Hmm . um - we have our - we have our RASTA features and - and presumably the neural nets are - are learning some sort of a nonlinear mapping  uh  from the - the - the features to - to this - this probability posterior space. Mm-hmm. Right? And  um - and each of the hidden units is learning some sort of - some sort of - some sort of pattern. Right? And it could be  like - like these  um - these auditory patterns that Michael is looking at. And then when you're looking at the - the  uh  um  the best features  you know  you can take out - you can do the - do this  uh  brain surgery by taking out  um  hidden units that don't really help at all. And this is k- sorta like - Mm-hmm. Or the - or features. Right? I mean  y- actually  you make me think a - a very important point here is that  um  Yeah. if we a- again try to look at how is this different from what we're already doing  uh  there's a - a  uh - a nasty argument that could be made th- that it's - it's not different at - at all  because  uh - if you ignore the - the selection part - because we are going into a - a very powerful  uh  nonlinearity that  uh  in fact is combining over time and frequency  and is coming up with its own - you know  better than Gabor functions - its  you know  neural net functions  its - whatever it finds to be best. Mm-hmm. @@ Um  so you could argue that in fact it - But I - I don't actually believe that argument because I know that  um  you can  uh - computing features is useful  even though in principle you haven't added anything - in fact  you subtracted something  from the original waveform - You know  uh  if you've - you've processed it in some way you've typically lost something - some information. And so  you've lost information and yet it does better with - with features than it does with the waveform. So  uh  I - I know that i- sometimes it's useful to - to constrain things. So that's why it really seems like the constraint - in - in all this stuff it's the constraints that are actually what matters. Because if it wasn't the constraints that mattered  then we would've completely solved this problem long ago  because long ago we already knew how to put waveforms into powerful statistical mechanisms. So. Yeah. Well  if we had infinite processing power and data  Right. Yeah- Uh  I guess  using the waveform could - then it would work. Yeah  I agree. Yeah. There's the problem. So  that's - Yeah. Then it would work. But - but  I mean  i- it's - With finite of those things - I mean  uh  we - we have done experiments where we literally have put waveforms in and - Mm-hmm. and - and  uh  we kept the number of parameters the same and so forth  and it used a lot of training data. And it - and it - it  uh - not infinite but a lot  and then compared to the number parameters - and it - it  uh - it just doesn't do nearly as well. Mm-hmm. So  anyway the point is that you want to suppress - it's not just having the maximum information  you want to suppress  uh  the aspects of the input signal that are not helpful for - for the discrimination you're trying to make. So. So maybe just briefly  uh - Well  that sort of segues into what - what I'm doing. Yeah. Um  so  uh  the big picture is k- um  come up with a set of  uh  intermediate categories  then build intermediate category classifiers  then do recognition  and  um  improve speech recognition in that way. Um  so right now I'm in - in the phase where I'm looking at - at  um  deciding on a initial set of intermediate categories. And I'm looking for data- data-driven methods that can help me find  um  a set of intermediate categories of speech that  uh  will help me to discriminate later down the line. And one of the ideas  um  that was to take a - take a neural net - train - train an ordinary neural net to - uh  to learn the posterior probabilities of phones. And so  um  at the end of the day you have this neural net and it has hidden - hidden units. And each of these hidden units is - um  is learning some sort of pattern. And so  um  what - what are these patterns? I don't know. Hmm. Um  and I'm gonna to try to - to look at those patterns to - to see  um  from those patterns - uh  presumably those are important patterns for discriminating between phone classes. And maybe - maybe some  uh  intermediate categories can come from just looking at the patterns of - um  that the neural net learns. Be- before you get on the next part l- let me just point out that s- there's - there's a - a pretty nice Yeah. relationship between what you're talking about doing and what you're talking about doing there. Right? So  it seems to me that  you know  if you take away the - the - the difference of this primary features  and  say  you use - as we had talked about maybe doing - you use P_- RASTA-P_L_P or something for the - the primary features  um  then this feature discovery  uh  uh  thing is just what he's talking about doing  too  except that he's talking about doing them in order to discover intermediate categories that correspond to these - uh  uh  what these sub-features are - are - are - are showing you. And  um  the other difference is that  um  he's doing this in a - in a multi-band setting  which means that he's constraining himself to look across time in some f- relatively limited  uh  uh  spectral extent. Right? And whereas in - in this case you're saying ""let's just do it unconstrained"". So they're - they're really pretty related and maybe they'll be - at some point where we'll see the - the connections a little better and Hmm. Mm-hmm. connect them. Um. Yeah  so - so that's the - that's the first part - uh  one - one of the ideas to get at some - some patterns of intermediate categories. Um  the other one was  um  to  uh  come up with a - a - a model - um  a graphical model  that treats the intermediate categories as hidden - hidden variables  latent variables  that we don't know anything about  but that through  um  s- statistical training and the E_M algorithm  um  at the end of the day  we have  um - we have learned something about these - these latent  um - latent variables which happen to correspond to intermediate categories. Um. Yeah  and so those are the - the two directions that I'm - I'm looking into right now. And  uh  um - Yeah. I guess that's - that's it. O_K. Should we do our digits and get ou- get our treats? Oh  tea time? Yeah. It's kind of like  you know  the little rats with the little thing dropping down to them. We do the digits and then we get our treats. That's ri- O_K. Transcript L_ dash three seven one. Two zero  five four  five five  three three  five nine. Four four five  one zero  four two three zero. Oops. Three three  one seven  five five  six seven  one zero. Eight  eight three one  one seven  two six three  zero. Five six zero  six zero four  nine seven two seven. Eight  one eight three  two nine  eight zero five  seven. Nine eight  five seven  two zero  eight eight  nine three. Six five three  four five eight  five four seven five. Transcript L_ dash three seven zero. Nine eight six  eight seven six  three seven two three. Four nine two  one six eight  one one zero. Six  seven one four  three eight  four eight eight  three. Four one eight four  seven three one seven  one nine one one. Three one six five  four three seven eight  nine six six nine. Two three one seven  O_  five six four. Four six O_  four one one  six one one. Seven four one eight  nine O_ seven five  O_ four four nine. Transcript L_ dash three six nine. Zero two six zero  nine nine eight five  four four nine four. Three  nine eight five  seven five  six four zero  three. Four nine six  two six  seven zero zero one. Nine four two  one two zero  seven six seven. One seven seven  four six eight  two eight nine. Two five  one nine  three seven  two eight  five two. One seven three  five seven seven  two eight six four. Seven eight eight  three one three  five seven six. Transcript L_ dash three six eight. Five nine six nine  three  eight nine zero. Six one five  six zero nine  three zero three. Zero seven zero three  eight eight three zero  two zero zero six. Four one three  six eight three  seven four three seven. Nine six  nine three  three three  six six  six seven. Eight four five  three zero  eight five seven six. Three three two  zero two one  eight two eight. Nine six zero  four seven  four five two three. Transcript L_ dash three six seven. Nine eight seven  two three one  nine two one one. Six  two zero seven  two two  two three eight  nine. Four seven five  seven three  eight nine seven zero. Three nine three  five eight seven  five four zero five. Seven six five eight  five four two zero  one eight four six. Five  one five nine  six eight  zero nine three  one. Two eight five  three one three  six three nine seven. Four eight zero one  six zero four nine  four six zero zero. O_K. O_K. ",Topics discussed by the Berkeley Meeting Recorder group included a potential collaboration with another ICSI member regarding the analysis of inference structures  efforts by speaker mn005 to detect speaker overlap  the current status on recordings and transcriptions  and future efforts to collect meeting data. In addition to weekly meetings by the BMR group  efforts are in progress to record meetings by other ICSI research groups  as well as routine discussions by non-ICSI members. The group will resume its discussion of speaker anonymization in a subsequent meeting. To save time  speaker mn005 will only mark the sample of transcribed data for regions of overlapping speech  as opposed to marking all acoustic events. The digits extraction task will be delegated to whomever is working on acoustics for the Meeting Recorder project. The group will inquire with other ICSI researchers and colleagues at the University of Washington about collecting additional meeting data. Echo cancellation experiments will be performed on Meeting Recorder digits data. Use of pronouns (e.g. he  she  you) during meetings is potentially problematic for indexing referents and analysing speech understanding. Sensitivity issues involved in mentioning individuals by name were also discussed. Data obtained via the close-talking microphone channels don't match meeting content yielded via the far-field microphones. The collection of meeting data from non-Meeting Recorder participants is likely to be more infrequent. A cursory review of a subset of Meeting Recorder data is in progress to determine whether it is suitable material for a collaboration with another ICSI researcher interested in analyzing inference structures. Efforts by speaker mn005 are in progress to detect overlapping speech. For a single transcribed meeting  speaker mn005 reported approximately 300 cases of overlap. Future work will involve manually deriving time marks from sections of overlapping speech for the same meeting  and then experimenting with different measures  e.g. energy increase  to determine a set of acoustically salient features for identifying speaker overlap. Approximately 12-13 hours of Meeting Recorder data have been collected  roughly 45 minutes of which have been transcribed. Additional meetings by other ICSI research groups will be recorded. A suggestion was made that multi-channel data also be collected in cooperation with local media broadcasters  and that such events might be recorded live from ICSI. The group aims to collect over 100 hours of Meeting Recorder data in total. Speaker consent forms are being revised. It was suggested that subjects should sign a new consent form after 10 recording sessions. 
"Yeah. Um  so. If we can't  we can't. But uh we're gonna try to make this an abbreviated meeting cuz the - the next - next occupants were pushing for it  so. Um. So. Agenda is - according to this  is transcription status  DARPA demos X_M_L tools  disks  backups  et cetera and CrossPads. Does anyone have anything to add to the agenda? O_K. Should we just go in order? Transcription status? Who's - that's probably you. I can do that quickly. Um I hired several more transcribers  They're making great progress. Seve- several  several. Seven? Oh. And uh - and uh  uh I've been uh finishing up the uh double checking. I hoped to have had that done by today but it's gonna take one more week. Um I g- as a somewhat segue into the next topic  um could I get a hold of uh the data even if it's not really corrected yet just so I can get the data formats and make sure the information retrieval stuff is working? Certainly. Yeah I mean  it's in the same place it's been. So can you just - Oh  it is. O_K. Just - So  ""transcripts"" Uh-huh. No change. Uh - is the sub-directory? Yes. Uh-huh. O_K. So I'll - I'll probably just make some copies of those rather than use the ones that are there. O_K. Um and then just - we'll have to remember to delete them once the corrections are made. O_K. O_K  wh- I also got anot- a short remark to the transcription. I've uh just processed the first five E_D_U meetings and they are chunked up so they would - they probably can be sent to I_B_M whenever they want them. Well the second one of those Cool. Yep. It's already at I_B_M  but the other ones - Yeah. is already at I_B_M. That's the one that we're waiting to hear from them on. Yeah. Yeah. O_K. These are separate from the ones that - I mean  these are - As soon as - They're the I_B_M set. It's this one. Yep. Excellent. Good. Yeah. Is my mike on? Yeah. And so as soon as we hear from Brian that this one is O_K and we get the transcript back and we find out that hopefully there are no problems matching up the transcript with what we gave them  then uh we'll be ready to go and we'll just send them the next four as a big batch  Excellent. and let them work on that. And so we're doing those as disjoint from the ones we're transcribing here? Yes  exactly. We're sort of doing things in parallel  that way we can get as much done a- at once. Yeah. O_K  good. Yeah  I think that's the right way to do it  especially for the information retrieval stuff. Anything else on transcription status? Hm-mmm. O_K. DARPA demos  we had the submeeting the other day. Right  which uh - So I've been working on using the THISL tools to do information retrieval on meeting data and the THISL tools are - there're two sets  there's a back-end and a front-end  so the front-end is the user interface and the back-end is the indexing tool and the querying tool. And so I've written some tools to convert everything into the right for- file formats. And the command line version of the indexing and the querying is now working. So at least on the one meeting that I had the transcript for uh conveniently you can now do information retrieval on it  do - type in a - a string and get back a list of start-end times for the meeting  uh of hits. What - what kind of uh - what does that look like? The string that you type in. What are you - are you - are they keywords  or are they - ? O_K. Keywords. I see. Right? And so - and then it munges it to pass it to the THISL I_R which uses an S_G_M_L-like format for everything. I see. And then does it play something back or that's something you're having to program? Um  right now  I have a tool that will do that on a command line using our standard tools  Yeah. but my intention is to do a prettier user interface based either - So - so that's the other thing I wanted to discuss  is well what should we do for the user interface? We have two tools that have already been written. Um the SoftSound guys did a web-based one  Mm-hmm. um  which I haven't used  haven't looked at. Dan says it's pretty good Mm-hmm. but it does mean you need to be running a web server. Mm-hmm. And so it - it's pretty big and complex. Uh and it would be difficult to port to Windows because it means porting the web server to Windows. Mm-hmm. Uh the other option is Dan did the Tcl-T_K THISL GUI front-end Yeah. for Broadcast News which I think looks great. I think that's a nice demo. Um and that would be much easier to port to Windows. And so I think that's the way we should go. I - Can I ask a question? So um Mm-hmm. as it stands within the - the Channeltrans interface  it's possible to do a find and a play. You can find a searched string and play. So e- Are you - So you're adding like um  I don't know  uh are they fuzzy matches or are they uh - ? It's a sort of standard  text-retrieval-based - So it's uh term frequency  inverse document frequency scoring. O_K. Um and then there are all sorts of metrics for spacing how far apart they have to be and things like that. So it - it's It's a lot more sophisticated than the uh i- it's like doing a Google query or anyth- anything else like that. the basically Windows-based - So i- it uses - So it pr- produces an index ahead of time so you don't - you're not doing a linear search through all the documents. O_K. Cuz you can imagine if - with - if we have the sixty hours' worth you do - wouldn't wanna do a search. Hm-mmm. Um you have to do preindexing and so that - these tools do all that. Good. And so the work to get the front-end to work would be porting it - well - uh to get it to work on the UNIX systems  our side is just rewriting them and modifying them to work for meetings. So that it understands that they're different speakers and that it's one big audio file instead of a bunch of little ones and just Mm-hmm. Mm-hmm. sorta things like that. Mm-hmm. So what does the user see as the result of the query? On which tool? THISL. The THISL GUI tool which is the one that Dan wrote  Tcl-T_K Yeah. um you type in a query and then you get back a list of hits and you can type on them and listen to them. Click on them rather with a mouse. Ah. So if you typed in ""small heads"" or something you could Mmm Right  you'd get - get back a uh uh something - something that would let you click and listen to some audio where that phrase had occurred or some- You - you'd get to listen to ""beep"". That was a really good look. It's too bad that that couldn't come into the - You couldn't get a video. Guess who I practice on? At some point we're gonna have to say what that private joke is  that keeps coming up. Yeah. And then again  maybe not. So  uh - Yeah  that soun- that sounds reasonable. Yeah  it loo- it - my - my recollection of it is it's - it's a pretty reasonable Right. uh demo sort of format. Yeah that sounds good. That sounds really neat. And so I think there'd be minimal effort to get it to work  minimally and then we'd wanna add things like query by speaker and by meeting and all that sort of stuff. Um Dave Gelbart expressed some interest in working on that so I'll work with him on it. And it - it's looking pretty good  you know  the fact that I got the query system working. So if we wanna just do a video-based one I think that'll be easy. Mm-hmm. If we wanna get it to Windows it's gonna be a little more work because the THISL I_R  the information retrieval tool's - um  I had difficulty just compiling them on Solaris. Mm-hmm. So getting them to compile on Windows might be challenging. Mm-hmm. But you were saying that - that the uh - So. that there's that set of tools  uh  Cygnus tools  that - It certainly helps. Um  I mean without those I wouldn't even attempt it. Uh-huh. Yeah. Mm-hmm. But what those - they - what those do is provide sort of a B_S_D compatibility layer  so that the normal UNIX function calls all work. Mm-hmm. Mm-hmm. Um  And you have to have all the o- But the problem is that - that the THISL tools didn't use anything like Autoconf and so you have the normal porting problems of different header files and th- some things are defined and some things aren't and uh different compiler work-arounds and so on. So the fact that um it took me a day to get it c- to compile under Solaris means it's probably gonna take me s- significantly more than that to get it to compile under Windows. How about having it run under free B_S_D? Well what you need - Free B_S_D would probably be easier. All you need to do is say to Dan ""gee it would be nice if this worked under Autoconf "" and it'll be done in a day. That's true. Right? Uh - Actually you know I should check because he did port it to SPRACHcore so he might have done that already. Right. I - I - I wouldn't be surprised. So - I'll check at that - How does it play? What I - But it would - what would serve - would serve both purposes  is if you contact him and ask him if he's already done it. Yeah  right. If he has then you learn  if he hasn't then he'll do it. Right. Wow. I hope he never listens to these meetings. That's right. So  and I've been corresponding with Dan and also with uh It's amazing. Yeah. uh  SoftSound guy  uh - Blanking on his name. Tony Robinson? Tony Robinson? Do I mean Tony? I guess I do. Yeah. Or S- or Steve Renals. James Christie. Which one do I mean? Steve Renal- Steve Renals. Steve Renals is not SoftSound  is he? No. My brain is not working  I don't remember who I've been corresponding with. O_K. Steve wro- i- it's Ste- Steve Renals wrote THISL I_R. Then it's Steve Renals. O_K. Oh  O_K. So uh just getting documentation and uh and f- and formats  so that's all going pretty well  I think we'll be O_K with that. Yeah. Right. Assuming we're - What about issues of playing sound files @@ between the two platforms? Um we have - Well  that's a good point too. Here's a - here's a crazy idea actually. I don't know. Why don't you try and merge Transcriber and THISL I_R? Well this is one of the reasons - They're both Tcl interfaces. This is the - one of the reasons that I'm gonna have uh Dave Gelbart - Gelbart - Having him volunteer to work on it is a really good thing because he's worked on the Transcriber stuff Right. and he's more familiar with Tcl-T_K than I am. And then you get - they - then you get the Windows media playing for free. Well that's Snack  not - not Transcriber. Right. But the point is that the Transcriber uses Snack and then you can - but you can use a - a lot of the same functionality and it's - Yeah  yeah  I mean  I - I think THISL - THISL GUI probably uses Snack. And so my intention was just to base it on that. Yeah. Well my thought was is that it would be nice - it would be nice to have the running transcripts And if it doesn't - um eh you know  from speaker to speaker. Right? Do you have - you have  you know  a speaker mark here and a speaker mark here? Right  we'll have to figure out a user interface for that  so. Right. Well that - eh my thought was if you had like Multitrans or whatever do it. Or whatever. Yeah. It might be fairly difficult to get that to work in the little short segments we'd be talking about and having the search tools and so on. We - we can look into it  but - Yeah. The thing I was asking about with  um  free B_S_D is that it might be easier to get PowerPoint shows running in free B_S_D than to get this other package running in - Yeah  I mean we have to - I have to sit down and try it before I make too many judgments  so uh Yeah. Um My experience with the Gnu compatibility library is really it's just as hard and just as easy to port to any system. Right? The Windows system isn't any harder because it - it looks like a B_S_D system. Mm-hmm. It's just  you know  just like all of them  the ""include"" files are a little different and the function calls are a little different. Right. So I - it might be a little easier but it's not gonna be a lot easier. O_K. So there was that demo  which was one of the main ones  then we talked about um some other stuff which would basically be um showing off the - the Transcriber interface itself and as you say  maybe we could even merge those in some sense  but - but um  uh - and part of that was showing off what the speech-non- uh nonspeech stuff that Thilo has done s- looks like. Yeah. Can I ask one more thing about THISL? Mm-hmm. So with the I_R stuff then you end up with a somewhat prioritized um - ? Mm-hmm  ranked. Excellent. Excellent. Yeah. So another idea I w- t- had just now actually for the demo was whether it might be of interest to sh- to show some of the prosody uh work that Don's been doing. Mm-hmm. Um actually show some of the features and then show for instance a task like finding sentence boundaries or finding turn boundaries. Um  you know  you can show that graphically  sort of what the features are doing. It  you know  it doesn't work great but it's definitely giving us something. Well I think at - I don't know if that would be of interest or not. at the very least we're gonna want something illustrative with that cuz I'm gonna want to talk about it and so i- if there's something that shows it graphically Yeah. it's much better than me just having a bullet point pointing at something I don't know much about  so. I mean  you're looking at this now - Are you looking at Waves or Matlab? Um yeah I'm starting to and um - Yeah we can probably find some examples of different type of prosodic Yeah def- events going on. S- so when we here were having this demo meeting  what we're sort of coming up with is that we wanna have all these pieces together  to first order  by the end of the month I- and then that'll give us a week or so. Oh  the end of this month or next month? Ooo. The end of - This month. Oh  you mean like today? Ju- Oh. Oh sorry  next month. Today isn't June first  is it. June. June. June. Next month. Sorry. O_K. Yeah. There's another one. Exactly. Uh - that'll - that'll give us - Sorry. that'll give us a week or so to uh - to port things over to my laptop and make sure that works  yeah. I think  I mean eh where - Yeah  I mean I'll be here. Yeah if d- if Don can sort of talk to whoever's - cuz we're doing this anyway as part of our - Yeah. you know  the research  visualizing what these features are doing and so either - Yeah. it might not be integrated but it - it could potentially be in it. Could find some. Yeah. Well  this is to an audience of researchers so I mean  you know  to let s- the goal is to let them know what it is we're doing. So that's - I mean it's different. I don't think anyone has done this on meeting data so it might be neat  you know. Yeah. Good. Done with that. X_M_L tools? Um. So I've been doing a bunch of X_M_L tools where you - we're sort of moving to X_M_L as the general format for everything and I think that's definitely the right way to go because there are a lot of tools that let you do extraction and reformatting of X_M_L tools. Um. So yet again we should probably meet to talk about transcription formats in X_M_L because I'm not particularly happy with what we have now. I mean it works with Transcriber but it - it's a pain to use it in other tools uh because it doesn't mark start and end. Start and end of each - ? Uh - Yeah. Utterance. Utterance. Just marks - ? So it's implicit in - in there but you have to do a lot of processing to get it. Yeah. Right. Right. And so - and also I'd like to do the indirect time line business. Um but regardless  I mean  w- that's something that you  me  and Jane can talk about later. Um  but I've installed X_M_L tools of various sorts in various languages and so if people are interested in doing - extracting any information from any of these files  either uh information on users because the user database is that way - I'm converting the Key files to X_M_L so that you can extract m- uh Cool. various inf- uh sorted information on individual meetings Yeah. and then also the transcripts. And so l- just let me know there - it's mostly Java and Perl but we can get other languages too if - if that's desirable. Oh  quick question on that. Is - do we have the - the seat information? In - in the Key files now? Mm-hmm. The seat information is on the Key files for the ones which Ah. Oh in - it's been recorded  yeah. For the new one- O_K. Great. Seat? Sea- yeah. Where - where you're sitting. Oh! Not - not the quality or anything. No. O_K. I see. n- Right. ""It's pretty soft and squishy."" Yeah. Alright . Yeah. O_K. Oh  but that might just be me. O_K. Um. Alright. That's more seat information than we wanted. Never mind. Hmm. I'm just trying to figure out  you know  when Morgan's voice appears on someone's microphone are they next to him or are they across from him? Maybe we should bleep that out. Yeah. Mmm  yeah. Wait a minute  how - how w- eh where is it in the Key file? Right. Yeah. The square bracket. Cuz I mean I haven't been putting it in and - in by - You haven't been putting it in. Right. I have not. And - Well bu- Oh  O_K. Isn't it always on the digits? Some of these are missing. Aren't they? Isn't it always on the digits forms? Some fall out of - Well it- Yeah so we can go back and fill them in for the ones we have. Ooo. I mean they're on th- right  these  but I just hadn't ever been putting it in the Key files. And I don't think Chuck was either cuz - Yeah I - I never - I never knew we were supposed to put it in the Key file. I had told you guys about it but - Oh  so we're both sorry. So - Oh really? I mean this is why I wanna use a g- a tool to do it rather than the plain text because with the plain text it's very easy to skip those things. O_K. O_K. O_K. So. Um if you use the Edit-key  or Key-edit - I think it's Edit-key  Edit-key. command - Did I show you guys that? Yep. I did show it to you  but I think you both said ""no  you'll just use You mentioned it  yeah. Yeah. text file"". Um it has it in there  a place to fill it in. Text. O_K. O_K. Yeah  and so if you don't fill it in  you're not gonna get it in the meetings. So. So if - Right. Well I - I just realized I hadn't been doing it and probably - So - Yep. u- Yeah and then the other thing also that Thilo noticed is  on the microphone  on channel zero it says hand-held mike or Crown mike  Yeah. Right. you actually have to say which one. So. I know - Yeah  I usually delete the - I don't  maybe I forgot to d- Oh! O_K. I didn't do that either. Yeah. Takes me no time at all to edit these. But it's almost - Yeah. Yeah that's cuz you kn- I - I know why. I'm not doing anything. And I was - I was looking at Chuck's  like  ""oh what did Chuck do  O_K I'll do that "". So. And then uh also in a couple of places instead of filling the participants under ""participants"" they were filled in under ""description"". Ah  O_K. Uh - And so that's also a problem. So anyway. We will do better. That's it. Oh uh also I'm working on another version of this tool  the - the one that shows up here  that will flash yellow if the mike isn't connected. And it's not quite ready to go yet because um it's hard to tell whether the mike's connected or not because the best quality ones  the Crown ones  are about the same level if they're off and no one's o- off or if they're on and no one's talking. Huh. Um these - these ones  they are much easier  there's a bigger difference. So I'm working on that and it - it sorta works and so eventually we will change to that and then you'll be able to see graphically if your mike is dropping in or out. Will that also include like batteries dying? Yep. Just a- any time the mike's putting out zeros basically. Yep. Yep. But with the screensaver kicking in  it - Now - But y- yeah. Well I'll turn off the screensaver too. Yeah. Yeah. Oops. Speaking of which. Um the other thing is as I've said before  it is actually on the thing. There's a little level meter but of course no one ever pays attention to it. So I think having it on the screen is more easy to notice. It would be nice if - if these had little light indicators  little L_E_Ds for - Uh buzzer. ""Bamp  bamp!"" Yeah  a buzzer. Small shocks administered to the - Yeah. Actually - O_K. Oh - O_K  disk backup  et cetera? Um I spoke with Dave Johnson about putting all the Meeting Recorder stuff on non-backed-up disk to save the overhead of backup and he pretty much said ""yeah  you could do that if you want"" but he thought it was a bad idea. In fact what he said is doing the manual one  doing uh N_W archive to copy it is a good idea and we should do that and have it backed up. He w- he's a firm believer in - in lots of different modalities of backup. I mean  his point was well taken. This data cannot be recovered. Yeah. And so if a mistake is made and we lose the backup we should have the archive and if then a mistake is made and we lose the archive we should have the backup. Well I guess it is true that even with something that's backed up it's not gonna - if it's stationary it's not going to go through the increment- it's not gonna burden things in the incremental backups. Just - just the monthly full. Hmm. Yeah  so the monthly full will be a bear but - Yeah. But he said that - that we sh- shouldn't worry too much about that  that we're getting a new backup system and we're far enough away from saturation on full backups that it's w- Really? probably O_K. And uh  so the only issue here is the timing between getting more disks and uh recording meetings. So I guess the idea is that we would be reserving the non-backed-up space for things that took less than twenty-four hours to recreate or something like that  right? Things that are recreatable easily and also - Yeah  basically things that are recreatable. Yeah. Yeah. The expanded files and things like that. O_K . They take up a lot more room anyway. Yeah. Uh but we do need more disk. So we can get more disk. Yeah. So. Yeah. And I - I think I agree with him. I mean his point was well taken that if we lose one of these we cannot get it back. O_K. I don't think there was any other et cetera there. Well I was allowing someone else to come up with something related that they had uh - I thought you guys were gonna burn C_Ds? Um unfortunately - we could burn C_Ds but first of all it's a pain. Yeah. Because you have to copy it down to the P_C and then burn it and that's a multi-step procedure. And second of all the - the write-once burners as opposed to a professional press don't last. Yeah. So I think burning them for distribution is fine but burning them for backup is not a good idea. I see. Cuz th- they - they O_K. fail after a couple years. Alright. I do have uh uh - It's a different topic. Can I add one top- topic? We have time? I wanted to ask  I know that uh that Thilo you were  um  bringing the Channeltrans interface onto the Windows machine? Yeah it's - it - Basically it's done  yeah. And I wanted to know is th- It's all done? That's g- wonderful. Great. Yeah. Yes  since Tcl-T_K runs on it  basically Yeah it - things'll just work. Yeah  it was just a problem with the Snack version and the Transcriber version but it's solved. So. Does - and that - does that mean  I - maybe I should know this but I don't. Does this mean that the - that this could be por- uh ported to a Think-Pad note- or some other type of uh - Yeah  basically uh I did install it on my laptop and yeah Wonderful. it worked. Hmm! Wonderful. Good. CrossPads? Uh got an email from uh James Landay who basically said ""if you're not using them  could you return them?"" So he said he doesn't need them  he just periodically w- at the end of each term sends out email to everyone who was recorded as having them and asks them if they're still using them. So we've never used them. We used them once. We - we used them a couple times  but - Once? Mm-hmm. Them? There's more than one? Couple times. Yeah  we have two. Yeah. i- Um. But - My opinion on it is  first  I never take notes anyway so I'm not gonna use it  um and second  it's another level of infrastructure that we have to deal with. And I have - uh so my - my feeling on it is that I think in principle it's a really nice idea  and you have the time tags which makes it better tha- than just taking ra- raw notes. On the other hand  I - the down side for me was that I think the pen is really noisy. So you have ka- kaplunk  kaplunk  kaplunk. And I - and I don't know if it's audible on the - but I - I sort of thought that was a disadvantage. I do take notes  I mean  I could be taking notes on these things and I guess the plus with the CrossPads would be the time markings but - I don't know. Uh  what is a CrossPad? So it's - it's um - it's a regular pad  just a regular pad of paper but there's this pen Thank you. which indicates position. And so you have time and position O_K. stuff stored so that you can - you have a record of whatever it is you've written. O_K. And then you can download it and they have O_C_R and searching and all sorts of things. O_K. O_K. So i- if you take notes it's a great little device. Could - Mm-hmm. But I don't take notes  so. And one of the reasons that it was brought up originally was because uh we were interested in - in higher-level things  not just the  you know  microphone stuff but also summarization and so forth and the question is if you were going to go to some gold standard of what wa- what was it that happened in the meeting you know  where would it come from? And um I think that was one of the Yeah. Yep. Yeah. things  right? And so the - it seemed like a neat idea. We'll have a - you know  have a scribe  have somebody uh take good notes and then that's part of the record of the meeting. And then we did it once or twice and we sort of - Yep  and then just sort of died out. probably chose the wrong scribe but it was - It's uh - I mean - Yeah that's right. Well I did it one time but um - Yep. Yeah. u- but I guess the - the other thing I'm thinking is if we wanted that kind of thing I wonder if we'd lose that much by having someone be a scribe by listening to the tape  to the recording afterwards and taking notes in some other interface. I mean we're transcribing it anyways  why do we need notes? Oh it's la- it's useful  have a summary and high points. Because that's summary. I think - Summary. there's also - there's this use that - Summarize it from the transcription. the - Well  what if you're sitting there and you just wanna make an X_ and you don't wanna take notes and you're - you just wanna Doodle. get the summary of the transcript from this time location like - you know  and - and then while you're bored you don't do anything and once in a while  maybe there's a joke and you put a X_ and - But - in - in other words you can use that just to highlight times in a very simple way. Also with - I was thinking and I know Morgan disagrees with me on this but suppose you have a group in here and you wanna let them note whenever they think there might be something later that they might not wanna distribute in terms of content  they could just sort of make an X_ near that point or a question mark that sort of alerts them that when they get the transcript back they c- could get some red flags in that transcript region and they can then look at it. So. I know we haven't been using it but I w- I can imagine it being useful just for sort of marking time periods which you then Right. get back in a transcript so. Well. I guess - so  you know  what - what makes one think i- is maybe we should actually schedule some periods where people go over something later and - and - and put some kind of summary or something uh you know  some - there'd be some scribe who would actually listen  w- who'd agreed to actually listen to the whole thing  not transcribe it  but just sort of write down things that struck them as important. But - then you don't - you don't have the time reference Right. uh that you'd have if you had it live. And you don't have a lot of other cues that might be useful  so. Yeah. How do you synchronize the time in the CrossPad and the time of the recording? I mean that was one of the issues we talked about originally and that- that's w- part of the difficulty is that we need an infrastructure for using the time - the CrossPads and so that means synchronizing the time - Uh. Mm-hmm. You know you want it pretty close and there's a fair amount of skew because it's a hand-held unit with a battery Well when - when I d- and so you - O_K. so you have to synchronize at the beginning of each meeting all the pads that are being used  so that it's synchronized with the time on that and then you have to download to an application  and then you have to figure out what the data formats are and convert it over if you wanna do anything with this information. w- Mm-hmm. And so there's a lot of infrastructure which Why - There is an alternative. unless someone - There is an alternative  I mean  it's still  there's uh - you know  your point stands about there be - needing to be an infrastructure  but it doesn't have to be synchronized with the little clock's timer on it. You c- I mean  I - when I - when I did it I synchronized it by voice  by whispering ""one  two  three  four"" onto the microphone and uh  you know. Hmm. Well  but then there's the infrastructure at the other end which someone has to listen to that and find that point  Right. Yeah  it's transcribed. It's in the transcript. and then mark it. So. Yeah. Well it's in the transcript. Well  could we keep one of these things for another year? Would h- I mean is there a big cau- We can keep all - both of them for the whole whole year. I mean  it's just - just - just in case we - even maybe some of the transcribers who might be wanting to annotate uh f- just there's a bunch of things that might be neat to do but I - it might not be the case that we can actually synchronize them and then do all the infrastructure but we could at least try it out. Well - one thing that we might try um is on some set of meetings  some collection of meetings  maybe E_D_U is the right one or maybe something else  we - we get somebody to buy into the idea of doing this as part of the task. I mean  Right. uh part of the reason - I think part of the reason that Adam was so interested in uh the SpeechCorder sort of f- idea from the beginning is he said from the beginning he hated taking notes and - Yep. and so forth so and - and Jane is more into it but eh uh you know I don't know if you wanna really do - do this all the time so I think the thing is to - to get someone to actually buy into it and have at least some series of meetings where we do it. Um - and if so  it's probably worth having one. The p- the - the problem with the - the more extended view  all these other you know with uh quibbling about particular applications of it is that it looks like it's hard to get people to um uh routinely use it  I mean it just hasn't happened anyway. But maybe if we can get a person to - Yeah I don't think it has to be part of a- what everybody does in a meeting but it might be a useful  neat part of the project that we can  you know  show off as a mechanism for synchronizing events in time that happen that you just wanna make a note of  like what Jane was talking about with some later browsing  just - just as a convenience  even if it's not a full-blown note taking substitute. Well if you wanted to do that maybe the right architecture for it is to get a P_D_A with a wireless card. And - and that way you can synchronize very easily with the - the - the meeting because you'll be synchroni- you can synchronize with the - the Linux server and uh - So what kind of input would you be - ? so - so  I mean  if you're not worried about - Buttons. You'd just be pressing like a - a - Well - well you have a P_D_A and may- and you could have the same sort of X_ interface or whatever  I mean  you'd have to do a little eh a little bit of coding to do it. Mm-hmm. But you could imagine  I mean  if - if all you really wanted was - you didn't want this Yeah  that be good. secondary note-taking channel but just sort of being able to use m- markers of some sort  a P_D_A with a l- a wireless card would be the - probably the right way to go. I mean even buttons you could do  sort of  I mean  as you said. I mean for what - what you've been describing buttons would be even more convenient than anything else  right? You have the - M- right. That would be fine too. I mean  I don't have  you know  grandiose ideas in mind but I'm just sort of thinking Right. well we've - we're getting into the next year now and we have a lot of these things worked out at - in terms of the speech maybe somebody will be interested in this and - I like this P_D_A idea. Yeah. Yeah  I do like the idea of having a couple buttons where like one - one button was ""uh-oh"" Well I'm sure there would - Yeah. Yeah. and then another button was ""that's great"" and another button ""that's f-"" Or like this is my ""I'm supposed to do this"" kind of button  like ""I better remember to -"" Yeah. Action item. Yeah something like that or - I mean I think the CrossPad idea is a good one. And then - Uh-huh. It's just a question of getting people to use it and getting the infrastructure set up in such a way that it's not a lot of extra work. I mean that's part of the reason why it hasn't happened is that Yeah. it's been a lot of extra work for me Right. and - Well  and not just for you. But it's also  it has this problem of having to W- go from an analog to a d- a digital record too  doesn't it? I mean - Well it's digital but it's in a format that But I mean  say  if i- if - is not particularly standard. if you're writing - if you're writing notes in it does - it - it can't do handwriting recognition  right? No  no  but it's just - it's just storing the pixel O_K. informa- position information  it's all digital. I - I guess what I'm thinking is that the P_D_A solution you h- you have it already without needing to go from the pixelization to a - to a - I mean - Right. You don't have to - The transfer function is less errorful  yes. Yeah  yeah. Yeah. Oh  nicely put. Yeah. Yeah. Well it also - it's maybe realistic cuz people are supposed to be bringing their P_D_As to the meeting eventually  right? That's why we have this little - I don't know what - I don't wanna cause more work for anyone but I can imagine some interesting things that you could do with it and so if we don't have to return it and we can keep it for a year - I don't know. Well - w- we don't - we certainly don't have to return it  as I said. All - all he said is that if you're not using it could you return it  if you are using it feel free to keep it. The point is that we haven't used it at all and are we going to? So we have no but - uh by I - I would suggest you return one. O_K. Yeah. Because we - we you know  we - we haven't used it at all. We c- We have some aspirations of using them and - One would probably be fine. Maybe we could do like a student project  you know  maybe someone who wants to do this as their main like Yeah. s- project for something would be cool. Yep. I mean if we had them out and sitting on the table people might use them a little more Maybe Jeremy could sit in some meetings and press a button when there - when - when somebody laughed. although there is a little - Well  I'm - yeah  that's not a bad - Yeah  yeah. Yeah. Jeremy's gonna be an - he's a new student starting on modeling brea- breath and laughter  actually  which sounds funny but I think it should be cool  so. Yeah. Sounds breathy to me. O_K. Breath and lau- ""ha-ha-ha-ha"". ""Ha-ha-ha."" ""Ha-ha-ha-ha."" Well dear. Um. Hmm. That reminded me of something. Oh well  too late. It slipped out. O_K. Oh  equipment. You're - you're gonna tease me? O_K. Ordered - Uh  well I'm always gonna do that. W- uh - We ordered uh more wireless  and so they should be coming in at some point. Great. And then at the same time I'll probably rewire the room as per Jane's suggestion so that uh the first N_ channels are wireless  eh are the m- the close-talking and the next N_ are far-field. You know what he means but isn't that funny sounding? ""We ordered more wireless. "" It's like wires are the things so you're wiring - you're - you're - you - we're - we ordered more absence of the thing. That's a very philosophical statement from Morgan. I just - it's sort of a anachronism  I mean it's like - wired less   wired more. Anyway. It's great. Should we do digits? Do we have anything else? Yeah. O_K. I mean there's - there's all this stuff going on uh between uh Andreas and - and - and Dave and Chuck and others with various kinds of runs uh um - recognition runs  trying to figure things out about the features but it's - it's all sort of in process  so there's not much to say right now. Uh why don't we start with our - our esteemed guest. O_K. Alright. So just the transcript number and then the - then the - This is - Yes  this is number two for me today. See all you have to do is go away to move way up in the - Oh. We could do simultaneous. Initiate him. We - we could. Should we do simultaneous? Well  I'm just thinking  are you gonna try to save the data before this next group comes in? Yeah. Yeah  absolutely. Yeah  so we might wanna do it simultaneous. I mean you hav- sorta have to. Right  so - Well O_K  so let's do one of those simultaneous ones. That sounds good. so we might n- we might need to do that actually. O_K. O_K. Everybody ready? Yeah. A one. You have to plug your ears  by the way uh Eric  or - Well I have to  I don't know about other people. O_K  alright. You don't have to. or you start laughing. O_K  a one and a two and a three. Transcript L_ one six five. Transcript L_ dash one six four. Transcript L_ one six one. Transcript L_ dash one six two. Transcript L_ one sixty. Transcript L_ one six six. Transcript L_ one five nine. L_ one six three. zero  one six zero  two seven  six four seven  eight zero nine six five  five  one five one three O_  six nine  three nine  four six  nine seven zero five  four eight  seven eight  eight seven  four five five four six  eight one  three two seven two three seven eight two  two three two five  nine five nine seven two six four seven  five  two nine eight five seven nine  five six  five O_ four one two  two five five  eight nine  one eight two  six three six five  O_ six  two three six seven six zero six two  zero five four five  eight five four three six four six one  three  three two two six three one one  six eight five five  two two four three six nine five nine  one  one one six five two nine seven  nine seven seven one  five O_ two four six four three  nine nine eight  three one nine two O_ five  four O_  nine six  three seven  one O_ nine three seven  seven six  three nine four eight four nine three  four zero two  five eight two seven nine five  four four seven  four three four six three zero one  four four  two one zero four seven three five  six two two  zero two six four five eight eight nine  nine  seven nine seven six six nine  four nine  four four  O_ nine four five  eight O_  four three  seven five  nine three three seven  four one  eight seven  four O_   two nine one zero one two  eight  zero six five Four  four eight six  five three  seven four five  two six nine seventy four  eight  five seven two four four  seven zero  five five  three zero  four three O_ eight  six eight  nine  five one nine three eight three eight  four  two four nine six one seven  three O_ six  three three four nine eight nine five  four three nine  O_ eight six four three four nine  nine six six three  zero four four five three two six two  eight eight three one  four two five one O_ five one  three two  two O_ seven three six five seven  six seven  two three three one two five one  seven two three  four eight six five five  five seven  eight eight  eight three  six three six  two three seven  three two  nine seven five  one seven one six three  four  five four four eight one  one four  three zero  five four  seven five nine  nine O_ two  three seven  eight three zero  eight four seven five  two nine seven  seven four five one zero seven seven  eight four  five six nine one four nine three  O_ eight one  four five seven three one three seven  six eight zero seven  three two four two nine four  three two  one seven  seven six  six three five five O_ five  three  two three five seven two four  nine two  zero eight  seven three nine  three six four  six three  nine six nine  six five four seven O_  nine  nine seven four six five five four  four three three three  O_ four two five. zero six four four  O_ four O_ three  three eight nine zero six five nine  two seven  two eight  one two. eight two four  three two eight  two four zero nine eight one six zero  four  O_ seven three Sorry. nine four two six  six nine eight five  five three four four. three zero zero nine  seven  eight four two. five four seven O_  nine  seven nine four four four two  five one  nine eight two five. two one four five  eight  four two six. four four five  four nine  nine six six six. Nine six  two three  four eight  eight eight  seven nine. O_K  babble  take five. ","The discussion concerned mainly ideas about data collection and the nature and generation of queries on meetings. Meeting notes taken by participants as standard minutes or summaries  or on devices like CrossPads can provide useful information. There is also interest in the speech community for fusion of speech with visual data. Taking some photos of the whiteboard and the positioning of participants is easy enough to do. Another option would be the recording by participants of short oral summaries of the meeting. Summaries could be used to bootstrap for queries  the exact nature of which remained nebulous. Candidate types are keyword searches  action items  elaboration on points of interest  and agreement between participants. An initial prototype system to test any hypotheses can be pipelined. The recorded data will be stored on CD-ROM's and sent to IBM for transcription. There is also work being done on the annotation of prosody. The corpus could be enriched with found data (public or collected by other projects)  if those prove appropriate for use in the project. Finally  project web pages and mailing list are being set up and UW are going to investigate the suitability of their recording equipment. Within the piloting of data collection ideas  it was decided that CrossPads are going to be used for detection of ""hot points"" during the meeting. Other ideas to be tested are the use of summaries or minutes -if a group normally produce them- and the recording of oral summaries by individual participants after the meeting. Photographing the contents of the board and the positions of the meeting participants will provide extra information. For query generation purposes  all participants will also be asked for their highlight of the meeting. The general goal regarding the corpus is to investigate the acquisition of further appropriate data through public sources or available collections of other institutions. As to recordings at ICSI  the group agreed that imposing rules of participation  in order to avoid speaker overlaps was not desirable. Instead  they will aim for collecting stylistically varied data (different group dynamics and types of meeting). Further action will also be taken to close other pending issues: the web pages will be organised  the recording room will be finalised and UW will also test their recording infrastructure. The recording of meetings and any possible additional tasks must be set up in a user-friendly way  otherwise it would be difficult to recruit volunteers. Asking participants to do more than they normally would in a meeting could put people off. The video-recording of meetings  apart from adding an extra level of instrumentation complexity  can also make people apprehensive. The usability and usefulness of CrossPads is not certain. Acquiring data from other sources will not be straightforward  as they may either not be suitable for this project or not publicly available. Querying is also a major issue: what users would ask from a system is not clear. How this system would resolve high-level queries (eg regarding agreement between participants) is also hard to tell at this stage. As transcription has not started yet  there was concern as to how IBM will deal with multi-channel data. The abundance of speaker overlaps may also affect the quality of the trascription. However  it was accepted that some problems with the transcription of jargon are  to an extent  unavoidable. Five hours of recorded pilot data are already available. IBM  who will be carrying out the transcribing  have been emailed the URL's for the online data set-up and for the transcribing tool. Some work has already been done with the annotation of speech features like prosody. "
"Alright. We're on. Test  um. Test  test  test. Guess that's me. Yeah. O_K. Ooh  Thursday. So. There's two sheets of paper in front of us. Yeah. So. What are these? This is the arm wrestling? Uh. Yeah  we formed a coalition actually. We already made it into one. Yeah. Almost. Oh  good. Excellent. Yeah. Yeah. That's the best thing. Mm-hmm. So  tell me about it. So it's - well  it's spectral subtraction or Wiener filtering  um  depending on if we put - if we square the transfer function or not. Right. And then with over-estimation of the noise  depending on the  uh - the S_N_R  with smoothing along time  um  smoothing along frequency. It's very simple  smoothing things. Mm-hmm. Mm-hmm. And  um  the best result is when we apply this procedure on F_F_T bins  uh  with a Wiener filter. Mm-hmm. And there is no noise addition after - after that. O_K. So it's good because it's difficult when we have to add noise to - to - to find the right level. O_K. Are you looking at one in - in particular of these two? Yeah. So the sh- it's the sheet that gives fifty-f- three point sixty-six. Mm-hmm. Um  the second sheet is abo- uh  about the same. It's the same  um  idea but it's working on mel bands  and it's a spectral subtraction instead of Wiener filter  and there is also a noise addition after  uh  cleaning up the mel bins. Mmm. Well  the results are similar. Yeah. I mean  it's - it's actually  uh  Mm-hmm. very similar. I mean  if you look at databases  uh  the  uh  one that has the smallest - smaller overall number is actually better on the Finnish and Spanish  uh  but it is  uh  It's worse on - worse on the  uh  Aurora - I mean on the  uh  T_I- T_I-digits  uh  uh. on the multi-condition in T_I-digits. Yeah. Um. Mmm. So  it probably doesn't matter that much either way. Yeah. But  um  when you say u- uh  unified do you mean  uh  it's one piece of software now  or - ? So now we are  yeah  setting up the software. Mm-hmm. Um  it should be ready  uh  very soon. Um  and we- So what's - what's happened? I think I've missed something. O_K. So a week ago - maybe you weren't around when - when - when Hynek and Guenther and I - ? @@ Hynek was here. Yeah. I didn't. Oh  O_K. So - Yeah  let's summarize. Um - And then if I summarize somebody can tell me if I'm wrong  which will also be possibly helpful. What did I just press here? I hope this is still working. Um. p-p-p- @@ We  uh - we looked at  uh - anyway we - after coming back from QualComm we had  you know  very strong feedback and  uh  I think it was Hynek and Guenter's and my opinion also that  um  you know  we sort of spread out to look at a number of different ways of doing noise suppression. But given the limited time  Mm-hmm. uh  it was sort of time to choose one. Mmm. Uh  and so  uh  th- the vector Taylor series hadn't really worked out that much. Uh  the subspace stuff  uh  had not been worked with so much. Um  so it sort of came down to spectral subtraction versus Wiener filtering. Hmm. Uh  we had a long discussion about how they were the same and how they were d- uh  completely different. Mm-hmm. And  uh  I mean  fundamentally they're the same sort of thing but the math is a little different so that there's a - a - there's an exponent difference in the index - you know  what's the ideal filtering  and depending on how you construct the problem. And  uh  I guess it's sort - you know  after - after that meeting it sort of made more sense to me because Uh-huh. um  if you're dealing with power spectra then how are you gonna choose your error? And typically you'll do - choose something like a variance. And so that means it'll be something like the square of the power spectra. Mm-hmm. Whereas when you're - when you're doing the - the  uh  um  looking at it the other way  you're gonna be dealing with signals and you're gonna end up looking at power - uh  noise power that you're trying to reduce. And so  eh - so there should be a difference of - you know  conceptually of - of  uh  a factor of two in the exponent. Mm-hmm. But there're so many different little factors that you adjust in terms of - of  uh  uh  over-subtraction and - and - and - and - and so forth  um  that arguably  you're c- and - and - and the choice of do you - do you operate on the mel bands or do you operate on the F_F_T beforehand. There're so many other choices to make that are - are almost - well  if not independent  certainly in addition to the choice of whether you  uh  do spectral subtraction or Wiener filtering  that  um  @@ again we sort of felt the gang should just sort of figure out which it is they wanna do and then let's pick it  go forward with it. So that's - that was - that was last week. And - and  uh  we said  uh  take a week  go arm wrestle  you know  Oh. figure it out. I mean  and th- the joke there was that each of them had specialized in one of them. And - and so they - Oh  O_K. so instead they went to Yosemite and bonded  and - and they came out with a single - single piece of software. So it's another - another victory for international collaboration. So. Uh. So - so you guys have combined - or you're going to be combining the software? Oh boy. Well  the piece of software has  like  plenty of options  like you can parse command-line arguments. So depending on that  it - it becomes either spectral subtraction or Wiener filtering. Oh  O_K. They're close enough. So  ye- Well  that's fine  but the thing is - the important thing is that there is a piece of software that you - that we all will be using now. Yes. Yeah. Yeah. There's just one piece of software. Yeah. Yeah. I need to allow it to do everything and even more - more than this. Well  if we want to  Right. like  optimize different parameters of - Parameters. Yeah. Yeah  we can do it later. Sure. But  still - so  there will be a piece of software with  uh  will give this system  the fifty-three point sixty-six  by default and - Mm-hmm. How - how is - how good is that? Mm-hmm. I - I - I don't have a sense of - It's just one percent off of the best proposal. @@ Best system. It's between - i- we are second actually if we take this system. Right? Yeah. Yeah. O_K. Compared to the last evaluation numbers? Yeah. But  uh - Mm-hmm. Yeah. Yeah. w- which we sort of were before but we were considerably far behind. And the thing is  this doesn't have neural net in yet for instance. You know? Mm-hmm. So it - so  um  it's - it- it's not using our full bal- bag of tricks  if you will. Hmm. Mm-hmm. And  uh  and it - it is  uh  very close in performance to the best thing that was there before. Uh  but  you know  looking at it another way  maybe more importantly  uh  we didn't have any explicit noise  uh  handling - stationary - dealing with - e- e- we didn't explicitly have anything to deal with stationary noise. Mm-hmm. And now we do. So will the neural net operate on the output from either the Wiener filtering or the spectral subtraction? Well  so - so - so argu- arguably  I mean  what we should do - I mean  I gather you have - Or will it operate on the original? it sounds like you have a few more days of - of nailing things down with the software and so on. But - and then - but  um  arguably what we should do is  even though the software can do many things  we should for now pick a set of things  th- these things I would guess  Mm-hmm. and not change that. And then focus on everything that's left. And I think  you know  that our goal should be by next week  when Hynek comes back  uh  to - uh  really just to have a firm path  uh  for the - you know  for the time he's gone  of - of  uh  what things will be attacked. But I would - I would - I would thought- think that what we would wanna do is not futz with this stuff for a while because what'll happen is we'll change many other things in the system  Mm-hmm. and then we'll probably wanna come back to this and possibly make some other choices. But  um. But just conceptually  where does the neural net go? Do - do you wanna h- run it on the output of the spectrally subtracted - ? Mmm. Well  depending on its size - Well  one question is  is it on the  um  server side or is it on the terminal side? Uh  if it's on the server side  it - you probably don't have to worry too much about size. Mm-hmm. So that's kind of an argument for that. We do still  however  have to consider its latency. So the issue is - is  um  for instance  could we have a neural net that only looked at the past? Right. Um  what we've done in uh - in the past is to use the neural net  uh  to transform  um  all of the features that we use. So this is done early on. This is essentially  um  um - I guess it's - it's more or less like a spee- a speech enhancement technique here - right? - where we're just kind of creating Mm-hmm. new - if not new speech at least new - new F_F_T's that - that have - you know  which could be turned into speech - Mm-hmm. uh  that - that have some of the noise removed. Mm-hmm. Um  after that we still do a mess of other things to - to produce a bunch of features. Right. And then those features are not now currently transformed by the neural net. And then the - the way that we had it in our proposal-two before  we had the neural net transformed features and we had the untransformed features  which I guess you - you actually did linearly transform with the K_L_T  but - but - but - uh  to orthogonalize them - but - Yeah. Yeah. Right. but they were not  uh  processed through a neural net. And Stephane's idea with that  as I recall  was that you'd have one part of the feature vector that was very discriminant and another part that wasn't  Mm-hmm. uh  which would smooth things a bit for those occasions when  uh  the testing set was quite different than what you'd trained your discriminant features for. So  um  all of that is - is  uh - still seems like a good idea. The thing is now we know some other constraints. We can't have unlimited amounts of latency. Uh  y- you know  that's still being debated by the - by people in Europe but  uh  no matter how they end up there  it's not going to be unlimited amounts  so we have to be a little conscious of that. Yeah. Um. So there's the neural net issue. There's the V_A_D issue. And  uh  there's the second stream thing. And I think those that we - last time we agreed that those are the three things that have to get  uh  focused on. What was the issue with the V_A_D? Well  better ones are good. And so the w- the default  uh  boundaries that they provide are - they're O_K  but they're not all that great? I guess they still allow two hundred milliseconds on either side or some- ? Is that what the deal is? Mm-hmm. Uh  so th- um  they keep two hundred milliseconds at the beginning and end of speech. And they keep all the - Yeah. Outside the beginnings and end. Uh-huh. And all the speech pauses  which is - Sometimes on the SpeechDat-Car you have pauses that are more than one or two seconds. Wow. More than one second for sure. Um. Hmm. Yeah. And  yeah  it seems to us that this way of just dropping the beginning and end is not - We cou- we can do better  I think  Mm-hmm. because  um  with this way of dropping the frames they improve over the baseline by fourteen percent and Sunil already showed that with our current V_A_D we can improve by more than twenty percent. On top of the V_A_D that they provide? No. @@ Just using either their V_A_D or Our way. our current V_A_D. Oh  O_K. So  our current V_A_D is - is more than twenty percent  while Theirs is fourteen? their is fourteen. Yeah. I see. Huh. So. Yeah. And another thing that we did also is that we have all this training data for - let's say  for SpeechDat-Car. We have channel zero which is clean  channel one which is far-field microphone. And if we just take only the  um  V_A_D probabilities computed on the clean signal and apply them on the far-field  uh  test utterances  Mm-hmm. then results are much better. In some cases it divides the error rate by two. Wow. So it means that there are stim- still - If - if we can have a good V_A_D  well  it would be great. How - how much latency does the  uh - does our V_A_D add? Is it significant  or - ? Uh  right now it's  um  a neural net with nine frames. So it's forty milliseconds plus  um  the rank ordering  which  uh  should be Like another ten frames. ten - Yeah. Rank. Oh. So  right now it's one hundred and forty milliseconds. With the rank ordering - ? I'm sorry. The - the - the smoothing - the m- the - the filtering of the probabilities. The - The  um - on the R_ . Yeah. It's not a median filtering. It's just - We don't take the median value. We take something - Um  so we have eleven  um  frames. And - for the V_A_D  yeah - and we take th- the third. Oh  this is for the V_A_D. Yeah. Yeah. Oh  O_K. Yeah. Dar- Um. Yeah. Um. Mmm. So - Yeah  I was just noticing on this that it makes reference to delay. So what's the - ? If you ignore - Um  the V_A_D is sort of in - in parallel  isn't i- isn't it  with - with the - ? I mean  it isn't additive with the - the  uh  L_D_A and the Wiener filtering  and so forth. Right? The L_D_A? Yeah. So - so what happened right now  we removed the delay of the L_D_A. Mm-hmm. Yeah. So we - I mean  if - so if we - if - so which is like if we reduce the delay of V_A- So  the f- the final delay's now ba- is f- determined by the delay of the V_A_D  because the L_D_A doesn't have any delay. So if we re- if we reduce the delay of the V_A_D  I mean  it's like effectively reducing the delay. How - how much  uh  delay was there on the L_D_A? So the L_D_A and the V_A_D both had a hundred millisecond delay. So and they were in parallel  so which means you pick either one of them - the - the biggest  whatever. Mmm. Mm-hmm. I see. So  right now the L_D_A delays are more. And there - Oh  O_K. And there didn't seem to be any  uh  penalty for that? Pardon? There didn't seem to be any penalty for making it causal? Oh  no. It actually made it  like  point one percent better or something  actually. Or something like that and - O_K. Well  may as well  then. And he says Wiener filter is - is forty milliseconds delay. Yeah. So that's the one which Stephane was discussing  like - Mmm. So is it - ? The smoothing? Yeah. The - you smooth it and then delay the decision by - So. Right. O_K. So that's - that's really not - not bad. So we may in fact - we'll see what they decide. We may in fact have  um  the - the  uh  latency time available for - to have a neural net. I mean  sounds like we probably will. So. Mm-hmm. That'd be good. Cuz I - cuz it certainly always helped us before. So. Uh. What amount of latency are you thinking about when you say that? Well  they're - you know  they're disputing it. You know  they're saying  uh - one group is saying a hundred and thirty milliseconds and another group is saying two hundred and fifty milliseconds. Mmm. Two hundred and fifty is what it was before actually. So  uh  some people are lobbying - lobbying to make it shorter. Oh. Hmm. Um. And  um. Were you thinking of the two-fifty or the one-thirty when you said we should have enough for the neural net? Well  it just - it - when we find that out it might change exactly how we do it  is all. I mean  how much effort do we put into making it causal? I mean  Oh  O_K. I think the neural net will probably do better if it looks at a little bit of the future. Mm-hmm. But  um  it will probably work to some extent to look only at the past. And we ha- you know  limited machine and human time  and effort. And  you know  how - how much time should we put into - into that? So it'd be helpful if we find out from the - the standards folks whether  you know  they're gonna restrict that or not. Mm-hmm. Um. But I think  you know  at this point our major concern is making the performance better and - and  um  if  uh  something has to take a little longer in latency in order to do it that's you know  a secondary issue. Mm-hmm. But if we get told otherwise then  you know  we may have to c- clamp down a bit more. Mmm. So  the one - one - one difference is that - was there is like we tried computing the delta and then doing the frame-dropping. S- Mm-hmm. The earlier system was do the frame-dropping and then compute the delta on the - Uh-huh. Ah. So this - Which could be a kind of a funny delta. Right? Yeah. Oh  oh. So that's fixed in this. Yeah  we talked about that. Yeah. Yeah. Uh-huh. So we have no delta. And then - So the frame-dropping is the last thing that we do. Good. So  yeah  what we do is we compute the silence probability  convert it to that binary flag  and then in the end you c- up- upsample it to Uh-huh. Mm-hmm. match the final features number of - Did that help then? It seems to be helping on the well-matched condition. So that's why this improvement I got from the last result. So. And it actually r- reduced a little bit on the high mismatch  so in the final weightage it's b- b- better because the well-matched is still weighted more than - So  @@ I mean  you were doing a lot of changes. Did you happen to notice how much  uh  the change was due to just this frame-dropping problem? What about this? Uh  y- you had something on it. Right? Just the frame-dropping problem. Yeah. But it's - it's difficult. Sometime we - we change two - two things together and - But it's around maybe - it's less than one percent. Uh-huh. Yeah. Well. It - But like we're saying  if there's four or five things like that then pretty sho- soon you're talking real improvement. @@ Yeah. Yeah. Yeah. And it - Yeah. And then we have to be careful with that also - with the neural net because in the proposal the neural net was also  uh  working on - after frame-dropping. Mm-hmm. Um. Oh  that's a real good point. So. Well  we'll have to be - to do the same kind of correction. It might be hard if it's at the server side. Right? Mmm. Well  we can do the frame-dropping on the server side or we can just be careful at the terminal side to send a couple of more frames before and after  and - So. I think it's O_K. O_K. You have  um - So when you - Uh  maybe I don't quite understand how this works  but  um  couldn't you just send all of the frames  but mark the ones that are supposed to be dropped? Cuz you have a bunch more bandwidth. Right? Well  you could. Yeah. I mean  it - it always seemed to us that it would be kind of nice to - in addition to  uh  reducing insertions  actually use up less bandwidth. But nobody seems to have Yeah. Yeah. cared about that in this evaluation. So. And that way the net could use - If the net's on the server side then it could use all of the frames. Yes  it could be. It's  like  you mean you just transferred everything and then finally drop the frames after the neural net. Right? Mm-hmm. Mm-hmm. Yeah. That's - that's one thing which - But you could even mark them  Yeah. Right now we are - before they get to the server. Uh  ri- Right now what - wha- what we did is  like  we just mark - we just have this additional bit which goes around the features  Ah. saying it's currently a - it's a speech or a nonspeech. Oh  O_K. So there is no frame-dropping till the final features  like  including the deltas are computed. I see. And after the deltas are computed  you just pick up the ones that are marked silence and then drop them. Mm-hmm. I see. So it would be more or less the same thing with the neural net  I guess  actually. I see. Mm-hmm. So. Yeah  that's what - that's what - that's what  uh  this is doing right now. I see. O_K. Yeah. Mm-hmm. Um. O_K. So  uh  what's  uh - ? That's - that's a good set of work that - that  uh - Just one more thing. Like  should we do something f- more for the noise estimation  because we still - ? @@ Yeah. I was wondering about that. That was - I - I had written that down there. Yeah. Mm-hmm. Um - So  we  uh - actually I did the first experiment. This is with just fifteen frames. Um. We take the first fifteen frame of each utterance to it  and average their power spectra. Yeah. Um. I tried just plugging the  um  uh  Guenter noise estimation on this system  and it - uh  it got worse. Um  but of course I didn't play with it. But - Mm-hmm. Uh-huh. Uh  I didn't do much more for noise estimation. I just tried this  and - Hmm. Yeah. Well  it's not surprising it'd be worse the first time. But  um  Mm-hmm. it does seem like  you know  i- i- i- i- some compromise between always depending on the first fifteen frames and a- a- always depending on a - a pause is - is - is a good idea. Uh  maybe you have to weight the estimate from the first -teen - fifteen frames more heavily than - than was done in your first attempt. But - Mm-hmm. but - Yeah  I guess. Yeah. Um. No  I mean - Um  do you have any way of assessing how well or how poorly the noise estimation is currently doing? Mmm. No  we don't. Yeah. We don't have nothing that - Is there - was there any experiment with - ? Well  I - I did - The only experiment where I tried was I used the channel zero VAD for the noise estimation and frame-dropping. So I don't have a - Yeah. I don't have a split  like which one helped more. So. It - it was the best result I could get. Mm-hmm. So  that's the - So that's something you could do with  um  this final system. Right? Just do this - everything that is in this final system except  uh  use the channel zero. Mm-hmm. For the noise estimation. Yeah. We can try something. Yeah. And then see how much better it gets. Mm-hmm. Sure. If it's  you know  essentially not better  then it's probably not worth Yeah. any more. Yeah. But the Guenter's argument is slightly different. It's  like  ev- even - even if I use a channel zero VAD  I'm just averaging the - the s- power spectrum. But the Guenter's argument is  like  if it is a non-stationary segment  then he doesn't update the noise spectrum. So he's  like - he tries to capture only the stationary part in it. So the averaging is  like  different from updating the noise spectrum only during stationary segments. So  th- the Guenter was arguing that  I mean  even if you have a very good V_A_D  averaging it  like  over the whole thing is not a good idea. Because you're averaging the stationary and the non-stationary  and finally you end up getting something I see. which is not really the s- because  you - anyway  you can't remove the stationary part fr- I mean  non-stationary part from the signal. So - Not using these methods anyway. Yeah. Yeah. So you just update only doing - or update only the stationary components. Yeah. So  that's - so that's still a slight difference from what Guenter is trying in - Well  yeah. And - and also there's just the fact that  um  eh  uh  although we're trying to do very well on this evaluation  um  we actually would like to have something that worked well in general. Yeah  yeah. And  um  relying on having fifteen frames at the front or something is - is pretty - I mean  you might  you might not. Mmm. Mm-hmm. So  um. Um  it'd certainly be more robust to different kinds of input if you had at least some updates. Um. Mm-hmm. But  um. Well  I don't know. What - what do you  uh - what do you guys see as - as being what you would be doing in the next week  given wha- what's happened? Cure the VAD? Yeah. What was that? @@ V_A_D. Oh. And - Oh - O_K. So  should we keep the same - ? I think we might try to keep the same idea of having a neural network  but training it on more data and adding better features  I think  but - because the current network is just P_L_P features. Well  it's trained on noisy P_L_P - Just the cepstra. Yeah. P_L_P features computed on noisy speech. But there is no- nothing particularly robust in these features. No. So  I- I- uh - There's no RASTA  no - So  uh  I - I don't remember what you said the answer to my  uh  question earlier. Will you - will you train the net on - after you've done the spectral subtraction or the Wiener filtering? This is a different net. Oh. So we have a V_A_D which is like neur- that's a neural net. Oh  yeah. Hmm. Oh  you're talking about the V_A_D net. O_K. I see. Yeah. Mm-hmm. So that - that V_A_D was trained on the noisy features. Mm-hmm. So  right now we have  like  uh - we have the cleaned-up features  so we can have a better Mm-hmm. V_A_D by training the net on the cleaned-up speech. I see. I see. Yeah  but we need a V_A_D for uh noise estimation also. So it's  like  where do we want to put the V_A_D? Uh  it's like - Can you use the same net to do both  or - ? For - Can you use the same net that you - that I was talking about to do the V_A_D? Mm-hmm. Uh  it actually comes at v- at the very end. So the net - the final net - I mean  which is the feature net - Mm-hmm. so that actually comes after a chain of  like  L_D_A plus everything. So it's  like  it takes a long time to get a decision out of it. And - and you can actually do it for final frame-dropping  but Mm-hmm. not for the V_A- f- noise estimation. You see  the idea is that the  um  initial decision to - that - that you're in silence or speech happens pretty quickly. Oh  O_K. Cuz that's used by some of these other - ? Oh  O_K. Hmm. And that - Yeah. And that's sort of fed forward  and - and you say ""well  flush everything  it's not speech anymore"". I see. Yeah. I thought that was only used for doing frame-dropping later on. Um  it is used  uh - Yeah  it's only used f- Well  it's used for frame-dropping. Um  it's used for end of utterance because  you know  there's - Mmm. if you have more than five hundred milliseconds of - of - of nonspeech then you figure it's end of utterance or something like that. So  Mm-hmm. um. And it seems important for  like  the on-line normalization. Um. We don't want to update the mean and variance during silen- long silence portions. Um. So it - it has to be done before Oh. I see. this mean and variance normalization. Um. Um. Yeah. So probably the V_A_D and - and maybe testing out the noise estimation a little bit. I mean  keeping the same method but - but  uh  seeing if you cou- but  um noise estimation could be improved. Mm-hmm. Those are sort of related issues. It probably makes sense to move from there. And then  uh  later on in the month I think we wanna start including the neural net at the end. Um. O_K. Anything else? The Half Dome was great. Good. Yeah. You didn't - didn't fall. That's good. Well  yeah. Our e- our effort would have been devastated if you guys had run into problems. So  Hynek is coming back next week  you said? Yeah  that's the plan. Hmm. @@ I guess the week after he'll be  uh  going back to Europe  and so we wanna - Is he in Europe right now or is he up at - ? Oh. No  no. He's - he's - he's dropped into the U_S. Yeah. Yeah. Hmm. So. Uh. So  uh. Uh  the idea was that  uh  we'd - we'd sort out where we were going next with this - with this work before he  uh  left on this next trip. Good. Uh  Barry  you just got through your quals  so I don't know if you have much to say. But  uh. Mmm. No  just  uh  looking into some - some of the things that  um  uh  John Ohala and Hynek  um  gave as feedback  um  as - as a starting point for the project. Um. In - in my proposal  I - I was thinking about starting from a set of  uh  phonological features  or a subset of them. Um  but that might not be necessarily a good idea according to  um  John. @@ Mm-hmm. He said  uh  um  these - these phonological features are - are sort of figments of imagination also. Um. S- Mm-hmm. In conversational speech in particular. I think you can - you can put them in pretty reliably in synthetic speech. But Ye- we don't have too much trouble recognizing synthetic speech since we create it in the first place. So  it's - Right. Yeah. So  um  a better way would be something more - more data-driven  just looking at the data and seeing what's similar and what's not similar. Mm-hmm. Mm-hmm. So  I'm - I'm  um  taking a look at some of  um  Sangita's work on - on TRAPS. She did something where  um - w- where the TRAPS learn- She clustered the - the temporal patterns of  um  certain - certain phonemes in - in m- averaged over many  many contexts. And  uh  some things tended to cluster. Mm-hmm. Right? You know  like stop - stop consonants clustered really well. Hmm. Um  silence was by its own self. And  uh  um  v- vocalic was clustered. And  Mm-hmm. Mm-hmm. um  so  those are interesting things to - So you're - now you're sort of looking to try to gather a set of these types of features? Right. Yeah. Just to Mm-hmm. see where - where I could start off from  uh  you know? Mm-hmm. A - a - a set of small features and continue to iterate and find  uh  a better set. Mm-hmm. Yeah. O_K. Well  short meeting. That's O_K. Yeah. O_K. So next week hopefully we'll - can get Hynek here to - to join us and  uh  uh. Digits  digits. Should we do digits? O_K  now . Go ahead  Morgan. You can start. Alright. Let me get my glasses on so I can see them. O_K. Transcript L_ dash three two seven. Eight two one  zero six  seven four zero zero. Eight zero zero one  four one seven six  one two eight one. Six three zero  two two four  one nine one two. Six five zero  eight six nine  four six two four. Eight nine one nine  one  four eight five. Six eight  four five  three eight  eight three  eight zero. Four five  one eight  eight two  nine one  three five. Zero  two three four  four four  eight one two  two. Transcript L_ dash three two eight. Nine nine zero  six zero  three nine five five. Nin- nine eight four  three one  six five three four. Two four three  one one four  four one six six. Four one  nine four  three three  seven six  five five. Five zero five  seven five four  zero seven five. Six six three zero  five  four eight seven. Seven zero one  eight one two  eight three one. Five seven  three four  eight seven  zero three  six eight. Transcript L_ dash three two nine. Nine nine seven  seven three zero  three six eight. Seven six two  seven one seven  zero nine nine six. Nine  three eight eight  nine six  nine eight seven  nine. Two  one two six  nine three  seven two zero  six. Six seven two  three zero eight  nine four nine. Eight  zero three two  six three  one four eight  nine. Six  four four four  two  six six nine. Three one eight  seven nine one  three two four seven. Transcript L_ dash three three zero. Two three six zero  nine  five nine three. Five four six  three four eight  six six seven five. Three seven zero four  three eight four four  eight six three six. Three six seven five  eight seven zero five  five seven three nine. One five zero  nine zero  one six two two. Four zero nine  two seven seven  seven zero one. Nine  two zero six  eight nine  six seven six  zero. One six four  one nine one  two four two eight. Transcript L_ dash three three one. Three seven eight eight  nine  one zero zero. Six two three  two seven  zero three eight five. One three eight  one eight  three seven nine five. Zero nine six  seven six one  zero one nine seven. Five four one two  four zero  four one  five six one two. Three zero nine one  six zero five seven  six five two eight. Three four one  one six four  seven three nine. Four five two eight  eight  five O_ seven. O_K. And we're off. Mm- ","Although the Meeting Recorder group only list two agenda items  this meeting explores transcription  and in particular  consent forms in depth  and at times results in heated debate. With regard to obtaining consent  the group discuss the extent to which they need to attempt to contact people  which methods are most appropriate  and how much responsibility rests on participants being available and checking their e-mail regularly. The group suggest sending reminder e-mails  although since many participants are local they can be contacted by other means if necessary. Transcriptions are back from IBM  and the group discuss the checking of these  particularly since the pre-segmenter has interfered with back-channel data. Checking of the NSA meetings has revealed that this non-native English meeting data contains transcription inaccuracies due to the use of foreign language terms and technical vocabulary. Additional topics covered more briefly in this meeting are disk space  the DARPA annual report  progress with the demo  conference submissions and attendance  and requests from the University of Washington for data. The group discuss whether this meeting will relate to either meeting recorder or speech recognition issues. They decide that covering such topics over alternate weeks will commence at the next meeting  although other topics will be discussed if time allows. The group decide that it would be good to set a date for having the non-native network services group data  and one or two weeks before the 15th of July is suggested. With regard to contacting participants to request consent  the group decide that no signature is required  and an e-mail would be enough. However for ""boundary cases"" legal advice would be sought. As soon as the next set of data is ready for checking  participants should be contacted  so that this process is on-going. When the deadline for giving consent is approaching  a reminder e-mail should be sent out. In cases where no consent response is given  participants could be chased up since many are local. Original uncensored copies of meetings will be kept  with all of the old signal deleted and replaced with new when censoring. Gaining consent from participants for the use of the meeting data is raising a number of issues for the group  some of which may have legal implications. Firstly a relatively arbitrary deadline of 15 July has been set  and since this is during the summer break  the group debate whether enough action has been made to contact participants. If people don't respond in time  the group discuss what facility should there be for later amendments  and whether a meeting can be used if they don't respond at all. Checking of the NSA meeting transcripts has shown that although acoustically fine  some errors have shown up  especially relating to foreign language terms and jargon or technical terminology. Additionally  the discussions reveal that there is a shortage of headphones amongst group members  and also that disk space is in short supply  especially if original copies of edited transcripts are retained. Progress has been made regarding gaining consent from participants to publish the meeting data. E-mails were sent out to request that the transcripts are checked  and corrected or censored  in time for the data to be used in the DARPA meeting in July. Transcriptions are back from IBM  although there was a small problem  this was simple to fix. They have not yet been checked to see whether they are correct  however  back-channels appear to be missed as these were not caught by the pre-segementer. The University of Washington has been in touch with the group requesting audio files and transcripts  and also  new headphones have been purchased for the transcribers which are much better than the previous ones. Disk space is again filling up fast  and a new 100 gig hard drive will soon be available. As  a temporary measure  the group will use up disk space on each other's machines. The annual report for DARPA will be written over the next week based on project status information; The DARPA demo is ongoing  with automatic alignment and tighter boundaries to be investigated. A conference abstract has been accepted. "
"O_K. Uh. Somebody else should run this. I'm sick of being the one to sort of go through and say  ""Well  what do you think about this?"" You wanna - ? Should we take turns? You want me to run it today? Yeah. Yeah. Why don't you run it today? O_K. O_K. O_K. Um. Let's see  maybe we should just get a list of items - things that we should talk about. Um  I guess there's the usual updates  everybody going around and saying  uh  you know  what they're working on  the things that happened the last week. But aside from that is there anything in particular that anybody wants to bring up Mmm. for today? No? O_K. So why don't we just around and people can give updates. Uh  do you want to start  Stephane? Oh. Alright. Um. Well  the first thing maybe is that the p- Eurospeech paper is  uh  accepted. Um. Yeah. This is - what - what do you  uh - what's in the paper there? So it's the paper that describe basically the  um  system that were proposed for the Aurora. The one that we s- we submitted the last round? Right  yeah. Uh-huh. Yeah. Um - Yeah. So and the  fff comments seems - from the reviewer are good. So. Hmm. Mmm - Yeah. Where - where's it gonna be this year? It's  uh  Aalborg in Denmark. And it's  yeah  Oh  O_K. September. Mmm. Mmm - Yeah. Then  uh  whhh well  I've been working on - on t- mainly on on-line normalization this week. Uh  I've been trying different - slightly - slightly different approaches. Um  the first thing is trying to play a little bit again with the  um  time constant. Uh  second thing is  uh  the training of  uh  on-line normalization with two different means  one mean for the silence and one for the speech. Um  and so I have two recursions which are controlled by the  um  probability of the voice activity detector. Mmm. This actually don't s- doesn't seem to help  although it doesn't hurt. So. But - well  both on-line normalization approach seems equivalent. Are the means pretty different for the two? Well  they - Yeah. They can be very different. Yeah. Mm-hmm. Hmm. So do you maybe make errors in different places? Different kinds of errors? I didn't look  uh  more closely. Um. It might be  yeah. Mm-hmm. @@ Um. Well  eh  there is one thing that we can observe  is that the mean are more different for - for C_zero and C_one than for the other coefficients. And - Yeah. And - Yeah  it - the C_one is - There are strange - strange thing happening with C_one  is that when you have different kind of noises  the mean for the - the silence portion is - can be different. Hmm. And - So when you look at the trajectory of C_one  it's - has a strange shape and I was expecting th- the s- that these two mean helps  especially because of the - the strange C_ze- C_one shape  uh  which can - like  yo- you can have  um  a trajectory for the speech and then when you are in the silence it goes somewhere  but if the noise is different it goes somewhere else. Oh. So which would mean that if we estimate the mean based on all the signal  even though we have frame dropping  but we don't frame ev- uh  drop everything  but - uh  this can - hurts the estimation of the mean for speech  and - Mmm. But I still have to investigate further  I think. Um  a third thing is  um  that instead of t- having a fixed time constant  I try to have a time constant that's smaller at the beginning of the utterances to adapt more quickly to the r- something that's closer to the right mean. T- t- um - Yeah. And then this time constant increases and I have a threshold that - Mm-hmm. well  if it's higher than a certain threshold  I keep it to this threshold to still  uh  adapt  um  the mean when - Mm-hmm. if the utterance is  uh  long enough to - to continue to adapt after  like  one second or - Mm-hmm. Mmm. Uh  well  this doesn't help neither  but this doesn't hurt. So  well. It seems pretty - Wasn't there some experiment you were gonna try where you did something differently for each  um  uh - I don't know whether it was each mel band or each  uh  um  F_F_T bin or someth- There was something you were gonna - uh  some parameter you were gonna vary depending on the frequency. I don't know if that was - I guess it was - I don't know. No. u- Maybe it's this - this idea of having different on-line normalization  um  tunings for the different M_F_C_C's. For each  uh - Mm-hmm. But - Mm-hmm. Yeah. I - I thought  Morgan  you brought it up a couple meetings ago. And then it was something about  uh  some- and then somebody said ""yeah  it does seem like  you know  C_zero is the one that's  you know  the major one"" or  uh  s- I can't remember exactly what it was now. Mmm. Yeah. There - uh  actually  yeah. S- um  it's very important to normalize C_zero and much less to normalize the other coefficients. And  um  actu- uh  well  at least with the current on-line normalization scheme. And we - I think  we kind of know that normalizing C_one doesn't help with the current scheme. And - and - Yeah. In my idea  I - I was thinking that the - the - the reason is maybe because of these funny things that happen between speech and silence which have different means. Um - Yeah. But maybe it's not so - so easy to - Um  I- I really would like to suggest looking  um  a little bit at the kinds of errors. I know you can get lost in that and go forever and not see too much  but - Mm-hmm. sometimes  but - but  um  just seeing that each of these things didn't make things better may not be enough. It may be that they're making them better in some ways and worse in others  or increasing insertions and decreasing deletions  or - Yeah. Mm-hmm. or  um  um  you know  helping with noisy case but hurting in quiet case. And if you saw that then maybe you - it would - something would occur to you of how to deal with that. Mm-hmm. Mm-hmm. Hmm. Alright. Mmm. Yeah. W- um  So that's it  I think  for the on-line normalization. Um - Yeah. I've been playing a little bit with some kind of thresholding  and  mmm  as a first experiment  I think I- Yeah. Well  what I did is t- is to take  um - to measure the average - no  the maximum energy of s- each utterance and then put a threshold - Well  this for each mel band. Then put a threshold that's fifteen D_B below - well  uh  a couple of D_B below this maximum  and - Mm-hmm. Mmm. Actually it was not a threshold  it was just adding noise. Mm-hmm. So I was adding a white noise energy  uh  that's fifteen D_B below the maximum energy of the utterance. And - Yeah. When we look at - at the  um  M_F_C_C that result from this  they are a lot more smoother. Um  when we compare  like  a channel zero and channel one utterance - um  so a clean and  uh  the same noisy utterance - well  there is almost no difference between the cepstral coefficients of the two. Hmm. Um. And - Yeah. And the result that we have in term of speech recognition  actually it's not - it's not worse  it's not better neither  Hmm. but it's  um  kind of surprising that it's not worse because Sorry. basically you add noise that's fifteen D_B - just fifteen D_B below the maximum energy. And So why does that m- smooth things out? I don't - I don't understand that. at least - Well  there's less difference. Right? Cuz it's - It's - I think  it's whitening - This - the portion that are more silent  as you add a white noise that are - has a very high energy  it whitens everything and - Huh. Oh  O_K. and the high-energy portion of the speech don't get much affected anyway by the other noise. And as the noise you add is the same is - the shape  it's also the same. Hmm. Yeah. So they have - the trajectory are very  very similar. And - and - So  I mean  again  if you trained in one kind of noise and tested in the same kind of noise  you'd - you know  given enough training data you don't do b- do badly. The reason that we d- that we have the problems we have is because it's different in training and test. Even if the general kind is the same  the exact instances are different. And - and Mm-hmm. so when you whiten it  then it's like you - the - the only noise - to - to first order  the only th- noise that you have is white noise and you've added the same thing to training and test. So it's  Mm-hmm. Hmm. uh - So would that be similar to  like  doing the smoothing  then  over time or - ? Mm-hmm. Well  it's a kind of smoothing  but - I think it's - I think it's different. It's - it's something that - yeah  that affects more or less the silence portions because - Mm-hmm. Well  anyway  the sp- the portion of speech that ha- have high energy are not ch- Mm-hmm. a lot affected by the noises in the Aurora database. If - if you compare th- the two shut channels of SpeechDat-Car during speech portion  it's n- n- n- the M_F_C_C are not very different. They are very different when energy's lower  like during fricatives or during speech pauses. And  uh - Yeah  but you're still getting more recognition errors  which means that the differences  even though they look like they're not so big  are - are hurting your recognition. Right? Ye- Yeah. So it distort the speech. Right. Um. Yeah. So performance went down? No. It didn't. But - Oh. Yeah. So  but in this case I - I really expect that maybe the - the two - these two stream of features  they are very different. I mean  and maybe we could gain something by combining them or - Well  the other thing is that you just picked one particular way of doing it. Uh  I mean  first place it's fifteen D_B  uh  down across the utterance. And maybe you'd want to have something that was a little more adaptive. Secondly  you happened to pick fifteen D_B and Mmm. Yeah. maybe twenty'd be better  or - or twelve. Yeah. Right. So what was the - what was the threshold part of it? Was the threshold  uh  how far down - ? Yeah. Well  he - yeah  he had to figure out how much to add. So he was looking - he was looking at the peak value. Uh-huh. Right? And then - Uh-huh. And - and so what's - ho- I don't understand. How does it go? If it - if - if the peak value's above some threshold  then you add the noise? Or if it's below s- I systematically add the noise  but the  um  noise level is just some kind of threshold below the peak. Oh  oh. I see. I see. Mmm. Um. Yeah. Yeah. Which is not really noise  actually. It's just adding a constant to each of the mel  uh  energy. Mm-hmm. To each of the mel filter bank. Yeah. I see. So  yeah  it's really  uh  white noise. I th- Mm-hmm. Yeah. So then afterwards a log is taken  and that's so- sort of why the - Mm-hmm. the little variation tends to go away. Um. Yeah. So may- Well  the - this threshold is still a factor that we have to look at. And I don't know  maybe a constant noise addition would - would be fine also  or - Um - Or - or not constant but - but  uh  varying over time in fact Mm-hmm. is another way to go. Mm-hmm. Um. @@ Yeah. Um - Were you using the - the normalization in addition to this? I mean  what was the rest of the Um - system? Yeah. It was - it was  uh  the same system. Mm-hmm. O_K. It was the same system. Mmm. Oh  yeah. A third thing is that  um  I play a little bit with the  um - finding what was different between  um  @@ And there were a couple of differences  like the L_D_A filters were not the same. Um  he had the France Telecom blind equalization in the system. Um  the number o- of M_F_C_C that was - were used was different. You used thirteen and we used fifteen. Well  a bunch of differences. And  um  actually the result that he - he got were much better on T_I-digits especially. So I'm kind of investigated to see what was the main factor for this difference. And it seems that the L_D_A filter is - is - was hurting. Um  so when we put s- some noise compensation the  um  L_D_A filter that - that's derived from noisy speech is not more - anymore optimal. And it makes a big difference  um  on T_I-digits trained on clean. Uh  if we use the - the old L_D_A filter  I mean the L_D_A filter that was in the proposal  we have  like  eighty-two point seven percent recognition rate  um  on noisy speech when the system is trained on clean speech. But - and when we use the filter that's derived from clean speech we jumped - so from eighty-two point seven to eighty-five point one  which is Mm-hmm. a huge leap. Um. Yeah. So now the results are more similar  and I don't - I will not  I think  investigate on the other differences  which is like the number of M_F_C_C that we keep and other small things that we can I think optimize later on anyway. Sure. But on the other hand if everybody is trying different kinds of noise suppression things and so forth  it might be good to standardize on the piece that we're not changing. Right? So if there's any particular reason to ha- pick one or the other  I mean - Which - which one is closer to what the proposal was that was submitted to Aurora? Are they - they both - ? Well  I mean - I think - Yeah. I think th- th- uh  the new system that I tested is  I guess  closer because it doesn't have - You mean the - it have less of - of France Telecom stuff  I - The - whatever you  uh  tested with recently. Right? Mmm? Yeah. Yeah? Well  no  I - I'm - I - Yeah  you're trying to add in France Telecom. Tell them about the rest of it. Like you said the number of filters might be But  we - different or something. Right? Or - The number of cepstral coefficients is what? Cep- Mm-hmm. Yeah. So  I mean  I think we'd wanna standardize there  wouldn't we? Yeah  yeah. So  sh- you guys should pick something and - Yeah. Yeah. Well  all th- all three of you. I think we were gonna work with - with this or this new system  or with - Uh  so the - the - right now  the - the system that is there in the - So - what we have in the repositories  with - uses fifteen. Right. Yeah. Yeah  so - Yeah  so - Yep. But we will use the - the L_D_A filters f- derived from clean speech. Yeah  yeah. So - Well  yeah  actually it's - it's not the - the L_D_A filter. It's something that's also short enough in - in latency. So. Yeah. Well. Yeah. So  we haven't - w- we have been always using  uh  fifteen coefficients  not thirteen? Yeah. Yeah. Mm-hmm. Well  uh  that's - something's - Um. Yeah. Then - mmm - @@ I think as long as you guys agree on it  it doesn't matter. I think we have a maximum of sixty  uh  features that we're allowed. So. Yeah. Yeah. Ma- maybe we can - I mean  at least  um  I'll t- s- run some experiments to see whether - once I have this noise compensation to see whether thirteen and fifteen really matters or not. Mm-hmm. Mm-hmm. Never tested it with the compensation  but without  uh  compensation it was like fifteen was s- slightly better than thirteen  so that's why we stuck to thirteen. Yeah. Yeah. And there is - there is also this log energy versus C_zero. Sorry  fifteen. Yeah  the log energy versus C_zero. Well. Uh  that's - that's the other thing. I mean  without noise compensation certainly C_zero is better than log energy. W- w- if - if - Be- I mean  because the - there are more  uh  mismatched conditions than the matching conditions for testing. Mm-hmm. You know  always for the matched condition  you always get a slightly better performance for log energy than C_zero. Mm-hmm. But not for - I mean  for matched and the clean condition both  you get log energy - I mean you get a better performance with log energy. Mm-hmm. Well  um  maybe once we have this noise compensation  I don't know  we have to try that also  whether we want to go for C_zero or log energy. Mm-hmm. We can see that. Yeah. Hmm. Mmm. So do you have more  Stephane  or - ? Uh  that's it  I think. Mmm. Do you have anything  Morgan  or - ? Uh  no. I'm just  you know  being a manager this week. So. How about you  Barry? Um  still working on my - my quals preparation stuff. Um  so I'm - I'm thinking about  um  starting some  uh  cheating experiments to  uh  determine the  um - the relative effectiveness of  um  some intermediate categories that I want to classify. So  for example  um  if I know where voicing occurs and everything  um  I would do a phone - um  phone recognition experiment  um  somehow putting in the - the  uh - the perfect knowledge that I have about voicing. So  um  in particular I was thinking  um  in - in the hybrid framework  just taking those L_N_A files  and  um  setting to zero those probabilities that  um - that these phones are not voicing. So say  like  I know this particular segment is voicing  Mm-hmm. um  I would say  uh  go into the corresponding L_N_A file and zonk out the - the posteriors for  um  those phonemes that  um  are not voiced  Mm-hmm. and then see what kinds of improvements I get. And so this would be a useful thing  um  to know Hmm. in terms of  like  which - which  um - which of these categories are - are good for  um  speech recognition. Mm-hmm. So  that's - I hope to get those  uh - those experiments done by - by the time quals come - come around in July. So do you just take the probabilities of the other ones and spread them out evenly among the - Yeah. I - I - I was thinking - O_K  so just set to - the remaining ones? set to some really low number  the - the non-voiced  um  phones. Right? And then renormalize. Mm-hmm. Mmm. Right. Cool. Mm-hmm. Yeah. That will be really interesting to see  you know. So then you're gonna feed the - those into some standard recognizer. Uh  wh- are you gonna do digits or - ? Mm-hmm. Yeah  m- Um  well  I'm gonna f- work with TIMIT - With TIMIT. O_K. TIMIT - uh  phone recognition with TIMIT. And  um - Mm-hmm. Oh  so then you'll feed those - Sorry. So where do the outputs of the net go into if you're doing phone recognition? Oh. Um  the outputs of the net go into the standard  h- um  ICSI hybrid  um  recognizer. So maybe  um  Chronos or - An- and you're gonna - the - you're gonna do phone recognition with that? O_K  O_K. Phone recognition. Right  right. I see. So. And  uh  another thing would be to extend this to  uh  digits or something where I can look at whole words. Mm-hmm. And I would be able to see  uh  not just  like  phoneme events  but  um  inter-phoneme events. Mm-hmm. So  like  this is from a stop to - to a vo- a vocalic segment. You know  so- something that is transitional in nature. Right. Cool. Yeah. Great. Uh - So that's - that's it. O_K. Um - Yeah. Let's see  I haven't done a whole lot on anything related to this this week. I've been focusing mainly on Meeting Recorder stuff. Oh. So  um  I guess I'll just pass it on to Dave. Uh  O_K. Well  in my lunch talk last week I - I said I'd tried phase normalization and gotten garbage results using that l- um  long-term mean subtraction approach. It turned out there was a bug in my Matlab code. So I tried it again  um  and  um  the results were - were better. I got intelligible speech back. But they still weren't as good as just subtracting the magnitude - the log magnitude means. And also I've been talking to  um  Andreas and Thilo about the  um  SmartKom language model and about coming up with a good model for  um  far mike use of the SmartKom system. So I'm gonna be working on  um  implementing this mean subtraction approach in the far-mike system - for the SmartKom system  I mean. And  um  one of the experiments we're gonna do is  um  we're gonna  um  train the - a Broadcast News net  which is because that's what we've been using so far  and  um  adapt it on some other data. Um  An- Andreas wants to use  um  data that resembles read speech  like these digit readings  because he feels that the SmartKom system interaction is not gonna be exactly conversational. Mm-hmm. S- so actually I was wondering  how long does it take to train that Broadcast News net? The big one takes a while. Yeah. That takes two  three weeks. Two  three weeks. So - but  you know  uh  you can get - I don't know if you even want to run the big one  uh  um  in the - in the final system  cuz  you know  it takes a little while to run it. So  um  you can scale it down by - I'm sorry  it was two  three weeks for training up for the large Broadcast News test set - training set. Oh. I don't know how much you'd be training on. O_K. The full? Uh  i- so if you trained on half as much and made the net  uh  uh  half as big  then it would be one fourth the amount of time and it'd be nearly as good. O_K. So. O_K. Yeah. Also  I guess we had - we've had these  uh  little di- discussions - I guess you ha- haven't had a chance to work with it too much - about - about  uh - uh  uh m- other ways of taking care of the phase. Mm-hmm. So  I mean  I - I guess that was something I could say would be that we've talked a little bit about you just doing it all with complex arithmetic and  uh - and not - not  uh  doing the polar representation with magnitude and phase. But it looks like there's ways that one could potentially just work with the complex numbers and - and - and in principle get rid of the effects of the average complex spectrum. But - And  um  actually  regarding the phase normalization - So I did two experiments  and one is - So  phases get added  modulo two pi  and - because you only know the phase of the complex number t- t- to a value modulo two pi. And so I thought at first  um  that  uh  what I should do is unwrap the phase because that will undo that. Um  but I actually got worse results doing that unwrapping using the simple phase unwrapper that's in Matlab than I did not unwrapping at all. Hmm. Mm-hmm. Yeah. P- So. And that's all I have to say. Hmm. Yeah. So I'm - I'm still hopeful that - that - I mean  we - we don't even know if the phase is something - the average phase is something that we do want to remove. I mean  maybe there's some deeper reason why it isn't the right thing to do. But  um  at least in principle it looks like there's - there's  uh  a couple potential ways to do it. One - one being to just work with the complex numbers  um  and  uh - in rectangular kind of coordinates. And the other is to  uh  do a Taylor series - Well. So you work with the complex numbers and then when you get the spectrum - the average complex spectrum - um  actually divide it out  um  as opposed to taking the log and subtracting. So then  um  um  you know  there might be some numerical issues. We don't really know that. The other thing we talked a little bit about was Taylor series expansion. And  um  uh  actually I was talking to Dick Karp about it a little bit  and - and - and  since I got thinking about it  and - and  uh  so one thing is that y- you'd have to do  I think  uh - we may have to do this on a whiteboard  but I think you have to be a little careful about scaling the numbers that you're taking - the complex numbers that you're taking the log of because the Taylor expansion for it has  you know  a square and a cube  and - and so forth. And - and so if - if you have a - a number that is modulus  you know  uh  very different from one - It should be right around one  if it's - cuz it's a expansion of log one - one minus epsilon or o- is - is one plus epsilon  or is it one plus - ? Well  there's an epsilon squared over two and an epsilon cubed over three  and so forth. So if epsilon is bigger than one  then it diverges. O_K. Oh. So you have to do some scaling. But that's not a big deal cuz it's the log of - of K_ times a complex number  then you can just - that's the same as log of K_ plus log of the complex number. Oh. O_K. Uh  so there's - converges. But. Hmm. O_K. How about you  Sunil? So  um  I've been  uh  implementing this  uh  Wiener filtering for this Aurora task. And  uh  I - I actually thought it was - it was doing fine when I tested it once. I- it's  like  using a small section of the code. And then I ran the whole recognition experiment with Italian and I got  like  worse results than not using it. Then I - So  I've been trying to find where the problem came from. And then it looks like I have some problem in the way - there is some - some very silly bug somewhere. And  ugh! I - I mean  i- uh  it actually - i- it actually made the whole thing worse. I was looking at the spectrograms that I got and it's  like - w- it's - it's very horrible. Like  when I - I - I missed the v- I'm sorry  I was - I was distracted. I missed the very first sentence. So then  I'm a little lost on the rest. What - what - what - ? Oh  I mean - Oh  yeah. I actually implemented the Wiener f- f- fil- filtering as a module and then tested it out separately. And it - it - it gave  like - I just got the signal out and it - it was O_K. Yeah  I see. Oh  O_K. So  I plugged it in somewhere and then - I mean  it's like I had to remove some part and then plugging it in somewhere. And then I - in that process I messed it up somewhere. So. O_K. So  it was real- I mean  I thought it was all fine and then I ran it  and I got something worse than not using it. So  I was like - I'm trying to find where the m- m- problem came  and it seems to be  like  somewhere - some silly stuff. Uh-huh. O_K. And  um  the other thing  uh  was  uh  uh - Well  Hynek showed up one - suddenly on one day and then I was t- talking wi- Right. Yeah. As - as he is wont to do. Yeah. Uh  yeah. So I was actually - that day I was thinking about d- doing something about the Wiener filtering  and then Carlos matter of stuff. And then he showed up and then I told him. And then he gave me a whole bunch of filters - what Carlos used for his  uh  uh  thesis and then that was something which came up. And then  um - So  uh  I'm actually  uh  thinking of using that also in this  uh  W- Wiener filtering because that is a m- modified Wiener filtering approach  where instead of using the current frame  it uses adjacent frames also in designing the Wiener filter. So instead of designing our own new Wiener filters  I may just use one of those Carlos filters in - in this implementation and see whether it - it actually gives me something better Mm-hmm. than using just the current f- current frame  which is in a way  uh  something like the smoothing - the Wiener filter - Mm-hmm. but @@ - S- so  I don't know  I was h- I'm - I'm - I'm  like - that - so that is the next thing. Once this - I - once I sort this pro- uh  problem out maybe I'll just go into that also. And the - the other thing was about the subspace approach. So  um  I  like  plugged some groupings for computing this eigen- uh  uh  uh  s- values and eigenvectors. So just - I just @@ some small block of things which I needed to put together for the subspace approach. And I'm in the process of  like  building up that stuff. And  um  uh - Yeah. I guess - Yep. I guess that's it. And  uh  th- th- that's where I am right now. So. Oh. How about you  Carmen? Mmm. I'm working with V_T_S. Um  I do several experiment with the Spanish database first  only with V_T_S and nothing more. Not V_A_D  no L_D_A  nothing more. What - what is V_T_S again? Eh  Vectorial Taylor Series. New - Oh  yes. Right  right. To remove the noise too. I think I ask you that every single meeting  don't I? I ask you that question every meeting. What? Yeah. If - Well - So  that'd be good from - for analysis. It's good to have some  uh  cases of the same utterance at different - different times. Yeah. Yeah. ""What is V_T_S?"" V_T_S. I'm sor- Well  um  the question is that - Well. Remove some noise but not too much. And when we put the - m- m- the  em  V_A_D  the result is better. And we put everything  the result is better  but it's not better than the result that we have without V_T_S. No  no. I see. So that @@ given that you're using the V_A_D also  the effect of the V_T_S is not so far - Is not. Do you - How much of that do you think is due to just the particular implementation and how much you're adjusting it? Or how much do you think is intrinsic to - ? Pfft. I don't know because - Hhh  Are you still using only the ten first frame for noise estimation or - ? Uh  I do the experiment using only the Or i- ? f- Yeah. onl- eh  to use on- only one fair estimation of the noise. Hmm. And also I did some experiment  uh  doing  um  a lying estimation of the noise. And  well  it's a little bit better but not - n- Maybe you have to standardize this thing also  noise estimation  because all the thing that you are testing use a different - Mmm. Mmm. They all need some - some noise - noise spectra but they use - every - all use a different one. No  I do that two - t- did two time. I have an idea. If - if  uh  uh  y- you're right. I mean  each of these require this. Um  given that we're going to have for this test at least of - uh  boundaries  what if initially we start off by using known sections of nonspeech for the estimation? Mm-hmm. Mm-hmm. Right? S- so  e- um  Yeah. Mm-hmm. first place  I mean even if ultimately we wouldn't be given the boundaries  uh  this would be a good initial experiment to separate out the effects of things. I mean  how much is the poor - you know  relatively  uh  unhelpful result that you're getting in this or this or this is due to some inherent limitation to the method for these tasks and how much of it is just due to the fact that you're not accurately finding enough regions that - that are really Mmm. Mm-hmm. n- noise? Mm-hmm. Um. So maybe if you tested it using that  you'd have more reliable stretches of nonspeech to do the estimation from and see if that helps. Yeah. Another thing is the  em - the codebook  the initial codebook. That maybe  well  it's too clean and - Mm-hmm. Cuz it's a - I don't know. The methods - If you want  you c- I can say something about the method. Mm-hmm. Yeah. In the - Because it's a little bit different of the other method. Well  we have - If this - if this is the noise signal  uh  in the log domain  we have something like this. Now  we have something like this. And the idea of these methods is to - n- given a  um - How do you say? Mm- hmm. I will read because it's better for my English. I- i- given is the estimate of the P_D_F of the noise signal when we have a  um  a statistic of the clean speech and an statistic of the noisy speech. And the clean speech - the statistic of the clean speech is from a codebook. Mmm? This is the idea. Well  like  this relation is not linear. The methods propose to develop this in a vectorial Taylor series approximation. I- I'm actually just confused about the equations you have up there. So  uh  the top equation is - is - is - No  this in the - it's - this is the log domain. I - I must to say that. Which is - which is the log domain? Is the T_ - is egual - is equal to  uh  log of - And - but Y_ is what? Y_ of - the spectrum or - ? Uh  this - this is this and this is this. No  no. The top Y_ is what? Mm-hmm. Is that power spectrum? No  is that power spectrum? Is it - ? Uh  this is the noisy speech. p- s- this - Yeah. I guess it's the power spectrum of noisy speech. Yeah. And - Yeah. It's the power spectrum. Oh  O_K. So that's uh - This is the noisy - Yeah  it's - of the value - O_K. Yeah  O_K. So this - it's the magnitude squared or something. O_K  so you have power spectrum added there Yeah. and down here you have - you - you put the - depends on T_   but - b- all of this is just - you just mean - w- o- Yeah. It's the same. you just mean the log of the - of the one up above. Yeah. Mm-hmm. And  uh  so that is X_ times  uh  Yeah  maybe - One - one plus N_ by X_. But  n- o- Well  y- we can expre- we can put this expression - X_ times one plus  uh  The - N_ - uh  N_ - N_ - N_ minus X_? Yeah. And then  And the noise signal. uh - So that's log of X_ plus log of one plus  uh - Well. Is that right? Log of - One plus N_ by X_. Well  mmm - I actually don't see how you get that. Well  if we apply the log  we have E_ is n- Uh. Mmm. uh  log - Uh  and - E_ is equal  oh  to log of X_ plus N_. Yeah. And  well  uh  we can say that E_ And  log of - is equal to log of  um  exponential of X_ plus exponential of N_. Uh - Mm-hmm. No. No. That doesn't follow. Well  if E_ restricts - Well  this is - this is in the ti- the time domain. Well  we have that  um - It is y- We have first that  for example  X_ is equal  uh - Well. This is the frequency domain and we can put u- that n- the log domain - Yeah. log of X_ omega  but  well  in the time domain we have an exponential. No? No? Oh  maybe it's I am - I'm problem. Yeah. I mean  just never mind what they are. Uh  it's just if X_ and N_ are variables - Right? What is  uh - ? The - the - the log of X_ plus N_ is not the same as the log of E_ to the X_ plus E_ to the N_. Yeah. But this i- Well  I don't - Well  uh  maybe - Maybe we can take it off-line  but I - I don't know. I - I can do this incorrectly. Well  the expression that appear in the - in the paper  is  uh - The log - the Taylor series expansion for log one plus N_ by X_ is - is X_ - Is it the first-order expansion? Yeah  I guess. Yeah. Uh-huh. O_K. I- i- Yeah  the first one. Yeah. Yeah. O_K. Yeah. Cuz it doesn't just follow what's there. It has to be some  uh  Taylor series - Y- yeah. If - if you take log X_ into log one plus N_ by X_  and then expand the log one plus N_ by X_ into Taylor series - Yeah. Now  this is the - and then - Yeah  but the - the second expression that you put is the first-order expansion of Not exactly. the nonlinear relation between - No. No  no  no. It's not the first space . Well  we have - pfft  uh  em - Well  we can put that X_ is equal - I_ is equal to log of  uh  mmm - That doesn't follow. Well  we can put  uh  this? Mmm. No. That - I mean  that - the f- top one does not imply the second one. The top? Because - cuz the log of a sum is not the same as th- I mean  as - Yeah. Yeah  yeah  yeah  yeah  yeah. But we can - uh  we - we know that  for example  the log of E_ plus B_ is equal to log of E_ plus log Right. to B_. And we can say here  it i- Right. So you could s- What is that? And we can  uh  put this inside. Yeah. And then we can  uh  N- no  but - you know - Yeah. Uh. I don't see how you get the second expression from the top one. The - I mean  just more generally here  if you say ""log of  um  A_ plus B_""  the log of - log of A_ plus B_ is not - or A_ plus B_ is not the  um  log of E_ to the A_ plus E_ to the B_. No  no  no  no  no  no  no. This not. No. Right? And that's what you seem to be saying. No. It's not. But this is the same - oh. Right? Cuz you - cuz you - up here you have the A_ plus B_ - No. I say if I apply log  I have  uh  log of E_ is equal to log of  uh - in this side  is equal to log of X_ Plus N_. plus N_. Right. No? Right. This is right. Right. And then how do you go from there to the - ? And then if I apply exponential  to have here Look. O_K  so let's - I mean  C_ equals A_ plus B_  E_ - It's log o- of capital Y_. Yeah  right. Capital Y_. Yeah. and then - X_. X_. This is X_  inside. Mm-hmm. We have this  no? Right. Yeah. That one's right. Mm-hmm. One and - S- uh  i- th- we can put here the set transformation. Oh. I see. No? I see. O_K  I understand now. Alright  thanks. Yeah. In this case  well  we can put here a Y_. O_K. So  yeah. It's just by definition that the individual - that the  uh - So  capital X_ is by definition the same as E_ to the little X_ because she's saying that the little X_ is - is the  uh - is the log. Alright. Now we can put this. No? Yeah. And here we can multiply by X_. Alright. I think these things are a lot clearer when you can use fonts - Oh  yes. different fonts there so you know which is which. But I - I under- I understand what you mean now. O_K. Yeah  yeah. That's true. That's true. But this - this is correct? And now I can do it  uh - pfff! Sure. I can put log Oh. of E_X plus log - Yes. I understand now. And that's where it comes from. Yeah. And this is - Yeah. Right. Right. Now it's correct. Right. O_K. Thanks. Well. The idea - Well  we have fixed this equa- O_K. So now once you get that - that one  then you - then you do a first- or second-order  or something  Taylor series expansion of this. Yeah. This is another linear relation that this - to develop this in vector s Taylor series. Yeah  sure. Right. Mm-hmm. And for that  well  the goal is to obtain  um - est- estimate a P_D_F for the noisy speech when we have a - a statistic for clean speech and for the noisy speech. Mmm? And when w- the way to obtain the P_D_F for the noisy speech is - well  we know this statistic and we know the noisy st- well  we can apply first order of the vector st- Taylor series of the - of the - of - well  the order that we want  increase the complexity of the problem. And then when we have a expression  uh  for the Mm-hmm. mean and variance of the noisy speech  we apply a technique of minimum mean-square estimation Mm-hmm. to obtain the expected value of the clean speech given the - this statistic for the noisy speech - Mm-hmm. the statistic for clean speech and the statistic of the noisy speech. This only that. But the idea is that - u- And the - the model of clean speech is a codebook. Right? Yeah. We have our codebook with different density Gaussian. Mm-hmm. We can expre- we can put that the P_D_F for the clean test  probability of the clean speech is equal to - Yeah. @@ Mm-hmm. So  um  how - h- how much - in - in the work they reported  how much noisy speech did you need to get  uh  good enough statistics for the - to get this mapping? I don't know exactly. Yeah. I - I need to s- I don't know exactly. Yeah. Cuz I think what's certainly characteristic of a lot of the data in this test is that  um  you don't have - the - the training set may not be a - a great estimator for the noise in the test set. Sometimes it is and sometimes it's not. Yeah. I - the clean speech - the codebook for clean speech  I am using TIMIT. And I have now  uh  sixty-four Gaus- Gaussian. Uh-huh. And what are you using for the noisy - ? Y- y- doing that strictly - Of the noise - I estimate the noises wi- Well  for the noises I only use one Gaussian. Mm-hmm. And - and you - and you train it up entirely from  uh  nonspeech sections in the test? Hmm. Uh  yes. The first experiment that I do it is solely to calculate the  mmm - well  this value - Yeah. uh  the compensation of the dictionary o- one time using the - the noise at the f- beginning of the sentence. This is the first experiment. And I fix this for all the - Mm-hmm. Yeah. all the sentences. Uh  because - well  the V_T_S methods - In fact the first thing that I do is to - to obtain  uh  an expression for E_ - probability e- expression of - of E_. That mean that the V_T_S - mmm  with the V_T_S we obtain  uh - well  we - we obtain the means for each Gaussian and the variance. Mm-hmm. This is one. Eh  this is the composition of the dictionary. Mm-hmm. This one thing. And the other thing that this - with these methods is to  uh  obtain - to calculate this value. Mm-hmm. Because we can write - uh  we can write that the estimation of the clean speech Mm-hmm. is equal at an expected value of the clean speech conditional to  uh  the noise signal - the probability f- of the - the statistic of the clean speech and the statistic of the noise. Mm-hmm. This is the methods that say that we're going obtain this. Mm-hmm. And we can put that this is equal to the estimated value of E_ minus a function that conditional to E_ to the T_ - to the noise signal. Well  this is - this function is the the term - after develop this  the term that we - we take. Give P_X and  uh  P_ the noise. X_ K_ C_ noise. Mmm. And I can put that this is equal to the noise signal minus - Well  I put before this name  uh - And I can calculate this. What is the first variable in that probability? Uh  this is the Gaussian. No  no. I'm sorry. In - in the one you pointed at. What's that variable? v- Uh  this is the - Weak. So probably it - it would do that. like this  but conditional. No  it's condition- it's not exactly this. It's It's one mixture of the model. Right? modify. Uh  if we have clean speech - we have the dictionary for the clean speech  we have a probability f- of - our - our weight for each Gaussian. No . And now  this weight is different now because it's conditional. And O_K. Yes. this I need to - to calcu- I know this and I know this because this is from the dictionary that you have. Uh-huh. Uh-huh. I need to calculate this. And for calculate this  Yes. I have an - I - I can develop an expression that is It's overlapping. that. I can calculate - I can - I calculated this value  uh  with the statistic of the noisy speech that I calculated before with the V_T_S approximation. Mm-hmm. And - well  normalizing. And I know everything. Uh  with the  nnn - when I develop this in s- Taylor - Taylor series  I can't  um  calculate the mean and the variance of the - for each of the Gaussian of the dictionary for the noisy speech. Now. And this is fixed. Mm-hmm. If I never do an estimat- a newer estimation of the noise  this mean as - mean and the variance are fixed. Mm-hmm. And for each s- uh  frame of the speech the only thing that I need to do is to calculate this in order to calculate the estimation of the clean speech given our noisy speech. So  I'm - I'm not following this perfectly but  um  I - @@ Are you saying that all of these estimates are done using  um  estimates of the probability density for the noise that are calculated only from the first ten frames? Yeah. And never change throughout anything else? Never cha- This is one of the approximations that I am doing. Per - per - per utterance  or per - ? Per utterance. Yes. Per utterance. Yes. And th- Per utterance. O_K. So it's done - it's done new for each new utterance. So this changes the whole mapping for every utterance. Yeah. It's not - Yeah. Yeah. It's fixed  the dictionary. And the other estimation is when I do the uh on-line estimation  I change the means and variance of th- for the noisy speech O_K. O_K. Yeah? each time that I detect noise. Mm-hmm. I do it uh again this develop. Estimate the new mean and the variance of the noisy speech. And with th- with this new s- new mean and variance I estimate again this. So you estimated  uh  f- completely forgetting what you had before? Um  no  no  no. It's not completely - No  it's - I am doing something like an adaptation of the noise. Uh  or is there some adaptation? O_K. Now do we know  either from their experience or from yours  that  uh  just having  uh  two parameters  the - the mean and variance  is enough? Yeah. I mean  I know you don't have a lot of data to estimate with  but - but  uh  um - I estimate mean and variance for each one of the Gaussian of the codebook. No  I'm talking about the noise. Oh  um. Well  only one - I am only - using only one. I don't know i- There's only one Gaussian. Right. And you - and - and it's  uh  uh - right  it's only - it's only one - Wait a minute. This is - what's the dimensionality of the Gaussian? This is - Uh  it's in - after the mel filter bank. @@ So this is twenty or something? Twenty? Twenty-three. So it's - Yeah. So it's actually forty numbers that you're getting. Yeah  maybe - Uh  the original paper say that only one Gaussian for the noise. maybe you don't have a - Well  yeah. But  I mean  no - no paper is - is a Bible  you know. This is - this is  uh - Yeah  maybe isn't the right thing. Yeah  yeah  yeah. The question is  um  whether it would be helpful  i- particularly if you used - if you had more - So  suppose you did - This is almost cheating. It certainly isn't real-time. But if y- suppose you use the real boundaries that - that you were - in fact were given by the V_A_D and so forth -th -th - or I - I guess we're gonna be given even better boundaries than that. And you look - you take all o- all of the nonspeech components in an utterance  so you have a fair amount. Do you benefit from having a better model for the noise? That would be another question. Maybe. So first question would be to what extent i- are the errors that you're still seeing based on the fact that you have poor boundaries for the  uh  uh  nonspeech? And the second question might be  given that you have good boundaries  could you do better if you used more parameters to characterize the noise? Um. Also another question might be - Um  they are doing - they're using first term only of the vector Taylor series? Yeah. Um  if you do a second term does it get too complicated cuz of the nonlinearity? Yeah. It's quite complicated. Yeah  O_K. No  I won't ask the next question then. Oh  it's - it's the - for me it's the first time that I am working with V_T_S. Uh - Yeah. No  it's interesting. Uh  w- we haven't had anybody work with it before  so it's interesting to get your - get your feedback about it. It's another type of approximation because i- because it's a statistic - statistic approximation to remove the noise. I don't know. Right. Great. O_K. Well  I guess we're about done. Um  so some of the digit forms don't have digits. Uh  we ran out there were some blanks in there  so not everybody will be reading digits. But  um  I guess you've got some. Right  Morgan? I have some. So  why don't you go ahead and start. And I think it's just us down here at this end that have them. So. um S- Uh  O_K. S- so  we switch off with this or n- ? Whenever you're ready. Uh  leave it on  uh  and the - No. O_K. They prefer to have them on just so that they're continuing to get the distant  uh  information. Yeah. O_K. O_K. Transcript L_ dash one six nine. Three nine three  zero nine five  seven nine eight. Two seven  six zero  five six  five eight  six seven. Six one one  one eight  two four s- two six. One nine zero zero  two  seven eight three. Eight eight one  two two six  seven four one nine. Seven nine four nine s- e- e- Eight five seven eight  seven nine zero nine. Two  three seven five  four one  seven six one  two. Three seven six two  six three six seven  two nine four two. Transcript L_ dash one sixty-seven. Seven five six zero  five  three seven three. One four seven  four four  nine two seven eight. One one  six seven  three eight  eight five  five seven. Eight  one six four  six four  eight two O_  three. Three seven seven  five  two six three. Two  eight five seven  nine five  six seven two  two. Nine O_  one eight  three two  two six  two three. Three two  five two  seven seven  nine four  nine six. Transcript L_ dash one six eight. Seven nine  nine nine  nine two  four eight  eight zero. Six nine eight  seven zero  one eight four eight. Zero three one  seven eight four  one seven eight two. Five  two three two  six five  nine eight three  six. Six five four zero  four one five zero  five eight eight two. Zero  three five seven  two six  zero six five  nine. Seven six two  eight one  nine five nine eight. One one  four two  zero eight  six seven  nine eight. Transcript L_ dash one seven four. Four nine one one  eight  four seven nine. Two four one two  seven  eight nine two. Five one six six  three six seven seven  three two eight nine. Seven six two  seven nine  three six one zero. Five nine three five  seven  five six zero. Four eight eight  eight eight two  nine eight eight. Five nine  six seven  two five  zero six  nine four. Eight two  two zero  one nine  two six  nine one. O_K. O_K. S- ","The first phase of the data collection has finished. There is a new wizard for phase two  during which subjects will be given more complex scenarios. Also finished are the modifications on SmartKom: the remaining glitches will take no more than a day to iron out. A big part of the meeting was covered by the presentation of the PRM of the proposed system. An alternative representation of the Bayes-net  it depicts context features as classes  and dependencies as relations between them. The current outputs show the desirability of a site  as well as its EVA mode. The fact that this model allows for instantiations of classes fits the research purposes much better than the extended belief-net. Following this  a visiting researcher presented an overview of a parallel project at the International University. It attempts to build a smart tutoring system for a computer science course. The assumption is that document searches can give more personalised results  if they take into account contextual parameters (user  situation). Although no detailed linguistic analysis takes place  it was suggested that the use of FrameNet could be a useful approach. There were also further suggestions for meetings with ICSI researchers. As the data collection is going into its second phase  more complex scenarios will be used to generate more intricate dialogues. Subjects can be recruited from within the Psychology department students  since such participation in experiments is compulsory in their syllabus. As the work on creating a PRM for the system has progressed  it is now necessary to find an implementation that can work as a PRM interpreter. There are no plans to build one from scratch  but it seems that other research groups at Stanford may already have one. Moreover  closer collaboration with the local PRM group will be pursued with additional participation in their meetings. An early version of the PRM of the system presented the same problem as a flat Bayes-net: the CPT's become too large. Another PRM issue that arose and remained unclear was how the probabilities are specified (instead of learnt) on the actual relations between the classes. Furthermore  the lack of a PRM interpreter is an open issue. It is not within the remit of the project to build one from scratch  therefore  an existing implementation has to be found. On the other hand  the discussion about the smart tutoring system being built at the International University showed the importance of finding out  which context parameters are influential in a given domain. It is easy to hack up a system for a small domain  but making it scalable is much more difficult. Finally  there was a passing mention of problems encountered with the speech synthesis module of SmartKom. The first phase of the data collection has been completed. Thirty subjects were recorded in total. Of those dialogues  ten have been transcribed. A new wizard will carry out the second phase. On top of this  a presentation of a PRM of the proposed system took place. The PRM comprises a set of classes  such as ""user""  ""site""  ""route""  ""time"" and ""query"" with relations between them. Another class incorporates a number of attributes (""money affordability""  ""travel compatibility"" etc) modelling the intermediate nodes of the Bayes-net  as well as references to the other classes in the model. This model is much cleaner  and it also makes it easier to specify the CPT's. At this stage  the model makes two decisions (outputs): how much a site fits a user's needs and what the user intention is in EVA terms. "
"O_K  we're on. O_K. So  I mean  everyone who's on the wireless check that they're on. Alright. C- we - I see. Yeah. Yeah. O_K  our agenda was quite short. Oh  could you close the door  maybe? Yeah. Sure. Two items  which was  uh  digits and possibly stuff on - on  uh  forced alignment  which Jane said that Liz and Andreas had in- information on  but they didn't  so. @@ Mm-hmm. I guess the only other thing  uh  for which I - We should do that second  because Liz might join us in time for that. O_K. Um. O_K  so there's digits  alignments  and  um  I guess the other thing  which I came unprepared for  uh  is  uh  to dis- s- s- see if there's anything anybody wants to discuss about the Saturday meeting. Right. So. Any - I mean  maybe not. Digits and alignments. But - Uh. Talk about aligning people's schedules. Yeah. Yeah. Mm-hmm. Yeah. I mean - Right. Yeah  I mean  it was - Yeah  it's forced alignment of people's schedules. Yeah. If we're very - Forced align. Yeah. Yeah. With - with - whatever it was  a month and a half or something ahead of time  the only time we could find in common - roughly in common  was on a Saturday. Yeah. Ugh. Yep. It's pretty sad. Yeah. Yeah. Have - Have we thought about having a conference call to include him in more of - in more of the meeting? I - I mean  I don't know  if we had the - if we had the telephone on the table - No. But  h- I mean  he probably has to go do something. Right? No  actually I - I have to - I have to shuttle kids from various places to various other places. I see. O_K. Yeah. So. And I don't have - and I don't  um  have a cell phone so I can't be having a conference call while driving. A cell phone? No. It's not good. That's not good. R- r- right. So we have to - we - Plus  it would make for interesting noise - background noise. @@ So we have to equip him with a - with a - with a head-mounted  uh  cell phone and - Yep. Uh - Ye- we- and we'd have to force you to read lots and lots of digits  so it could get real - real car noise. Oh  yeah. Oh  yeah. Take advantage. Yeah. And with the kids in the background. Yeah. I'll let - I'd let - I let  uh  my five-year-old have a try at the digits  eh. Yeah. So  anyway  I can talk about digits. Um  did everyone get the results or shall I go over them again? I mean that it was basically - the only thing that was even slightly surprising was that the lapel did so well. Um  and in retrospect that's not as surprising as maybe i- it shouldn't have been as surprising as I - as - as I felt it was. The lapel mike is a very high-quality microphone. And as Morgan pointed out  that there are actually some advantages to it in terms of breath noises and clothes rustling if no one else is talking. Exactly. Yeah. Mm-hmm. Um  so  uh - It's g- it - Well  it's - Yeah  sort of the bre- the breath noises and the mouth clicks and so forth like that  the lapel's gonna be better on. Or the cross-talk. Yeah. The lapel is typically worse on the - on clothes rustling  but if no one's rustling their clothes  Right. I mean  a lot of people are just sort of leaning over and reading the digits  so it's - it's a very it's - it's - different task than sort of the natural. So. Yeah. You don't move much during reading digits  I think. Yeah. Yeah. Right. Probably the fact that it picks up other people's speakers - other people's talking is an indication of that it - the fact it is a good microphone. Yeah. Right. Right. Right. So in the digits  in most - most cases  there weren't other people talking. So. So. D- do the lapel mikes have any directionality to them? There typically don't  no. Because I - I suppose you could make some that have sort of - that you have to orient towards your mouth  and then it would - They have a little bit  but they're not noise-cancelling. So  uh - They're - they're intended to be omni-directional. Right. Mm-hmm. And th- it's - and because you don't know how people are gonna put them on  you know. Right. So  also  Andreas  on that one the - the back part of it should be right against your head. And that will he- keep it from flopping aro- up and down as much. It is against my head. O_K. Yeah. Um. Yeah  we actually talked about this in the  uh  front-end meeting this morning  too. Uh-huh. Much the same thing  and - and it was - uh  I mean  there the point of interest to the group was primarily that  um  the  uh - the system that we had that was based on H_T_ K  that's used by  you know  all the participants in Aurora  Everybody. was so much worse than the - than the S_R_ I. And the interesting thing is that even though  yes  it's a digits task and that's a relatively small number of words and there's a bunch of digits that you train on  it's just not as good as having a - a l- very large amount of data and training up a - a - a nice good big H_M_M. Um  also you had the adaptation in the S_R_I system  which we didn't have in this. Um. So. Um. And we know - Di- did I send you some results without adaptation? No. Or if you did  I didn't include them  cuz it was - I- s- I think Stephane  uh  had seen them. So - Yeah  I think I did  actually. So there was a significant loss from not doing the adaptation. Yeah. Um. A - a - a couple percent or some- I mean - Well  I don't know it - Overall - Uh  I - I don't remember  but there was - there was a significant  um  loss or win from adaptation - with - with adaptation. And  um  that was the phone-loop adaptation. And then there was a very small - like point one percent on the natives - uh  win from doing  um  you know  adaptation to the recognition hypotheses. And I tried both means adaptation and means and variances  and the variances added another - or subtracted another point one percent. So  it's  um - that's the number there. Point six  I believe  is what you get with Right. both  uh  means and variance adaptation. But I think one thing is that  uh  I would presume - Hav- Have you ever t- Have you ever tried this exact same recognizer out on the actual T_I-digits test set? This exact same recognizer? No. It might be interesting to do that. Cuz my - my - cuz my sense  um - But - but  I have - I mean  people - people at S_R_I are actually working on digits. I bet it would do even slightly better. I could - and they are using a system that's  um - you know  h- is actually trained on digits  um  but h- h- otherwise uses the same  you know  decoder  the same  Mm-hmm. uh  training methods  and so forth  and I could ask them what they get on T_I-digits. Yeah  bu- although I'd be - I think it'd be interesting to just take this exact actual system so that these numbers were comparable and try it out on T_I-digits. Mm-hmm. Well  Adam knows how to run it  so you just Yeah. No problem. Yeah. Yeah. Yeah. Cuz our sense from the other - from the Aurora  uh  task is that - I mean  cuz we were getting sub one percent make a f- And try it with T_I-digits? Mm-hmm. Mm-hmm. numbers on T_I-digits also with the tandem thing. So  Mmm. one - so there were a number of things we noted from this. One is  yeah  the S_R_I system is a lot better than the H_T_K - Hmm. this  you know  very limited training H_T_K system. Mm-hmm. Uh  but the other is that  um  the digits recorded here in this room with these close mikes  i- uh  are actually a lot harder than the studio-recording T_I-digits. I think  you know  one reason for that  uh  might be that there's still - even though it's close-talking  there still is some noise and some room acoustics. Mm-hmm. Mm-hmm. And another might be that  uh  I'd - I would presume that in the studio  uh  uh  situation recording read speech that if somebody did something a little funny or n- pronounced something a little funny or made a little - that they didn't include it  they made them do it again. They didn't include it. Whereas  I took out the ones that I noticed that were blatant - that were correctable. Mmm. Yeah. So that  if someone just read the wrong digit  I corrected it. And then there was another one where Jose Yeah. couldn't tell whether - I couldn't tell whether he was saying zero or six. And I asked him and he couldn't tell either. Hmm. So I just cut it out. You know  so I just e- edited out the first  i- uh  word of the utterance. Yeah. Um  so there's a little bit of correction but it's definitely not as clean as T_I-digits. So my expectations is T_I-digits would  especially - I think T_I-digits is all American English. Right? So it would probably do even a little better still Mm-hmm. on the S_R_I system  but we could give it a try. Well. But remember  we're using a telephone bandwidth front-end here  uh  on this  uh - on this S_R_I system  so  um  I was - I thought that maybe that's actually a good thing because it - it gets rid of some of the - uh  the noises  um  you know  in the - the - below and above the - Mm-hmm. um  the  you know  speech bandwidth and  Mm-hmm. um  I suspect that to get sort of the last bit out of these higher-quality recordings you would have to in fact  uh  use models that  uh  were trained on wider-band data. And of course we can't do that or - Wha- what's T_I-digits? I thought t- It's wide-band  yeah. It's - in - in fact  we looked it up and it was actually twenty kilohertz sampling. It is wide-band. O_K. Oh  that's right. I - I did look that up. I couldn't remember whether that was T_I-digits or one of the other digit tasks. Mm-hmm. Yeah. Right. But - but  I would - Yeah. It's - it's easy enough to try  just run it on - Mm-hmm. So  Morgan  you're getting a little breath noise. You might wanna move the mike down a little bit. Yeah. See w- Now  eh  does - one - one issue - one issue with - with that is that um  the system has this  uh  notion of a speaker to - which is used in adaptation  variance norm- uh  you know  both in  uh  mean and variance normalization and also in Mm-hmm. the V_T_L estimation. So - Yeah  I noticed the script that extracted it. Do y- ? Is - ? So does - so th- so does - does  um  the T_I-digits database have speakers that are known? Yep. Yep. And is there - is there enough data or a comparable - comparable amount of data to - to what we have in our recordings here? That I don't know. I don't know. I don't know how many speakers there are  O_K. Yeah. and - and how many speakers per utterance. Well  the other thing would be to do it without the adaptation and compare to these numbers without the adaptation. That would - Right. Uh  but I'm not so much worried about the adaptation  actually  than - than the  um  um - the  uh  V_T_L estimation. If you have only one utterance per speaker you might actually screw up on estimating the - Right. the warping  uh  factor. So  um - I strongly suspect that they have more speakers than we do. Right. But it's not the amount of speakers  it's the num- it's the amount of data per speaker. So  uh - Right. So we - we could probably do an extraction that was roughly equivalent. Right. Right. So - Um. So  although I - I sort of know how to run it  there are a little - a f- few details here and there that I'll have to dig out. O_K. The key - So th- the system actually extracts the speaker I_D from the waveform names. Right. I saw that. And there's a - there's a script - and that is actually all in one script. So there's this one script that parses waveform names and extracts things like the  um  speaker  uh  I_D or something that can stand in as a speaker I_D. So  we might have to modify that script to recognize the  um  speakers  Right. um  in the - in the  uh  um  T_I-digits database. Right. And that  uh - Or you can fake - you can fake names for these waveforms that resemble the names that we use here for the - for the meetings. Right. That would be the  sort of - probably the safest way to do - I might have to do that anyway to - to do - because we may have to do an extract to get the amount of data per speaker about right. Uh-huh. The other thing is  isn't T_I-digits isolated digits? Right. Or is that another one? I'm - I looked through a bunch of the digits t- corp- corpora  and now they're all blurring. Mm-hmm. Cuz one of them was literally people reading a single digit. And then others were connected digits. Yeah. Most of T_I-digits is connected digits  I think. The - I mean  we had a Bellcore corpus that we were using. It was - O_K. Maybe it's the Bell Gram . that's - that was isolated digits. Bell Digits. Alright. Um. By the way  I think we can improve these numbers if we care to compr- improve them by  um  not starting with the Switchboard models but by taking the Switchboard models and doing supervised adaptation on a small amount of digit data collected Yep. in this setting. Because that would adapt your models to the room acoustics and f- for the far-field microphones  you know  to the noise. And that should really improve things  um  further. And then you use those adapted models  which are not speaker adapted but sort of acous- you know  channel adapted - Channel adapted. use that as the starting models for your speaker adaptation. Yeah. But the thing is  uh - I mean  w- when you - it depends whether you're ju- were just using this as a - a starter task for - you know  to get things going for conversational or if we're really interested i- in connected digits. And I - I think the answer is both. Well  I don't know. And for - for connected digits over the telephone you don't actually want to put a whole lot of effort into adaptation because somebody gets on the phone and says a number and then you just want it. This is - this - that one's better. Mm-hmm. You don't - don't  uh - Right. Um  but  you know  I - uh  my impression was that you were actually interested in the far-field microphone  uh  problem  I mean. So  you want to - you want to - That's the obvious thing to try. Right? Then  eh - because Oh. Oh. Right. you - you don't have any - That's where the most m- acoustic mismatch is between the currently used models and the - the r- the set up here. So. Yeah. Right. Yeah. So that'd be anoth- another interesting data point. I mean  I - I guess I'm saying I don't know if we'd want to do that as the - as - Mm-hmm. Other way. Liz - Other way. Now you're all watching me. Alright. This way. It f- it clips over your ears. There you go. If you have a strong fe- if you have a strong preference  you could use this. It's just we - we think it has some spikes. So  uh  we - we didn't use that one. You're all watching. This is terrible. I'll get it. But you could if you want. I don't know. And Andre- Andreas  your - your microphone's a little bit low. Yeah. At any rate  I don't know if w- It is? Uh. Yeah. I don't know if we wanna use that as the - Yeah. Uh  it pivots. So if you see the picture and then you have to scr- It - it - like this. I- I - I - I already adjusted this a number of times. I - I Eh. Yeah  I think these mikes are not working as well as I would like. can't quite seem to - Yeah  I think this contraption around your head is not working so well. Too many adju- too many adjustments. Yeah. Anyway  what I was saying is that I - I think I probably wouldn't want to see that as sort of like the norm  that we compared all things to. That looks good. Yeah. To  uh  the - to have - have all this ad- all this  uh  adaptation. But I think it's an important data point  if you're - if - Yeah. Right. Um. The other thing that - that  uh - of course  what Barry was looking at was - was just that  the near versus far. And  yeah  the adaptation would get Mm-hmm. th- some of that. But  I think even - even if there was  uh  only a factor of two or something  like I was saying in the email  I think that's - that's a big factor. So - Mm-hmm. N- Liz  you could also just use the other mike if you're having problems with that one. Well. O_K. Yeah. This would be O_K. We - we - we think that this has spikes on it  so it's not as good acoustically  but - It's this thing's - This is too big for my head. Yeah  basically your ears are too big. I mean  mine are too. No  my - my - But this is too big for my head. So  I mean  E- th- everybody's ears are too big for these things. Uh - it doesn't - you know  it's sit- Well  if you'd rather have this one then it's - O_K. Yeah. Oh  well. It's great. So the - To get that  uh  pivoted this way  it pivots like this. No this way. Yeah. Yeah. There you go. And there's a screw that you can tighten. And then it - Right. Right. I already tried to get it close. Good. So if it doesn't bounce around too much  that's actually good placement. O_K. That looks good. But it looks like it's gonna bounce a lot. So  where were we? Uh - Yeah. Yeah. Digits. Adaptation. Uh  adaptation  non-adaptation  um  factor of two  um - What k- u- By the way  wh- what factor of two did you - ? I mean - Oh  yeah. I know what I was go- w- Oh  no  no. It's tha- that - that we were saying  you know  well is - how much worse is far than near  you know. And I mean it depends on which one you're looking at  but for the everybody  it's Oh  th- O_K. That factor of two. Mm-hmm. little under a factor or two. Yeah. I - I know what I was thinking was that maybe  uh  i- i- we could actually t- t- try at least looking at  uh  some of the - the large vocabulary speech from a far microphone  at least from the good one. Mm-hmm. I mean  before I thought we'd get  you know  a hundred and fifty percent error or something  but if - Mm-hmm. if  uh - if we're getting thirty-five  forty percent or something  Actually if you run  u- um - though  on a close-talking mike over the whole meeting  during all those silences  you get  like  four hundred percent word error. Mm-hmm. Right. I understand. But doing the same kind of limited thing - Or - or some high number. Yeah  sure. Get all these insertions. But I'm saying if you do the same kind of limited thing Yeah. as people have done in Switchboard evaluations or as - a- Where you know who the speaker is and there's no overlap? Yeah. And you do just the far-field for those regions? Yeah. The same sort of numbers that we got those graphs from. Right? Could we do exactly the same thing that we're doing now  but do it with a far-field mike? Yeah  do it with one of - on- Cuz we extract the times from the near-field mike  but you use the acoustics from the far-field mike. Right. I understand that. I just meant that - so you have three choices. There's  um - You can use times where that person is talking only from the transcripts but the segmentations were - were synchronized. Or you can do a forced alignment on the close-talking to determine that  the- you know  within this segment  these really were the times that this person was talking and elsewhere in the segment other people are overlapping and just front-end those pieces. Or you can run it on the whole data  which is - But - but - but how did we get the - how did we determine which is  you know  a - the links  uh  that we're testing on in the stuff we reported? In the H_L_ T paper we took segments that are channel - time-aligned  which is now h- being changed in the transcription process  which is good  and we took cases where the transcribers said there was only one person talking here  because no one else had time - any words in that segment and called that ""non-overlap"". And tha- And that's what we were getting those numbers from. Right. Yes. Tho- good - the good numbers. The bad numbers were from the segments where there was overlap. Well  we could start with the good ones. But anyway - so I think that we should try it once with Yeah. the same conditions that were used to create those  and in those same segments just use one of the P_Z_Ms. Right. So we - we can do that. Yeah. And then  you know  I mean  the thing is if we were getting  uh - what  thirty-five  forty percent  something like that on - on that particular set  uh  does it go to seventy or eighty? Or  does it use up so much memory we can't decode it? Right. It might also depend on which speaker th- it is and how close they are to the P_Z_M? I don't know how different they are from each other. Uh - You want to probably choose the P_Z_M channel that is closest to the speaker. To be best - For this particular digit ones  I just picked that one. Yeah. f- Well - O_K. So we would then use that one  too  or - ? Oh  O_K. So - This is kind of central. You know  it's - so i- but I would - I'd pick that one. It'll be less good for some people than for other  but I - I'd like to see it on the same - exact same data set that - that we did the other thing on. Right? Actually - I sh- actually should've picked a different one  because that could be why the P_D_A is worse. Because it's further away from most of the people reading digits. It's further away. Yeah. Yeah. That's probably one of the reasons. Hmm. Mm-hmm. Well  yeah. You could look at  I guess  that P_Z_M or something. Yep. But the other is  it's very  uh - I mean  even though there's - I'm sure the f- f- the - the S_R_I  uh  front-end has some kind of pre-emphasis  it's - it's  uh - Mm-hmm. still  th- it's picking up lots of low-frequency energy. So  even discriminating against it  I'm sure some of it's getting through. Um. But  yeah  you're right. Prob- a part of it is just the distance. And aren't these pretty bad microphones? Well  they're bad. Yep. I mean - But  I mean  if you listen to it  it sounds O_K. You know? Yeah. When you listen to it  u- Yeah. uh  the P_Z_M and the P_D_A - Yeah  th- the P_D_A has higher sound floor but not by a lot. It's really pretty - uh  pretty much the same. I just remember you saying you got them to be cheap on purpose. Cheap in terms of their quality. So. Well  they're twenty-five cents or so. Yeah. Th- we wanted them to be - to be typical of what would be in a P_D_A. So they are - Mm-hmm. they're not the P_Z_M three hundred dollar type. They're the twenty-five cent  Yeah. buy them in packs of thousand type. I see. But  I mean  the thing is people use those little mikes for everything because they're really not bad. Everything. I mean  if you're not Mm-hmm. doing something ridiculous like feeding it to a speech recognizer  they - they - they - you know  you can hear the sou- hear the sounds just fine. You know  it's - Right. They - I mean  i- it's more or less the same principles as these other mikes are built under  it's just that there's less quality control. They just  you know  churn them out and don't check them. Um. So. So that was - Yeah. So that was i- interesting result. So like I said  the front-end guys are very much interested in - in this is as - as well and So - so  but where is this now? I mean  what's - where do we go from here? I mean  Yeah. That was gonna be my question. we - so we have a - we have a - a system that works pretty well but it's not  you know  the system that people here are used to using - to working with. So what - what do we do now? Well  I think what we wanna do is we want to - eh  and we've talked about this in other contexts - we want to have the ability to feed it different features. Mm-hmm. O_K. And then  um  from the point of view of the front-end research  it would be s- uh  substituting for H_T_K. O_K. I think that's the key thing. And then if we can feed it different features  then we can try all the different things that we're trying there. O_K. Alright. And then  um  uh  also Dave is - is thinking about using the data in different ways  uh  to Mm-hmm. um  uh  explicitly work on reverberation starting with some techniques that some other people have found somewhat useful  and - Yeah. O_K. So - so the key thing that's missing here is basically the ability to feed  you know  other features i- into the recognizer and also then to train the system. Right. Right. O_K. And  uh  es- I don't know when Chuck will be back but that's exactly what he - he's gonna - H- h- He's - he's sort of back  but he drove for fourteen hours an- and wasn't gonna make it in today. Oh  O_K. So  I think that's one of the things that he said he would be working on. Um. Just sort of t- to make sure that we can do that and - Yeah. Yeah. Um. Right. It's - uh  I mean  the - the front-end is f- i- tha- that's in the S_R_I recognizer is very nice in that it does a lot of things on the fly but it unfortunately is not designed and  um - like the  uh  ICSI system is  where you can feed it from a pipeline of - of the command. So  the - what that means probably for the foreseeable future is that you have to  uh  dump out  um - you know  if you want to use some new features  you have to dump them into individual files and give those files to the recognizer. We do - we tend to do that anyway. O_K. Oh. So  although you - you can pipe it as well  we tend to do it that way because that way you can concentrate on one block and not keep re-doing it over and over. Oh  O_K. Alright. Yeah. Yeah. So I've - I - So tha- that's exactly what the P_file is for. Yeah. Yeah  the - the - the cumbersome thing is - is  um - is that you actually have to dump out little - little files. So for each segment that you want to recognize you have to dump out a separate file. Uh - Uh-huh. Just like i- th- like th- as if there were these waveform segments  but instead you have sort of feature file segments. But  you know - So. Cool. O_K. So the s- the - the next thing we had on the agenda was something about alignments? Oh. Yes  we have - I don't know  did you wanna talk about it  or - ? I can give a - I was just telling this to Jane and - and - W- we - we were able to get some definite improvement on the forced alignments by looking at them first and then realizing the kinds of errors that were occurring and um  some of the errors occurring very frequently are just things like the first word being moved to as early as possible in the recognition  which is a um  I think was both a - a pruning problem and possibly a problem with needing constraints on word locations. And so we tried both of these st- things. We tried saying - I don't know  I got this whacky idea that - just from looking at the data  that when people talk their words are usually chunked together. It's not that they say one word and then there's a bunch of words together. They're might say one word and then another word far away if they were doing just backchannels? But in general  if there's  like  five or six words and one word's far away from it  that's probably wrong on average. So  um - And then also  ca- the pruning  of course  was too - too severe. So that's actually interesting. The pruning was the same value that we used for recognition. And we had lowered that - we had used tighter pruning after Liz ran some experiments showing that  you know  it runs slower and there's no real difference in - No gain. Actually it was better with - slightly better or about th- it was the same with tighter pruning. Right. So for free recognition  this - the lower pruning value is better. You - It's probably cuz the recognition's just bad en- at a point where it's bad enough that - that you don't lose anything. Correct. Right. Um  but it turned out for - for - to get accurate alignments it was really important to open up the pruning Right. significantly. Hmm. Um because otherwise it would sort of do greedy alignment  um  in regions where there was no real speech yet from the foreground speaker. Mm-hmm. Um  so that was one big factor that helped improve things and then the other thing was that  you know  as Liz said the - we f- enforce the fact that  uh  the foreground speech has to be continuous. It cannot be - you cannot have a background speech hypothesis in the middle of the foreground speech. You can only have background speech at the beginning and the end. Yeah. I mean  yeah  it isn't always true  and I think what we really want is some clever way to do this  where  um  you know  from the data or from maybe some hand-corrected alignments from transcribers that things like words that do occur just by themselves a- alone  like backchannels or something that we did allow to have background speech around it - Yeah. those would be able to do that  but the rest would be constrained. Sorry. So  I think we have a version that's pretty good for the native speakers. I don't know yet about the non-native speakers. And  um  we basically also made noise models for the different - sort of grouped some of the mouth noises together. Um  so  and then there's a background speech model. And we also - There was some neat - or  interesting cases  like there's one meeting where  um  Jose's giving a presentation and he's talking about  um  the word ""mixed signal"" and someone didn't understand  uh  that you were saying "" mixed "" - I think  Morgan. Yeah  yeah. And so your speech -ch was s- saying something about mixed signal. And the next turn was a lot of people saying ""mixed""  like ""he means mixed signal"" or ""I think it's mixed"". And the word ""mixed"" in this segment occurs  like  a bunch of times. Sh- And Chuck's on the lapel here  and he also says ""mixed"" but it's at the last one  and of course the aligner th- aligns it everywhere else to everybody else's ""mixed""  Yeah. cuz there's no adaptation yet. So there's - I think there's some issues about - u- We probably want to adapt at least the foreground speaker. But  I guess Andreas tried adapting both the foreground and a background generic speaker  and that's actually a little bit of a f- funky model. Like  it gives you some weird alignments  just because often the background speakers match better to the foreground than the foreground speaker. So there's some things there  especially when you get lots of the same words  Oh - Yeah. Oh. uh  occurring in the - Well  the - I - I think you can do better by uh  cloning - so we have a reject phone. And you - and what we wanted to try with - you know  once we have this paper written and have a little more time  uh  t- cloning that reject model and then one copy of it would be adapted to the foreground speaker to capture the rejects in the foreground  like fragments and stuff  and the other copy would be adapted to the background speaker. Right. I mean  in general we actually - And - Right now the words like partial words are reject models Mm-hmm. and you normally allow those to match to any word. But then the background speech was also a reject model  and so this constraint of not allowing rejects in between - you know  it needs to differentiate between the two. So just sort of working through a bunch of Right. debugging kinds of issues. And another one is turns  like people starting with ""well I think"" and someone else is ""well how about"". So the word ""well"" is in this - in this segment multiple times  and as soon as it occurs usually the aligner will try to align it to the first person who says it. But then that constraint of sort of - uh  proximity constraint will push it over to the person who really said it in general. Is the proximity constraint a hard constraint  or did you do some sort of probabilistic weighting distance  or - ? We - we didn't - No. We - w- Right now it's a kluge. O_K. We - it's straightforward to actually just have a - a penalty that doesn't completely disallows it but discourages it. But  um  we just didn't have time to play with  you know  tuning yet another - yet another parameter. The ve- level. Yeah. Yeah. And really the reason we can't do it is just that we don't have a - we don't have ground truth for these. So  we would need a hand-marked  um  word-level alignments or at least sort of the boundaries of the speech betw- you know  between the speakers. Um  and then use that as a reference and tune the parameters of the - of the model  uh  to op- to get the best performance. Yeah. G- given - I - I mean  I wa- I wa- I was gonna ask you anyway  uh  how you assessed that things were better. Mm-hmm. I looked at them. I spent two days - um  in Waves - O_K. Oh  it was painful because the thing is  you know the alignments share a lot in common  so - And you're - yo- you're looking at these segments where there's a lot of speech. I mean  a lot of them have a lot of words. Yeah. Not by every speaker but by some speaker there's a lot of words. Yeah. Ju- No  not - I mean that if you look at the individual Yeah. segments from just one person you don't see a lot of words  but altogether you'll see a lot of words up there. And so the reject is also mapping and pauses - Mm-hmm. Yeah. Yeah. So I looked at them all in Waves and just lined up all the alignments  and  at first it sort of looked like a mess and then the more I looked at it  I thought ""O_K  well it's moving these words leftward and -"" You know  it wasn't that bad. It was just doing certain things wrong. So - But  I don't  you know  have time to l- to look at all of them and it would be really useful to have  like  a - a transcriber who could use Waves  um  just mark  like  the beginning and end of the foreground speaker's real words - like  the beginning of the first word  the end of the last word - and then we could  Yeah. I - O_K. I have to ask you something  which is  first of all  um  you know  do some adjustments. is i- does it have to be Waves? Because if we could benefit from what you did  incorporate that into the present transcripts  that would help. No. And then  um  the other thing is  I believe that I did hand- So. One of these transcripts was gone over by a transcriber and then I hand-marked it myself so that we do have  uh  the beginning and ending of individual utterances. Mm-hmm. Um  I didn't do it word level  but - but in terms - So I - so for - for one of the N_S_ A groups. And also I went back to the original one that I first transcribed and - and did it Mm-hmm. w- uh  w- uh  utterance by utterance for that particular one. So I think you do have - if that's a sufficient unit  I think that you do have hand-marking for that. But it'd be wonderful to be able to benefit from your Waves stuff. Mm-hmm. We don't care what - what tool you use. O_K. I used it in Transcriber and it's - it's in the - Yeah. I mean  if - if you can  um - if you wanna - well  Jane and I were - U- uh - just in terms of the tool  talking about this. I guess Sue had had some reactions. You know  interface-wise if you're looking at speech  you wanna be able to know really where the words are. And so  Yeah  that's right. Middle of the word  or - we can give you some examples of sort of what this output looks like  um  and see if you can in- maybe incorporate it into the Transcriber tool some way  or - Well  I th- I'm thinking just ch- e- e- incorporating it into the representation. I mean  if it's - if it's - Um. You mean like - Yeah  word start insights. if you have start points  if you have  like  time tags  which is what I assume. Isn't that what - what you - ? Well  see  Adam would be - Right. Yeah  whatever you use. I mean  we convert it to this format Yeah. that the  um  NIST scoring tool unders- uh  C_T_M. Conversation Time-Marked file. And - and then that's the - that's what the - I think Transcriber  uh  outputs C_T_M. If it - ? O_K. So you would know this more than I would. I think so. Yeah. So  I mean - Right. It seems like she - if she's g- if she's moving time marks around  since our representation in Transcriber uses time marks  it seems like there should be some way of - of using that - benefitting from that. Right. Yeah  it wou- the advantage would just be that when you brought up a bin you would be able - if you were zoomed in enough in Transcriber to see all the words  you would be able to  like  have the words sort of located in time  Mm-hmm. if you wanted to do that. So. So - so if we e- e- even just had a - a - It sounds like w- we - we almost do. Uh  if we - We have two. Yeah. We have two. Just ha- uh  trying out the alignment Mm-hmm. procedure that you have on that you could actually get something  um - uh  uh  get an objective measure. Mm-hmm. Uh - You mean on - on the hand-marked  um - So we - we only r- hav- I only looked at actually alignments from one meeting that we chose  I think M_R four  just randomly  um - Yeah. And - Actually  not randomly. We knew - we knew that it had these insertion errors from - Yeah. Not randomly - It had sort of average recognition performance in a bunch of speakers and Yeah. it was a Meeting Recorder meeting. Um. But  yeah  we should try to use what you have. I did re-run recognition on your new version of Oh  good. Good! M_R one. I - I mean the - the one with Dan Ellis in it and Eric. Uh-huh. Yeah  exactly. Yeah. Yeah. I don't think that was the new version. Um - That - Yeah  actually it wasn't the new new  it was the medium new. But - but we would - we should do the - the latest version. It was the one from last week. O_K. O_K. Yeah. You - did you adjust the - the utterance times  um  for each channel? Yes. Yes  I did. And furthermore  I found that there were a certain number where - not - not a lot  but several times I actually moved an utterance from Adam's channel to Dan's or from Dan's to Adam's. So there was some speaker identif- And the reason was because I transcribed that at a point before - uh  before we had the multiple audio available f- so I couldn't switch between the audio. I - I transcribed it off of the mixed channel entirely  which meant in overlaps  I was at a - at a terrific disadvantage. Right. Right. In addition it was before the channelized  uh  possibility was there. And finally I did it using the speakers of my  um - of - you know  off the C_P_U on my - on my machine cuz I didn't have a headphone. So it @@   like  I mean - Right. Yeah  I - I mean  i- in retrospect it would've been good to ha- have got- I should've gotten a headphone. But in any case  um  thi- this is - this was transcribed in a - in a  uh  less optimal way than - than the ones that came after it  and I was able to - you know  an- and this meant that there were some speaker identif- identifications which were Well  I know there were some speaker labeling problems  um  after interruptions. Is that what you're referring to? changes. Yeah. Fixed that. Oh  well - I mean  cuz there's this one instance when  for example  you're running down the stairs. I remember this meeting really well. Yeah. Don - Don has had - He knows - he can just read it like a play. Right. It's a - Yeah  I've - I've - I'm very well acquainted with this meeting. Yeah. Yeah  I can s- ""And then she said  and then he said."" Yeah  I know it by heart. So  um  there's one point when you're running down the stairs. Right? And  like  there's an interruption. Uh-oh. You interrupt somebody  but then there's no line after that. For example  there's no speaker identification after that line. Uh-huh. Is that what you're talking about? Or were there mislabelings as far as  like  the a- Adam was - ? That was fixed  um  before - i- i- i- I think I- I think I understood that pretty - Thank you for mentioning. Yeah  no  tha- that - Yeah. Cuz I thought I let you know about that. Yeah. That I think went away a couple of versions ago  but it's good to know. O_K. But you're actually saying that certain  uh  speakers were mis- mis-identified. Yeah. So  with - under - um  uh  listening to the mixed channel  there were times when  as surprising as that is  I got Adam's voice confused with Dan's and vice versa - not for long utterances  but jus- just a couple of places  O_K. O_K. Yeah. and embedde- embedded in overlaps. The other thing that was w- interesting to me was Mm-hmm. that I picked up a lot of  um  backchannels which were hidden in the mixed signal  which  you know  I mean  you c- not - not too surprising. Right. But the other thing that - I - I hadn't thought about this  but I thou- I wanted to raise this when you were - uh  with respect to also a strategy which might help with the alignments potentially  but that's - When I was looking at these backchannels  they were turning up usually - very often in - w- well  I won't say ""usually"" - but anyway  very often  I picked them up in a channel w- which was the person who had asked a question. S- so  like  someone says ""an- and have you done the so-and-so?"" And then there would be backchannels  but it would be the person who asked the question. Other people weren't really doing much backchanneling. And  you know  sometimes you have the - Yeah  uh-huh. Well  that's interesting. I mean  i- it wouldn't be perfect  but - but it does seem more natural to give a backchannel when - Yeah. No  that's really interesting. when you're somehow involved in the topic  and the most natural way is for you to have initiated the topic by asking a question. Mm-hmm. Well  That's interesting. I think - No. I think it's - actually I think what's going on is backchanneling is something that happens in two-party conversations. And if you ask someone a question  you essentially initiating a little two-party conversation. Mm-hmm. Yeah. Well  actu- Yeah  when we looked at this - So then you're - so and then you're expected to backchannel because the person is addressing you directly and not everybody. Exactly. Exactly. Exactly my point. An- and so this is the expectation thing that - uh  uh  just the dyadic - But in addition  Yeah. Yeah. Right. Mm-hmm. Right. H- you know  if someone has done this analysis himself and isn't involved in the dyad  but they might also give backchannels to verify what - what the answer is that this - that the - the answerer's given - I tell you  I say - I say ""uh-huh"" a lot  Right. It's - There you go. Well  but it's interesting cuz  uh - while people are talking to each other. There you go. Yeah. Yeah. But there are fewer - I think there are fewer ""uh-huhs"". I mean  just from - We were looking at word frequency lists to try to find the cases that we would allow to have the reject words in between in doing the alignment. You know the ones we wouldn't constrain to be next to the other words. Oh  yeah. And ""uh-huh"" is not as frequent as it sort of would be in Switchboard  if you looked at just a word frequency list of one-word short utterances. And "" yeah "" is way up there  but not ""uh-huh"". And so I was thinking thi- it's not like you're being encouraged by everybody else to keep talking in the meeting. And uh  that's all  I- I'll stop there  cuz I- I think what you say makes a lot of sense. But it was sort of - Well  that's right. And that would - Well  an- And what you say is the - is the re- uh  o- other side of this  which is that  you know  so th- there are lots of channels where you don't have these backchannels  w- when a question has been asked and - and these - Right. There's just probably less backchanneling Mm-hmm. So that's good news  really. in general  even if you consider every other person altogether one person in the meeting  but we'll find out anyway. We were - I guess the other thing we're - we're - I should say is that we're gonna  um try - compare this type of overlap analysis to Switchboard  And CallHome. where - and CallHome  where we have both sides  so that we can try to answer this question of  you know  Mm-hmm. Mm-hmm. is there really more overlap in meetings or is it just because we don't have the other channel in Switchboard and we don't know what people are doing. Try to create a paper out of that. Yeah. I mean  y- y- you folks have probably already told me  but were - were you intending to do a Eurospeech submission  or - ? Um  you mean the one due tomorrow? Yeah. Yeah. Well  we're still  like  writing the scripts for doing the research  and we will - Yes  we're gonna try. Mm-hmm. And I was telling Don  do not take this as an example of how people should work. Do as I say  don't do as I do. Yeah. That's r- So  we will try. It'll probably be a little late  but I'm gonna try it. Well - It is different. In previous years  Eurospeech only had the abstract due by now  not the full paper. Right. Right. And so all our timing was off. I've given up on trying to do digits. I just don't think that what I have so far makes a Eurospeech paper. Well  I'm no- We may be in the same position  and I figured we'll try  because that'll at least get us to the point where we have - We have this really nice database format that Andreas and I were working out that - It - it's not very fancy. It's just a ASCII line by line format  but it does give you information - It's the - it's the spurt format. It - Yeah  we're calling these ""spurts"" after Chafe. I was trying to find what's a word for a continuous region with pauses around it? Hmm. Yeah. I know that th- the Telecom people use - use ""spurt"" for that. Yes. Good. They do? Oh! Oh. I would jus- Oh. And that's - I mean  I - I was using that for a while when I was doing the rate of speech stuff  because I - because I looked up in some books and I found - O_K  I wanna find a spurt Ah  right! It's just  like  defined by the acoustics. in which - and - an- because - cuz it's another question about how many pauses they put in between them. But how fast do they Horrible. Right. do the words within the spurt? Yeah. you know "" Burst "" also? Isn't ""burst"" is used also? Right. Well  that's what we were calling It's gonna - Burst. spurt  so - Spurt has the horrible name overloading with other - with hardware at ICSI. Here @@ - Here. Just very locally  yeah. But - but that just - Well  well  Chafe had this wor- I think it was Chafe  or somebody had a - the word ""spurt"" originally  and so I - Actually - But tha- that's good to know. Was thi- it's Chafe? Well  see  I know S- Sue wrote about spurts of development. But  in any case  I think it's a good term  and  uh - So maybe we should talk - Maybe it was Sue - ? Y- Hmm! So we have spurts and we have spurt-ify dot shell and spurt-ify Yeah. And ma- maybe - maybe Chafe did. I know - I know Ch- Chafe dealt with - Yeah. Uh. So s- That's cool. And then it's got all - it's a verb now. W- uh  w- Chafe speaks about intonation units. Yes. Right. But maybe he speaks about spurts as well and I just don't know. Yeah  go ahead. We- So what we're doing - uh  this - this is just - maybe someone has s- some - some ideas about how to do it better  but we - I've heard ""burst"" also. Mmm. So we're taking these  uh  alignments from the individual channels. We're - from each alignment we're producing  uh  one of these C_T_M files  which essentially has - it's just a linear sequence of words with the begin times for every word and the duration. Great. And - and - and of course - It looks like a Waves label file almost. Right? It's just - Right. But it has - one - the first column has the meeting name  so it could actually contain several meetings. Um. And the second column is the channel. Third column is the  um  start times of the words and the fourth column is the duration of the words. And then we're  um - O_K. Then we have a messy alignment process where we actually insert into the sequence of words the  uh  tags for  like  where - where sentence - ends of sentence  question marks  um  various other things. Uh. Yeah. These are things that we had Don - So  Don sort of  Right. um  propagated the punctuation from the original transcriber - so whether it was  like  question mark or period or  um  you know  comma and things like that  and we kept the - and disfluency dashes - Mm-hmm. uh  kept those in because we sort of wanna know where those are relative to the spurt overlaps - sp- overlaps  or - Right. So - so those are actually sort of retro-fitted into the time alignment. And then we merge all the alignments from the various channels and we sort them by time. And then there's a - then there's a process where you now determine the spurts. That is - Actually  no  you do that before you merge the various channels. So you - you id- identify by some criterion  which is pause length - you identify the beginnings and ends of these spurts  and you put another set of tags in there to keep those straight. Mm-hmm. And then you merge everything in terms of  you know  linearizing the sequence based on the time marks. And then you extract the individual channels again  but this time you know where the other people start and end talking - you know  where their spurts start and end. And so you extract the individual channels  uh  one sp- spurt by spurt as it were. Um  and inside the words or between the words you now have begin and end tags for overlaps. So  you - you basically have everything sort of lined up and in a form where you can look at the individual speakers and how their speech relates to the other speakers' speech. Right. Uh  I mean  I think that's actually really u- useful also because And - even if you weren't studying overlaps  if you wanna get a transcription for the far-field mikes  how are you gonna know which words from which speakers occurred at which times relative to each other? You have to be able to get a transcript like - like this anyway  just for doing far-field recognition. So  you know  Yeah. it's - it's sort of - I thi- it's just an issue we haven't dealt with before  how you time-align things that are overlapping anyway. That's wonderful. So - Well - And - and we - I mean  i- I never thought about it before  but - Y- yes. I mean  s- when I came up with the original data - suggested data format based on the transcription graph  there's capability of doing In - that sort of thing in there. Right. But you can't get it directly from the transcription. Yeah  this is like a poor man's ver- formatting version. But it's  you know - It's clean  it's just not fancy. Mm-hmm. Yeah  that's right. Right. Well  this is - this is just - Well  there's lots of little things. It's like there're twelve different scripts which you run and then at the end you have what you want. But  um  Right. Um. at the very last stage we throw away the actual time information. All we care about is whether - that there's a certain word was overlapped by someone else's word. So you sort of - at that point  you discretize things into just having overlap or no overlap. Because we figure that's about the level of analysis that we want to do for this paper. Mm-hmm. But if you wanted to do a more fine-grained analysis and say  you know  how far into the word is the overlap  you could do that. It's just - it'll just require more - Yeah. Just sort of huge. you know  slightly different - What's interesting is it's exactly what  um  i- in discussing with  um  Sue about this  um  she  um  Yeah. i- i- i- indicated that that - you know  that's very important for overlap analysis. Yeah. It's - it's nice to know  and also I think as a human  like  I don't always hear these in the actual order that they occur. So I can have two foreground speakers  you know  Morgan an- and Right. um  Adam and Jane could all be talking  and I could align each of them to be starting their utterance at the correct time  and then look where they are relative to each other  and that's not really what I heard. And that's another thing she said. This is - This is Bever's - Bever's effect  when - where - Cuz it's just hard to do. Y- Yeah. It's sort of - In psy- ps- psycho-linguistics you have these experiments where people have perceptual biases a- as to what they hear  that - that - Yeah  you sort of move things around until you get to a low information point and yo- then you can bring in the other person. So it's Not the best - actually not even possible  I think  for any person to listen to a mixed signal  even equalize  and make sure that they have all the words in the right order. Mm-hmm. So  I guess  we'll try to write this Eurospeech paper. I mean  we will write it. Whether they accept it late or not  I don't know. Superb. Um  and the good thing is that we have - It's sort of a beginning of what Don can use to link the prosodic features from each file to each other. Yeah. Yeah. That's the good thing about these pape- Hmm? Plus  mayb- So. i- You know  might as well. We- I ju- Otherwise we won't get the work done I don't know  m- I mean  u- u- Jane likes to look at data. Maybe  you know  you could - you could look at this format and see if you find anything interesting. on our deadline. Yeah. I don't know. Yeah. No  it's - that's the good thing about these pape- paper deadlines and  uh  you know  class projects  and - and things like that  because you - you really get g- Yeah. Well  what I'm thinking is - Yeah. Yeah. Yeah. Right. Well  my - Mm-hmm. Well th- th- the other thing that - that - that yo- that you usually don't tell your graduate students is that these deadlines are actually not that  Yeah. Forces you to do the work. Exactly. um  you know  strictly enforced  because the - Strict. @@ Oh  now it's out in the public  this - this - this secret information. Yeah. I think we can ha- Right. So - because - bec- b- Nah - No. No. Nah. i- Because these - the conference organizers actually have an interest in getting lots of submissions. I mean  a - a monetary interest. So - Right. Right. Yeah. Um. Th- that's - that's true. And good ones  good ones  which sometimes means a little extra time. And good submission- Right. Well - That's - That's true. That's another issue  but - By th- by the way  this is totally unfair  you may - you may feel  but the - the  uh - the morning meeting folks actually have an - an extra month or so. Mm-hmm. Yep. Yep. The Aurora - there's a special Aurora - When - Uh - There's a special Aurora session and the Aurora pe- people involved in Aurora have till Ma- uh  early May or something to turn in their paper. Oh. Mmm. Oh. Mmm. Well  then you can just - Oh  well maybe we'll submit to s- Actually - Maybe you can submit the digits paper on e- for the Aurora session. Yeah. Yeah. Yeah. Oh  I could! I could submit that to Aurora. That would be pretty - pretty - I- if it w- Yeah. Well - Yeah. i- it has - Yeah. S- That wouldn't work. It's not Aurora. @@ No  it wouldn't work. It's - it's not the Aurora - I mean  it - it's - it's actually the Aurora task. Maybe they'll get s- Aurora's very specific. It- Well  maybe it won't be after this deadline extension. Maybe they'll - But - but the people - I mean  a - a paper that is not on Aurora would probably be more interesting at that point because everybody's so sick and tired of the Aurora task. Yeah. Oh  I thought you meant this was just the digits section. I didn't know you meant it was Aurora digits. Yeah. Well  no. If you - if you have - it's to - if you discuss some relation to the Aurora task  like if you use the same - This is not the Aurora task. So they just do a little grep for - Do - uh  d- d- Um. Do not - do not - we are not setting a good example. This is not a - Well  a relation other than negation  maybe  um. So. I don't know. Anyway. But the good thing is this does - Well  I- I don't know. I mean  you could - you could do a paper on what's wrong with the Aurora task by comparing it to other ways of doing it. How well does an Aurora system do on - on - you know  on digits collected in a - in this environment? Yeah. @@ Different way. Yeah. Maybe. Maybe. Pretty hokey. I think it's a littl- little far-fetched. Nah  I mean  the thing is Aurora's pretty closed community. I mean  you know  the people who were involved in the - Yep. Mm-hmm. the only people who are allowed to test on that are people who - who made it above a certain threshold in the first round  It's very specific. uh w- in ninety-nine and it's - it's sort of a - it's - not like a - Well  that's maybe why they don't f- know that they have a crummy system. I mean  a crummy back-end. No  I mean - I mean  seriously  if you - if you have a very - No  I'm sorry. Uh  ""beep"" ""bee-"" No. I didn't mean I mean  th- anybody - any particular system. I meant this H_T_ K back-end. If they - Oh  you don't like H_T_K? Yeah. I don't h- I don't have any stock in H_T_K or Entropic or anything. No. I mean  this - it- it's the H_T_K that is trained on a very limited amount of data. Yeah. It's d- it's very specific. Right. But so  if you - But maybe you should  you know  consider more - using more data  or - I mean - Oh  yeah. I - I really think that that's true. And they i- i- If yo- if you sort of hermetically stay within one task and don't look left and right  then you're gonna - But they - they had - i- But - They had something very specific in mind when they designed it. Right. Well  u- i- Right? And so - so you can - you can argue about maybe that wasn't the right thing to do  but  you know  they - they - But  one of the reasons I have Chuck's messing around with - with the back-end that you're not supposed to touch - they had something specific. Mm-hmm. I mean  for the evaluations  yes  we'll run a version that hasn't been touched. Mm-hmm. But  uh  one of the reasons I have him messing around with that  because I think it's sort of an open question that we don't know the answer to. People always say very glibly that i- if you s- show improvement on a bad system  that doesn't mean anything  cuz it may not be - show - uh  because  you know  it doesn't tell you anything about the good system. Mm-hmm. And I - I've always sort of felt that that depends. You know  that if some peopl- If you're actually are getting at something that has some conceptual substance to it  it will port. Mm-hmm. And in fact  most methods that people now use were originally tried with something that was not their absolute best system at some level. But of course  sometimes it doesn't  uh  port. So I think that's - that's an interesting question. If we're getting three percent error on  uh  u- uh  English  uh  nati- native speakers  um  using the Aurora system  and we do some improvements and bring it from three to two  do those same improvements bring  uh  th- you know  the S_R_I system from one point three to - you know  to Hmm. Mm-hmm. Zero. point eight? Well. You know  so that's - that's something we can test. Mmm. Right. So. Anyway. O_K. I think we've - we've covered that one up extremely well. Mm-hmm. Whew! O_K. So  um - Yeah. So tha- so we'll - you know  maybe you guys'll have - have one. Uh  you - you and  uh - and Dan have - have a paper that - that's going in. You know  that's - that's pretty solid  on the segmentation stuff. Yeah. Yeah. Yeah. I will send you the - the final version  which is not - Yeah. And the Aurora folks here will - will definitely get something in on Aurora  so. Actually this - this  um - So  there's another paper. It's a Eurospeech paper but not related to meetings. But it's on digits. So  um  uh  a colleague at S_R_I developed a improved version of M_M_I_E training. Uh-huh. And he tested it mostly on digits because it's sort of a - you know  it doesn't take weeks to train it. Right. Um. And got some very impressive results  um  with  you know  discriminative  uh  Gaussian training. Um  you know  like  um  error rates go from - I don't know  in very noisy environment  like from  uh  uh - I for- now I - O_K  now I have the order of magnit- I'm not sure about the order of magnitude. Was it like from ten percent to eight percent or from e- e- you know  point - you know  from one percent to point eight percent? I mean  it's a - H- i- it got - it got better. Yeah  yeah. It got better. That's the important thing. Yeah. But it's - Hey  that's the same percent relative  so - Yeah. Yeah. Yeah. Right. It's  uh  something in - Right. Yeah. Twenty percent relative gain. Yeah. Yeah. Yeah. Um  let's see. I think the only thing we had left was - unless somebody else - Well  there's a couple things. Uh  one is anything that  um  anybody has to say about Saturday? Anything we should do in prep for Saturday? Um - I guess everybody knows about - I mean  u- um  Mari was asking - was trying to come up with something like an agenda and we're sort of fitting around people's times a bit. But  um  clearly when we actually get here we'll move things around this  as we need to  but - so you can't absolutely count on it. But - but  uh - O_K. Yeah. Are we meeting in here probably or - ? O_K. Yeah. That was my thought. I think this is - Are we recording it? Yeah. @@ We won't have enough microphones  but - u- No. I - I hadn't in- intended to. We won- we wanna - I mean  they're - There's no way. O_K. there's gonna be  uh  Jeff  Katrin  Mari and two students. So there's five from there. And Brian. But you know th- And Brian's coming  so that's six. Mm-hmm. And plus all of us. Can use the Oprah mike. Uh - Depends how fast you can throw it. It's just - It seems like too many - too much coming and going. Mm-hmm. Yeah. Well - We don't even have enough channel - Because it would be a different kind of meeting  that's what I'm - Yeah. Well - Yeah. But - I hadn't really thought of it  but - Maybe just - maybe not the whole day but just  you know  maybe some - I mean  part of it? Maybe part of it. Maybe part of it. Make everyone read digits. At the same time. At the same time. At the same time. Please. Yeah. @@ We c- I don't know. Any- That's their initiation into our Into our - our - our cult. w- Yeah  our - Yeah  our - Maybe the sections that are not right afte- you know  after lunch when everybody's still munching and - O_K. Well - So can you send out a schedule once you know it  jus- ? O_K. Yeah. I guess I sent it around a little bit. But - Is - is there a r- ? There's a res- Is it changed now  or - ? I hadn't heard back from Mari after I - I u- u- uh  brought up the point abou- about Andreas's schedule. So  um  maybe when I get back there'll be some - some mail from her. O_K. I'm looking forward to seeing your representation. That'd be  uh - So  I'll make a - And w- we should get the two meetings from y- I mean  I know about the first meeting  um  I'd like to see that. Yeah. but the other one that you did  the N_S_A one  which we hadn't done cuz we weren't running recognition on it  because the non-native speaker - Mm-hmm. there were five non-native speakers. Mm-hmm. I see. Mm-hmm. But  it would be useful for the - to see what we get with that one. So. Which N_S_A meeting was that? Great. O_K. It's  uh  two thousand eleven twenty-one one thousand. Oh  we did - That was the last one I gave you. Yeah  three. Right. So - Mm-hmm. Great. I sent email when I finished the - that one. That was sort of son- N_S_ A three  I think. Yeah. Yeah  that's right. That's right. That's much simpler. I don't know what they said but I know the number. Th- that part's definitely gonna confuse somebody who looks at these later. I mean  this is - we- we're recording secret N_S_A meetings? I mean  it's - Right. I remember the date. So. Um. Not the - Yeah. Uh. The - th- the - Yeah. Not that N_S_A. It's network services and applications. They are hard to understand. They're very  uh  out there. I have no idea what they're talking about. Wait. The - Yeah  they're cryptic. The  um - Yeah. th- the other good thing about the alignments is that  um  it's not always the machine's fault if it doesn't work. So  you can actually find  um  It's the person's fault. It's Morgan's fault. problem - uh  proble- You can find - You can find  uh  problems with - with the transcripts  um  you know  It's always Morgan's fault. Oh. and go back and fix them. But - Yeah. Tha- There are some cases like where the - the wrong speaker - uh  these ca- Not a lot  but where the - the wrong person - the - the speech is addre- attached to the wrong speaker and you can tell that when you run it. Or at least you can get clues to it. So these are from the early transcriptions that people did on the mixed signals  like what you have. Interesting. I guess it does w- Mm-hmm. It also raises the possibility of  um  using that kind of representation - I mean  I don't know  this'd be something we'd wanna check  but maybe using that representation for data entry and then displaying it on the channelized  uh  representation  cuz it - I think that the - I mean  my - my preference in terms of  like  looking at the data is to see it in this kind of musical score format. Mm-hmm. And also  s- you know  Sue's preference as well. Yeah  if you can get it to - And - and - but  I mean  this - if this is a better interface for making these kinds of  uh  you know  lo- clos- local changes  then that'd be fine  too. I don't - I have no idea. I think this is something that would need to be checked. Yeah. O_K. Th- the other thing I had actually was  I - I didn't realize this till today  but  uh  this is  uh  Jose's last day. Is my last - my last day. My - my last meeting about meetings. Yeah. Oh! Oh! Oh! Oh. You're not gonna be here tomorrow? Oh  that's right. Tomorrow - Yeah. Because  eh  I leave  eh  the next Sunday. I will come back to home - to Spain. The last meeting meeting? It's off. Mm-hmm. Oh. Yeah. I d- so I - I jus- Mm-hmm. And I - I would like to - to - to say thank you very much  eh  to all people in the group and at ICSI  because I - I enjoyed @@ very much  uh. Oh. Mm-hmm. Yeah. It was good having you. Mmm. Yeah. Mmm. And I'm sorry by the result of overlapping  because  eh  I haven't good results  eh  yet but  eh  I - I pretend to - to continuing out to Spain  eh  during the - the following months  eh  because I have  eh  another ideas Uh-huh. but  eh  I haven't enough time to - to - with six months it's not enough to - Yep. Yeah. to - to research  eh  and e- i- I mean  if  eh  the topic is  eh  so difficult  uh  in my opinion  there isn't - Yeah. Maybe somebody else will come along and will be  uh  interested in working on it and could start off from where you are also  you know. They'd make use of - of what you've done. Yeah. Yeah. Yeah. But  eh  I - I will try to recommend  eh  at  eh  the Spanish government but  eh  the following @@ scholarship  eh  eh  eh  will be here more time  because eh  i- in my opinion is - is better  eh  for us to - to spend more time here and to work more time i- i- in a topic. No ? Yeah  it's a very short time. Yeah. Yeah. But  uh - Yeah  six months is hard. I think a year Yeah. It is . Yeah. Yeah. It's difficult. is a lot better. You - e- you have  eh - you are lucky  and you - you find a solution in - in - in some few tim- uh  months  eh? O_K. But  eh  I think it's not  eh  common. But  eh  anyway  thank you. Thank you very much. Eh  I - I bring the chocolate  eh  to - Mmm. to tear  uh  with - with you  uh. Oh. Ah. Nice. I - I hope if you need  eh  something  eh  from us in the future  I - I will be at Spain  to you help  uh. Great. Well. Great. Thank you. Thank you  Jose. Right. And  thank you very much. Thank you. Have a good trip. Yeah. Yeah. Thank you. Yeah. Keep in touch. O_K. I guess  uh  unless somebody has something else  we'll read - read our digits and we'll get our - Digits? Uh. Oops. Are we gonna do them simultaneously or - ? get our last bit of  uh  Jose's - Jose - Jose's digit - Uh  I'm sorry? You - eh - Ye- ye- you prefer  eh  to eat  eh  chocolate  eh  at the coffee break  eh  at the - ? Or you prefer now  before after - ? Well  we have a time - No  we prefer to keep it for ourselves. Well  we have a s- a time - time constraint. Yeah. Yeah. During - So keep it away from that end of the table. Yeah  yeah. Yeah. during digits. Yeah. Why is it that I can read your mind? Well  we've gotta wait until after di- after we take the mikes off. No  no. So are we gonna do digits simultaneously or what? Well? Yeah. You - This is our reward if we do our digi- Yeah. O_K. I - I think  eh  it's enough  eh  for more peopl- for more people after. But  eh - Simultaneous digit chocolate task. We're gonna - we're gonna do digits at the same - Oh. I'm going to get a ticket if people don't eat some. Mmm! That's nice. Wow. Mm-hmm. So  I have a vested interest. Um. Well - Oh  thanks  Jose. To Andreas  the idea is - is good. s- To eat here. Mmm. Wow. Very nice. Oh. Tha- that's - that looks great. Alright  so in the interest of getting to the - Oh  wow. Oh  yeah. Th- it doesn't - it won't leave this room. We could do digits while other people eat. So it's background crunching. Yeah. Yeah. Mmm. Yeah. Is  eh  a - another acoustic event. Nice. We don't have background chewing. Background crunch. Yeah. No  we don't have any data with background eating. Mmm. Yeah. I'm serious. You- She's - she's serious. She is serious. It's just the rest of the digits - the rest of the digits are very clean  I am serious. Mmm. Are you - ? Oh  they're clean. Well - ? um  without a lot of background noise  so I'm just not sure - Yeah ! And it - You have to write down  like  while y- what you're - what ch- chocolate you're eating cuz they might make different sounds  like n- nuts - chocolate with nuts  chocolate without nuts. Chocolate adaptation. Oh. Crunchy frogs. Um - Actually - actually kind of careful cuz I have a strong allergy to nuts  so I have to sort of figure out one without th- That w- Oh  yeah  they - they might. It's hard to - hard to say. Maybe those? They're so - I don't know. Um - I don't know. This is - You know  this is a different kind of speech  looking at chocolates  deciding - you know  it's another style. Well - Take - take several. Mmm. Mmm. Yeah. I may - I may hold off. But if I was - eh  but maybe I'll get some later. Mmm. Thanks. Well - well  why don't we - ? He - he's worried about a ticket. Why don't we do a simultaneous one? Yeah. Thank you! O_K. O_K. O_K. Mmm. Simultaneous one? O_K. Remember to read the transcript number  please. And you laughed at me  too  f- the first time I said that. Right. O_K. Oops. Yeah. I have to what? You laughed at me  too  the first time I sa- said - I did  and now I love it so much. You really shouldn't  uh  te- O_K  everyone ready? You have to sort of  um - Jose  if you haven't done this  you have to plug your ears while you're t- talking so that you don't get confused  I guess. Transcript L_ four four. W- wait - wait a minute - wait a minute. W- we want - we want - we want it synchronized. Yeah. Hey  you've done this before. Haven't you? Yeah. Oh  you've done this one before? Together? That's - You've read digits together with us  haven't you - I mean  at the same time? No. Oh  you haven't! Oh  O_K. I'm not - we - we - Oh  and you haven't done this either. I- the first time is traumatic  but - O_K. Oh  yeah. We- Y- Yeah  bu- Oh  and the groupings are important  so yo- you're supposed to pause between the groupings. Mmm. The grouping. Yeah. O_K. So  uh - Yeah. You mean that the - the grouping is supposed to be synchronized? Yeah  sure. No. No. No  no. Synchronized digits. No? Oh  wow. No? That'd be good. We- we'll give everybody the same sheet but they say different - It's like a - like a Greek - like a Greek choir? You know? Like - Yeah. Hey  what a good idea. We could do the same sheet for everyone. Have them all read them at once. Yes. Well  different digits but same groupings. Eh - Or - or just same digits. See if anyone notices. So they would all be - Yeah. Yeah. That'd be good. And then - then we can sing them next time. There's so many possibilities. Uh. O_K  why don't we go? O_K. Uh  one two three - Go! L_ forty four  Transcript L_ twenty-nine. Transcript L_ dash four one. Transcript L_ forty-six. Four  eight eight seven  two six  one one two  one. Transcript L_ forty three. Transcript L_ forty five. Six eight six  nine three two  nine two three. Two  eight six eight  two three  two five three  eight. s- seven nine six seven  eight  four eight five  Four  zero three six  three five  one four three  one. Nine six one  eight five  nine seven seven five. Zero five five three  two  one nine zero. Forty nine  seventy seven  one six  zero three  five zero. Nine three two six  six five  nine three  three four one one. Seven one O_ eight  one nine eight four  five eight seven seven. seven four one nine  two six eight nine  four nine seven one. Zero four eight  nine four three  two three one. Two nine nine  nine four five  three four nine three. Eight two seven  two one six  four three six. Three nine five nine  five nine one eight  five one seven five. I'm starting over. Seven  five six two  six nine  three three four  four. Four nine  seven seven  one six  zero three  five zero  Two eight four three  three six two three  O_ eight five four. Zero six six  five four  two four three zero. Five nine one zero  zero nine six six  eight zero eight eight. Nine three nine  six zero five  four seven seven. three  two five four  six four  one  five six eight  Four one two six  four one five four  three four seven three. Six eight seven  eight five eight  eight one four. Three one three  O_ nine six  nine four nine. Five three  seven three  six seven  seven five zero seven. Five six one  four nine six  three six four six. Seven  seven nine two  six nine  four five zero  four. two zero nine seven  seven nine nine seven  seven eight seven eight. Five  two seven one  six one  zero five eight  one. Three six one  three six nine  Oops. Three seven nine  four O_ seven. five O_ three  three two five  five two zero nine  Six four seven seven  two zero  eight two  four seven. Zero six eight two  eight  six one nine. Two two eight eight  one eight four seven  one one two seven. Nine zero nine  three nine  two five two five. Seven eight three zero  eight five eight one  nine seven two eight. One five five  two seven  three eight nine two. Eight one  nine six  nine O_  three six  three six. Three zero three three  zero four four five  one six zero fi- six. Seven four one  zero five five  nine eight two. Seven one one eight  four one seven five  five five zero one. two seven  two four  two nine  six three  five six  Nine O_ five seven  six eight four five  one five three eight  Eight four eight nine  one  one five six. One four two two  seven four three one  nine nine three three. Seven  three five one  three four  two three one  zero. Six nine  one four  zero five  seven nine  five six  nine two. One seven three  four eight  zero seven nine three. Twenty-s- Oh  sorry. Two one eight  seven one six  five one eight  two nine  eight four  eight two  O_ three  five two. Two seven  five one  zero two  five zero  two two. Eight four seven two  O_ seven two three  O_ one seven O_. Nine seven  three seven  five nine  seven seven  two three. Four eight four three  four seven five one  eight five nine eight. eight two nine zero  six four three one  six eight five five  One nine  one six  five four  eight four  three one. four eh five eh seven  six eight  eh nine zero one two Two nine eight  one two five  two two nine nine. Seven seven five  zero eight three  nine five four seven. t- two two five eight  five two eight nine  five seven seven eight. four six nine  one five six  four seven eight one  Mmm! five four eight seven  three  five five five  one six three one  two  eight three eight  eight zero nine  zero seven four  one six four seven. Did you read it twice or what? And Andreas has the last word. He's try- No  he's trying to get good recognition performance. He had the h- Yeah. Yeah. He had the - the long form. No. And we're off. ",The Berkeley Meeting Recorder group discussed recording equipment and setup issues  recent developments in the transcription effort  other potential types of tagging to be assigned to transcribers  and the post-processing of waveforms. The discussion was largely focused on efforts to facilitate transcriptions  including the improvement of strategies for transcribing overlapping speech  and achieving greater uniformity in the type of equipment used during recordings and the manner in which recording devices are worn by speakers. To achieve greater uniformity in across-speaker recording conditions  the group decided to purchase three additional head-mounted microphones. Future work will include recording more varied meeting data from non-ICSI discussion groups. It was proposed by speaker fe016 that better communication be established between researchers involved in post-processing of the waveform and ASR. Use of dissimilar microphones adds an extra  unwanted variable to individual speaker recordings. Similarly  differences in the type of recording equipment used and the manner in which microphones are worn by speakers causes problems for the transcription effort. Setting up a microphone array and performing video recordings (in a possible collaboration with NIST) are problematic due to the types of changes in infrastructure they require. IBM's single-channel approach to transcriptions may pose problems for the post-processing of waveforms and forced alignments  as the group foresees difficulties in referencing chopped segments back to the original times/locations from which they were extracted. Another post-processing problem involves cross-talk  and  in particular  situations in which a speaker whose contributions to the discussion are relatively sparse but whose microphone picks up signals from the other speakers. Modifications are being made to multi-trans to enable tight time markings at the boundaries of overlapping speech segments  and facilitate the transcription of such segments. Pre-segmentation continues to be very beneficial to the transcription effort. Work by speaker mn014 is in progress to compare the energies of different channels for detecting speech/non-speech portions  facilitating transcriptions and potentially providing speaker identification information. Efforts are being developed to create a cross-correlation setup linking recorded data with a map of where individual speakers were seated. The transcriber pool has been performing within the expected range of work completed per the amount of time spent transcribing. IBM has a team of people employed to transcribe meeting data  and who are transcribing single versus multiple channels. The group discussed the potential for assigning additional tasks to ICSI's transcriber pool  including tagging more fine-grained acoustic information  and discourse and disfluency tagging. 
"O_K. Mike. Mike-one? Ah. We're on? Have a good meeting. Do you want this closed? Yes  please. I mean  we're testing noise robustness but let's not get silly. O_K  so  uh  you've got some  uh  Xerox things to pass out? That are - Yeah. Yeah  um. Yeah. Yeah  I'm sorry for the table  but as it grows in size  uh  it. Uh  so for th- the last column we use our imagination. O_K. Ah  yeah. Ah. Uh   yeah. This one's nice  though. This has nice big font. Uh  do you want @@ . Yeah. @@ Let's see. Yeah. Yeah. Chop! When you get older you have these different perspectives. I mean  lowering the word hour rate is fine  but having big font! So I- That's what's - Next time we will put colors or something . Yeah. It's mostly big font. Uh. O_K. Uh - O_K  s- Go ahead. so there is kind of summary of what has been done - It's this. Summary of experiments since  well  since last week and also since the - Oh. O_K. we've started to run - work on this. Um. So since last week we've started to fill the column with um uh features w- with nets trained on P_L_P with on-line normalization but with delta also  because the column was not completely - well  it's still not completely filled  but Mm-hmm. Mm-hmm. we have more results to compare with network using without P_L_P and finally  hhh  um ehhh P_L- uh delta seems very important. Uh I don't know. If you take um  let's say  anyway Aurora-two-B_  so  the next - t- the second  uh  part of the table  Mm-hmm. uh when we use the large training set using French  Spanish  and English  you have one hundred and six Mm-hmm. without delta and eighty-nine with the delta. a- And again all of these numbers are with a hundred percent being  uh  the baseline performance  but with a mel cepstra system going straight into the H_T_K? Yes. Yeah  on the baseline  yeah. So - Yeah. Yeah. So now we see that the gap between the different training set is much uh uh much smaller It's out of the way. um - Right. But  actually  um  for English training on TIMIT is still better than the other languages. And Mmm  Yeah. And f- also for Italian  actually. If you take the second set of experiment for Italian  so  the mismatched condition  Mm-hmm. um when we use the training on TIMIT so  it's multi-English  we have a ninety-one number  Mm-hmm. and training with other languages is a little bit worse. Um - Oh  I see. Down near the bottom of this sheet. Uh  yes. So  yeah. O_K. And  yeah  and here the gap is still more important between using delta and not using delta. If y- if I take the training s- the large training set  it's - we have one hundred and seventy-two  Yes. Yeah. and one hundred and four when we use delta. Mm-hmm. Uh. Even if the contexts used is quite the same  because without delta we use seventeenths - seventeen frames. Uh. Yeah  um  so the second point is that we have no single cross-language experiments  uh  that we did not have last week. Uh  so this is training the net on French only  or on English only  and testing on Italian. Mm-hmm. And training the net on French only and Spanish only and testing on  uh T_I-digits. Mm-hmm. And  fff um  yeah. What we see is that these nets are not as good  except for the multi-English  which is always one of the best. Yeah  then we started to work on a large dat- database containing  uh  sentences from the French  from the Spanish  from the TIMIT  from SPINE  uh from uh English digits  and from Italian digits. So this is the - another line - another set of lines in the table. Ah  yes. Mm-hmm. Uh  @@ with SPINE and uh  actually we did this before knowing the result of all the data  uh  so we have to- to redo the uh - the experiment training the net with  uh P_L_P  but with delta. Mm-hmm. But um this - this net performed quite well. Well  it's - it's better than the net using French  Spanish  and English only. Uh. So  uh  yeah. We have also started feature combination experiments. Uh many experiments using features and net outputs together. And this is - The results are on the other document. Uh  we can discuss this after  perhaps - well  just  @@ . Yeah  so basically there are four - four kind of systems. The first one  yeah  is combining  um  two feature streams  uh using - and each feature stream has its own M_P_L. So it's the - kind of similar to the tandem that was proposed for the first. The multi-stream tandem for the first proposal. The second is using features and K_L_T transformed M_L_P outputs. And the third one is to u- use a single K_L_T trans- transform features as well as M_L_P outputs. Um  yeah. Mmm. You know you can - you can comment these results  or - Yes  I can s- I would like to say that  for example  um  mmm  if we doesn't use the delta-delta  uh we have an improve when we use s- some combination. But when w- Yeah  we- ju- just to be clear  the numbers here are Yeah  this - uh recognition accuracy. Yeah  this number recognition acc- So it's not the - Again we switch to another - Yes  and the baseline - the baseline have - i- is eighty-two. Mm-hmm. Baseline is eighty-two. Yeah So it's experiment only on the Italian mismatched for the moment for this. Uh  this is Italian mismatched. O_K. Um. Yeah  by the moment. Mm-hmm. And first in the experiment-one I - I do - I - I use different M_L_P  Mm-hmm. and is obviously that the multi-English M_L_P is the better. Um. for the ne - rest of experiment I use multi-English  only multi-English. And I try to combine different type of feature  but the result is that the M_S_G-three feature doesn't work for the Italian database because never help to increase the accuracy. Yeah  eh  actually  if w- we look at the table  the huge table  Mm-hmm. um  we see that for T_I-digits M_S_G perform as well as the P_L_P  but this is not the case for Italian what - where the error rate is c- is almost uh twice the error rate of P_L_P. Mm-hmm. So  um uh  well  I don't think this is a bug but this - this is something in - probably in the M_S_G um process that uh I don't know what exactly. Perhaps the fact that the - the - there's no low-pass filter  well  or no pre-emp- pre-emphasis filter and that there is some D_C offset in the Italian  or  well  something simple like that. But - that we need to sort out if want to Mm-hmm. uh get improvement by combining P_L_P and M_S_G because for the moment M_S_G do- doesn't bring much information. And Mm-hmm. I- as Carmen said  if we combine the two  we have the result  basically  of P_L_P. Um  the uh  baseline system - when you said the baseline system was uh  uh eighty-two percent  that was trained on what and tested on what? Mm-hmm. That was  uh Italian mismatched d- uh  uh  digits  uh  is the testing  and the training is Italian digits? Yeah. Yeah. So the ""mismatch"" just refers to the noise and - and  uh microphone and so forth  right? Yeah. Yeah. So  um did we have - So would that then correspond to the first line here of where the training is - is the uh Italian digits? The - 3x The train- the training of the H_T_K? Yes. Ah yes! This h- Yes. Th- Yes. Yes. Yes. Training of the net  yeah. Yeah. So  um - So what that says is that in a matched condition  we end up with a fair amount worse putting in the uh P_L_P. Now w- would - do we have a number  I suppose for the matched - I - I don't mean matched  but uh use of Italian - training in Italian digits for P_L_P only? Uh yes? Uh yeah  so this is - basically this is in the table. Uh so the number is fifty-two  Another table. uh - Fifty-two percent. Fift- So - No. No  fifty-two percent of eighty-two? No  it's - it's the - Of - of - of uh eighteen - of eighteen. Eighty. Eighty. So it's - it's error rate  basically. It's er- error rate ratio. So - It's plus six. Oh this is accuracy! Oy! O_K. Yeah. Uh  so we have nine - nine - let's say ninety percent. Ninety. Yeah. Um which is uh what we have also if use P_L_P and M_S_G together  eighty-nine point seven. Yeah. O_K  so even just P_L_P  uh  it is not  in the matched condition - Um I wonder if it's a difference between P_L_P and mel cepstra  or whether it's that the net half  for some reason  is not helping. Uh. P_- P_L_P and Mel cepstra give the same - Same result pretty much? same results. Well  we have these results. I don't know. It's not - So  s- Do you have this result with P_L_P alone  j- fee- feeding H_T_K? That - That's what you mean? Yeah  yeah yeah yeah yeah  at the first - Just P_L_P at the input of H_T_K. and the - Yeah. Yeah. So  P_L_P - Eighty-eight point six. Yeah. Um  so adding M_S_G Um - um - Well  but that's - yeah  that's without the neural net  right? Yeah  that's without the neural net and that's the result basically that O_G_I has also with the M_F_C_C with on-line normalization. But she had said eighty- two. Right? This is the - w- well  but this is without on-line normalization. Oh  this - Yeah. @@ the eighty- two. Eighty-two is the - it's the Aurora baseline  so M_F_C_C. Then we can use - well  O_G_I  they use M_F_C_C - th- the baseline M_F_C_C Oh  I'm sorry  I k- I keep getting confused because this is accuracy. plus on-line normalization @@ Yeah  sorry. Yeah. O_K. Yeah. Alright. Alright. So this is - I was thinking all this was worse. O_K so this is all better because eighty- nine is bigger than eighty-two. O_K. Yeah. Yes  better. Mm-hmm. Yeah. I'm - I'm all better now. O_K  go ahead. So what happ- what happens is that when we apply on-line normalization Yeah. we jump to almost ninety percent. Mm-hmm. Uh  when we apply a neural network  is the same. We j- jump to ninety percent. Yeah. Nnn  we don't know exactly. And - And um - whatever the normalization  actually. If we use n- neural network  even if the features are not correctly normalized  we jump to ninety percent. So we go from eighty-si- eighty-eight point six to - to ninety  or something. So - Well  ninety - No  I - I mean ninety- It's around eighty-nine  ninety  eighty-eight . Eighty-nine. Yeah. Well  there are minor - minor differences. And then adding the M_S_G does nothing  basically. No. Yeah. O_K. Uh For Italian  yeah. For this case  right? Mm-hmm. Um. Alright. So  um - So actually  the answer for experiments with one is that adding M_S_G  if you - uh does not help in that case. Mm-hmm. Um - The other ones  we'd have to look at it  but - But w- Yeah. And the multi-English  does uh - So if we think of this in error rates  we start off with  uh eighteen percent error rate  roughly. Mm-hmm. Um and we uh almost  uh cut that in half by um putting in the on-line normalization and the neural net. Yeah And the M_S_G doesn't however particularly affect things. No. And we cut off  I guess about twenty-five percent of the error. Uh no  not quite that  is it. Uh  two point six out of eighteen. About  um sixteen percent or something of the error  um  if we use multi-English instead of the matching condition. Mm-hmm. Not matching condition  but uh  the uh  Italian training. Yeah. Yeah. Mm-hmm. O_K. Mmm. We select these - these - these tasks because it's the more difficult. Yes  good. O_K? So then you're assuming multi-English is closer to the kind of thing that you could use since you're not gonna have matching  uh  data for the - uh for the new - for the other languages and so forth. Um  one qu- thing is that  uh - I think I asked you this before  but I wanna double check. When you say ""M_E"" in these other tests  that's the multi-English  but it is not all of the multi-English  right? It is some piece of - part of it. That's - it's a part - it's - Or  one million frames. And the multi-English is how much? You have here the information. It's one million and a half. Yeah. Oh  so you used almost all- You used two thirds of it  Yeah. you think. So  it- it's still - it hurts you - seems to hurt you a fair amount to add in this French and Spanish. Mmm. Yeah. I wonder why Yeah. Uh. Well Stephane was saying that they weren't hand-labeled  Yeah. Yeah  it's - Yeah. the French and the Spanish. The Spanish. Maybe for that. Yeah. Hmm. Mmm. It's still - O_K. Alright  go ahead. And then - then - Um. Mmm  with the experiment type-two  I - first I tried to- to combine  nnn  some feature from the M_L_P and other feature - another feature. Mm-hmm. And we s- we can - first the feature are without delta and delta-delta  and we can see that in the situation  uh  the M_S_G-three  the same help nothing. Mm-hmm. And then I do the same but with the delta and delta-delta - P_L_P delta and delta-delta. And they all p- but they all put off the M_L_P is it without delta and delta-delta. And we have a l- little bit less result than the - Mm-hmm. the - the baseline P_L_P with delta and delta-delta. Maybe if - when we have the new - the new neural network trained with P_L_P delta and delta-delta  maybe the final result must be better. I don't know. Uh - Actually  just to be some more - Do- This number  this eighty-seven point one number  has to be compared with the um - Which number? Yes  yeah  I mean it can't be compared with the other cuz this is  uh - with multi-English  uh  training. So you have to compare it with the one over that you've got in a box  which is that  uh the eighty-four point six. Mm-hmm. Mm-hmm. Right? Uh. So - Yeah  but I mean in this case for the eighty-seven point one we used M_L_P outputs for the P_L_P net Yeah. and straight features with delta-delta. Yeah. Mm-hmm. And straight features with delta-delta gives you what's on the first sheet. It's eight- eighty-eight point six. Not t- not tr- No. No. No. Not trained with multi- English. Yes. Uh  yeah  but th- this is the second configuration. So we use No  but they - they feature @@ without - feature out- uh  net outputs together with features. So yeah  this is not - perhaps not clear here but in this table  the first column is for M_L_P and the second for the features. Eh. Oh  I see. Ah. So you're saying w- so asking the question  ""What - what has adding the M_L_P done to improve over the  uh - Yes. So  just - Yeah so  actually it - it - it decreased the - the accuracy. Yeah. Uh- huh. Because we have eighty-eight point six. And even the M_L_P alone - What gives the M_L_P alone? Multi-English P_L_P. Oh no  it gives eighty- three point six. But - So we have our eighty-three point six and now eighty-eighty point six  that gives eighty-seven point one. Mm-hmm. Eighty-s- I thought it was eighty- Oh  O_K  eighty-three point six and eighty - eighty- eight point six. Eighty-three point six. Eighty - O_K. Is th- is that right? Yeah? Yeah. But - I don't know - but maybe if we have the neural network trained with the P_L_P delta and delta-delta  maybe tha- this can help. Perhaps  yeah. Well  that's - that's one thing  but see the other thing is that  um  I mean it's good to take the difficult case  but let's - let's consider what that means. What - what we're saying is that one o- one of the things that - I mean my interpretation of your - your s- original suggestion is something like this  as motivation. When we train on data that is in one sense or another  similar to the testing data  Mm-hmm. then we get a win by having discriminant training. When we train on something that's quite different  Mm-hmm. we have a potential to have some problems. And  um  if we get something that helps us when it's somewhat similar  and doesn't hurt us too much when it - when it's quite different  Yeah. that's maybe not so bad. So the question is  if you took the same combination  Mmm. and you tried it out on  uh - on say digits  On T_I-digits? O_K. Yeah. you know  d- Was that experiment done? No  not yet. Yeah  O_K. Uh  then does that  eh - you know maybe with similar noise conditions and so forth  does it - does it then look much better? Mm-hmm. And so what is the range over these different kinds of uh - of tests? So  an- anyway. O_K  go ahead. Yeah. Mm-hmm. And  with this type of configuration which I do on experiment using the new neural net with name broad klatt s- twenty-seven  uh  d- I have found more or less the same result. Mm-hmm. So  it's Little bit better? slightly better  yeah. Slightly better. Yeah. Slightly bet- better. Yes  is better. And - and you know again maybe if you use the  uh  delta Yeah  maybe. Maybe. Maybe. there  uh  you would bring it up to where it was  uh you know at least about the same for a difficult case. Yeah. Oh  yeah. Yeah. Oh  yeah. So. Well  so perhaps let's - let's jump at the last experiment. It's Yeah. either less information from the neural network if we use only the silence output. i- Mm-hmm. It's again better. So it's eighty-nine point - point one. Mm-hmm. Yeah  and we have only forty - forty feature So. because in this situation we have one hundred and three feature. Yeah. Yeah. And then w- with the first configuration  I f- Yeah. I am found that work  uh  doesn't work - uh  well  work  but is better  the second configuration. Because I - for the del- Engli- P_L_P delta and delta-delta  here I have eighty-five point three accuracy  and with the second configuration I have eighty-seven point one. Um  by the way  there is a- another  uh  suggestion that would apply  uh  to the second configuration  um  which  uh  was made  uh  by  uh  Hari. And that was that  um  if you have - uh feed two streams into H_T_K  um  and you  uh  change the  uh variances - if you scale the variances associated with  uh these streams um  you can effectively scale the streams. Right? So  um  Mmm. you know  without changing the scripts for H_T_K  which is the rule here  Mm-hmm. uh  you can still change the variances which would effectively change the scale of these - these  uh  two streams that come in. Uh  yeah. And  um  so  um  if you do that  for instance it may be the case that  um  the M_L_P should not be considered as strongly  for instance. Mmm. And  um  so this is just setting them to be  excuse me  of equal - equal weight. Maybe it shouldn't be equal weight. Maybe. Right? You know  I- I'm sorry to say that gives more experiments if we wanted to look at that  but - but  uh  um  you know on the other hand it's just experiments at the level of the H_T_K recognition. It's not even the H_T_K  uh  Mmm. Yeah. Yeah. uh - Well  I guess you have to do the H_T_K training also. Uh  do you? Yeah. so this is what we decided to do. Let me think. Maybe you don't. Uh. Yeah  you have to change the - No  you can just do it in - as - once you've done the training - And then you can vary it. Yeah. Yeah  the training is just coming up with the variances so I guess you could - you could just scale them all. Scale the - ? Variances. Yeah. But - Is it - i- th- I mean the H_T_K models are diagonal covariances  so I d- That's uh  exactly the point  I think  that if you change - Is it - um  Hmm. change what they are - Mm-hmm. It's diagonal covariance matrices  but you say what those variances are. Mm-hmm. So  that - you know  it's diagonal  but the diagonal means th- that then you're gonna - it's gonna - it's gonna internally multiply it - and - and uh  uh  i- it im- uh implicitly exponentiated to get probabilities  and so it's - it's gonna - Mmm. it's - it's going to affect the range of things if you change the - change the variances of some of the features. Mmm. do? So  i- it's precisely given that model you can very simply affect  uh  the s- the strength that you apply the features. That was - that was  uh  Hari's suggestion. Yeah. Yeah. So  um - Yeah. Yeah. So. So it could just be that h- treating them equally  tea- treating two streams equally is just - just not the right thing to do. Of course it's potentially opening a can of worms because  you know  maybe it should be a different number for - for each kind of test set  or something  but - Mm-hmm. O_K. Yeah . So I guess the other thing is to take - you know - if one were to take  uh  you know  a couple of the most successful of these  Yeah  and test across everything. and uh - Yeah  try all these different tests. Mmm. Yeah. Yeah. Alright. Uh. So  the next point  yeah  we've had some discussion with Steve and Shawn  um  about their um  uh  articulatory stuff  um. So we'll perhaps start something next week. Mm-hmm. Um  discussion with Hynek  Sunil and Pratibha for trying to plug in their our - our networks with their - within their block diagram  uh  where to plug in the - the network  uh  after the - the feature  before as um a- as a plugin or as a- anoth- another path  discussion about multi-band and TRAPS  um  actually Hynek would like to see  perhaps if you remember the block diagram there is  uh  temporal L_D_A followed b- by a spectral L_D_A for each uh critical band. And he would like to replace these by a network which would  uh  make the system look like a TRAP. Well  basically  it would be a TRAP system. Basically  this is a TRAP system - kind of TRAP system  I mean  but where the neural network are replaced by L_D_A. Hmm. Um  yeah  and about multi-band  uh  I started multi-band M_L_P trainings  um mmh Actually  I w- I w- hhh prefer to do exactly what I did when I was in Belgium. So I take exactly the same configurations  seven bands with nine frames of context  and we just train on TIMIT  and on the large database  so  with SPINE and everything. And  mmm  I'm starting to train also  networks with larger contexts. So  this would - would be something between TRAPS and multi-band because we still have quite large bands  and - but with a lot of context also. So Um Yeah  we still have to work on Finnish  um  basically  to make a decision on which M_L_P can be the best across the different languages. For the moment it's the TIMIT network  and perhaps the network trained on everything. So. Now we can test these two networks on - with - with delta and large networks. Well  test them also on Finnish and see Mmm. which one is the - the - the best. Uh  well  the next part of the document is  well  basically  a kind of summary of what - everything that has been done. So. We have seventy-nine M_L_Ps trained on one  two  three  four  uh  three  four  five  six  seven ten - on ten different databases. Mm-hmm. Uh  the number of frames is bad also  so we have one million and a half for some  three million for other  and six million for the last one. Uh  yeah! As we mentioned  TIMIT is the only that's hand-labeled  and perhaps this is what makes the difference. Um. Yeah  the other are just Viterbi-aligned. So these seventy-nine M_L_P differ on different things. First  um with respect to the on-line normalization  there are - that use bad on-line normalization  and other good on-line normalization. Um. With respect to the features  with respect to the use of delta or no  uh with respect to the hidden layer size and to the targets. Uh  but of course we don't have all the combination of these different parameters Um. s- What's this? We only have two hundred eighty six different tests Ugh! And no- not two thousand. I was impressed boy  two thousand. O_K. Yeah. Ah  yes. I say this morning that @@ thought it was the - Alright  now I'm just slightly impressed  O_K. Um. Yeah  basically the observation is what we discussed already. The M_S_G problem  um  the fact that the M_L_P trained on target task decreased the error rate. but when the M_- M_L_P is trained on the um - is not trained on the target task  it increased the error rate compared to using straight features. Except if the features are bad - uh  actually except if the features are not correctly on-line normalized. In this case the tandem is still better even if it's trained on - not on the target digits. Yeah. So it sounds like yeah  the net corrects some of the problems with some poor normalization. Yeah. But if you can do good normalization it's - Yeah. Yeah. it's uh - Uh  so the fourth point is  yeah  O_K. the TIMIT plus noise seems to be the training set that gives better - So the best network. Mm-hmm. So- Let me - bef- before you go on to the possible issues. So  on the M_S_G uh problem um  I think that in - in the - um  in the short time solution um  that is  um  trying to figure out what we can proceed forward with to make the greatest progress  Mm-hmm. uh  much as I said with J_RASTA  even though I really like J_RASTA and I really like M_S_G  Mm-hmm. I think it's kind of in category that it's  it - it may be complicated. Yeah. And uh it might be - if someone's interested in it  uh  certainly encourage anybody to look into it in the longer term  once we get out of this particular rush uh for results. But in the short term  unless you have some - some s- strong idea of what's wrong  Mm-hmm. I don't know at all but uh - I've - perhaps - I have the feeling that it's something that's quite - quite simple or Yeah  probably. just like nnn  no high-pass filter or - Mmm. Yeah. My - But I don't know. There's supposed to - well M_S_G is supposed to have a- an on-line normalization though  right? It's - There is  yeah  an A_G_C- kind of A_G_C. Yeah. Yeah  but also there's an on-line norm- besides the A_G_C  there's an on-line normalization that's supposed to be uh  Yeah. Yeah. yeah  Mmm. taking out means and variances and so forth. So. Yeah. In fac- in fact the on-line normalization that we're using came from the M_S_G design  so it's - Um. Yeah  but - Yeah. But this was the bad on-line normalization. Actually. Uh. Are your results are still with the bad - the bad - Maybe  may - No? With the better - No? With the O_- O_L_N-two? Yes. Ah yeah  you have - Oh! Yeah  yeah  yeah! With ""two""  with ""on-line-two"". Yeah  yeah  yeah. you have O_L_N-two  yeah. ""On-line-two"" is good. Yep  So it's  is the good yeah. ""Two"" is good? it's a good. And - No  ""two"" is bad. Yeah. O_K. Well  actually  it's good with the ch- with the good. Yeah. So - Yeah  I - I agree. It's probably something simple uh  i- if - if uh someone  you know  uh  wants to play with it for a little bit. I mean  you're gonna do what you're gonna do but - Mmm. but my - my guess would be But - that it's something that is a simple thing that could take a while to find. Yeah. Mmm. I see  yeah. Yeah. And - Uh. And the other - the results uh  observations two and three  Mmm. Um  is uh - Yeah  that's pretty much what we've seen. That's - that - what we were concerned about is that if it's not on the target task - If it's on the target task then it - it - it helps to have the M_L_P transforming it. Mmm. If it uh - if it's not on the target task  then  depending on how different it is  uh you can get uh  a reduction in performance. Mmm. And the question is now how to - how to get one and not the other? Or how to - how to ameliorate the - the problems. Mmm. Um  because it - it certainly does - is nice to have in there  when it - when there is something like the Mm-hmm. training data. Um. Yeah. So  the - the reason - Yeah  the reason is that the - perhaps the target - the - the task dependency - the language dependency  So that's what you say th- there. I see. and the noise dependency - Well  the e- e- But this is still not clear because  um  I - I - I don't think we have enough result to talk about the - the language dependency. Well  the TIMIT network is still the best but there is also an- the other difference  the fact that it's - it's hand-labeled. Hey! Sorry  I'm very late  uh  Am I still accommodated  or - ? Um  just - you can just sit here. Uh  I d- I don't think we want to mess with the microphones but it's uh - Just uh  have a seat. Um. s- Summary of the first uh  uh forty-five minutes is that some stuff work and - works  and some stuff doesn't O_K  We still have uh this - One of these perhaps? Mm-hmm. Yeah. Yeah  I guess we can do a little better than that but - I think if you - if you start off with the other one  actually  that sort of has it in words and then th- that has it the associated results. O_K. Um. So you're saying that um  um  although from what we see  yes there's what you would expect in terms of a language dependency and a noise dependency. That is  uh  when the neural net is trained on one of those and tested on something different  we don't do as well as in the target thing. But you're saying Mm-hmm. that uh  it is - Although that general thing is observable so far  there's something you're not completely convinced about. And - and what is that? I mean  you say ""not clear yet"". What - what do you mean? Uh  mmm  uh  I mean  that the - the fact that s- Well  for - for T_I-digits the TIMIT net is the best  which is the English net. Mm-hmm. But the other are slightly worse. But you have two - two effects  the effect of changing language and the effect of training on something that's Viterbi-aligned instead of hand - hand-labeled. Yeah. So. Um. Yeah. Do you think the alignments are bad? I mean  have you looked at the alignments at all? What the Viterbi alignment's doing? Mmm. I don't - I don't know. Did- did you look at the Spanish alignments Carmen? Mmm  no. Might be interesting to look at it. @@ Because  I mean  that is just looking but um  um - It's not clear to me you necessarily would do so badly from a Viterbi alignment. It depends how good the recognizer is that's - Mm-hmm. that - the - the engine is that's doing the alignment. Yeah. But - Yeah. But  perhaps it's not really the - the alignment that's bad but the - just the ph- phoneme string that's used for the alignment or - Aha! Mmm. I mean for - Yeah. @@ The pronunciation models and so forth We - It's single pronunciation  uh - Aha. I see. French - French s- uh  phoneme strings were corrected manually so we asked people to listen to the um - the sentence and we gave the phoneme string and they kind of correct them. But still  @@ there - there might be errors just in the - in - in the ph- string of phonemes. Mmm. Um. Yeah  so this is not really the Viterbi alignment  in fact  yeah. Um  the third - The third uh issue is the noise dependency perhaps but  well  this is not clear yet because all our nets are trained on the same noises and - Mmm. I thought some of the nets were trained with SPINE and so forth. So it - And that has other noise. Yeah. So - Yeah. But - Yeah. Results are only coming for - for this net. Mmm. O_K  yeah  just don't - just need more - more results there with that @@ . Yeah. Um. So. Uh  from these results we have some questions with answers. What should be the network input? Um  P_L_P work as well as M_F_C_C  I mean. Um. But it seems impor- important to use the delta. Uh  with respect to the network size  there's one experiment that's still running and we should have the result today  comparing network with five hundred and one thousand units. So  nnn  still no answer actually. Hm-hmm. Uh  the training set  well  some kind of answer. We can  we can tell which training set gives the best result  but we don't know exactly why. Uh. Uh  so. Right  I mean the multi-English so far is - is the best. Yeah. ""Multi- multi-English"" just means ""TIMIT""  right? Yeah. Yeah. So uh That's - Yeah. So. And - and when you add other things in to - to broaden it  it gets worse uh typically. Yeah. Mmm. Mm-hmm. Then uh O_K. some questions without answers. Uh  training set  um  Uh-huh. uh  training targets - I like that. The training set is both questions  with answers and without answers. It's sort of  yes - it's mul- it's multi-uh-purpose. O_K. It's - Yeah. Yeah. Yeah. Uh  training s- Right . So - Yeah  the training targets actually  the two of the main issues perhaps are still the language dependency and the noise dependency. And perhaps to try to reduce the language dependency  we should focus on finding some other kind of training targets. Mm-hmm. And labeling s- labeling seems important uh  because of TIMIT results. Mm-hmm. Uh. For moment you use - we use phonetic targets but we could also use articulatory targets  soft targets  and perhaps even  um use networks that doesn't do classification but just regression so uh  train to have neural networks that um  um  Mm-hmm. uh  does a regression and well  basically com- com- compute features and noit- not  nnn  features without noise. I mean uh  transform the fea- noisy features in other features that are not noisy. But continuous features. Not uh Mm-hmm. uh  hard targets. Mm-hmm. Uh - Yeah  that seems like a good thing to do  probably  uh  not uh again a short-term Yeah. sort of thing. I mean one of the things about that is that um it's - e- u- the ri- I guess the major risk you have there of being - is being dependent on - very dependent on the kind of noise and - and so forth. Yeah. f- Uh. But  yeah. So  this is w- w- But it's another thing to try. i- wa- wa- this is one thing  this - this could be - could help - could help perhaps to reduce language dependency and for the noise part um we could combine this with other approaches  like  well  the Kleinschmidt approach. So the d- the idea of putting all the noise that we can find inside a database. Mm-hmm. I think Kleinschmidt was using more than fifty different noises to train his network  Yeah. and - So this is one approach Mm-hmm. and the other is multi-band Mm-hmm. uh  that I think is more robust to the noisy changes. So perhaps  I think something like multi-band trained on a lot of noises with uh  features-based targets could - Yeah  if you - i- i- It's interesting thought maybe if you just trained up - could - could help. I mean w- yeah  one - one fantasy would be you have something like articulatory targets and you have um some reasonable database  um but then - which is um copied over many times with a range of different noises  Mm-hmm. And uh - If - Cuz what you're trying to do is come up with a - a core  reasonable feature set which is then gonna be used uh  by the - the uh H_M_M system. So. Mm-hmm. Yeah  O_K. So  um  yeah. The future work is  well  try to connect to the - to make - to plug in the system to the O_G_I system. Um  there are still open questions there  where to put the M_L_P Mm-hmm. basically. Um. And I guess  you know  the - the - the real open question  I mean  e- u- there's lots of open questions  but one of the core quote ""open questions"" for that is um  um  if we take the uh - you know  the best ones here  maybe not just the best one  but the best few or something - Mm-hmm. You want the most promising group from these other experiments. Um  how well do they do over a range of these different tests  not just the Italian? Mmm  Yeah  yeah. Um. And y- Right? And then um - then see  again  how - We know that there's a mis- there's a uh - a - a loss in performance when the neural net is trained on conditions that are different than - than  uh we're gonna test on  but well  if you look over a range of these different tests um  how well do these different ways of combining the straight features with the M_L_P features  uh stand up over that range? Mm-hmm. That's - that - that seems like the - the - the real question. And if you know that - So if you just take P_L_P with uh  the double-deltas. Assume that's the p- the feature. look at these different ways of combining it. And uh  take - let's say  just take uh multi-English cause that works pretty well for the training. Mm-hmm. And just look - take that case and then look over all the different things. How does that - How does that compare between the - So all the - all the test sets you mean  yeah. Yeah. All the different test sets  and for - and for the couple different ways that you have of - of - of combining them. And - Yeah. Um. How well do they stand up  over the - Mmm. Mm-hmm. And perhaps doing this for - cha- changing the variance of the streams and so on getting different scaling - That's another possibility if you have time  yeah. Yeah. O_K. Um. @@ Yeah  so thi- this sh- would be more working on the M_L_P as an additional path instead of an insert to the - to their diagram. Cuz - Yeah. Perhaps the insert idea is kind of strange because nnn  they - they make L_D_A and then we will again add a network does discriminate anal- nnn  Yeah. It's a little strange but on the other hand they that discriminates  or - ? Mmm? did it before. Mmm. And - and - and yeah. And because also perhaps we know that Um the- the - when we have very good features the M_L_P doesn't help. So. I don't know. Um  the other thing  though  is that um - So. Uh  we - we wanna get their path running here  right? If so  we can add this other stuff. Um. as an additional path right? Yeah  the - the way we want to do - Cuz they're doing L_D_A RASTA. The d- What? They're doing L_D_A RASTA  yeah? Yeah  the way we want to do it perhaps is to - just to get the V_A_D labels and the final features. So they will send us the - Well  I see. provide us with the feature files  I see. and with V_A_D uh  binary labels so that we can uh  get our M_L_P features and filter them with the V_A_D and then combine them with their f- feature stream. I see. So we - So. First thing of course we'd wanna do there is to make sure that when we get those labels of final features is that we get the same results as them. So. @@ Without putting in a second path. Uh. You mean - Oh  yeah! Just re- re- retraining r- Yeah just th- w- i- i- retraining the H_T_K? Just to make sure that we have - we understand properly what things are  our very first thing to do is to - is to double check that we get the exact same results as them on H_T_K. Oh yeah. @@ Yeah  O_K. Mmm. Uh  I mean  I don't know that we need to r- Yeah. Yeah. Um Do we need to retrain I mean we can just take the re- their training files also. But. But  uh just for the testing  jus- just make sure that we get the same results so we can duplicate it before we add in another - Mmm. O_K. Cuz otherwise  you know  we won't know what things mean. Oh  yeah. O_K. And um. Yeah  so fff  LogRASTA  I don't know if we want to - We can try networks with LogRASTA filtered features. Maybe. Mmm. Would you be using on-line normalization with that? I'm sorry? Would you be using on-line normalization with LogRASTA-P_L_P? Yeah. Well - Yeah. Oh! You know  the other thing is when you say comb- But - I'm - I'm sorry  I'm interrupting. that u- Um  uh  when you're talking about combining multiple features  um - Suppose we said  ""O_K  we've got these different features and so forth  but P_L_P seems pretty good."" If we take the approach that Mike did and have - I mean  one of the situations we have is we have these different conditions. We have different languages  we have different - different noises  Um If we have some drastically different conditions and we just train up different M_L_ Ps with them. Mm-hmm. And put - put them together. What - what - What Mike found  for the reverberation case at least  I mean - I mean  who knows if it'll work for these other ones. That you did have nice interpolative effects. That is  that yes  if you knew what the reverberation condition was gonna be and you trained for that  then you got the best results. But if you had  say  a heavily-reverberation ca- heavy-reverberation case and a no-reverberation case  Mm-hmm. uh  and then you fed the thing  uh something that was a modest amount of reverberation then you'd get some result in between the two. So it was sort of - behaved reasonably. Is tha- that a fair - Yeah. Yeah. Um. It also seems like whe- if you try to train  like  a single M_L_P with too much noise  um  you'll get some nice interval of power for unseen cases but um but any unmatched cases it'll start  um  interfering. Yeah. So you - you think it's perhaps better to have several M_L_Ps? Well  it's easier. Um  that way you can turn things off @@ turn things on. Um. But you then - Yeah but - Mmm. A single M_L_P of course will work a little bit better because uh it'll have uh - It's a nonlinear kind of merging of the features. It works better if what? Yea- Uh  well  in general nonlinear mergings seems to work a little better then just the straight linear coefficients   but um it just means it's - you have to have bigger nets and more training time  and if you want to turn things off um that's harder to do. I see. Well  see  i- oc- You were doing some- something that was - So maybe the analogy isn't quite right. You were doing something that was in way a little better behaved. You had reverb- for a single variable which was re- uh  uh  reverberation. Here the problem seems to be is that we don't have a hug- a really huge net with a really huge amount of training data. But we have s- f- for this kind of task  I would think  sort of a modest amount. I mean  a million frames actually isn't that much. We have a modest amount of - of uh training data from a couple different conditions  and then uh - in - yeah  that - and the real situation is that there's enormous variability that we anticipate in the test set in terms of language  and noise type uh  and uh  uh  channel characteristic  sort of all over the map. A bunch of different dimensions. And so  I'm just concerned that we don't really have um  the data to train up - I mean one of the things that we were seeing is that when we added in - we still don't have a good explanation for this  but we are seeing that we're adding in uh  a fe- few different databases and uh the performance is getting worse and uh  when we just take one of those databases that's a pretty good one  it actually is - is - is - is - is better. And uh that says to me  yes  that  you know  there might be some problems with the pronunciation models that some of the databases we're adding in or something like that. But one way or another we don't have uh  seemingly  the ability to represent  in the neural net of the size that we have  um  all of the variability that we're gonna be covering. So that I'm - I'm - I'm hoping that um  this is another take on the efficiency argument you're making  which is I'm hoping that with moderate size neural nets  uh  that uh if we - if they look at more constrained conditions they - they'll have enough parameters to really represent them. I think the way that Hynek or - or Malik had - had told me was that if you try to train a classifier on too much - too many conditions then it'll do good on none of them  but it'll start doing something else. Mm-hmm. Which means  if you have something Mm-hmm. else in there it'll be nicer @@ I'm not sure @@ Mm-hmm. I also have some - some - a new theory on why um LogRASTA-P_L_P - uh P_L_P with on-line normalization might be a little bit better than LogRASTA-P_L_P with on-line normalization. It has to do with certain distribution characteristics. But if you take away the on-line normalization  LogRASTA seems to do better than P_L_P but not in all cases. Yeah. So doing both is - is not - is not right  you mean  or - ? Um. not so much not right  but it if you throw in the on-line normalization then it might not be necessary to use the LogRASTA-P_L_P. Yeah. Yeah. I - I just sort of have a feeling - But - Yeah. Mm-hmm. Yeah. I mean - i- i- e- The um - I think it's true that the O_G_I folk found that using L_D_A RASTA  which is a kind of LogRASTA  it's just that they have the - I mean it's done in the log domain  as I recall  and it's - it uh - it's just that they d- it's trained up  right? Mm-hmm. That that um benefitted from on-line normalization. So they did - At least in their case  it did seem to be somewhat complimentary. So will it be in our case  where we're using the neural net? I mean they - they were not - not using the neural net. Uh I don't know. O_K  so the other things you have here are uh  trying to improve results from a single - Yeah. Make stuff better. O_K. Uh. Yeah. And C_P_U memory issues. Yeah. We've been sort of ignoring that  haven't we? Yeah  so I don't know. But - But we have to address the problem of C_P_U and memory we - Yeah  but I li- Well  I think - My impression - You - you folks have been looking at this more than me. But my impression was that uh  there was a - a - a - a strict constraint on the delay  Yeah. but beyond that it was kind of that uh using less memory was better  and using less C_P_U was better. Something like that  right? Yeah  but - Yeah. So  yeah  but we've - I don't know. We have to get some reference point to where we - Well  what's a reasonable number? Perhaps be- because if it's - if it's too large or - large or @@ - Um  well I don't think we're um completely off the wall. I mean I think that if we - if we have - Uh  I mean the ultimate fall back that we could do - If we find uh - I mean we may find that we - we're not really gonna worry about the M_L_ P. You know  if the M_L_P ultimately  after all is said and done  doesn't really help then we won't have it in. Mmm. If the M_L_P does  we find  help us enough in some conditions  uh  we might even have more than one M_L_P. We could simply say that is uh  done on the uh  server. Mmm. And it's uh - We do the other manipulations that we're doing before that. So  I - I - I think - I think that's - that's O_K. And - Yeah. So I think the key thing was um  this plug into O_G_I. Um  what - what are they - What are they gonna be working - Do we know what they're gonna be working on while we take their features  and - ? They're - They're starting to wor- work on some kind of multi-band. So. Um - This - that was Pratibha. Sunil  what was he doing  do you remember? Sunil? Yeah. He was doing something new or - ? I - I don't re- I didn't remember. Maybe he's working with I don't think so. Trying to tune wha- networks? neural network. Yeah  I think so. I think they were also mainly  well  working a little bit of new things  like networks and multi-band  but mainly trying to tune their - their system as it is now to - just Yeah. trying to get the best from this - this architecture. Mmm. O_K. So I guess the way it would work is that you'd get - There'd be some point where you say  ""O_K  this is their version-one"" or whatever  and we get these V_A_D labels and features and so forth for all these test sets from them  Mm-hmm. and then um  uh  that's what we work with. We have a certain level we try to improve it with this other path and then um  uh  when it gets to be uh  January some point uh  we say  ""O_K we - we have shown that we can improve this  in this way. So now uh um what's your newest version?"" And then maybe they'll have something that's better and then we - we'd combine it. This is always hard. I mean I - I - I used to work with uh folks who were trying to improve a good uh  H_M_M system with uh - with a neural net system and uh  it was a common problem that you'd - Oh  and this - Actually  this is true not just for neural nets but just for - in general if people were working with uh  rescoring uh  N_best lists or lattices that come - came from uh  a mainstream recognizer. Uh  You get something from the - the other site at one point and you work really hard on making it better with rescoring. But they're working really hard  too. So by the time you have uh  improved their score  Mmm. they have also improved their score and now there isn't any difference  because the other - Yeah. Yeah. So  um  I guess at some point we'll have to uh - So it's - Uh  I - I don't know. I think we're - we're integrated a little more tightly than happens in a lot of those cases. I think at the moment they - they say that they have a better thing we can - we - e- e- Mmm. What takes all the time here is that th- we're trying so many things  presumably uh  in a - in a day we could turn around uh  taking a new set of things from them and - and rescoring it  right? So. Mmm. Yeah. Yeah  perhaps we could. Yeah. Well  O_K. No  this is - I think this is good. I think that the most wide open thing is the issues about the uh  you know  different trainings. You know  da- training targets and Mmm. noises and so forth. That's sort of wide open. So we - we can for - we c- we can forget combining multiple features and M_L_G perhaps  or focus more on the targets and on the training data and - ? Yeah  I think for right now um  I th- I - I really liked M_S_G. And I think that  you know  one of the things I liked about it is has such different temporal properties. And um  I think that there is ultimately a really good uh  potential for  you know  bringing in things with different temporal properties. Um  but um  uh  we only have limited time and there's a lot of other things we have to look at. And it seems like much more core questions are issues about the training set Mmm. and the training targets  and fitting in uh what we're doing with what they're doing  and  you know  with limited time. Yeah. I think we have to start cutting down. So uh - Mmm. I think so  yeah. And then  you know  once we - Um  having gone through this process and trying many different things  I would imagine that certain things uh  come up that you are curious about uh  that you'd not getting to and so when the dust settles from the evaluation uh  I think that would time to go back and take whatever intrigued you most  you know  got you most interested uh and uh - and - and work with it  you know  for the next round. @@ Uh  as you can tell from these numbers uh  nothing that any of us is gonna do is actually gonna completely solve the problem. So. Mmm. So  there'll still be plenty to do. Barry  you've been pretty quiet. @@ Just listening. Well I figured that  but - That - what - what - what were you involved in in this primarily? Um  helping out uh  preparing - Well  they've been kind of running all the experiments and stuff and I've been uh  uh w- doing some work on the - on the - preparing all - all the data for them to - to um  train and to test on. Um Yeah. Right now  I'm - I'm focusing mainly on this final project I'm working on in Jordan's class. Ah! I see. Right. What's - what's that? Yeah. Um  I'm trying to um - So there was a paper in I_C_S_L_P about um this - this multi-band um  belief-net structure. Mm-hmm. This guy did - uh basically it was two H_M_Ms with - with a - with a dependency arrow between the two H_M_Ms Uh-huh. And so I wanna try - try coupling them instead of t- having an arrow that - that flows from one sub-band to another sub-band. I wanna try having the arrows go both ways. And um  I'm just gonna see if - if that - that better models um  uh asynchrony in any way or um - Yeah. Oh! O_K. Well  that sounds interesting. Yeah. O_K. Alright. Anything to - you wanted to - No. O_K. Silent partner in the - in the meeting. Oh  we got a laugh out of him  that's good. O_K  everyone h- must contribute to the - our - our sound - sound files here. O_K  so speaking of which  if we don't have anything else that we need - You happy with where we are? Know - know wher- know where we're going? Mmm. Uh - I think so  yeah. Yeah  yeah. You - you happy? Mmm. You're happy. O_K everyone should be happy. O_K. You don't have to be happy. You're almost done. @@ . Yeah  yeah. O_K. Al- actually I should mention - So if - um  about the Linux machine ""Swede."" So it looks like the um  neural net tools are installed there. And um Yeah. Mmm. Dan Ellis I believe knows something about using that machine so Mmm. If people are interested in - in getting jobs running on that maybe I could help with that. Yeah  but I don't know if we really need now a lot of machines. Well. we could start computing another huge table but - yeah  we - Well. Yeah  I think we want a different table  at least Yeah  sure. Right? I mean there's - there's some different things that we're But - trying to get at now. But - Yeah. Mmm. So. Yeah  as far as you can tell  you're actually O_K on C_- on C_P_U uh  for training and so on? Yeah. Ah yeah. I think so. Well  more is always better  but mmm  I don't think we have to train a lot of networks  now that we know - We just select what works fine and O_K. O_K. Yeah. And we're O_K on - to work try to improve this and - And we're O_K on disk? It's O_K  yeah. Well sometimes we have some problems. Some problems with the - You know. But they're correctable  uh problems. Yeah  restarting the script basically and - Yes. Yeah  I'm familiar with that one  O_K. Alright  so uh  since uh  we didn't ha- get a channel on for you  you don't have to read any digits but the rest of us will. Light's on here. @@ Yeah. Uh  is it on? Well. We didn't uh - I think I won't touch anything cuz I'm afraid of making the driver crash which it seems to do  pretty easily. O_K  thanks. O_K  so we'll uh - I'll start off the uh um connect the - My battery is low. Well  let's hope it works. Maybe you should go first and see so that you're - O_K. I'm reading transcript two five seven one  two five nine O_. @@ batteries? one nine four three Yeah  your battery's going down too. two six three four five seven one eight three zero six seven four four nine three O_ four nine O_ O_ O_ nine nine eight four zero one two O_ two eight six nine one four one five six nine zero seven five four six six seven eight nine O_ O_ O_ one two seven three O_ seven three Transcript uh two - Carmen's battery is d- going down too. Oh  O_K. Yeah. Why don't you go next then. Uh  transcript number two five one one dash two five three zero channel one. nine zero five O_ O_ four seven one three zero zero six three seven two five five four four five four three three one four seven O_ five six seven zero seven nine one O_ four zero four seven one seven five two O_ four O_ two nine three four zero five five two six eight six three eight nine zero nine zero seven six four seven eight six one nine O_K. Um  transcript two five three one dash two five five zero O_ zero zero one O_ three five two one seven three one one zero six five six four three three six four six three five eight one one two four four six nine nine seven eight O_ one eight O_ O_ nine zero one six zero one two three eight one two six zero four three nine four nine five five six zero four eight three eight two four two nine three three three O_ five I'm reading transcript two five five one dash two five seven O_ zero six six nine five four seven one two three O_ O_ five one six two five seven seven five eight nine O_ zero zero five zero eight nine six two one three O_ five three five seven two three six zero four three five six nine nine six seven seven O_ six eight zero six five zero O_ three eight five four zero two I'm reading transcript two four nine one dash two five one zero on channel two. eight nine zero seven nine zero two one four O_ six two six three one three eight two O_ five eight four four five six O_ eight eight one two zero six nine six two zero four four three O_ seven three zero seven one two O_ five four one five one five six six nine seven five nine one three eight seven two nine seven two six Guess we're done. O_K  uh so. Just finished digits. O_K. I'll be right up. O_K. Yeah  so. Uh Well  it's good. I think - I guess we can turn off our microphones now. Just pull the batteries out. ",The ICSI Meeting Recorder Group of Berkeley met for the first time in two weeks. Group members reported their progress in the areas of spectral subtraction  Wiener filtering and noise estimation. They also discusses topics relating to the rules and preferences of the project they are working on  including single vs multiple passes. A number of the group also took time to explain the basics of their approaches to the group. There are hopes that a visitor coming for three weeks  may lead to a longer term collaboration. The visitor works on spectral subtraction  so speaker me026 will make sure he talks to him. Speaker mn007 agreed  at me013's suggestion  to try his noise compensation scheme in compensation with the prior work on spectral subtraction. In implementing smoothing to the spectral subtraction  latency has been increased; while some feel this is nothing to worry about  others feel it is better to worry now  in case it turns out to be something to worry about. Speaker me026 has been experimenting with spectral subtraction using different data window sizes. One possible idea is to use increasing windows as more data becomes available. Speaker mn049 has been working on Wiener filtering  and testing with just the base system provides 30% improvement. Using a second stage of filtering led to even more improvement. Speaker mn007 is working on spectral subtraction  still with minimal results. Smoothing seems to help  and implementing alongside the neural net should also be positive. He has also been working on noise estimation with an energy minima approach that does not require the voice activity detector. 
"So I guess this is more or less now just to get you up to date  Johno. This is what  uh  This is a meeting for me. um  Eva  Bhaskara  and I did. Did you add more stuff to it? later? Um. Why? Um. I don't know. There were  like  the - you know  @@ and all that stuff. But. I thought you - you said you were adding stuff but I don't know. Uh  no. This is - Um  Ha! Very nice. Um  so we thought that  We can write up uh  an element  and - for each of the situation nodes that we observed in the Bayes-net? So. What's the situation like at the entity that is mentioned? if we know anything about it? Is it under construction? Or is it on fire or something happening to it? Or is it stable? and so forth  going all the way um  f- through Parking  Location  Hotel  Car  Restroom  @@ Riots  Fairs  Strikes  or Disasters. So is - This is - A situation are - is all the things which can be happening right now? Or  what is the situation type? That's basically just specifying the - the input for the - w- what's Oh  I see y- Why are you specifying it in X_M_L? Um. Just because it forces us to be specific about the values here? O_K. And  also  I mean  this is a - what the input is going to be. Right? So  we will  uh - This is a schema. This is - Well  yeah. I just don't know if this is th- l- what the - Does - This is what Java Bayes takes? as a No  because I mean if we - Bayes-net spec? I mean we're sure gonna interface to - We're gonna get an X_M_L document from somewhere. Right? And that X_M_L document will say ""We are able to - We were able to observe that w- the element  um  @@ of the Location that the car is near. "" So that's gonna be - Um. So this is the situational context  everything in it. Is that what Situation is short for  shi- situational context? Yep. O_K. So this is just  again  a- an X_M_L schemata which defines a set of possible  uh  permissible X_M_L structures  which we view as input into the Bayes-net. Right? And then we can r- uh possibly run one of them uh transformations? That put it into the format that the Bayes n- or Java Bayes or whatever wants? Yea- Are you talking - are you talking about the - the structure? Well it - I mean when you observe a node. When you - when you say the input to the v- Java Bayes  Um-hmm. it takes a certain format  right? Which I don't think is this. Although I don't know. No  it's certainly not this. Nuh. So you could just - Couldn't you just run a - X_S_L . Yeah. Yeah. To convert it into the Java Bayes for- format? Yep. O_K. That's - That's no problem  but I even think that  um - I mean  once - Once you have this sort of as - running as a module - Right? What you want is - You wanna say  ""O_K  give me the posterior probabilities of the Go-there node  when this is happening."" Right? When the person said this  the car is there  it's raining  and this is happening. And with this you can specify the - what's happening in the situation  and what's happening with the user. So we get - After we are done  through the Situation we get the User Vector. So  this is a - So this is just a specification of all the possible inputs? Yep. And  all the possible outputs  too. O_K. So  we have  um  for example  the  uh  Go-there decision node which has two elements  going-there and its posterior probability  and not-going-there and its posterior probability  because the output is always gonna be all the decision nodes and all the - the - a- all the posterior probabilities for all the values. And then we would just look at the  eh  Struct that we wanna look at in terms of if - if we're only asking about one of the - So like  if I'm just interested in the going-there node  I would just pull that information out of the Struct that gets return- that would - that Java Bayes would output? Um  pretty much  yes  but I think it's a little bit more complex. As  if I understand it correctly  it always gives you all the posterior probabilities for all the values of all decision nodes. So  when we input something  we always get the  uh  posterior probabilities for all of these. Right? O_K. So there is no way of telling it t- not to tell us about the EVA values. Yeah  wait I agree  that's - yeah  use - oh  uh Yeah  O_K. So - so we get this whole list of - of  um  things  and the question is what to do with it  what to hand on  how to interpret it  in a sense. So y- you said if you - ""I'm only interested in whether he wants to go there or not""  then I just look at that node  look which one - Look at that Struct in the output  right? Yep. Look at that Struct in the - the output  even though I wouldn't call it a ""Struct"". But. Well i- well  it's an X_M_L Structure that's being res- returned  right? Oh. Mm-hmm. So every part of a structure is a ""Struct"". Yeah. Yeah  I just uh - I just was - abbreviated it to Struct in my head  and started going with that. That element or object  I would say. Not a C_Struct. That's not what I was trying to k- though yeah. Yeah. O_K. And  um  the reason is - why I think it's a little bit more complex or why - why we can even think about it as an interesting problem in and of itself is - Um. So. The  uh - Let's look at an example. Well  w- wouldn't we just take the structure that's outputted and then run another transformation on it  that would just dump the one that we wanted out? Yeah. w- We'd need to prune. Right? Throw things away. Well  actually  you don't even need to do that with X_M_L. D- Can't you just look at one specific - No- Yeah  exactly. The - @@ Xerxes allows you to say  u- ""Just give me the value of that  and that  and that."" But  we don't really know what we're interested in before we look at the complete - at - at the overall result. So the person said  um  ""Where is X_?"" and so  we want to know  um  is - Does he want info? o- on this? or know the location? Or does he want to go there? Let's assume this is our - our question. Sure. Nuh? So. Um. @@ Do this in Perl. So we get - O_K . Let's assume this is the output. So. We should con- be able to conclude from that that - I mean. It's always gonna give us a value of how likely we think i- it is that he wants to go there and doesn't want to go there  or how likely it is that he wants to get information. But  maybe w- we should just reverse this to make it a little bit more delicate. So  does he wanna know where it is? or does he wanna go there? He wants to know where it is. Right. I - I - I tend to agree. And if it's - If - Well now  y- I mean  you could - And i- if there's sort of a clear winner here  and  um - and this is pretty  uh - indifferent  then we - then we might conclude that he actually wants to just know where  uh t- uh  he does want to go there. Uh  out of curiosity  is there a reason why we wouldn't combine these three nodes? into one smaller subnet? that would just basically be the question for - We have ""where is X_?"" is the question  right? That would just be Info-on or Location? Based upon - Or Go-there. A lot of people ask that  if they actually just wanna go there. People come up to you on campus and say  ""Where's the library?"" You're gonna say - y- you're gonna say  g- ""Go down that way."" You're not gonna say ""It's - It's five hundred yards away from you"" or ""It's north of you""  or - ""it's located -"" Well  I mean - But the - there's - So you just have three decisions for the final node  that would link thes- these three nodes in the net together. Um. I don't know whether I understand what you mean. But. Again  in this - Given this input  we  also in some situations  may wanna postulate an opinion whether that person wants to go there now the nicest way  use a cab  or so s- wants to know it - wants to know where it is because he wants something fixed there  because he wants to visit t- it or whatever. So  it - n- I mean - a- All I'm saying is  whatever our input is  we're always gonna get the full output. And some - some things will always be sort of too - not significant enough. Wha- Or i- or i- it'll be tight. You won't - it'll be hard to decide. But I mean  I guess - I guess the thing is  Yep. uh  this is another  smaller  case of reasoning in the case of an uncertainty  which makes me think Bayes-net should be the way to solve these things. So if you had - If for every construction  right? Oh! you could say  ""Well  there - Here's the Where-Is construction."" And for the Where-Is construction  we know we need to l- look at this node  that merges these three things together Mm-hmm. as for th- to decide the response. And since we have a finite number of constructions that we can deal with  we could have a finite number of nodes. O_K. Say  if we had to y- deal with arbitrary language  it wouldn't make any sense to do that  because Mm-hmm. there'd be no way to generate the nodes for every possible sentence. Mm-hmm. But since we can only deal with a finite amount of stuff - So  basically  the idea is to f- to feed the output of that belief-net into another belief-net. Yeah  so basically take these three things and then put them into another belief-net. But  why - why - why only those three? Why not the whol- Well  I mean  d- For the Where-Is question. So we'd have a node for the Where-Is question. Yeah. But we believe that all the decision nodes are - can be relevant for the Where-Is  and the Where - How-do-I-get-to or the Tell-me-something-about. Is food not allowed in here? You can come in if you want. Yes  it is allowed. As long as y- you're not wearing your h- your h- headphones. Alright. Just a second. I'll be back. Well  I do- I - See  I don't know if this is a good idea or not. I'm just throwing it out. But uh  it seems like we could have - I mea- or uh we could put all of the- all of the r- information that could also be relevant into the Where-Is node answer Mm-hmm. Yep. node thing stuff. And uh - O_K. I mean - Let's not forget we're gonna get some very strong input from these sub- dis- from these discourse things  right? So. ""Tell me the location of X_."" Nuh? Or ""Where is X_ located We u- at?"" Nuh? Yeah  I know  but the Bayes-net would be able to - The weights on the - on the nodes in the Bayes-net would be able to do all that  wouldn't it? Mm-hmm. Here's a k- Oh! Oh  I'll wait until you're plugged in. Oh  don't sit there. Sit here. You know how you don't like that one. It's O_K. Oh  do I not? That's the weird one. That's the one that's painful. That hurts. It hurts so bad. I'm h- I'm happy that they're recording that. That headphone. The headphone that you have to put on backwards  with the little - little thing - and the little - little foam block on it? It's a painful  painful microphone. I think it's th- called ""the Crown"". The crown? What? Yeah  versus ""the Sony"". The Crown? Is that the actual name? Mm-hmm. O_K. The manufacturer. I don't see a manufacturer on it. You w- Oh  wait  here it is. h- This thingy. Yeah  it's ""The Crown"". The crown of pain! Yes. You're on-line? Are you - are your mike o- Is your mike on? O_K. Indeed. So you've been working with these guys? You know what's going on? Yes  I have. And  I do. Yeah  alright. s- So where are we? Excellent! We're discussing this. I don't think it can handle French  but anyway. So. Assume we have something coming in. A person says  ""Where is X_?""  and we get a certain - We have a Situation vector and a User vector and everything is fine? An- an- and - and our - and our - Did you just sti- Did you just stick the m- the - the - the microphone actually in the tea? No. And  um  I'm not drinking tea. What are you talking about? Oh  yeah. Sorry. let's just assume our Bayes-net just has three decision nodes for the time being. These three  he wants to know something about it  he wants to know where it is  he wants to go there. In terms of  these would be wha- how we would answer the question Where-Is  right? We u- This is - i- That's what you s- it seemed like  explained it to me earlier w- We - we're - we wanna know how to answer the question ""Where is X_?"" Yeah  but  mmm. Yeah. @@ No  I can - I can do the Timing node in here  too  and say ""O_K."" Well  yeah  but in the s- uh  let's just deal with the s- the simple case of we're not worrying about timing or anything. We just want to know how we should answer ""Where is X_?"" O_K. And  um  O_K  and  Go-there has two values  right?  Go-there and not-Go-there. Let's assume those are the posterior probabilities of that. Mm-hmm. Info-on has True or False and Location. So  he wants to know something about it  and he wants to know something - he wants to know Where-it-is  Excuse me. has these values. And  um  Oh  I see why we can't do that. And  um  in this case we would probably all agree that he wants to go there. Our belief-net thinks he wants to go there  right? In the  uh  whatever  if we have something like Yeah. Mm-hmm. this here  and this like that and maybe here also some - You should probably make them out of - Yeah. Well  it- something like that  then we would guess  ""Aha! He  our belief-net  has s- stronger beliefs that he wants to know where it is  than actually wants to go there."" Right? That it - Doesn't this assume  though  that they're evenly weighted? True. Like - I guess they are evenly weighted. The different decision nodes  you mean? Yeah  the Go-there  the Info-on  and the Location? Well  d- yeah  this is making the assumption. Yes. Like - What do you mean by ""differently weighted""? They don't feed into anything really anymore. Or I jus- But I mean  why do we - @@ Le- If we trusted the Go-there node more th- much more than we trusted the other ones  then we would conclude  even in this situation  that he wanted to go there. So  in that sense  we weight them equally O_K. Makes sense. Yeah. right now. So the- But I guess the- k- the question - But - that I was as- er- wondering or maybe Robert was proposing to me is - How do we d- make the decision on - as to - which one to listen to? Yeah  so  the final d- decision is the combination of these three. So again  it's - it's some kind of  uh - Bayes-net. Yeah  sure. O_K so  then  the question i- So then my question is t- to you then  would be - So is the only r- reason we can make all these smaller Bayes-nets  because we know we can only deal with a finite set of constructions? Cuz oth- If we're just taking arbitrary language in  we couldn't have a node for every possible question  you know? A decision node for every possible question  you mean? Well  I - like  in the case of - Yeah. In the ca- Any piece of language  we wouldn't be able to answer it with this system  b- if we just h- Cuz we wouldn't have the correct node. Basically  w- what you're s- proposing is a n- Where-Is node  right? Yeah. And - and if we - And if someone - says  you know  uh  something in Mandarin So is - Yeah. to the system  we'd- wouldn't know which node to look at to answer that question  right? Yeah. Mmm? So  but - but if we have a finite - What? I don't see your point. What - what - what I am thinking  or what we're about to propose here is we're always gonna get the whole list of values and their posterior probabilities. And now we need an expert system or belief-net or something that interprets that  that looks at all the values and says  ""The winner is Timing. Now  go there."" ""Uh  go there  Timing  now."" Or  ""The winner is Info-on  Function-Off."" So  he wants to know something about it  and what it does. Nuh? Uh  regardless of - of - of the input. Wh- Regardle- Yeah  but- But how does the expert - but how does the expert system know - how- who- which one to declare the winner  if it doesn't know the question it is  and how that question should be answered? Based on the k- what the question was  so what the discourse  the ontology  the situation and the user model gave us  we came up with these values for these decisions. Yeah I know. But how do we weight what we get out? As  which one i- Which ones are important? So my i- So  if we were to it with a Bayes-net  we'd have to have a node - for every question that we knew how to deal with  that would take all of the inputs and weight them appropriately for that question. Mm-hmm. Does that make sense? Yay  nay? Um  I mean  are you saying that  what happens if you try to scale this up to the situation  or are we just dealing with arbitrary language? We - Is that your point? Well  no. I - I guess my question is  Is the reason that we can make a node f- or - O_K. So  lemme see if I'm confused. Are we going to make a node for every question? Does that make sense? - Or not. For every question? Like - Every construction. Hmm. I don't - Not necessarily  I would think. I mean  it's not based on constructions  it's based on things like  uh  there's gonna be a node for Go-there or not  and there's gonna be a node for Enter  View  Approach. Wel- W- O_K. So  someone asked a question. Yeah. How do we decide how to answer it? @@ Well  look at - look - Face yourself with this pr- question. You get this - You'll have - y- This is what you get. And now you have to make a decision. What do we think? What does this tell us? @@ And not knowing what was asked  and what happened  and whether the person was a tourist or a local  because all of these factors have presumably already gone into making these posterior probabilities. Yeah. What - what we need is a - just a mechanism that says  ""Aha! There is -"" I just don't think a ""winner-take-all"" type of thing is the - I mean  in general  like  we won't just have those three  right? We'll have  uh  like  many  many nodes. Yep. So we have to  like - So that it's no longer possible to just look at the nodes themselves and figure out what the person is trying to say. Because there are interdependencies  right? The uh - Uh  no. So if - if for example  the Go-there posterior possibility is so high  um  uh  w- if it's - if it has reached - reached a certain height  then all of this becomes irrelevant. So. If - even if - if the function or the history or something is scoring pretty good on the true node  true value - Wel- I don't know about that  cuz that would suggest that - I mean - He wants to go there and know something about it? Do they have to be mutual- Yeah. Do they have to be mutually exclusive? I think to some extent they are. Or maybe they're not. Cuz I  uh - The way you describe what they meant  they weren't mutu- uh  they didn't seem mutually exclusive to me. Well  if he doesn't want to go there  even if the Enter posterior proba- So. Wel- Go-there is No. Enter is High  and Info-on is High. Well  yeah  just out of the other three  though  that you had in the - Hmm? those three nodes. The- d- They didn't seem like they were mutually exclusive. No  there's - No. But - It's through the - So th- s- so  yeah  but some - So  some things would drop out  and some things would still be important. Mm-hmm. But I guess what's confusing me is  if we have a Bayes-net to deal w- another Bayes-net to deal with this stuff  Mm-hmm. you know  uh  is the only reason - O_K  so  I guess  if we have a Ba- another Bayes-net to deal with this stuff  the only r- reason we can design it is cuz we know what each question is asking? Yeah. I think that's true. And then  so  the only reason - way we would know what question he's asking is based upon - Oh  so if - Let's say I had a construction parser  and I plug this in  I would know what each construction - the communicative intent of the construction was Mm-hmm. and so then I would know how to weight the nodes appropriately  in response. So no matter what they said  if I could map it onto a Where-Is construction  Ge- Mm-hmm. I could say  ""ah! well the- the intent  here  was Where-Is""  O_K  right. and I could look at those. Yeah. Yes  I mean. Sure. You do need to know - I mean  to have that kind of information. Hmm. Yeah  I'm also agreeing that a simple pru- Take the ones where we have a clear winner. Forget about the ones where it's all sort of middle ground. Prune those out and just hand over the ones where we have a winner. Yeah  because that would be the easiest way. We just compose as an output an X_M_L mes- message that says. ""Go there now."" ""Enter historical information."" And not care whether that's consistent with anything. Right? But in this case if we say  "" definitely he doesn't want to go there. He just wants to know where it is."" or let's call this - this ""Look-At-H-"" He wants to know something about the history of. So he said  ""Tell me something about the history of that."" Now  the e- But for some reason the Endpoint-Approach gets a really high score  too. We can't expect this to be sort of at O_ point three  three  three  O_ point  three  three  three  O_ point  three  three  three. Right? Somebody needs to zap that. You know? Or know - There needs to be some knowledge that - We - Yeah  but  the Bayes-net that would merge - I just realized that I had my hand in between my mouth and my micr- er  my- and my microphone. So then  the Bayes-net that would merge there  that would make the decision between Go-there  Info-on  and Location  would have a node to tell you which one of those three you wanted  and based upon that node  then you would look at the other stuff. Yep. I mean  it- i- Yep. Does that make sense? Yep. It's sort of one of those  that's - It's more like a decision tree  if - if you want. You first look o- at the lowball ones  and then - Yeah  i- Yeah  I didn't intend to say that every possible - O_K. There was a confusion there  k- I didn't intend to say every possible thing should go into the Bayes-net  because some of the things aren't relevant in the Bayes-net for a specific question. Like the Endpoint is not necessarily relevant in the Bayes-net for Where-Is until after you've decided whether you wanna go there or not. Mm-hmm. Right. Show us the way  Bhaskara. I guess the other thing is that um  yeah. I mean  when you're asked a specific question and you don't even - Like  if you're asked a Where-Is question  you may not even look - like  ask for the posterior probability of the  uh  E_V_A node  right? Cuz  that's what - I mean  in the Bayes-net you always ask for the posterior probability of a specific node. So  I mean  you may not even bother to compute things you don't need. Um. Aren't we always computing all? No. You can compute  uh  the posterior probability of one subset of the nodes  given some other nodes  but totally ignore some other nodes  also. Basically  things you ignore get marginalized over. Yeah  but that's - that's just shifting the problem. Then you would have to make a decision  ""O_K  if it's a Where-Is question  which Yeah. So you have to make - Yeah. decision nodes do I query?"" Yes. That's un- But I would think that's what you want to do. Right? Mmm. Well  eventually  you still have to pick out which ones you look at. So it's pretty much the same problem  isn't it? Yeah. Yeah - it's - it's - it's apples and oranges. Nuh? I mean  maybe it does make a difference in terms of performance  computational time. So either you Mm-hmm. always have it compute all the posterior possibilities for all the values for all nodes  and then prune Mmm. the ones you think that are irrelevant  or you just make a p- @@ a priori estimate of what you think might be relevant and query those. Yeah. So basically  you'd have a decision tree query  Go-there. If k- if that's false  query this one. If that's true  query that one. And just basically do a binary search through the - ? I don't know if it would necessarily be that  uh  complicated. But  uh - I mean  it w- Well  in the case of Go-there  it would be. In the case - Cuz if you needed an- If y- If Go-there was true  you'd wanna know what endpoint was. And if it was false  you'd wanna d- look at either Lo- Income Info-on or History. Yeah. That's true  I guess. Yeah  so  in a way you would have that. Also  I'm somewhat boggled by that Hugin software. O_K  why's that? I can't figure out how to get the probabilities into it. Like  I'd look at - Mm-hmm. It's somewha- It's boggling me. O_K. Alright. Well  hopefully it's fixable. Ju- Oh yeah  yeah. I d- I just think I haven't It's - there's a - figured out what - the terms in Hugin mean  versus what Java Bayes terms are. O_K. Um  by the way  are - Do we know whether Jerry and Nancy are coming? Or - ? So we can figure this out. They should come when they're done their stuff  basically  whenever that is. So. What d- what do they need to do left? Um  I guess  Jerry needs to enter marks  but I don't know if he's gonna do that now or later. But  uh  if he's gonna enter marks  it's gonna take him awhile  I guess  and he won't be here. And what's Nancy doing? Nancy? Um  she was sorta finishing up the  uh  calculation of marks and assigning of grades  but I don't know if she should be here. Well - or  she should be free after that  so - assuming she's coming to this meeting. I don't know if she knows about it. She's on the email list  right? Is she? O_K. Mm-hmm. O_K. Because basically  what - where we also have decided  prior to this meeting is that we would have a rerun of the three of us sitting together O_K. sometime this week again O_K. and finish up the  uh  values of this. So we have  uh - Believe it or not  we have all the bottom ones here. Well  I - You added a bunch of nodes  for - ? Yep. O_K. We - we - we have - Actually what we have is this line. Uh  what do the  uh  Right? structures Hmm? do? So the - the - the - For instance  this Location node's got two inputs  that one you - Four inputs. Hmm. Four. Those are - The bottom things are inputs  also. Oh  I see. Yeah. O_K  that was- O_K. That makes a lot more sense to me now. Yep. Cuz I thought it was like  that one in Stuart's book about  you know  the - Alarm in the dog? U- Yeah. Yeah. Or the earthquake and the alarm. Sorry. Yeah  I'm confusing two. Yeah  there's a dog one  too  but that's in Java Bayes  isn't it? Right. Maybe. But there's something about bowel problems or something with the dog. Yeah. And we have all the top ones  all the ones to which no arrows are pointing. What we're missing are the - these  where arrows are pointing  where we're combining top ones. So  we have to come up with values for this  and this  this  this  and so forth. And maybe just fiddle around with it a little bit more. And  um. And then it's just  uh  edges  many of edges. And  um  we won't meet next Monday. So. Cuz of Memorial Day? Yep. We'll meet next Tuesday  I guess. Yeah. When's Jerry leaving for - Italia? On - on Friday. Which Friday? This - this Friday. O_K. Oh. This Friday? Ugh. This Friday. As in  four days? Yep. Or  three days? Is he - How long is he gone for? Two weeks. Italy  huh? What's  uh - what's there? Well  it's a country. Buildings. People. But it's not a conference or anything. He's just visiting. Pasta. Hmm? Right. Just visiting. Vacation. It's a pretty nice place  in my brief  uh  encounter with it. Do you guys - Oh  yeah. So. Part of what we actually want to do is sort of schedule out what we want to surprise him with when - when he comes back. Um  so - Oh  I think we should disappoint him. Yeah? You - or have a finished construction parser and a working belief-net  and uh - That wouldn't be disappointing. I think w- we should do absolutely no work for the two weeks that he's gone. Well  that's actually what I had planned  personally. I had - I - I had sort of scheduled out in my mind that you guys do a lot of work  and I do nothing. And then  I sort of - Oh  yeah  that sounds good  too. sort of bask in - in your glory. But  uh  i- do you guys have any vacation plans  because I myself am going to be  um  gone  but this is actually not really important. Just this weekend we're going camping. Yeah  I'm wanna be this - gone this weekend  too. Ah. But we're all going to be here on Tuesday again? Looks like it? Yeah. O_K  then. Let's meet - meet again next Tuesday. And  um  finish up this Bayes-net. And once we have finished it  I guess we can  um - and that's going to be more just you and me  because Bhaskara is doing probabilistic  recursive  structured  object-oriented  uh  Killing machines! reasoning machines. Yes. And  um - Killing  reasoning. What's the difference? Wait. So you're saying  next Tuesday  is it the whole group meeting  or just Uh. us three working on it  or - or - ? The whole group. And we present our results  our final  O_K. definite - So  when you were saying we need to do a re-run of  like - h- What? What - Like  just working out the rest of the - Yeah. We should do this th- the upcoming days. This week? So  this week  yeah. O_K. When you say  ""the whole group""  you mean the four of us  and Keith? And  Ami might. Ami might be here  and it's possible that Nancy'll be here? Yep. So  yeah. Because  th- you know  once we have the belief-net done - You're just gonna have to explain it to me  then  on Tuesday  how it's all gonna work out. You know. We will. O_K. Because then  once we have it sort of up and running  then we can start you know  defining the interfaces and then feed stuff into it and get stuff out of it  and then hook it up to some fake construction parser and - That you will have in about nine months or so. Yeah. Yeah. And  um  The first bad version'll be done in nine months. Yeah  I can worry about the ontology interface and you can - Keith can worry about the discourse. I mean  this is pretty - Um  I mean  I - I - I hope everybody uh knows that these are just going to be uh dummy values  right? Which - where the - Which ones? S- so - so if the endpoint - if the Go-there is Yes and No  then Go-there- discourse will just be fifty-fifty. Right? Um  what do you mean? If the Go-there says No  then the Go-there is - I don't get it. I don't u- understand. Um. Like  the Go-there depends on all those four things. Yep. Yeah. But  what are the values of the Go-there- discourse? Well  it depends on the situation. If the discourse is strongly indicating that - Yeah  but  uh  we have no discourse input. Oh  I see. The d- See  uh  specifically in our situation  D_ and O_ are gonna be  uh - Yeah. Sure. So  whatever. So  so far we have - Is that what the Keith node is? Yep. O_K. And you're taking it out? for now? Or - ? Well  this is D_ - O_K  this  I can - I can get it in here. All the D_ 's are - I can get it in here  so th- We have the  uh  um  sk- let's - let's call it ""Keith-Johno node"". Johno? There is an H_ somewhere printed . There you go. Yeah. People have the same problem with my name. Yeah. Oops. And  um  Does th- th- does the H_ go b- before the A_ or after the A_? Oh  in my name? Before the A_. Yeah. O_K  good. Cuz you kn- When you said people have the same problem  I thought - Cuz my H_ goes after the uh e- e- e- the v- People have the inverse problem with my name. O_K. I always have to check  every time y- I send you an email  a past email of yours  to make sure I'm spelling your name correctly. Yeah. That's good. I worry about you. I appreciate that. But  when you abbreviate yourself as the ""Basman""   you don't use any H_'s. ""Basman""? Yeah  it's because of the chessplayer named Michael Basman  who is my hero. O_K. You're a geek. It's O_ K. I- How do you pronou- O_K. How do you pronounce your name? Eva. Eva? Yeah. Not Eva? What if I were - What if I were to call you Eva? I'd probably still respond to it. @@ I've had people call me Eva  but I don't know. No  not just Eva  Eva. Like if I u- take the V_ and s- pronounce it like it was a German V_ ? Which is F_. Yeah. Um  no idea then. Voiced. What? It sounds like an F_. There's also an F_ in German  which is why I - I - O_K. Well  it's just the difference between voiced and unvoiced. O_K. Yeah. As long as that's O_ K. I mean  I might slip out and say it accidentally. That's all I'm saying. Um. That's fine. Yeah. It doesn't matter what those nodes are  anyway  because we'll just make the weights ""zero"" for now. Yep. We'll make them zero for now  because it - who - who knows what they come up with  what's gonna come in there. O_K. And  um  then should we start on Thursday? O_K. And not meet tomorrow? Sure. O_K. I'll send an email  make a time suggestion. Wait  maybe it's O_K  so that - that - that we can - that we have one node per construction. Cuz even in people  like  they don't know what you're talking about if you're using some sort of strange construction. Yeah  they would still c- sort of get the closest  best fit. Well  yeah  but I mean  the - uh  I mean  that's what the construction parser would do. Uh  I mean  if you said something completely arbitrary  it would f- find the Mm-hmm. closest construction  right? But if you said something that was completel- er - h- theoretically the construction parser would do that - O_K. But if you said something for which there was no construction whatsoever  n- people wouldn't have any idea what you were talking about. Mm-hmm. Like ""Bus dog fried egg."" I mean. You know. Or  if even something Chinese  for example. Or  something in Mandarin  yeah. Or Cantonese  as the case may be. What do you think about that  Bhaskara? I mean - Well - But how many constructions do - could we possibly have nodes for? In this system  or in r- No  we. Like  when people do this kind of thing. Oh  when p- How many constructions do people have? Yeah. I have not the slightest idea. Is it considered to be like in - are they considered to be like very  uh  sort of s- abstract things? Every noun is a construction. O_K  so it's like in the thousands. The - Yeah. Any - any form- meaning pair  to my understanding  is a construction. O_K. So. And form u- starts at the level of noun - Or actually  maybe even sounds. Yeah. Phoneme. Yep. And goes upwards until you get the ditransitive construction. And then  of course  the c- I guess  maybe there can be the - S- Can there be combinations of the dit- Discourse-level Yeah. constructions. The ""giving a speech"" construction  Rhetorical constructions. Yeah. Yes. But  I mean  you know  you can probably count - count the ways. I mean. It's probab- Yeah  I would s- definitely say it's finite. Yeah. And at least in compilers  that's all that really matters  as long as your analysis is finite. How's that? How it can be finite  again? Nah  I can't think of a way it would be infinite. Well  you can come up with new constructions. Yeah. If the - if your - if your brain was totally non-deterministic  then perhaps there's a way to get  uh  infin- an infinite number of constructions that you'd have to worry about. But  I mean  in the practical sense  it's impossible. Right. Cuz if we have a fixed number of neurons - ? Yeah. So the best-case scenario would be the number of constructions - or  the worst-case scenario is the number of constructions equals the number of neurons. Well  two to the power of the number of neurons. Right. But still finite. O_K. No  wait. Not necessarily  is it? We can end the meeting. I just - Can't you use different var- different levels of activation? across  uh - Mm-hmm. lots of different neurons  to specify different values? Um  yeah  but there's  like  a certain level of - There's a bandwidth issue  right? Yeah. Bandw- Yeah  so you can't do better than something. Turn off the mikes. Otherwise it gets really tough for the tr- ",The main topic for discussion by the Berkeley Meeting Recorder group was progress on the experiments run as part of the groups main project  a speech recogniser for the cellular industry. This included reporting the results  and making conclusions to shape future work. Also discussed were the details of the continued collaboration with project partner OGI. Further investigation into the lack of difference using MSG features makes should not be made while they are on their current short time scale for results. Same goes for anything else that comes up and looks interesting  leave it for just now. Really should pick which results are looking the best at this stage  and take only them further. Someone should look closely at the non-TIMIT databases  their Viterbi alignments  and their phoneme strings to see is that is why TIMIT is better. Need to get OGI's system from them  and get it running like they do  before integrating into it. It is unclear if the English TIMIT database is providing the best results because English is the best language or TIMIT is the most accurately labelled dataset because it was hand labelled. The results table was very large and difficult to follow; it was unclear which of the numbers were error or accuracy rate  and straight rates or percentages of the baseline. There is very limited training data  over only a few conditions. Test and real data is likely to encompass much more variability. Speakers mn007 and fn002 have made further progress into the series of experiments they have been running in previous weeks; results were varied. The main conclusions include that training on task data is good  and the best broad training data is the English TIMIT database. Other results show that MSG makes little difference  adding MLP improves when trained on task data  decreases figures when not  while using delta generally improves the situation  as does on-line normalization. Starting work with a new broad database drawn from English  French  TIMIT  SPINE and English and Italian digits. mn007 has also started work on multi-band MLP trainings  with large context. OGI have a block diagram explaining their system  and the group are trying to fit their work into it. Speaker me006 has been helping prepare data  but is mainly doing work for a class he takes  looking at modelling asynchrony. 
"So. O_K . Doesn't look like it crashed. That's great. So I think maybe what's causing it to crash is I keep starting it and then stopping it to see if it's working. And so I think starting it and then stopping it and starting it again causes it to crash. So  I won't do that anymore. And it looks like you've found a way of uh mapping the location to the - without having people have to give their names each time? Sounds like an initialization thing. I mean it's like you have the - So you know that - No. I mean  are you going to write down that I sat here? I'm gonna collect the digit forms and write it down. O_K. Oh  O_K. So - @@ So they should be right with what's on the digit forms. O_K  so I'll go ahead and start with digits. Um  reading transcript one two five one dash one two seven zero. u- seven five three nine one two zero one two seven three O_ four five two six nine six three nine three zero six four seven eight two O_ O_ six three O_ nine O_ zero two two nine three four nine five O_ four five five four six seven eight O_ four two two zero two one. And I should say that uh  you just pau- you just read each line an- and then pause briefly. And start by giving the transcript number. O_K. Transcript number one one nine one dash one two one zero. eight zero one one O_ one nine seven four zero two six one six two eight four eight three three seven three four five O_ five O_  eight five six seven three eight eight eight two nine three nine seven eight four O_ O_ eight six three zero nine four nine seven four four one two zero eight four w- one five four eight four zero six eight three O_ three eight seven seven eight Transcript one two one one dash one two three zero. nine O_ one five two three one two five nine one three seven zero zero four five zero one four five three five six seven nine four one nine zero O_ two seven zero five one four two three O_ two four O_ three eight six two four seven four nine eight four one eight seven five O_ nine six zero. Transcript nine five one nine seven zero. nine O_ zero zero six two zero one zero six one four nine five three five O_ four two O_ five six eight seven nine eight five eight nine O_ one O_ two nine nine zero one zero five nine one six zero zero two six six two zero zero six three nine three one zero four five six O_ seven seven O_ nine one four O_ three nine two Tran- Transcript - Uh. O_K  O_K. Oh sorry  go ahead. Transcript one two three one dash one two five zero. O_ zero one zero zero zero one three two four  four five three six four five five six eight O_ six seven eight nine zero five zero two nine one zero three seven zero one six five nine nine two nine eight  one one two seven three six four five six zero five seven seven eight zero eight one three six two nine two one O_ eight four one O_ five seven Transcript number one one three one dash one one five zero. six five two nine seven eight eight nine O_ O_ one one eight two four nine two three five seven eight zero four five zero two five six eight six seven nine one four O_ six two zero six eight one nine eight two nine two three four six one O_ six O_ five nine. Transcript one one five one  one one seven O_. seven three eight six eight nine three six O_ zero zero two one O_ three two nine  six O_ six eight four three zero zero five six six five zero eight seven nine zero six three eight nine zero two two five zero one five one five two five three five three four five O_ six. Transcript  uh  nine seven one  nine nine O_ t- O_ three six one six zero nine zero  five four zero three one two O_ O_ three zer- three zero five five one nine one five six five seven two seven nine O_ eight nine three four three O_ O_ nine six O_ O_ zero zero nine two seven two one three zero one zero zero three three six O_ one nine eight four four five seven zero six seven eight O_ O_ three seven eight. So uh  you see  Don  the unbridled excitement of the work that we have on this project. It's just uh - O_K. Umh. Uh  you know  it doesn't seem like a bad idea to have that information. And I'm surprised I sort of - I'm surprised I forgot that  but uh I think that would be a good thing to add. Yeah  I - I'd - I think it's some- After I just printed out a zillion of them. Yeah  well  that's - Um  so I - I do have a - a- an agenda suggestion. Uh  we - I think the things that we talk about in this meeting uh tend to be a mixture of uh procedural uh mundane things and uh research points and um I was thinking I think it was a meeting a couple of weeks ago that we - we spent much of the time talking about the mundane stuff cuz that's easier to get out of the way and then we sort of drifted into the research and maybe five minutes into that Andreas had to leave. So uh I'm suggesting we turn it around and - and uh sort of we have - anybody has some mundane points that we could send an email later  uh hold them for a bit  and let's talk about the - the research-y kind of things. Um  so um the one th- one thing I know that we have on that is uh we had talked a - a couple weeks before um uh about the uh - the stuff you were doing with - with uh um uh l- l- attempting to locate events  we had a little go around trying to figure out what you meant by "" events "" but I think  you know  what we had meant by ""events"" I guess was uh points of overlap between speakers. But I th- I gather from our discussion a little earlier today that you also mean uh interruptions with something else like some other noise. Yeah. Yes? Uh-huh. Yeah. You mean that as an event also. So at any rate you were - you've - you've done some work on that and um To- right. then the other thing would be it might be nice to have a preliminary discussion of some of the other uh research uh areas that uh we're thinking about doing. Um  I think especially since you - you haven't been in - in these meetings for a little bit  maybe you have some discussion of some of the p- the plausible things to look at now that we're starting to get data  uh and one of the things I know that also came up uh is some discussions that - that uh - that uh Jane had with Lokendra uh about some - some - some um uh work about I - I - I d- I - I don't want to try to say cuz I - I'll say it wrong  but anyway some - some potential collaboration there about - about the - about the - working with these data. Oh. Sure. So. Yeah. So  uh. You wanna just go around? Uh. Well  I don't know if we - if this is sort of like everybody has something to contribute sort of thing  I think there's just just a couple - a couple people primarily um but um Uh  wh- why don't - Actually I think that - that last one I just said we could do fairly quickly so why don't you - you start with that. O_K. Shall I - shall I just start? O_K. Yeah  just explain what it was. Um  so  uh  he was interested in the question of - you know  relating to his - to the research he presented recently  um of inference structures  and uh  the need to build in  um  this - this sort of uh mechanism for understanding of language. And he gave the example in his talk about how um  e- a- I'm remembering it just off the top of my head right now  but it's something about how um  i- ""Joe slipped"" you know  ""John had washed the floor"" or something like that. And I don't have it quite right  but that kind of thing  where you have to draw the inference that  O_K  there's this time sequence  but also the - the - the causal aspects of the uh floor and - and how it might have been the cause of the fall and that um it was the other person who fell than the one who cleaned it and it - These sorts of things. So  I looked through the transcript that we have so far  and um  fou- identified a couple different types of things of that type and um  one of them was something like uh  during the course of the transcript  um um  w- we had gone through the part where everyone said which channel they were on and which device they were on  and um  the question was raised ""Well  should we restart the recording at this point?"" And - and Dan Ellis said  ""Well  we're just so far ahead of the game right now we really don't need to"". Now  how would you interpret that without a lot of inference? So  the inferences that are involved are things like  O_K  so  how do you interpret ""ahead of the game""? You know. So it's the - it's Hmm  metaphorically. i- What you - what you int- what you draw - you know  the conclusions that you need to draw are that space is involved in recording  that um  i- that i- we have enough space  and he continues  like ""we're so ahead of the game cuz now we have built-in downsampling"". So you have to sort of get the idea that um  ""ahead of the game"" is sp- speaking with respect to space limitations  that um that in fact downsampling is gaining us enough space  and that therefore we can keep the recording we've done so far. But there are a lot of different things like that. So  do you think his interest is in using this as a data source  or training material  or what? Well  I - I should maybe interject to say this started off with a discussion that I had with him  so um we were trying to think of ways that his interests could interact with ours and um Mm-hmm. uh I thought that if we were going to project into the future when we had a lot of data  uh and um such things might be useful for that in or- before we invested too much uh effort into that he should uh  with Jane's help  look into some of the data that we're - already have Mm-hmm. and see  is there anything to this at all? Is there any point which you think that  you know  you could gain some advantage and some potential use for it. Cuz it could be that you'd look through it and you say ""well  this is just the wrong task for - for him to pursue his -"" Wrong  yeah. And - and uh I got the impression from your mail that in fact there was enough things like this just in the little sample that - that you looked at that - that it's plausible at least. It's possible. Uh  he was - he - he - you know - We met and he was gonna go and uh you know  y- look through them more systematically and then uh meet again. So it's  you know  not a matter of a - Yeah. Yeah. Yeah. But  yeah  I think - I think it was optimistic. So anyway  that's - that's e- a quite different thing from anything we've talked about that  you know  might - might - might come out from some of this. But he can use text  basically. I mean  he's talking about just using text pretty much  or - ? That's his major - I mentioned several that w- had to do with implications drawn from intonational contours and that wasn't as directly relevant to what he's doing. O_K. He's interested in these - these knowledge structures  inferences that you draw i- from - Yeah  interesting. I mean  he certainly could use text  but we were in fact looking to see if there - is there - is there something in common between our interest in meetings and his interest in - in - in this stuff. So. And I imagine that transcripts of speech - I mean text that is speech - probably has more of those than sort of prepared writing. I - I don't know whether it would or not  but it seems like it would. I don't know  probably de- probably depends on what the prepared writing was. But. Yeah  I don't think I would make that leap  because i- in narratives  you know - I mean  if you spell out everything in a narrative  it can be really tedious  so. Mm-hmm. Yeah  I'm just thinking  you know  when you're - when you're face to face  you have a lot of backchannel and - Oh. That aspect. And - Yeah. And so I think it's just easier to do that sort of broad inference jumping if it's face to face. Well - I mean  so  if I just read that Dan was saying ""we're ahead of the game"" in that - in that context  Yeah. I might not realize that he was talking about disk space as opposed to anything else. I - you know  I - I had several that had to do with backchannels and this wasn't one of them. This - this one really does um m- make you leap from - So he said  you know  ""we're ahead of the game  w- we have built-in downsampling"". Uh-huh. Mm-hmm. And the inference  i- if you had it written down  would be - I guess it would be the same. Uh-huh. But there are others that have backchanneling  it's just he was less interested in those. Can I - Sorry to interrupt. Um  I f- f- f- I've - @@ d- A minute - uh  several minutes ago  I  like  briefly was - was not listening and - So who is "" he "" in this context? Yeah  there's a lot of pronoun - O_K. So I was just realizing we've - You guys have been talking about ""he"" um I believe it. for at least uh  I don't know  three - three four minutes without ever mentioning the person's name again. Yeah. So this is - this is - Actually to make it worse  It's in my notes. this is - gonna be a big  big problem if you want to later do uh  you know  indexing  or uh  Morgan uses ""you"" and ""you"" with gaze and no identification  or - speech understanding of any sort. I just wrote this down. Yeah  actually. Cuz Morgan will say well  "" you had some ideas"" and he never said Li- He looked - You just wrote this? Yeah. Well  I think he's doing that intentionally  aren't you? Right  so it's great. So this is really great because the thing is  because he's looking at the per- even for addressees in the conversation  I bet you could pick that up in the acoustics. Just because your gaze is also correlated with the directionality of your voice. Right. Yeah. Mm-hmm. Uh-huh. Could be. Yeah. That would be tou- Can we- Oh  that would be interesting. Yeah. Yeah  so that  I mean  to even know um when - Yeah  if you have the P_Z_Ms you should be able to pick up what a person is looking at from their voice. Well  especially with Morgan  with the way we have the microphones arranged. I'm sort of right on axis and it would be very hard to tell. Right. Put Morgan always like this and - Uh. Oh  but you'd have the - You'd have fainter - Wouldn't you get fainter reception out here? Well  these - Sure  but I think if I'm talking like this? Right now I'm looking at Jane and talking  now I'm looking at Chuck and talking  I don't think the microphones would pick up that difference. But you don't have this - this problem. Morgan is the one who does this most. I see. So if I'm talking at you  or I'm talking at you. I probably been affect- No  I th- I think I've been affected by too many conversations where we were talking about lawyers and talking about - and concerns about ""oh gee is somebody going to say something bad?"" and so on. Lawyers. And so I - so I'm - I'm tending to stay away from people's names even though uh - I am too. Even though you could pick up later on  just from the acoustics who you were t- who you were looking at. I am too. And we did mention who ""he"" was. Yeah. Early in the conversation. Yeah. Right  but I missed it. But - it was uh - Do - Sh- Can I say or - or is that just too sensitive? Yeah  yeah. Yeah. Yeah. No no  there's - No no  it isn't sensitive at all. I was just - Yeah. Well - I was just - I was overreacting just because we've been talking about it. It's O_K to - And in fact  it is - it is - it is sensitive. I - I came up with something from the Human Subjects people that I wanted to No  but that - it's interesting. mention. I mean  it fits into the m- area of the mundane  but they did say - You know  I asked her very specifically about this clause of how  um  you know  it says ""no individuals will be identified uh  ""in any publication using the data."" O_K  well  individuals being identified  let's say you have a - a snippet that says  ""Joe s- uh thinks such-and-such about - about this field  but I think he's wrongheaded."" Now I mean  we're - we're gonna be careful not to have the "" wrongheaded "" part in there  but - but you know  let's say we say  you know  ""Joe used to think so-and-so about this area  in his publication he says that but I think he's changed his mind."" or whatever. b- But I - Then the issue of - of being able to trace Joe  because we know he's well-known in this field  and all this and - and tie it to the speaker  whose name was just mentioned a moment ago  can be sensitive. So I think it's really - really kind of adaptive and wise to not mention names any more than we have to because if there's a slanderous aspect to it  then how much to we wanna be able to have to remove? Yeah  well  there's that. But I - I mean I think also to some extent it's just educating the Human Subjects people  in a way  because there's - If uh - You know  there's court transcripts  there's - there's transcripts of radio shows - I mean people say people's names all the time. So I think it - it can't be bad to say people's names. It's just that - i- I mean you're right that there's more poten- If we never say anybody's name  then there's no chance of - of - of slandering anybody  but - But  then it won't - I mean  if we - if we - Yeah. I mean we should do whatever's natural in a meeting if - if we weren't being recorded. It's not a meeting. Yeah. Right  so I - So my behavior is probably not natural. So. ""If Person X_ -"" Well  my feeling on it was that it wasn't really important who said it  you know. Yeah. Well  if you ha- since you have to um go over the transcripts later anyway  you could make it one of the jobs of the people who do that to mark u- Well  we t- we t- we talked about this during the anon- anonymization. If we wanna go through and extract from the Right. audio and the written every time someone says a name. And I thought that our conclusion was that we didn't want to do that. Yeah  we really can't. But a- actually  I'm sorry. I really would like to push - finish this off. So it's - I understand. No I just - I just was suggesting that it's not a bad policy p- potentially. So  we need to talk about this later. Yeah  I di- I didn't intend it an a policy though. It was - it was just it was just unconscious - well  semi-conscious behavior. I sorta knew I was doing it but it was - Uh-huh. Well  I still don't know who ""he"" is. No  you have to say  you still don't know who "" he "" is  with that prosody. I - I do- I don't remember who ""he"" is. Ah. Uh  we were talking about Dan at one point and we were talking about Lokendra at another point. And I don't - I don't remember which - which part. Oh. Yeah  depends on which one you mean. It's ambiguous  so it's O_K. Uh  I think - Well  the inference structures was Lokendra. But no. The inference stuff was - was - was Lokendra. O_K. That makes sense  yeah. Yeah. Yeah. Yeah. And the downsampling must have been Dan. Um - Yeah. Good - Yeah. Yeah  you could do all these inferences  yeah. It's an inference. Yeah. Yeah. Um  I - I would like to move it into - into uh what Jose uh has been doing because he's actually been doing something. So. Yeah. Uh-huh. O_K. Well- As opposed to the rest of us. Right. O_K. I - I remind that me - my first objective eh  in the project is to - to study difference parameters to - to find a - a good solution to detect eh  the overlapping zone in eh speech recorded. But eh  tsk  ehhh In that way I - I - I begin to - to study and to analyze the ehn - the recorded speech eh the different session to - to find and to locate and to mark eh the - the different overlapping zone. And eh so eh I was eh - I am transcribing the - the first session and I - I have found eh  eh one thousand acoustic events  eh besides the overlapping zones  eh I - I - I mean the eh breaths eh aspiration eh  eh  talk eh  eh  clap  eh - I don't know what is the different names eh you use to - to name the - the n- Nonspeech sounds? speech Yeah. Oh  I don't think we've been doing it at that level of detail. So. Yeah. Eh  I - I - I do- I don't need to - to - to mmm - to m- to label the - the different acoustic  but I prefer because eh I would like to - to study if eh  I - I will find eh  eh  a good eh parameters eh to detect overlapping I would like to - to - to test these parameters eh with the - another eh  eh acoustic events  to nnn - to eh - to find what is the ehm - the false - eh  the false eh hypothesis eh  nnn  which eh are produced when we use the - the ehm - this eh parameter - eh I mean pitch eh  eh  difference eh  feature - Mm-hmm. You know - So it was - I think some of these um that are the nonspeech overlapping events Umh. may be difficult even for humans to tell that there's two there. Yeah. I mean  if it's a tapping sound  you wouldn't necessarily - or  you know  something like that  it'd be - it might be hard to know that it was two separate events. Yeah. Yeah. Yeah. Yeah. Well - You weren't talking about just overlaps were you? You were just talking about acoustic events. Ye- I - I - I - I t- I t- Someone starts  someone stops - I talk eh about eh acoustic events in general  Yeah. but eh my - my objective eh will be eh to study eh overlapping zone. Oh. Mm-hmm. Eh? How many overlaps were there uh in it? n- Eh in twelve minutes I found eh  eh one thousand acoustic events. No no  how many of them were the overlaps of speech  though? How many? Eh almost eh three hundred eh in one session in five - eh in forty-five minutes. Oh  God! Ugh. Three hundred overlapping speech - Alm- Three hundred overlapping zone. Overlapping speech. With the overlapping zone  overlapping speech - speech what eh different duration. Sure. Mm-hmm. Does this - ? So if you had an overlap involving three people  how many times was that counted? Yeah  three people  two people. Eh  um I would like to consider eh one people with difference noise eh in the background  be- No no  but I think what she's asking is if at some particular for some particular stretch you had three people talking  instead of two  did you call that one event? Oh. Oh. Yeah. I consider one event eh for th- for that eh for all the zone. This - th- Well - I - I - I con- I consider - I consider eh an acoustic event  the overlapping zone  the period where three speaker or eh - are talking together. So let's - For- Umh. So let's say me and Jane are talking at the same time  and then Liz starts talking also over all of us. How many events would that be? So- I don't understand. So  two people are talking  Yeah? and then a third person starts talking. Is there an event right here? Hmm. Eh no. No no. For me is the overlapping zone  because - So i- if two or more people are talking. I see. because you - you have s- you have more one - eh  more one voice eh  eh produced in a - in - in a moment. O_K. Yeah. So I think - Yeah. We just wanted to understand how you're defining it. So then  Yeah. If- in the region between - since there - there is some continuous region  in between regions where there is only one person speaking. And one contiguous region like that you're calling an event. Uh-huh. Uh-huh. Yeah. Is it - Are you calling the beginning or the end of it the event  or are you calling the entire length of it the event? I consider the - the  nnn - the nnn  nnn - eh  the entirety eh  eh  all - all the time there were - the voice has overlapped. O_K. This is the idea. But eh I - I don't distinguish between the - the numbers of eh speaker. Uh  I'm not considering eh the - the - ehm eh  the fact of eh  eh  for example  what did you say? Eh at first eh  eh two talkers are uh  eh speaking  and eh  eh a third person eh join to - to that. For me  it's eh - it's eh  all overlap zone  with eh several numbers of speakers is eh  eh the same acoustic event. Wi- but - uh  without any mark between the zone - of the overlapping zone with two speakers eh speaking together  and the zone with the three speakers. It - That would j- just be one. One. One. O_K. Eh  with eh  a beginning mark and the ending mark. Got it. Because eh for me  is the - is the zone with eh some kind of eh distortion the spectral. I don't mind - By the moment  by the moment. I - I don't - Well  but - But you could imagine that three people talking has a different spectral characteristic than two. So. Yeah  but eh - but eh I have to study. Could. You had to start somewhere. Yeah. What will happen in a general way  I - We just w- So there's a lot of overlap. So. Yep. I don't know what eh will - will happen with the - That's a lot of overlap  yeah  for forty-five minutes. So again  that's - that's three - three hundred in forty-five minutes that are - that are speakers  just speakers. Yeah? Yeah. Yeah. But a - a - a th- Uh- huh. O_K. Yeah. So that's about eight per minute. Yeah. Yeah  but - But a thousand events in twelve minutes  that's - Yeah. Uh. But that can include taps. But - Well  but a thousand taps in eight minutes is a l- in twelve minutes is a lot. General. Yeah. Actually - I - I con- I consider - I consider acoustic events eh  the silent too. Silent. Silence starting or silence ending - Yeah  silent  ground to - bec- to detect - eh because I consider acoustic event all the things are not eh speech. Oh  O_K. Mm-hmm. Oh. In ge- in - in - in a general point of view. Oh. O_K  so how many of those thousand were silence? Not speech - not speech or too much speech. Alright. in the per- Too much speech. Right. So how many of those thousand were silence  silent sections? Yeah. Uh silent  I - I - I - I don't - I - I haven't the - eh I - I would like to - to do a stylistic study and give you Yeah. eh with the report eh from eh Yeah. Yeah. the - the study from the - the - the session - one session. And I - I found that eh another thing. When eh eh I w- I - I was eh look at eh nnn  the difference speech file  um  for example  eh if eh we use the ehm - the mixed file  to - to transcribe  the - the events and the words  I - I saw that eh the eh speech signal  collected by the eh this kind of mike - eh of this kind of mike  eh are different Yep. from the eh mixed signal eh  we eh - collected by headphone. And - Right. Yeah. It's right. But the problem is the following. The - the - the - I - I - I knew that eh the signal eh  eh would be different  but eh the - the problem is eh  eh we eh detected eh difference events in the speech file eh collected by - by that mike uh qui- compared with the mixed file. Well - And so if - when you transcribe eh only eh using the nnn - the mixed file  it's possible - eh if you use the transcription to evaluate a different system  it's possible you eh - in the eh i- and you use the eh speech file collected by the eh fet mike  to eh - to nnn - to do the experiments with the - the system  Mm-hmm. Right. its possible to evaluate eh  eh - or to consider eh acoustic events that - which you marked eh in the mixed file  but eh they don't appear in the eh speech signal eh collected by the - by the mike. Right. The - the reason that I generated the mixed file was for I_B_M to do word level transcription  not speech event transcription. Yeah. Yeah. Oh  it's a good idea. It's a good idea I think. So I agree that if someone wants to do speech event transcription  that Yeah. the mixed signals here - I mean  if I'm tapping on the table  you- it's not gonna show up on any of the mikes  Yeah. but it's gonna show up rather loudly in the P_Z_M. So. Yeah. Yeah. So and I - I - I say eh that eh  eh  or this eh only because eh I c- I - I - in my opinion  it's necessary to eh - to eh - to put the transcription on the speech file  collected by the objective signal. I mean the - the - the signal collected by the - Mm-hmm. eh  the real mike in the future  in the prototype Mm-hmm. The - the - the far-field  yeah. to - to eh correct the initial eh segmentation eh with the eh real speech you have to - to analyze - you have to - to process. Because I - I found a difference. Yeah  well  just - I mean  just in that - that one s- Mm-hmm. ten second  or whatever it was  example that Adam had that - that we - we passed on to others a few months ago  there was that business where I g- I guess it was Adam and Jane were talking at the same time and - Mm-hmm. and uh  in the close-talking mikes you couldn't hear the overlap  and in the distant mike you could. So yeah  it's clear that if you wanna study - That's good. if you wanna find all the places where there were overlap  it's probably better to use a distant mike. On the other hand  there's other phenomena that are going on at the same time for which it might be useful to look at the close-talking mikes  so it's - Yeah. But why can't you use the combination of the close-talking mikes  time aligned? If you use the combination of the close-talking mikes  you would hear Jane interrupting me  but you wouldn't hear the paper rustling. And so if you're interested in - Were you interrupting him or was he interrupting you? Some of it's masking - masked. I - I mean if you're interested in speakers overlapping other speakers and not the other kinds of nonspeech  that's not a problem  right? Yeah. Right. Right. Yeah. Although the other issue is that the mixed close-talking mikes - I mean  I'm doing weird normalizations and things like that. Yeah. But it's known. I mean  the normalization you do is over the whole conversation isn't it  over the whole meeting. Yep. Right. Yep. So if you wanted to study people overlapping people  that's not a problem. Yeah. Right. I - I - I think eh - I saw the nnn - the - eh but eh I eh - I have eh any results. I - I - I saw the - the speech file collected by eh the fet mike  and eh eh signal eh to eh - to noise eh relation is eh low. Mm-hmm. It's low. It's very low. You would comp- if we compare it with eh the headphone. And Yep. I - I found that nnn - that eh  ehm  Did - Did you k- pr- probably  I'm not sure eh by the moment  but it's - it's probably that eh a lot of eh  eh for example  in the overlapping zone  on eh - in - in several eh parts of the files where you - you can find eh  eh eh  smooth eh eh speech eh from eh one eh eh talker Mm-hmm. Mm-hmm. in the - in the meeting  it's probably in - in that eh - in - in those files you - you can not find - you can not process because eh it's confused with - with noise. Mm-hmm. And there are a lot of - I think. But I have to study with more detail. But eh my idea is to - to process only nnn  this eh - nnn  this kind of s- of eh speech. Because I think it's more realistic. I'm not sure it's a good idea  but eh - No - i- Well  it's more realistic but it'll - it'll be a lot harder. Yeah. Well  it'd be hard  but on the other hand as you point out  if your - if i- if - if your concern is to get uh the overlapping people - people's speech  you will - you will get that somewhat better. Um  Mm-hmm. Yeah. Are you making any use - uh you were - you were working with th- the data that had already been transcribed. With - By Jane. Does it uh - Yes. Now um did you make any use of that? Yeah. See I was wondering cuz we st- we have these ten hours of other stuff that is not yet transcribed. Do you - Yeah. Yeah. The - the transcription by Jane  t- eh i- eh  I - I - I want to use to - to nnn  eh to put - i- i- it's a reference for me. But eh the transcription - eh for example  I - I don't - I - I'm not interested in the - in the - in the words  transcription words  eh transcribed eh eh in - eh follow in the - in the - in the speech file  but eh eh Jane eh for example eh put a mark eh at the beginning eh of each eh talker  in the - in the meeting  um eh she - she nnn includes information about the zone where eh there are eh - there is an overlapping zone. Mm-hmm. But eh there isn't any - any mark  time - temporal mark  to - to c- eh - to mmm - e-heh  to label O_K. the beginning and the end of the - of the ta- Right  so she is - I'm - I - I - I think eh we need this information to nnn - Right. So the twelve - you - you - it took you twelve hours - of course this included maybe some - some time where you were learning about what - what you wanted to do  but - but uh  it took you something like twelve hours to mark the forty-five minutes  your s- Twelve minutes. Twelve minutes. Twelve minutes! Twelve minutes. Twelve. I thought you did forty-five minutes of - No  forty-five minutes is the - is the session  all the session. Oh  you haven't done the whole session. This is just twelve minutes. Oh. Oh. Yeah  all is the the session. Tw- twelve hours of work to - to segment eh and label eh twelve minutes from a session of part - So let me back up again. So the - when you said there were three hundred speaker overlaps  that's in twelve minutes? of f- Yeah. No no no. I - I consider all the - all the session because eh I - I count the nnn - the nnn - the overlappings marked by - by Jane  Oh  O_K. in - in - in - in the fin- in - in the forty-five minutes. O_K . Oh  I see. So it's three hundred in forty-five minutes  but you have - you have time uh  uh marked - twelve minute - the - the - the um overlaps in twelve minutes of it. Got it. Yeah. Well  not just the overlaps  everything. So  can I ask - can I ask whether you found - uh  you know  how accurate uh Jane's uh uh labels were as far as - you know  did she miss some overlaps? or did she n- ? But  by - by the moment  I - I don't compare  my - my temporal mark with eh Jane  but eh Mm-hmm. Mm-hmm. I - I want to do it. Because eh eh i- per- perhaps I have eh errors in the - in the marks  Mm-hmm. I - and if I - I compare with eh Jane  Yeah. Well - it's probably I - I - I can correct and - and - and - to get eh eh a more accurately eh eh transcription in the file. I- Well  also Jane - Jane was doing word level. Yeah. So we weren't concerned with exactly when an overlap started and stopped. Yeah. Right. Right. I'm expect- I'm not expecting - Well - Well  not only a word level  but actually I mean  you didn't need to No  it's - show the exact point of interruption  you just were showing at the level of the phrase or the level of the speech spurt  or - Right. Mm-hmm. Yep. Well - Yeah. Yeah. Well  yeah  b- yeah  I would say time bin. So my - my goal is to get words with reference to a time bin  beginning and end point. And - and sometimes  you know  it was like you could have an overlap where someone said something in the middle  but  Yeah. Yeah. Yeah. Right. Yeah. yeah  w- it just wasn't important for our purposes to have it that - i- disrupt that unit in order to have  you know  a- the words in the order in which they were spoken  it would have - Yeah. it would have been hard with the interface that we have. Now  my - a- Adam's working on a of course  on a revised overlapping interface  but - Right. Uh-huh. I - I - I think - It's - it's a good eh work  but eh I think we need eh eh more information. No  of course. I expect you to find more overlaps than - than Jane because you're looking at it at a much more detailed level. Yeah. Always need more for - No  no. I - I have to go to - I want eh - I wanted to eh compare the - the transcription. Yeah. I have - But if it takes sixty to one - Well  I- but I have a suggestion about that. Um  obviously this is very  very time-consuming  and you're finding lots of things which I'm sure are gonna be very interesting  but in the interests of making progress  uh might I s- how - how would it affect your time if you only marked speaker overlaps? Only. Yes. Do not mark any other events  but only mark speaker - Do you think that would speed it up quite a bit? Yeah. Uh-huh. O_K. O_K. Do y- do you think that would speed it up? Uh  speed up your - your - your marking? I - I - I - I w- I - I wanted to - nnn  I don't understand very. It took you a long time to mark twelve minutes. Now  my suggestion was for the other thirty-three - Yeah. Oh  yeah  yeah. On- only to mark - only to mark overlapping zone  but - Yeah  and my question is  if you did that  if you followed my suggestion  would it take much less time? Oh  yeah. Sure. Yeah sure. Sure sure. Sure  Yeah O_K. Then I think it's a good idea. Then I think it's a good idea  because it- because I - I need a lot of time to - to put the label or to do that. Yeah. Yeah  I mean  we- we know that there's noise. And- Uh-huh. There's - there's uh continual noise uh from fans and so forth  and there is uh more impulsive noise from uh taps and so forth and - and something in between with paper rustling. Yeah. We know that all that's there and it's a g- worthwhile thing to study  Mm-hmm. but obviously it takes a lot of time to mark all of these things. Yeah. Whereas th- i- I would think that uh you - we can study more or less as a distinct phenomenon the overlapping of people talking. Uh-huh. O_K. O_K. So. Then you can get the - Cuz you need - If it's three hundred uh - i- i- it sounds like you probably only have fifty or sixty or seventy events right now that are really - Yeah. And - and you need to have a lot more than that to have any kind of uh even visual sense of - of what's going on  much less any kind of reasonable statistics. Right. Now  why do you need to mark speaker overlap by hand if you can infer it from the relative energy in the - I mean  you shouldn't need to do this p- completely by hand  right? Well  that's - That's what I was gonna bring up. Um  O_K  yeah. So let's back up because you weren't here for an earlier conversation. I'm sorry. So the idea was that what he was going to be doing was experimenting with different measures such as the increase in energy  such as the energy in the L_P_C residuals  such as - I mean there's a bunch of things - Mm-hmm. I mean  increased energy is -is sort of an obvious one. Yeah. Um  and In the far-field mike. Oh  O_K. uh  it's not obvious  I mean  you could - you could do the dumbest thing and get - get it ninety percent of the time. But when you start going past that and trying to do better  it's not obvious what combination of features is gonna give you the - you know  the right detector. So the idea is to have some ground truth first. And so the i- the idea of the manual marking was to say ""O_K this  i- you know  it's - it's really here"". But I think Liz is saying why not get it out of the transcripts? What I mean is get it from the close-talking mikes. Uh  yeah. We t- we t- w- A- or ge- get a first pass from those  and then go through sort of - It'd be a lot faster probably to - we t- we talked about that. And you can - Yeah  that's his  uh - We - we - we talked about that. s- But so it's a bootstrapping thing and the thing is  Yeah  I just - the idea was  i- we i- i- we thought it would be useful for him to look at the data anyway  and - and then whatever he could mark would be helpful  Right. and we could - Uh it's a question of what you bootstrap from. You know  do you bootstrap from a simple measurement which is right most of the time and then you g- do better  or do you bootstrap from some human being looking at it and then - then do your simple measurements  uh from the close-talking mike. I mean  even with the close-talking mike you're not gonna get it right all the time. Well  that's what I wonder  because um - or how bad it is  be- um  because that would be interesting especially because the bottleneck is the transcription. Right? Well- I'm working on a program to do that  and - I mean  we've got a lot more data than we have transcriptions for. We have the audio data  we have the close-talking mike  so I mean it seems like one kind of project that's not perfect  but - Yeah. um  that you can get the training data for pretty quickly is  you know  if you infer form the close-talking mikes where the on-off points are of speech  you know  how can we detect that from a far-field? Right  we discussed that. And - Oh. I've - I've written a program to do that  and it  uh - O_K  I'm sorry I missed the - @@ It's O_K. and - so - but it's - it's doing something very  very simple. It just takes a threshold  based on - on the volume  Uh-huh. Or you can set the threshold low and then weed out the false alarms by hand. Yeah. Right  by hand. Yeah. um  and then it does a median filter  and then it looks for runs. And  it seems to work  I've - I'm sort of fiddling with the parameters  to get it to actually generate something  and I haven't - I don't - what I'm working on - was working on - was getting it to a form where we can import it into the user interface that we have  into Transcriber. And so - I told - I said it would take about a day. I've worked on it for about half a day  so give me another half day and I- we'll have something we can play with. I have to go. O_K. See  this is where we really need the Meeting Recorder query stuff to be working  because we've had these meetings and we've had this discussion about this  and I'm sort of remembering a little bit about what we decided  Right. I'm sorry. I just - but I couldn't remember all of it. So  It- But - I think it was partly that  you know  give somebody a chance to actually look at the data and see what these are like  partly that we have e- some ground truth to compare against  you know  when - when he - he gets his thing going  Mm-hmm. uh  and - Well  it's definitely good to have somebody look at it. I was just thinking as a way to speed up That was - that was exactly the notion that - that - that we discussed. So. Mm-hmm. you know  the amount of - Yeah. O_K. Thanks. Another thing we discussed was um that - It looks good. I have to go. I'll be in touch. Thanks. S- See ya. Just give me an email. O_K. Yeah. Was that um there m- there was this already a script I believe uh that Dan had written  that uh handle bleedthrough  I mean cuz you have this - this close - you have contamination from other people who speak loudly. Yeah  and I haven't tried using that. It would probably help the program that I'm doing to first feed it through that. It's a cross-correlation filter. So I - I haven't tried that  but that - If - It - it might be something - it might be a good way of cleaning it up a little. So  some thought of maybe having - Yeah  having that be a preprocessor and then run it through yours. Exactly. But - but that's a refinement and I think we wanna see - try the simple thing first  cuz you add this complex thing up uh afterwards that does something good y- y- yo- you sort of wanna see what the simple thing does first. Yep. That's what we were discussing. Yep. But uh  having - having somebody have some experience  again  with - with uh - with marking it from a human standpoint  we're - I mean  I don't expect Jose to - to do it for uh f- fifty hours of - of speech  but I mean we - if uh - if he could speed up what he was doing by just getting the speaker overlaps so that we had it  say  for forty-five minutes  Yeah. Sure. Sure. then at least we'd have three hundred examples of it. And when - when uh Adam was doing his automatic thing he could then compare to that and see what it was different. Yeah. Oh yeah  definitely. Yeah. You know  I did - I did uh something almost identical to this at one of my previous jobs  and it works pretty well. I mean  i- almost exactly what you described  an energy detector with a median filter  you look for runs. And uh  you know  you can - It seemed like the right thing to do. Yeah. I mean  you - you can get y- I mean  you get them pretty close. That was with zero literature search. And so I think doing that to generate these possibilities and then going through and saying yes or no on them would be a quick way to - to do it. Yeah. That's good validation. Is this proprietary? Yeah  do you have a patent on it? Uh. No. No. It was when I was working for the government. Oh  then everybody owns it. It's the people. Well  I mean  is this something that we could just co-opt  or is it - ? No. O_K. Nah. @@ @@ Well  i- i- i- he's pretty close  anyway. I think - I think it's - Yeah  he's - it - it doesn't take a long time. Right. I just thought if it was tried and true  then - and he's gone through additional levels of - of development. Just output. Although if you - if you have some parameters like what's a good window size for the median filter - Yeah. Oh! I have to remember. I'll think about it  and try to remember. And it might be different for government people. That's alright . Yeah  good enough for government work  as they say. They - they - Di- dif- different - different bandwidth. They d- I was doing pretty short  you know  tenth of a second  sorts of numbers. Mm-hmm. O_K. Uh  I don't know  it - if - if we want to uh - So  uh  maybe we should move on to other - other things in limited time. Can I ask one question about his statistics? So - so in the tw- twelve minutes  Yeah. Yeah. um  if we took three hundred and divided it by four  which is about the length of twelve minutes  i- Um  I'd expect like there should be seventy-five overlaps. Did you find uh more than seventy-five overlaps in that period  or - ? More than? More than - How many overlaps in your twelve minutes? How many? Eh  not @@ I- Onl- only I - I transcribe eh only twelve minutes Mm-hmm. Yeah. from the but eh I - I don't co- eh - I don't count eh the - the overlap. The overlaps. O_K. I consider I - I - The - the nnn - The - the three hundred is eh considered only you - your transcription. I have to - to finish transcribing. So. I b- I bet they're more  because the beginning of the meeting had a lot more overlaps than - than sort of the middle. Yeah. Middle or end. Yeah. Because i- we're - we're dealing with the - Uh  in the early meetings  we're recording while we're saying who's talking on what microphone  and things like that  and that seems to be a lot of overlap. I'm not sure. Yeah. Yeah. I think it's an empirical question. I think we could find that out. I'm - I'm not sure that the beginning had more. Yeah. Yep. So - so I was gonna ask  I guess about any - any other things that - that - that either of you wanted to talk about  especially since Andreas is leaving in five minutes  that - that you wanna go with. Can I just ask about the data  like very straightforward question is where we are on the amount of data and the amount of transcribed data  just cuz I'm - I wanted to get a feel for that to sort of be able to know what - what can be done first and Right so there's this - this - like how many meetings are we recording and - There's this forty-five minute piece that Jane transcribed. That piece was then uh sent to I_B_M so they could transcribe so we have some comparison point. Then there's s- a larger piece that's been recorded and @@ uh put on C_D-ROM and sent uh to I_B_M. Right? And then we don't know. How many meetings is that? Like - how many - t- ten - It's like ten meetings or something? What's that? That was about ten hours  and there was about - Yeah  something like that. And then - then we r- Uh-huh. O_K. Ten meetings that have been sent to I_B_M? And - Yeah. Well  I haven't sent them yet because I was having this problem with the missing files. Oh. O_K. Oh  that's right  that had - those have not been sent. H- how many total have we recorded now  altogether? We're saying about twelve hours. About twelve by now. Twelve or thirteen. Uh-huh. And we're recording only this meeting  like continuously we're only recording this one now? or - ? O_K. No. Nope. No  so the - the - that's the - that's the biggest one - uh  chunk so far  It was the morning one. O_K. but there's at least one meeting recorded of uh the uh uh natural language guys. Jerry. Do they meet every week  or every - And then there - Uh  they do. w- w- And we talked to them about recording some more and we're going to  uh  we've started having a morning meeting  today uh i- starting a w- a week or two ago  on the uh front-end issues  and we're recording those  uh there's a network services and applications group here who's agreed to have their meetings recorded  Great. and we're gonna start recording them. They're - They meet on Tuesdays. We're gonna start recording them next week. So actually  we're gonna h- start having a - a pretty significant chunk and so  you know  Adam's sort of struggling with trying to get things to be less buggy  and come up quicker when they do crash and stuff - things like that  now that uh - the things are starting to happen. So right now  yeah  I th- I'd say the data is predominantly meeting meetings  but there are scattered other meetings in it and that - that amount is gonna grow uh so that the meeting meetings will probably ultimately - i- if we're - if we collect fifty or sixty hours  the meeting meetings it will probably be  you know  twenty or thirty percent of it  not - not - not eighty or ninety. But. So there's probably - there's three to four a week  That's what we're aiming for. that we're aiming for. Yeah. And they're each about an hour or something. Yeah  yeah. Although - Yeah. We'll find out tomorrow whether we can really do this or not. So - O_K. Yeah and th- the - the other thing is I'm not pos- I'm sort of thinking as we've been through this a few times  that I really don't know - maybe you wanna do it once for the novelty  but I don't know if in general we wanna have meetings that we record from outside this group do the digits. Right. Because it's just an added bunch of weird stuff. And  you know  we - we h- we're highly motivated. Yeah. Uh in fact  the morning group is really motivated cuz they're working on connected digits  so it's - Actually that's something I wanted to ask  is I have a bunch of scripts to help with the transcription of the digits. Yeah. We don't have to hand - transcribe the digits because we're reading them and I have those. Right. Yeah. And so I have some scripts that let you very quickly extract the sections of each utterance. But I haven't been ru- I haven't been doing that. Um  if I did that  is someone gonna be working on it? Uh  yeah  I - I think I mean  is it something of interest? definitely s- so- Absolutely. Yeah  whoever we have working on O_K. Hmm. the acoustics for the Meeting Recorder are gonna start with that. I mean  I- I'm - I'm interested in it  I just don't have time to do it now. I was - these meetings - I'm sure someone thought of this  but these - So this uh reading of the numbers would be extremely helpful to do um adaptation. Um. Yep. Yep. Actually I have o- @@ I - I would really like someone to do adaptation. So if we got someone interested in that  I think it would be great for Meeting Recorder. Mm-hmm. Well - I mean  one of the things I wanted to do  uh  that I- I talked to - Since it's the same people over and over. to Don about  is one of the possible things he could do or m- also  we could have someone else do it  Mm-hmm. is to do block echo cancellation  to try to get rid of some of the effects of the - the - the far-field effects. Mm-hmm. Um  I mean we have - the party line has been that echo cancellation is not the right way to handle the situation because people move around  and uh  if - if it's - if it's uh not a simple echo  like a cross-talk kind of echo  but it's actually room acoustics  it's - it's - it's - Mm-hmm. you can't really do inversion  and even echo cancellation is going to uh be something - It may - you - Someone may be moving enough that you are not able to adapt quickly and so the tack that we've taken is more ""lets come up with feature approaches and multi-stream approaches and so forth  that will be robust to it for the recognizer and not try to create a clean signal"". Mm-hmm. Uh  that's the party line. But it occurred to me a few months ago that uh party lines are always  you know  sort of dangerous. It's good - good to sort of test them  actually. And so we haven't had anybody try to do a good serious job on echo cancellation and we should know how well that can do. So that's something I'd like somebody to do at some point  just take these digits  take the far-field mike signal  and the close uh mike signal  and apply really good echo cancellation. Um  Hmm. there was a - have been some nice talks recently by - by Lucent on - on their b- the block echo cancellation particularly appealed to me  uh you know  trying and change it sample by sample  but you have some reasonable sized blocks. And um  you know  th- W- what is the um - Ciao. the artifact you try to - you're trying to get rid of when you do that? Uh so it's - it - you have a - a direct uh - Uh  what's the difference in - If you were trying to construct a linear filter  that would um - I'm signing off. Yeah. that would subtract off the um uh parts of the signal that were the aspects of the signal that were different between the close-talk and the distant. You know  so - so uh um I guess in most echo cancellation - Yeah  so you - Given that um - Yeah  so you're trying to - So you'd - There's a - a distance between the close and the distant mikes so there's a time delay there  and after the time delay  there's these various reflections. And if you figure out well what's the - there's a - a least squares algorithm that adjusts itself - adjusts the weight so that you try to subtract - essentially to subtract off uh different uh - different reflections. Right? So let's take the simple case where you just had - you had some uh some delay in a satellite connection or something and then there's a - there's an echo. It comes back. And you want to adjust this filter so that it will maximally reduce the effect of this echo. So that would mean like if you were listening to the data that was recorded on one of those. Uh  just the raw data  you would - you might hear kind of an echo? And - and then this - noise cancellation would get- Well  I'm - I'm - I'm saying - That's a simplified version of what's really happening. What's really happening is - Well  when I'm talking to you right now  you're getting the direct sound from my speech  but you're also getting  uh  the indirect sound that's bounced around the room a number of times. O_K? So now  if you um try to r- you - To completely remove the effect of that is sort of impractical for a number of technical reasons  but I - but - not to try to completely remove it  that is  invert the - the room response  but just to try to uh uh eliminate some of the - the effect of some of the echos. Um  a number of people have done this so that  say  if you're talking to a speakerphone  uh it makes it more like it would be  if you were talking right up to it. So this is sort of the st- the straight-forward approach. You say I - I - I want to use this uh - this item but I want to subtract off various kinds of echos. So you construct a filter  and you have this - this filtered version uh of the speech um gets uh uh - gets subtracted off from the original speech. Then you try to - you try to minimize the energy in some sense. And so um - uh with some constraints. Kind of a clean up thing  that - O_K. It's a clean up thing. Right. So  echo cancelling is - is  you know  commonly done in telephony  and - and - and it's sort of the obvious thing to do in this situation if you - if  you know  you're gonna be talking some distance from a mike. When uh  I would have meetings with the folks in Cambridge when I was at B_B_N over the phone  they had a um - some kind of a special speaker phone and when they would first connect me  it would come on and we'd hear all this noise. Yeah. And then it was uh - And then it would come on and it was very clear  you know. Right. So it's taking samples  it's doing adaptation  it's adjusting weights  and then it's getting the sum. So um  uh anyway that's - that's kind of a reasonable thing that I'd like to have somebody try - somebody look - And - and the digits would be a reasonable thing to do that with. I think that'd be enough data - plenty of data to do that with  and i- for that sort of task you wouldn't care whether it was uh large vocabulary speech or anything. Uh. Is Brian Kingsbury's work related to that  or is it a different type of reverberation? Um Brian's Kingsbury's work is an example of what we did f- f- from the opposite dogma. Right? Which is what I was calling the ""party line""  which is that uh doing that sort of thing is not really what we want. We want something more flexible  uh i- i- where people might change their position  and there might be  you know - There's also um oh yeah  noise. So the echo cancellation does not really allow for noise. It's if you have a clean situation but you just have some delays  Then we'll figure out the right - the right set of weights for your taps for your filter in order to produce the effect of those - those echos. But um if there's noise  then the very signal that it's looking at is corrupted so that it's decision about what the right - you know  right - right uh - delays are - is  uh - is - right delayed signal is - is - is - uh is incorrect. And so  in a noisy situation  um  also in a - in a situation that's very reverberant - with long reverberation times and really long delays  it's - it's sort of typically impractical. So for those kind of reasons  and also a - a c- a complete inversion  if you actually - I mentioned that it's kind of hard to really do the inversion of the room acoustics. Um  that's difficult because um often times the - the um - the system transfer function is such that when it's inverted you get something that's unstable  and so  if you - you do your estimate of what the system is  and then you try to invert it  you get a filter that actually uh  you know  rings  and - and uh goes to infinity. So it's - so there's - there's - there's that sort of technical reason  and the fact that things move  and there's air currents - I mean there's all sorts of - all sorts of reasons why it's not really practical. So for all those kinds of reasons  uh we - we - we sort of um  concluded we didn't want to in- do inversion  and we're even pretty skeptical of echo cancellation  which isn't really inversion  and um we decided to do this approach of taking - uh  just picking uh features  which were - uh will give you more - something that was more stable  in the presence of  or absence of  room reverberation  and that's what Brian was trying to do. So  um  let me just say a couple things that I was - I was gonna bring up. Uh. Let's see. I guess you - you actually already said this thing about the uh - about the consent forms  which was that we now don't have to - So this was the human subjects folks who said this  or that - that - ? The a- apparently - I mean  we're gonna do a revised form  of course. Um but once a person has signed it once  then that's valid for a certain number of meetings. She wanted me to actually estimate how many meetings and put that on the consent form. I told her that would be a little bit difficult to say. So I think from a s- practical standpoint  maybe we could have them do it once every ten meetings  or something. It won't be that many people who do it that often  but um just  you know  so long as they don't forget that they've done it  I guess. O_K. Um  back on the data thing  so there's this sort of one hour  ten hour  a hundred hour sort of thing that - that we have. We have - we have an hour uh that - that is transcribed  we have - we have twelve hours that's recorded but not transcribed  and at the rate we're going  uh by the end of the semester we'll have  I don't know  forty or fifty or something  if we - if this really uh - Well  do we have that much? Let's see  we have - Not really. It's three to four per week. So that's what - You know  that - uh eight weeks  uh is - So that's not a lot of hours. Um - Eight weeks times three hours is twenty-four  so that's - Yeah  so like thirty - Three - Three hours. Yeah. thirty hours? I mean  is there - I know this sounds tough but we've got the room set up. Um I was starting to think of some projects where you would use well  similar to what we talked about with uh energy detection on the close-talking mikes. There are a number of interesting questions that you can ask about how interactions happen in a meeting  that don't require any transcription. So what are the patterns  the energy patterns over the meeting? Mm-hmm. And I'm really interested in this but we don't have a whole lot of data. So I was thinking  you know  we've got the room set up and you can always think of  also for political reasons  if ICSI collected you know  two hundred hours  that looks different than forty hours  even if we don't transcribe it ourselves  so - But I don't think we're gonna stop at the end of this semester. Right? So  I th- I think that if we are able to keep that up for a few months  we are gonna have more like a hundred hours. I mean  is there - Are there any other meetings here that we can record  especially meetings that have some kind of conflict in them or some kind of deci- I mean  that are less well - I don't - uh  that have some more emotional aspects to them  or strong - We had some good ones earlier. There's laughter  um I'm talking more about strong differences of opinion meetings  maybe with manager types  or - I think it's hard to record those. To be allowed to record them? O_K. Mm-hmm. Yeah  people will get - It's also likely that people will cancel out afterwards. But I - but I wanted to raise the K_P_F_A idea. O_K. Well  if there is  anyway. Yeah  I was gonna mention that. Oh  that's a good idea. That's - That would be a good match. Yeah. Yeah. So - Yeah. So I - I - uh  I - I'd mentioned to Adam  and - that was another thing I was gonna talk - uh  mention to them before - that uh there's uh - It - it oc- it occurred to me that we might be able to get some additional data by talking to uh acquaintances in local broadcast media. Because  you know  we had talked before about the problem about using found data  that - that uh it's just set up however they have it set up and we don't have any say about it and it's typically one microphone  Mm-hmm. in a  uh  uh - or - and - and so it doesn't really give us the - the - the uh characteristics we want. Mm-hmm. Um and so I do think we're gonna continue recording here and record what we can. But um  it did occur to me that we could go to friends in broadcast media and say ""hey you have this panel show  or this - you know  this discussion show  and um can you record multi-channel?"" And uh they may be willing to record it uh with - With lapel mikes or something? Well  they probably already use lapel  but they might be able to have it - it wouldn't be that weird for them to have another mike that was somewhat distant. It wouldn't be exactly this setup  Right. but it would be that sort of thing  Hunh. and what we were gonna get from U_W  you know  assuming they - they - they start recording  isn't - als- also is not going to be this exact setup. Right. No  I think that'd be great  So  if we can get more data. I - I - I - I was thinking of looking into that. the other thing that occurred to me after we had that discussion  in fact  is that it's even possible  since of course  many radio shows are not live  uh that we could invite them to have like some of their - record some of their shows here. Hmm! Wow! Well - Or - The thing is  they're not as averse to wearing one of these head-mount- I mean  they're on the radio  right? So. Right  as we are. Right. Um  I think that'd be fantastic cuz those kinds of panels and - Those have interesting Yeah. Th- that's an - a side of style - a style that we're not collecting here  so it'd be great. And - and the - I mean  the other side to it was the - what - which is where we were coming from - I'll - I'll talk to you more about it later is that - is that there's - there's uh the radio stations and television stations already have stuff worked out presumably  uh related to  you know  legal issues and - and permissions and all that. I mean  they already do what they do - do whatever they do. So it's - Mm-hmm. uh  it's - So it's - so it's another source. So I think it's something we should look into  you know  we'll collect what we collect here hopefully they will collect more at U_W also and um - and maybe we have this other source. But yeah I think that it's not unreasonable to aim at getting  you know  significantly in excess of a hundred hours. I mean  that was sort of our goal. Mm-hmm. The thing was  I was hoping that we could - @@ in the - under this controlled situation we could at least collect  you know  thirty to fifty hours. And at the rate we're going we'll get pretty close to that I think this semester. And if we continue to collect some next semester  I think we should  uh - Right. Yeah I was mostly trying to think  ""O_K  if you start a project  within say a month  you know  how much data do you have to work with. And you - you wanna s- you wanna sort of fr- freeze your - your data for awhile so um right now - and we don't have the transcripts back yet from I_B_M right? Well  we don't even have it for this f- you know  forty-five minutes  that was - Do - Oh  do we now? So um  not complaining  I was just trying to think  you know  what kinds of projects can you do now versus uh six months from now and they're pretty different  because um - Yeah. Right. Yeah. So I was thinking right now it's sort of this exploratory stuff where you - you look at the data  you use some primitive measures and get a feeling for what the scatter plots look like  and - Right. Right  right. and - and uh - and meanwhile we collect  and it's more like yeah  three months from now  or six months from now you can - you can do a lot of other things. Cuz I'm not actually sure  just logistically that I can spend - you know  I don't wanna charge the time that I have on the project too early  before there's enough data to make good use of the time. And that's - and especially with the student Right. uh for instance this guy who seems - Yeah. Uh anyway  I shouldn't say too much  but um if someone came that was great and wanted to do some real work and they have to end by the end of this school year in the spring  how much data will I have to work with  with that person. And so it's - Right. i- Yeah  so I would think  exploratory things now. Uh  three months from now - Um  I mean the transcriptions I think are a bit of an unknown cuz we haven't gotten those back yet as far as the timing  but I think as far as the collection  it doesn't seem to me l- like  uh  unreasonable to say that uh in January  you know  ro- roughly uh - which is roughly three months from now  Hmm. we should have at least something like  you know  twenty-five  thirty hours. @@ And we just don't know about the transcription part of that  so. I mean  it - So that's - Yeah  we need to - I think that there's a possibility that the transcript will need to be adjusted afterwards  and uh Yep. es- especially since these people won't be Right. Yeah. uh used to dealing with multi-channel uh transcriptions. So I think that we'll need to adjust some - And also if we wanna add things like Right. um  well  more refined coding of overlaps  then definitely I think we should count on having an extra pass through. I wanted to ask another a- a- aspect of the data collection. There'd be no reason why a person couldn't get together several uh  you know  friends  and come and argue about a topic if they wanted to  right? If they really have something they wanna talk about as opposed to something @@ - I mean  what we're trying to stay away from was artificial constructions  but I think if it's a real - Why not? Yeah. I mean  I'm thinking  politically - Stage some political debates. You could do this  you know. You could. Well yeah  or just if you're - if you ha- If there are meetings here that happen that we can record even if we don't um have them do the digits  or maybe have them do a shorter digit thing like if it was  you know  We don't have to do the digits at all if we don't want to. uh  one string of digits  or something  they'd probably be willing to do. Then  having the data is very valuable  cuz I think it's um politically better for us to say we have this many hours of audio data  especially with the I_T_R  if we put in a proposal on it. It'll just look like ICSI's collected a lot more audio data. Um  whether it's transcribed or not um  is another issue  but there's - there are research questions you can answer without the transcriptions  or at least that you can start to answer. It seems like you could hold some meetings. You know  you and maybe Adam? You - you could - you could maybe hold some additional meetings  if you wanted. Yep. So. Would it help at all - I mean  we're already talking about sort of two levels of detail in meetings. One is uh um without doing the digits - Or  I guess the full-blown one is where you do the digits  and everything  and then talk about doing it without digits  what if we had another level  just to collect data  which is without the headsets and we just did the table-mounted stuff. Need the close-talking mikes. You do  O_K. I mean  absolutely  yeah. I'm really scared - Yeah. Yeah. It seems like it's a big part of this corpus is to have the close-talking mikes. Um or at least  like  me personally? I would - I - couldn't use that data. I see  O_K. Yeah. I agree. And Mari also  we had - This came up when she- she was here. That's important. Um. Yeah  I - I - So it's a great idea  and if it were true than I would just do that  but it's not that bad - like the room is not the bottleneck  and we have enough time in the room  it's getting the people to come in and put on the - and get the setup going. b- By the - by the way  I don't think the transcriptions are actually  in the long run  such a big bottleneck. I think the issue is just that we're - we're blazing that path. Right? And - and um - d- Do you have any idea when - when uh the - you'll be able to send uh the ten hours to them? Well  I've been burning two C_Ds a day  which is about all I can do with the time I have. So it'll be early next week. Yeah. Yeah. Yeah  O_K. So early next week we send it to them  and then - then we check with them to see if they've got it and we - we start  you know Yep. asking about the timing for it. So I think once they get it sorted out about how they're gonna do it  which I think they're pretty well along on  cuz they were able to read the files and so on. Right? Yep. Yeah  but - Well - Yeah  who knows where they are. Have they ever responded to you? Nope. Hhh. Yeah  but - You know  so they - they - they have - What if - you know  they're volunteering their time and they have a lot of other things to do  right? But they - Yeah  you - we can't complain. Yeah. But at any rate  they'll - I - I think once they get that sorted out  they're - they're making cassettes there  then they're handing it to someone who they - who's - who is doing it  and uh I think it's not going to be - I don't think it's going to be that much more of a deal for them to do thirty hours then to do one hour  I think. It's not going to be thirty t- Yep. I think that's probably true. Really? So it's the amount of - It's - it's just getting it going. It's pipeline  pipeline issues. Once the pipeline fills. Right. What about these lunch meetings - I mean  I don't know  if there's any way without too much more overhead  even if we don't ship it right away to I_B_M even if we just collect it here for awhile  to record you know  two or three more meeting a week  just to have the data  even if they're um not doing the digits  but they do wear the headphones? But the lunch meetings are pretty much one person getting up and - No  I meant  um  sorry  the meetings where people eat their lunch downstairs  maybe they don't wanna be recorded  but - Oh  and we're just chatting? Just the ch- the chatting. I actually - Yeah  we have a lot of those. I actually think that's useful data  um the chatting  but - Yeah  the problem with that is I would - I think I would feel a little constrained to - You know? O_K. You don't wanna do it  cuz - O_K. Uh  some of the meetings - You know  our ""soccer ball"" meeting? I guess none of you were there for our soccer ball meeting. Alright. Alright  so I'll just throw it out there  if anyone knows of one more m- or two more wee- meetings per week that happen at ICSI  um That was hilarious. that we could record  I think it would be worth it. Yeah. Well  we should also check with Mari again  because they - because they were really intending  you know  maybe just didn't happen  but they were really intending to be duplicating this in some level. So then that would double what we had. O_K. Right. Uh. And there's a lot of different meetings at U_W uh - I mean really m- a lot more than we have here right cuz we're not right on campus  so. Right. Is the uh  notion of recording any of Chuck's meetings dead in the water  or is that still a possibility? Uh  they seem to have some problems with it. We can - we can talk about that later. Um  but  again  Jerry is - Jerry's open - So I mean  we have two speech meetings  one uh network meeting  uh Jerry was open to it but I - I s- One of the things that I think is a little - a little bit of a limitation  there is a think when the people are not involved uh in our work  we probably can't do it every week. You know? I - I - I - I think that - that people are gonna feel uh - are gonna feel a little bit constrained. Now  it might get a little better if we don't have them do the digits all the time. And the - then - so then they can just really sort of try to - put the mikes on and then just charge in and - and - Yep. What if we give people - you know  we cater a lunch in exchange for them having their meeting here or something? Well  you know  I - I do think eating while you're doing a meeting is going to be increasing the noise. But I had another question  which is um  you know  in principle  O_K. Alright  alright  alright. w- um  I know that you don't want artificial topics  but um it does seem to me that we might be able to get subjects from campus to come down and do something that wouldn't be too artificial. I mean  we could - political discussions  or - or something or other  and No   definitely. i- you know  people who are - Because  you know  there's also this constraint. We d- it's like  you know  the - the - uh goldibears - goldi- goldilocks  it's like you don't want meetings that are too large  but you don't want meetings that are too small. And um - a- and it just seems like maybe we could exploit the subj- human subject p- p- pool  in the positive sense of the word. Well  even - I mean  coming down from campus is sort of a big thing  but what about We could pay subjects. or what about people in the - in the building? Yeah  I was thinking  there's all these other peo- Yeah. I mean  there's the State of California downstairs  and - I mean - I just really doubt that uh any of the State of California meetings would be recordable and then releasable to the general public. Yeah. Oh. Mm-hmm. So I - I mean I talked with some people at the Haas Business School who are i- who are interested in speech recognition Alright  well. and  they sort of hummed and hawed and said ""well maybe we could have meetings down here""  but then I got email from them that said ""no  we decided we're not really interested and we don't wanna come down and hold meetings."" So  I think it's gonna be a problem to get people regularly. What about Joachim  maybe he can - But - but we c- But I think  you know  we get some scattered things from this and that. And I - I d- I do think that maybe we can get somewhere with the - with the radio. Uh i- I have better contacts in radio than in television  but - Mm-hmm. You could get a lot of lively discussions from those radio ones. Yep. Well  and they're already - they're - these things are already recorded  we don't have to ask them to - even - and I'm not sure wh- how they record it  but they must record from individual - Yeah. Yeah. n- Well - No  I'm not talking about ones that are already recorded. I'm talking about new ones because - because - because we would be asking them to do something different. Why - why not? Well  we can find out. I know for instance Mark Liberman was interested uh in - in L_D_C getting data  uh  and - Right  that's the found data idea. But what I'm saying is uh if I talk to people that I know who do these th- who produce these things we could ask them if they could record an extra channel  Yeah. let's say  of a distant mike. Mm-hmm. And u- I think routinely they would not do this. So  since I'm interested in the distant mike stuff  I wanna make sure that there is at least that somewhere Right. Great . O_K. and uh - But if we ask them to do that they might be intrigued enough by the idea that they uh might be e- e- willing to - the - I might be able to talk them into it. Mm-hmm. Um. We're getting towards the end of our disk space  so we should think about trying to wrap up here. O_K. Well I don't - why don't we - why d- u- why don't we uh uh turn them - turn the - That's a good way to end a meeting. O_K  leave - leave them on for a moment until I turn this off  cuz that's when it crashed last time. Oh. That's good to know. Turning off the microphone made it crash. Well - That's good to know. O_K. ",The translation of SmartKom to english is in its final stages. The synthesis module will be the last one to do  after the english syntax trees are completed. The system is still buggy and unstable  but it will soon be ready for a demonstration. This is the first of two working demos required for the project. Further than that  there are no restrictions on the focus of the research or its possible applications. For example  issues like spatial descriptions could be investigated. The variety of linguistic conventions seem to develop around an ego/allo-centric and a proximal/distal paradigm. The latter is also reflected in neuro-physiological data. From an engineering perspective  the belief-net for the AVE task should be completed within a few weeks. The majority of the nodes are already there. This leaves the dependencies between them and the rules of computation to be set. Since the whole system is going to be re-designed  there are major decisions to be taken regarding the parser and the ontology  as well as what can be re-used from past EML projects. In parallel  another team is working on formalisation and notation. Finally  more ideas are expected to come from students and their research. The final english SmartKom demo will be presented to the whole institute once the system is de-bugged and stabilised. After the demo  the focus of research can switch towards purely scientific goals  including issues on ontology  deep semantic constructions  execution engines etc. Moreover  a new system will be designed for the project and at least some parts of it should be built. Similarly  the construction analyser should be a single  general tool working for both the tourist domain and child language modelling. The focus for the next meeting will be on the belief-net  of which a working demo should be complete in the next few weeks. Since there are not enough data  its connections and weights will have to be designed. Although JavaBayes has been the tool of choice until now  the possibility that Hugin could be a better option should be investigated. In order to promote the collaboration with EML  the group-ware server there will be updated with all progress being made in the two sites. A talk on some of the issues will also be organised to take place at DFKI. The german SmartKom version available on the server does not work. The english version  although still under development  does work  however  the system is still unstable as -apart from other reasons- it was initially built to work with a touch screen. De-bugging and cleaning up has to take place before any new modules are added on it. As regards the belief-net  no connections and dependencies have been built into it. These will have to be guessed instead of learnt through data  as not enough data is available for such a task. Finally  it has been noted that the JavaBayes GUI does not satisfy all the presentation requirements for this belief-net and modifying the underlying code would be too time-consuming. The german SmartKom system has been translated to English up to the speech synthesis level. The german syntax trees are currently being adapted to English. These also contribute information to the synthesis module in order to achieve better pronunciation. The current english version is probably the best working one  since some of the problems with the original system have been corrected. The design of the belief-net has also progressed significantly: the vast majority of the nodes have been identified and the feasibility of the task from a technical point of view has been confirmed. 
"Let's see. Test? Test? Yeah. O_K. Channel one. Hello? Hello? Test. I was saying Hynek'll be here next week  uh  Wednesday through Friday - uh  through Saturday  and  um  I won't be here Thursday and Friday. But my suggestion is that  uh  at least for this meeting  people should go ahead  uh  cuz Hynek will be here  and  you know  we don't have any Czech accent yet  uh  as far as I know  so - O_K. There we go. Um. So other than reading digits  what's our agenda? I don't really have  uh  anything new. Been working on Meeting Recorder stuff. So. O_K. Um. Do you think that would be the case for next week also? Or is - is  uh - ? What's your projection on - ? Um. Cuz the one thing - the one thing that seems to me we really should try  if you hadn't tried it before  because it hadn't occurred to me - it was sort of an obvious thing - is  um  adjusting the  uh  sca- the scaling and  uh  insertion penalty sorta stuff. I did play with that  actually  a little bit. Um. What happens is  uh  when you get to the noisy stuff  you start getting lots of insertions. Right. And  um  so I've tried playing around a little bit with  um  the insertion penalties and things like that. Yeah. Um. I mean  it - it didn't make a whole lot of difference. Like for the well-matched case  it seemed like it was pretty good. Um. I could do more playing with that  though. And  uh - and see. But you were looking at mel cepstrum. Yes. Right. Oh  you're talking about for th- for our features. @@ Right. So  I mean  i- it- it's not the direction that you were working with that we were saying what's the - uh  what's the best you can do with - with mel cepstrum. Mmm. But  they raised a very valid point  which  I guess - So  to first order - I mean  you have other things you were gonna do  but to first order  I would say that the conclusion is that if you  um  do  uh  some monkeying around with  uh  the exact H_T_K training and @@ with  uh  you know  how many states and so forth  that it - it doesn't particularly improve the performance. In other words  that even though it sounds pretty dumb  just applying the same number of states to everything  more or less  no matter what language  isn't so bad. Right? And I guess you hadn't gotten to all the experiments you wanted to do with number of Gaussians  Right. but  um  let's just - If we had to - if we had to draw a conclusion on the information we have so far  we'd say something like that. Right? Mm-hmm. Uh  so the next question to ask  which is I think the one that - that - that Andreas was dre- addressing himself to in the lunch meeting  is  um  we're not supposed to adjust the back-end  but anybody using the system would. Yeah. So  if you were just adjusting the back-end  how much better would you do  uh  in noise? Uh  because the language scaling and insertion penalties and so forth are probably set to be about right for mel cepstrum. Mm-hmm. But  um  they're probably not at all set right for these things  particularly these things that look over  uh  larger time windows  in one way or another with - with L_D_A and K_L_T and neural nets and all these things. In the fa- past we've always found that we had to increase the insertion penalty to - to correspond to such things. So  I think that's  uh  @@ that's kind of a first-order thing that - that we should try. So for th- so the experiment is to  um  run our front-end like normal  with the default  uh  insertion penalties and so forth  and then tweak that a little bit and see how much of a difference it makes So by ""our front-end"" I mean take  you know  the Aurora-two s- take some version that Stephane has that is  if we were - Mm-hmm. you know  our current best version of something. Um. I mean  y- don't wanna do this over a hundred different things that they've tried but  you know  for some version that you say is a good one. You know? Um. How - how much  uh  does it improve if you actually adjust that? O_K. But it is interesting. You say you - you have for the noisy - How about for the - for the mismatched or - or - or - or the - or the medium mismatched conditions? Have you - ? When you adjusted those numbers for mel cepstrum  did it - ? Uh  I - I don't remember off the top of my head. Um. Yeah. I didn't even write them down. I - I - I don't remember. I would need to - Well  I did write down  um - So  when I was doing - I just wrote down some numbers for the well-matched case. Yeah. Um. Looking at the - I wrote down what the deletions  substitutions  and insertions were  uh  for different numbers of states per phone. Yeah. Um  but  uh  that - that's all I wrote down. O_K. So. I - I would - Yeah. I would need to do that. O_K. So - I can do that for next week. Yeah. And  um - Yeah. Also  eh  eh  sometimes if you run behind on some of these things  maybe we can get someone else to do it and you can supervise or something. But - O_K. but I think it would be - it'd be good to know that. I just need to get  um  front-end  uh  stuff from you or you point me to some files that you've already calculated. Hmm. Yeah. Alright. O_K. Uh. I probably will have time to do that and time to play a little bit with the silence Mm-hmm. model. So maybe I can have that for next week when Hynek's here. Yeah. Mm-hmm. Yeah. Cuz  I mean  the - the other - That  in fact  might have been part of what  uh  the difference was - at least part of it that - that we were seeing. Remember we were seeing the S_R_I system was so much better than the tandem system. Hmm. Part of it might just be that the S_R_I system  they - they - they always adjust these things to be sort of optimized  and - Is there - ? I wonder if there's anything that we could do to the front-end that would affect the insertion - Yes. I think you can. What could you do? Well  um - uh  part of what's going on  um  is the  uh  the range of values. So  if you have something that has a much smaller range or a much larger range  Oh. and taking the appropriate root. Mm-hmm. You know? If something is kind of like the equivalent of a bunch of probabilities multiplied together  you can take a root of some sort. If it's like seven probabilities together  you can take the seventh root of it or something  or if it's in the log domain  divide it by seven. Mm-hmm. But - but  um  that has a similar effect because it changes the scale of the numbers - of the differences between different candidates from the acoustic model Oh  right. So that w- Right. So  in effect  as opposed to what's coming from the language model. that's changing the value of your insertion penalty. Yeah. I mean  it's more directly like the - the language scaling or the  uh - the model scaling or acoustic scaling  That's interesting. but you know that those things have kind of a similar effect to the insertion penalty anyway. They're a slightly different way of - Mm-hmm. Right. of handling it. So  um - So if we know what the insertion penalty is  then we can get an idea about what range our number should be in  so that they match with that. I think so. Yeah. Yeah. So that's why I think that's another reason other than curiosity as to why i- it would in fact be kinda neat to find out if we're way off. Mm-hmm. I mean  the other thing is  are- aren't we seeing - ? Y- y- I'm sure you've already looked at this bu- in these noisy cases  are - ? We are seeing lots of insertions. Right? The insertion number is quite high? @@ I know the V_A_D takes pre- care of part of that  but - Yeah. Yeah. I've seen that with the mel cepstrum. I don't - I don't know about the Aurora front-end  but - Yeah. I think it's much more balanced with  uh - when the front-end is more robust. Yeah. I could look at it - at this. Yeah. Yeah. Wha- what's a typical number? Mm-hmm. I don't - I don't know. Do we - ? Oh  you - oh  you don't know. O_K. I don't have this in - I'm sure it's more balanced  but it - it - it wouldn't surprise me if there's still - Mm-hmm. I mean  in - in the - the - the old systems we used to do  I - I - Mm-hmm. uh  I remember numbers kind of like insertions being half the number of deletions  as being - and both numbers being - tend to be on the small side comparing to - to  uh  substitutions. Mm-hmm. Well  this - the whole problem with insertions was what I think  um  we talked about when the guy from O_G_I came down that one time and - and that was when people were saying  well we should have a  uh  uh  voice activity detector - Right. that  because all that stuff that we're getting thr- the silence that's getting through is causing insertions. So. Mmm. Right. I'll bet you there's still a lot of insertions. Mm-hmm. Yeah. And it may be less of a critical thing. I mean  the fact that some get by may be less of a critical thing if you  uh  get things in the right range. Mm-hmm. So  I mean  the insertions is - is a symptom. It's a symptom that there's something  uh  wrong with the range. Right. But there's - uh  your - your - your substitutions tend to go up as well. So  uh  I - I - I think that  Mm-hmm. uh  the most obvious thing is just the insertions  @@ . But - Uh - um. If you're operating in the wrong range - I mean  that's why just in general  if you change what these - these penalties and scaling factors are  you reach some point that's a - that's a minimum. So. Um. Um. We do have to do well over a range of different conditions  some of which are noisier than others. Um. But  um  I think we may get a better handle on that if we - if we see - Um  I mean we ca- it's if we actually could pick a - a - a more stable value for the range of these features  it  um  uh  could - Uh - Even though it's - it's - it's true that in a real situation you can in fact adjust the - these - these scaling factors in the back-end  and it's ar- artificial here that we're not adjusting those  you certainly don't wanna be adjusting those all the time. And if you have a nice front-end that's in roughly the right range - Hmm. I remember after we got our stuff more or less together in the previous systems we built  that we tended to set those scaling factors at kind of a standard level  and we would rarely adjust them again  even though you could get a - Mm-hmm. for an evaluation you can get an extra point or something if you tweaked it a little bit. But  once we knew what rou- roughly the right operating range was  it was pretty stable  and - Uh  we might just not even be in the right operating range. So  would the - ? Uh  would a good idea be to try to map it into the same range that you get in the well-matched case? So  if we computed what the range was in well-matched  and then when we get our noisy conditions out we try to make it have the same range as - ? No. You don't wanna change it for different conditions. No. No. I - I - I - What - what I'm saying - Oh  I wasn't suggesting change it for different conditions. I was just saying that when we pick a range  we - we wanna pick a range that we map our numbers into - we should probably pick it based on Yeah. the range that we get in the well-matched case. Otherwise  I mean  what range are we gonna choose to - to map everything into? Well. It depends how much we wanna do gamesmanship and how much we wanna do - I mean  i- if he- it - to me  actually  even if you wanna be - play on the gamesmanship side  it can be kinda tricky. So  I mean  what you would do is set the - set the scaling factors  uh  so that you got the best number for this point four five times the - you know  and so on. Mm-hmm. But they might change that - those weightings. Yeah. Um. So - Uh - I just sorta think we need to explore the space. Mm-hmm. Just take a look at it a little bit. And we - we - we may just find that - that we're way off. O_K. Mm-hmm. Maybe we're not. You know? As for these other things  it may turn out that  uh  it's kind of reasonable. But then - I mean  Andreas gave a very reasonable response  and he's probably not gonna be the only one who's gonna say this in the future - of  you know  people - people within this tight-knit community who are doing this evaluation are accepting  uh  more or less  that these are the rules. Yeah. But  people outside of it who look in at the broader picture are certainly gonna say ""Well  wait a minute. You're doing all this standing on your head  uh  on the front-end  when all you could do is just adjust this in the back-end with one s- one knob."" And Mm-hmm. so we have to at least  I think  determine that that's not true  which would be O_K  or determine that it is true  in which case we want to adjust that and then continue with - with what we're doing. And as you say - as you point out - Right. finding ways to then compensate for that in the front-end also then becomes a priority for this particular test  and saying you don't have to do that. Mm-hmm. So. O_K. So  uh - What's new with you? Uh. So there's nothing new. Uh  what's old with you that's developed? You - Um. I'm sorry? O_K. What's old with you that has developed over the last week or two? Mmm. Well  so we've been mainly working on the report and - and - Yeah. Mainly working on what? On the report of the work that was already Oh. done. Um. Mm-hmm. That's all. How about that - ? Any- anything new on the thing that  uh  you were working on with the  uh - ? I don't have results yet. No results? Yeah. What was that? The - the  uh  Voicing thing. voicing detector. I mean  what- what's - what's going on now? What are you doing? Uh  to try to found  nnn  robust feature for detect between voice and unvoice. And we - w- we try to use the variance of the es- difference between the F_F_T spectrum and mel filter bank spectrum. Yeah. Uh  also the - another parameter is - relates with the auto-correlation function. Uh-huh. R_ze- energy and the variance a- also of the auto-correlation function. Uh-huh. So  that's - Yeah. That's what you were describing  I guess  a week or two ago. So. Yeah. But we don't have res- we don't have result of the AURO- for Aurora yet. We need to train the neural network and - Mm-hmm. So you're training neural networks now? No  not yet. So  what - wha- wh- wha- what- what's going on? Well  we work in the report  too  because we have a lot of result  they are very dispersed  and was necessary to - to look in all the directory to - to - Uh-huh. to give some more structure. So. B- So - Yeah. I- if I can summarize  basically what's going on is that you're going over a lot of material that you have generated in Yea- furious fashion  f- generating many results and doing many experiments and trying to pull it together into some coherent Hm-hmm. form to be able to see wha- see what happens. Yes? Uh  y- yeah. Basically we- we've stopped  uh  experimenting  I mean. We're just writing some kind of technical report. And - Is this a report that's for Aurora? Or is it just like a tech report for ICSI  or - ? No. Yeah. For ICSI. Ah. I see. Yeah. Just summary of the experiment and the conclusion and something like that. Yeah. Mm-hmm. O_K. So  my suggestion  though  is that you - you not necessarily finish that. But that you put it all together so that it's - you've got - you've got a clearer structure to it. You know what things are  you have things documented  you've looked things up that you needed to look up. So that  you know - so that such a thing can be written. Mm-hmm. And  um - When - when - when do you leave again? Uh  in July. First of July. First of July? O_K. And that you figure on actually finishing it in - in June. Because  you know  you're gonna have another bunch of results to fit in there anyway. Mm-hmm. Mm-hmm. And right now it's kind of important that we actually It's not. go forward with experiments. So - so  I - I think it's good to pause  and to gather everything together and make sure it's in good shape  so that other people can get access to it and so that it can go into a report in June. But I think to - to really work on - on fine-tuning the report n- at this point is - is probably bad timing  Mm-hmm. I - I think. Yeah. Well  we didn't - we just planned to work on it one week on this report  not - no more  anyway. Um. But you ma- you may really wanna add other things later anyway because you - Yeah. Mm-hmm. Mmm. There's more to go? Yeah. Well  so I don't know. There are small things that we started to - to do. But - Are you discovering anything  uh  that makes you scratch your head as you write this report  like why did we do that  or why didn't we do this  or - ? Uh. Yeah. Yeah. And - Actually  there were some tables that were also with partial results. We just noticed that  wh- while gathering the result that for some conditions we didn't have everything. Mmm. But anyway. Um. Yeah  yeah. We have  yeah  extracted actually the noises from the SpeechDat-Car. And so  we can train neural network with speech and these noises. Um. It's difficult to say what it will give  because when we look at the Aurora - the T_I-digits experiments  um  they have these three conditions that have different noises  and apparently this system perform as well on the seen noises - on the unseen noises and on the seen noises. But  I think this is something we have to try anyway. So - adding the noises from - from the SpeechDat-Car. Um. That's - @@ that's  uh - that's permitted? Uh. Well  O_G_I does - did that. Um. At some point they did that for - for the voice Uh  for a v- V_A_D. activity detector. Right? Um. Could you say it again? What - what exactly did they do? They used some parts of the  um  Italian database to train the voice activity detector  I think. Yeah. I guess the thing is - Yeah. I guess that's a matter of interpretation. The rules as I understand it  is that in principle It - the Italian and the Spanish and the English - no  Italian and the Finnish and the English ? - were development data Yeah. And Spanish  yeah. on which you could adjust things. And the - and the German and Danish were the evaluation data. Mm-hmm. And then when they finally actually evaluated things they used everything. Yeah. That's right. Uh - So - Uh  and it is true that the performance  uh  on the German was - I mean  even though the improvement wasn't so good  the pre- the raw performance was really pretty good. Mm-hmm. So - And  uh  it - it doesn't appear that there's strong evidence that even though things were somewhat tuned on those three or four languages  that - that going to a different language really hurt you. And the noises were not exactly the same. Right? Because it was taken from a different  uh - I mean they were different Different cars. Yeah. drives. I mean  it was - it was actual different cars and so on. So. Yeah. Um  it's somewhat tuned. It's tuned more than  you know  a - a - a - a - Mm-hmm. You'd really like to have something that needed no particular noise at all  maybe just some white noise or something like that a- at most. Mm-hmm. But that's not really what this contest is. So. Um  I guess it's O_K. Mm-hmm. I think it's - That's something I'd like to understand before we actually use something from it  because it would - it's probably something that  mmm  the - you know  the  uh  experiment designers didn't really think about  because I think most people aren't doing trained systems  or  you know  uh  systems that are like ours  where you actually use the data to build models. I mean  they just doing signal-processing. So. Yeah. Well  it's true  except that  uh  that's what we used in Aurora one  and then they designed the things for Aurora-two knowing that we were doing that. Yeah. That's true. Um. And they didn't forbid us - right? - to build models on the data? No. But  I think - I think that it - it - it probably would be the case that if  say  we trained on Italian  uh  data and then  uh  we tested on Danish data and it did terribly  uh  that - that it would look bad. And I think someone would notice and would say ""Well  look. This is not generalizing."" I would hope tha- I would hope they would. Mm-hmm. Um. But  uh  it's true. You know  maybe there's parameters that other people have used - you know  th- that they have tuned in some way for other things. So it's - it's  uh - We should - we should - Maybe - that's maybe a topic - Especially if you talk with him when I'm not here  that's a topic you should discuss with Hynek to  you know  double check it's O_K. Mm-hmm. Do we know anything about the speakers for each of the  uh  training utterances? What do you mean? We - we - Do you have speaker information? Social security number That would be good. Like  we have male  female  Bank PIN. Hmm. Just male f- female? at least. Mmm. What kind of information do you mean? Well  I was thinking about things like  you know  gender  uh - you know  gender-specific nets and  uh  vocal tract length normalization. Mm-hmm. Things like that. I d- I don't - I didn't know what information we have about the speakers that we could try to take advantage of. Mm-hmm. Hmm. Uh. Right. I mean  again  i- if you had the whole system you were optimizing  that would be easy to see. But if you're supposedly just using a fixed back-end and you're just coming up with a feature vector  w- w- I'm not sure - I mean  having the two nets - Suppose you detected that it was male  it was female - you come up with different - Well  you could put them both in as separate streams or something. Uh. Mm-hmm. Maybe. I don't know. I was just wondering if there was other information we could exploit. Mm-hmm. Hmm. Yeah  it's an interesting thought. Maybe having something along the - I mean  you can't really do vocal tract normalization. But something that had some of that effect Yeah. being applied to the data in some way. Mm-hmm. Um. Do you have something simple in mind for - I mean  vocal tract length normalization? Uh no. I hadn't - I hadn't thought - it was - thought too much about it  really. It just - something that popped into my head just now. And so I - I - I mean  you could maybe use the ideas - a similar idea to what they do in vocal tract length normalization. You know  you have some sort of a  uh  general speech model  you know  maybe just a mixture of Gaussians that you evaluate every utterance against  and then you see where each  you know  utterance - like  the likelihood of each utterance. You divide the - the range of the likelihoods up into discrete bins and then each bin's got some knob - Yeah. But just listen to yourself. I mean  that uh really doesn't sound like a real-time thing with less than two hundred milliseconds  uh  uh  setting. Yeah. Yeah. Mm-hmm. latency that - and where you're not adjusting the statistical engine at all. Yeah. That's true. You know  that just - Right. Hmm. Could be expensive. I mean - Yeah. No. Well not just expensive. I - I - I don't see how you could possibly do it. You can't look at the whole utterance and do anything. You know  you can only - Right? Oh  right. Each frame comes in and it's gotta go out the other end. So  uh - Right. So whatever it was  it would have to be uh sort of on a per frame basis. Yeah. Mm-hmm. Yeah. I mean  you can do  um - Yeah. Fairly quickly you can do male female - f- male female stuff. Yeah. But as far as  I mean - Like I thought B_B_N did a thing with  uh  uh  vocal tract normalization a ways back. Maybe other people did too. With - with  uh  uh  l- trying to identify third formant - average third formant - using that as an indicator of - I don't know. So. You know  third formant - I- if you imagine that to first order what happens with  uh  changing vocal tract is that  uh  the formants get moved out by some proportion - Mm-hmm. So  if you had a first formant that was one hundred hertz before  if the fifty - if the vocal tract is fifty percent shorter  then it would be out at seven fifty hertz  and so on. So  that's a move of two hundred fifty hertz. Whereas the third formant which might have started off at twenty-five hundred hertz  you know  might be out to thirty-seven fifty  you know so it's at - So  although  you frequently get less distinct higher formants  it's still - third formant's kind of a reasonable compromise  and - Mm-hmm. So  I think  eh  if I recall correctly  they did something like that. And - and - But - Hmm. Um  that doesn't work for just having one frame or something. You know? That's more like looking at third formant over - over a turn or something like that  and - Yeah. Mm-hmm. Mm-hmm. Right. Um. So. But on the other hand  male female is a - is a - is a much simpler categorization than figuring out a - a factor to  Mm-hmm. uh  squish or expand the - the spectrum. So  um. Y- you could imagine that - I mean  just like we're saying voiced-unvoiced is good to know - uh  male female is good to know also. Um. Mm-hmm. But  you'd have to figure out a way to - to - to  uh  incorporate it on the fly. Uh  I mean  I guess  as you say  one thing you could do is simply  uh  have the - the male and female output vectors - you know  tr- nets trained only on males and n- trained only on females or - or  uh  you know. But - Um. I don't know if that would really help  because you already have males and females and it's mm-hmm putting into one net. So is it - ? Is it balanced  um  in terms of gender - the data? Do you know? Mmm. Almost  yeah. Hmm. Mm-hmm. Hmm. O_K. Y- you're - you were saying before - ? Uh. Yeah. So  this noise  um - Yeah. The M_S_G - Um. Mmm. There is something - perhaps  I could spend some days to look at this thing  cuz it seems that when we train networks on - let's say  on TIMIT with M_S_G features  they - they look as good as networks trained on P_L_P. But  um  when they are used on - on the SpeechDat-Car data  it's not the case - oh  well . The M_S_G features are much worse  and so maybe they're  um  less - more sensitive to different recording conditions  Shouldn't be. They should be less so. R- right? or - Shou- Yeah. But - Wh- ? But let me ask you this. What - what's the  um - ? Mmm. Do you kno- recall if the insertions were - were higher with M_S_G? I don't know. I cannot tell. But - It's - it - the - the error rate is higher. Yeah. But you should always look at insertions  deletions  and substitutions. So - So  I don- Yeah. Mm-hmm. Mm-hmm. so  uh - @@ M_S_G is very  very dif- Eh  P_L_P is very much like mel cepstrum. M_S_G is very different from both of them. Mm-hmm. So  if it's very different  then this is the sort of thing - I mean I'm really glad Andreas brought this point up. I sort of had forgotten to discuss it. Um. You always have to look at how this - uh  these adjustments  uh  affect things. And even though we're not allowed to do that  again we maybe could reflect that back to our use of the features. So if it - if in fact  uh - The problem might be that the range of the M_S_G features is quite different than the range Mm-hmm. Mm-hmm. of the P_L_P or mel cepstrum. And you might wanna change that. Mm-hmm. But - Yeah. But  it's d- it's after - Well  it's tandem features  so - Mmm. Yeah. Yeah. We - we have estimation of post- posteriors Yeah. with P_L_P and with M_S_G as input  so I don- Well. I don't know. That means they're between zero and one. Mm-hmm. But i- it - it - it - it doesn't necessarily - You know  they could be  um - Do- doesn't tell you what the variance of the things is. Mmm. Mm-hmm. Right? Cuz if you're taking the log of these things  it could be  uh - Knowing what the sum of the probabilities are  doesn't tell you what the sum of the logs are. Mm-hmm. Yeah. So. Yeah. So we should look at the likelihood  or - or what? Or - well  at the log  perhaps  and - Yeah. Yeah. Mm-hmm. Or what - you know  what you're uh - the thing you're actually looking at. Mm-hmm. So your - your - the values that are - are actually being fed into H_T_K. Mm-hmm. But - What do they look like? No- And so th- the  uh - for the tandem system  the values that come out of the net don't go through the sigmoid. Right? They're sort of the pre-nonlinearity values? Right. Yes. So they're kinda like log probabilities is what I was saying. And those - O_K. And tho- that's what goes into H_T_K? Uh  almost. But then you actually do a K_L_T on them. O_K. Um. They aren't normalized after that  are they? Mmm. No  they are not - no. No. O_K. So  um. Right. So the question is - Yeah. Whatever they are at that point  um  are they something for which taking a square root or cube root or fourth root or something like that is - is gonna be a good or a bad thing? So. Mm-hmm. Uh  and that's something that nothing - nothing else after that is gonna - Uh  things are gonna scale it - Uh  you know  subtract things from it  scale it from it  but nothing will have that same effect. Um. So. Um. Yeah. Cuz if - if the log probs that are coming out Anyway  eh - Well  the - of the M_S_G are really big  the standard insertion penalty is gonna have very little effect Right. compared to  you know  Yeah. a smaller set of log probs. No. Again you don't really look at that. It's something - that  and then it's going through this transformation that's probably pretty close to - It's  eh  whatever the K_L_T is doing. But it's probably pretty close to what a - a - a discrete cosine transformation is doing. Yeah. But still it's - it's not gonna probably radically change the scale of things. I would think. And  uh - Yeah. It may be entirely off and - and it may be - at the very least it may be quite different for M_S_G than it is for mel cepstrum or P_L_P. So that would be - So the first thing I'd look at without adjusting anything would just be to go back to the experiment and look at the  uh  substitutions  insertions  and deletions. And if the - if the  uh - i- if there's a fairly large effect of the difference  say  uh  uh  the r- ratio between insertions and deletions Mm-hmm. for the two cases then that would be  uh  an indicator that it might - might be in that direction. Mm-hmm. Anything else? Yeah. But  my - my point was more that it - it works sometimes and - Yeah. but sometimes it doesn't work. So. Well. And it works on T_I-digits and on SpeechDat-Car it doesn't work  and - Yeah. Mm-hmm. Yeah. Well. But  you know  some problems are harder than others  and - Mm-hmm. Yeah. And  uh  sometimes  you know  there's enough evidence for something to work and then it's harder  it breaks. You know  so it's - Mm-hmm. But it - but  um  i- it - it could be that when you say it works maybe we could be doing much better  even in T_I-digits. Right? Yeah. Yeah  sure. So. Uh. Hmm? Yeah. Yeah. Well  there is also the spectral subtraction  which  um - I think maybe we should  uh  try to integrate it in - in our system. Yeah. Mmm. Mm-hmm. Right. O- But  I think that would involve to - to mmm use a big - a - al- already a big bunch of the system of Ericsson. Because he has spectral subtraction  then it's followed by  um  other kind of processing that's - are dependent on the - uh  if it's speech or noi- or silence. Mm-hmm. And there is this kind of spectral flattening after - if it's silence  and - and s- I - I think it's important  um  to reduce this musical noise and this - this increase of variance during silence portions. So. Well. This was in- this would involve to take almost everything from - from the - this proposal and - and then just add some kind of on-line normalization in - in the neural network. O_K. Well  this'll be  I think  something for discussion with Hynek next week. Mmm. Yeah. Mm-hmm. Yeah. O_K. Right. So. How are  uh  uh - how are things going with what you're doing? Oh. Well  um  I took a lot of time just getting my taxes out of the way - multi-national taxes. So  I'm - I'm starting to write code now for my work but I don't have any results yet. Um  i- it would be good for me to talk to Hynek  I think  when he's here. Yeah. Do you know what his schedule will be like? Uh  he'll be around for three days. O_K. So  y- Uh  we'll have a lot of time. So  uh - O_K. Um. I'll  uh - You know  he's - he'll - he'll be talking with everybody in this room So. But you said you won't - you won't be here next Thursday? Not Thursday and Friday. Yeah. Cuz I will be at faculty retreat. Hmm. So. I'll try to connect with him and people as - as I can on - on Wednesday. But - Um. Oh  how'd taxes go? Taxes go O_K? Mmm. Yeah. Yeah. Oh  good. Yeah. Yeah. That's just - that's - that's one of the big advantages of not making much money is the taxes are easier. Yeah. Unless you're getting money in two countries. They both want their cut. I think you are. Aren't you? Hmm. Hmm. Yeah. Right? Yeah. Yeah. Huh. Canada w- Canada wants a cut? Mm-hmm. Have to do - So you - you have to do two returns? Mmm. W- uh  for two thousand I did. Yeah. Oh  oh. Yeah. For tw- That's right  ju- But not for this next year? Two thousand. Yeah. Probably not this next year  I guess. Yeah. Ye- Yeah. Um. Uh  I'll - I'll still have a bit of Canadian income but it'll be less complicated because I will not be a - considered a resident of Canada anymore  so I won't have to declare my American income on my Canadian return. O_K. Alright. Uh. Barry  do you wanna say something about your stuff here? Oh  um. Right. I just  um  continuing looking at  uh  ph- uh  phonetic events  and  uh  this Tuesday gonna be  uh  meeting with John Ohala with Chuck to talk some more about these  uh  ph- um  phonetic events. Um  came up with  uh  a plan of attack  uh  gonna execute  and um - Yeah. It's - that's pretty much it. Oh  well. No- Um  why don't you say something about what it is? Oh  you - oh  you want - you want details. Hmm. O_K. Well  we're all gathered here together. I thought we'd  you know - I was hoping I could wave my hands. Um. So  um. So  once wa- I - I was thinking getting - getting us a set of acoustic events to - um  to be able to distinguish between  uh  phones and words and stuff. And um  once we - we would figure out a set of these events that can be  you know  um  hand-labeled or - or derived  uh  from h- the hand-labeled phone targets. Um  we could take these events and  um  do some cheating experiments  um  where we feed  um  these events into an S_R_I system  um  eh  and evaluate its performance on a Switchboard task. Uh  yeah. Hey  Barry? Can you give an example of an event? Yeah. Sure. Um  I - I can give you an example of twenty-odd events. Um - So  he- In this paper  um  it's talking about phoneme recognition using acoustic events. So  things like frication or  uh  nasality. Whose paper is it? Um  this is a paper by Hubener and Cardson Benson - Bernds- Berndsen. Yeah. Huh. From  uh  University of Hamburg and Bielefeld. Mm-hmm. O_K. Um. Yeah. I think the - just to expand a little bit on the idea of acoustic event. There's  um - in my mind  anyways  there's a difference between  Mm-hmm. um  acoustic features and acoustic events. And I think of acoustic features as being  um  things that linguists talk about  like  um - So  stuff that's not based on data. Stuff that's not based on data  necessarily. Right. That's not based on  Yeah. Oh  O_K. Yeah. Yeah  O_K. you know  acoustic data. So they talk about features for phones  like  uh  its height  its tenseness  laxness  things like that  which may or may not be all that easy to measure in the acoustic signal. Yeah. Mm-hmm. Versus an acoustic event  which is just some- something in the acoustic signal that is fairly easy to measure. Um. So it's  um - it's a little different  in - at least in my mind. I mean  when we did the SPAM work - I mean  there we had - we had this notion of an  uh  auditory - @@ auditory event. Good. That's great. And  uh  um  called them "" avents ""  Mm-hmm. uh  uh  uh  with an A_ at the front. Uh. And the - the - the idea was something that occurred that is important to a bunch of neurons somewhere. So. Mm-hmm. Um. A sudden change or a relatively rapid change in some spectral characteristic will - will do sort of this. I mean  there's certainly a bunch of - a bunch of places where you know that neurons are gonna fire because something novel has happened. That was - that was the main thing that we were focusing on there. But there's certainly other things beyond what we talked about there that aren't just sort of rapid changes  but - It's kinda like the difference between top-down and bottom-up. Yeah. I think of the acoustic - you know  phonetic features as being top-down. You know  you look at the phone and you say this phone is supposed to be - you know  have this feature  this feature  and this feature. Whether tha- those features show up in the acoustic signal is sort of irrelevant. Whereas  an acoustic event goes the other way. Here's the signal. Here's some event. What - ? Mm-hmm. And then that - you know  that may map to this phone sometimes  and sometimes it may not. It just depen- maybe depends on the context  things like that. And so it's sort of a different way of looking. Mm-hmm. Mm-hmm. Yeah. So. Yeah. O_K. Mm-hmm. Um - Using these - these events  um  you know  we can - we can perform these - these  uh  cheating experiments. See how - how - how good they are  um  in  um - in terms of phoneme recognition or word recognition. And  um - and then from that point on  I would  uh  s- design robust event detectors  um  in a similar  um  wa- spirit that Saul has done w- uh  with his graphical models  and this - this probabilistic AND-OR model that he uses. Um  eh  try to extend it to  um - to account for other - other phenomena like  um  C_M_R co-modulation release. And  um - and maybe also investigate ways to - to modify the structure of these models  um  in a data-driven way  uh  similar to the way that  uh  Jeff - Jeff  uh  Bilmes did his work. Um  and while I'm - I'm doing these  um  event detectors  you know  I can ma- mea- measure my progress by comparing  um  the error rates in clean and noisy conditions to something like  uh  neural nets. Um  and - So - so  once we have these - these  uh  event detectors  um  we could put them together and - and feed the outputs of the event detectors into - into the S_R_I  um  H_M_M - H_M_M system  and  um - and test it on - on Switchboard or  um  maybe even Aurora stuff. And  that's pretty much the - the big picture of - of um  the plan. By the way  um  there's  uh  a couple people who are gonna be here - I forget if I already told you this  but  a couple people who are gonna be here for six months. Uh - Mm-hmm. uh  there's a Professor Kollmeier  uh  from Germany who's  uh  uh  quite big in the  uh  hearing-aid signal-processing area and  um  Michael Kleinschmidt  who's worked with him  who also looks at auditory properties inspired by various  uh  brain function things. Hmm. So  um  um  I think they'll be interesting to talk to  in this sort of issue as these detectors are - are  uh  developing. Hmm. O_K. So  he looks at interesting - interesting things in - in the - different ways of looking at spectra in order to - to get various speech properties out. So. O_K. O_K. Well  short meeting  but that's O_K. And  uh  we might as well do our digits. And like I say  I - I encourage you to go ahead and meet  uh  next week with  uh  uh  Hynek. Alright  I'll - I'll start. It's  uh  one thirty-five. Uh  O_K  I'm doing transcript L_ seventy-six. zero three two three six five five five zero seven zero five eight five nine nine two four six five seven seventeen eight zero three four six zero one five five four four eight four four five zero eight eight three six six six six zero one seven two six nine seven one two three five one five eight eight four eight two one eight zero four two three seven seven zero five two eight five seven eight eight six seven four Transcript L_ dash seventy-seven. five eight four five four seven four one six three one three zero two eight seven four five two one two one one six one four two two three eight nine one four eight three eight one seven four zero six five one eight six seven six two nine three three one three two six one three four two four zero two four three two one four one three one three seven zero zero one nine five zero seven nine five six Transcript L_ dash seventy-eight. one five four three five eight nine two seven six four four nine seven four six six four six seven eight seven three three seven six one eight two six eight nine O_ three three three one three three seven seven five one seven five five three six one one four one three zero zero seven five one O_ seven eight two four six one six four three eight two two five O_ three Transcript L_ dash seventy-nine. eight eight five two five two six one seven four four five one nine one two eight eight four one three one six eight four three zero zero four six six eight three nine eight six four eight six six seven five zero three one four one seven zero eight five nine five O_ three seven six O_ two six O_ eight two three O_ seven one nine eight seven five seven four two eight eight three seven one six eight Transcript L_ dash eight zero. nine five six six four three nine seven eight three O_ two six four three six one two eight nine three four four O_ five seven nine eight one three nine O_ eight eight O_ one O_ two zero nine nine five eight nine eight one eight nine five four eight seven nine six one eight seven eight eight three zero nine six seven six two nine zero five seven five six zero seven two nine seven Transcript L_ dash eighty-one. seven nine one five nine zero eight two one six one six eight six four zero one three four four zero five five six four two one nine four two one zero five one two seven two one two nine five eight eight six three nine two five eight eight four three six zero five three zero two three three two six two four three six five two six two four two eight five one four five O_K- ","An idea for future work was suggested during the visit of the german project manager: the possibility to use the same system for language generation. Having a system able to ask questions could contribute significantly to training the belief-net. Setting up certain inputs in the Bayes-net would imply certain intentions  which would trigger dialogues. There is potential to make a conference paper out of presenting the current work and the project aspirations within a parsing paradigm. The focus should be the Bayes-net  to which all other modules interface. Situation  User  Discourse and Ontology feed into the net to infer user intentions. Someone asking where the castle is after having asked about the admission fee  indicates that -given that the castle is open to tourists- they want to go there  as opposed to knowing its whereabouts. It was suggested that they start analysing what the Discourse and Ontology would give as inputs to the Bayes-net by working on simple utterances like ""where is X?"". With this addition  all input layers of the net would be functioning. Although this function would be limited  it would allow for the Bayes-net to be tested in its entirety and  henceforth  extended. The possibility of incorporating language generation into the system will have to be discussed further. Similarly  as no one could recall some of the points of the conference call  the group will have to meet again and define the exact structure and content of the paper they are going to submit. The Bayes-net is going to be the focus of the presentation. In order to complete a functioning prototype of the belief-net  it was decided to start expanding the Ontology and Discourse nodes by working with a simple construction  like ""where is X?"". A robust analysis of such a basic utterance will indicate what the limits of the information derived from the construction are  as well as ways to design the whole module and fit other constructions in. The idea to create a language generation module for the system  along with the language understanding  was met with interest  although it was made clear that generation is not just the inverse of understanding. Understanding what a construction entails does not mean the system can use the construction in all appropriate circumstances. A dialogue producing system would be useful for training the system further  even though the number of input permutations could render the process computationally unwieldy. Regarding the conference paper  it was noted that at this stage they have not completed any big parts of the system and there is no evaluation. Similarly  the length of the paper would not allow for presentation of the formalism in detail. The focus would have to be on cognitive motivations of the research  and not on system design  anyway. Such motivations also apply to the belief-net: there are various direct or indirect ways to link features of the Ontology or Discourse with specific intentions. The originating observation behind the whole project is that utterances like ""Where is X?"" are seemingly unambiguous  but  in context  they can acquire much more complex interpretations. The SmartKom prototype was in need of de-bugging  which is now on its way. Similarly  the work on XML is going to be finished within a day. On the other hand  the data recording has started: almost twenty subjects have already taken part and the transcription of the recordings is running in parallel. Meanwhile a new person  who is also a possible replacement for the wizard's task in the data collection  has been hired. "
O_K So uh today we're looking at a number of uh things we're trying and uh fortunately for listeners to this uh we lost some of it's visual but um got tables in front of us. Um what is - what does combo mean? So combo is um a system where we have these features that go through a network and then this same string of features but low-pass filtered with the low-pass filter used in the M_S_G features. And so these low-pass filtered goes through M_ eh - another M_L_P and then the linear output of these two M_L_P's are combined just by adding the values and then there is this K_L_T. Um the output is used as uh features as well. Um so let me try to restate this and see if I have it right. There is uh - there is the features uh there's the O_G_I features and then um those features um go through a contextual - uh l- l- let's take this bottom arr- one pointed to by the bottom arrow. Um those features go through a contextualized K_L_T. Yeah. Then these features also uh get um low-pass filtered Yeah so yeah I could perhaps draw this on the blackboard Sure. Yeah. Yeah. Yeah. The graph  yeah another one. @@ Yeah  that's good. So So we have these features from O_G_I Yeah. that goes through the three paths. Three  O_K. The first is a K_L_T using several frames Yeah. of the features. Yeah. The second path is uh M_L_P also using nine frames - several frames of features Yeah. Uh-huh. The third path is this low-pass filter. Uh-huh. Uh  M_L_P Aha! aha! Adding the outputs just like in the second propose- the - the proposal from - for the first evaluation. Yeah? Yeah. Yeah. And then No  the K_L_T. the K_L_T And those two together. and then the two together again. That's it. Two H_T_K. Um. O_K so that's - that's this bottom one. So this is - yeah And so uh and then the - the - the one at the top - and I presume these things that uh are in yellow are in yellow because overall they're the best? Yeah that's the reason  yeah. Oh let's focus on them then- so what's the block diagram for the one above it? For the f- the f- first yellow line you mean? Yeah. Yeah so it's uh basically s- the same except that we don't have this Step. uh low-pass filtering so we have only two streams. Well. There's - there's no low - low-pass processing Mm-hmm. used as additional feature stream. Mm-hmm. Um Do you e- um they mentioned - made some - uh when I was on the phone with Sunil they - they mentioned some weighting scheme that was used to evaluate all of these numbers. Yeah. Uh actually the way things seems to um well it's uh forty percent for T_I-digit  sixty for all the SpeechDat-Cars  well all these languages. Ehm the well match is forty  medium thirty five and high mismatch twenty-five. Yeah. Um and we don't have the T_I-digits part yet? Uh  no. O_K. But yeah. Generally what you observe with T_I-digits is that the result are very close whatever the - the system. O_K. And so Yeah. have you put all these numbers together into a single number representing that? I mean not - Uh not yet. No. O_K so that should be pretty easy to do and that would be good - then we could compare the two and say what was better. Mmm yeah  yeah. Mmm. Yeah. Um and how does this compare to the numbers - oh so O_G_I two is just the top - Yeah. top row? So yeah to - actually O_G_I two is the - the baseline with the O_G_I features but this is not exactly the result that they have because they've - they're still made some changes in the features O_K. and - well but uh actually our results are better than their results. Um I don't know by how much because they did not send us the new results O_K. Uh Uh O_K so the one - one place where it looks like we're messing things up a bit is in the highly mismatched Italian. Yeah. Yeah. An- Yeah there is something funny happening here because - yeah. Yeah. But there are thirty-six and then sometimes we are - we are - we are around forty-two and Now up but Uh so one of the ideas that you had mentioned last time was having a - a second um silence detection. Yeah. So there are some results here For the Italian. For this one. uh so the third and the fifth line of the table So filt is what that is? Filt  yeah Yeah. Um yeah so it seems f- for the - the well match and mismatched condition it's uh it brings something. Uh but uh actually apparently there are - there's no room left for any silence detector at the server side because of the delay. Uh Oh we can't do it. Oh O_K. well No. For that - for that we - Oh. Too bad. Uh Good idea  but can't do it. O_K. Yeah. Except I don't know because they - I think they are still working well. Uh-huh. Uh t- two days ago they were still working on this trying to reduce the delay of the silence detector so but yeah if we had time perhaps we could try to find uh some kind of compromise between the delay that's on the handset and on the server side. Perhaps try to reduce the delay on the handset and - but well hmm For the moment they have this large delay on the - the feature computation and O_K. So so we don't Alright so for now at least that's not there you have some results with low-pass filter cepstrum doesn't have a huge effect but it - but it looks like it you know maybe could help in a couple places. I th- Yeah. Uh little bit. Um and um um Yeah and uh let's see What else did we have in there? Uh I guess it makes a l- um at this point this is I - I guess I should probably look at these others a little bit uh And you - you yellowed these out uh but uh uh Oh I see yeah that - that one you can't use because of the delay. Those look pretty good. Um let's see that one Well even the - just the - the second row doesn't look that bad right? That's just uh Yep. yeah? And - and Mmm yeah. that looks like an interesting one too. Uh Actually the - yeah the second line is uh pretty much like the first line in yellow except that we don't have this K_L_T on the first - on the left part of the diagram. We just have the features as they are. Mm-hmm. Um Yeah. Yeah so when we do this weighted measure we should compare the two cuz it might even come out better. Mm-hmm. And it's - it's - it's a little - slightly simpler. Yeah. So - so there's - so I - I would put that one also as a - as a maybe. Uh and it - yeah and it's actually does - does significantly better on the uh uh highly mismatched Italian  so s- and little worse on the mis- on the M_M case  but uh Well yeah it's worse than a few things Mm-hmm. so uh let's see how that c- that c- c- see how that comes out on their - their measure and - are - are we running this uh for T_I-digits or uh Yeah. Yeah. Now is T_I di - is- is that part of the result that they get for the uh development - th- the results that they're supposed to get at the end of - end of the month  the T_I-digits are there also? Yeah. It's included  yeah. Oh O_K. O_K. And see what else there is here. Um Oh I see - the one - I was looking down here at the - the o- the row below the lower yellowed one. Uh that's Mm-hmm? uh that's with the reduced uh K_L_T size - Yeah. reduced dimensionality. Yeah. What happens there is it's around the same and so you could reduce the dimension as you were saying before a bit perhaps. Yeah  it's - it's significantly worse well but - Mm-hmm. It's significantly worse - it's - Exc- except for the H_M it's uh For many a mismatch it's worse. but it's - it's mostly worse. Yeah. But it is little. I mean not - not by a huge amount  I don't know. What are - what are the sizes of any of these sets  I - I'm - I'm sure you told me before  but I've forgotten. So - you know how many words Uh are in uh one of these test sets? I don't remember. Um About? it's - it depends - well - the well matched is generally larger than the other sets and I think it's around Ye- two thousand or three thousand words perhaps  at least. But words - Hmm? well word - I don't know. The words  yeah. Sentences. S- sentences. Some sets have five hundred sentences  so. Yeah. Mmm. So the - so the sets - so the test sets are between five hundred and two thousand sentences  let's say and each sentence on the average has four or five digits or is it - most of them longer or Yeah for the Italian Yeah. It - it d- even seven digits y- more or less but sometime Seven digits. the sentence have only one digit and sometime uh like uh the number of uh credit cards  something like that. Mm-hmm. Right  so between one and sixteen. See the - I mean the reason I'm asking is - is - is we have all these small differences and I don't know Mm-hmm? how seriously to take them  right? So Yeah. uh i- if - if you had uh just you know - to give an example  if you had uh um if you had a thousand words then uh a - a tenth of a percent would just be one word  right? So - so it wouldn't mean anything. Yeah. Yeah. Oh - um so um yeah it be kind of - I'd kind of like to know what the sizes of these test sets were actually. The size that we have? Yeah. We could - we could run - run some kind of significance tests or Yeah since these - well also just to know the numbers  right. So these - these are word error rates so this is on how many words. Yeah. Yeah. Yep. Yeah we have the result that the output of the H_T_K Yeah. The number of - of sentences  no it's the number isn't. Yeah sure - sure. Yeah sure. Yeah. Yeah so anyway if you could just mail out what those numbers are and then - then - that - that be great. Yeah. Um what else is there here? Um see the second - second from the bottom it says S_I_L  but this is some different kind of silence or thing or - what was that? Uh It the - the output silence of the M_L_P. Oh yeah I see. It's only one small experiment to know what happened. To apply also to in- include also the - the silence of the M_L_P we have the fifty-six form and the silence to pick up the silence and we include those. Yes. Uh-huh  uh-huh. The silence plus the K_L_T output? Oh so you're only using the silence. Yeah. Yeah  because when we apply the K_L_T No they're - I think there is this silence in addition to the um K_L_T outputs it is because we - we - we just keep No. in addition  yes. In addition t- uh we don't keep all the dimensions after the K_L_T and - yeah. and we not s- we are not sure if we pick - we have the silence. So we try to add the silence also in addition to the - these twenty-eight dimensions. I see. O_K. And what - and what's O_G_I forty-five? Uh it's o- it's O_G_I two  it's - so the - th- it's the features from the first line The bottom one there? It's in fact O_G_I two. and - yeah. S- Right  but I mean what's the - what does the last row mean? So it's uh basically this but without the K_L_T on the - from the left path. I thought that was the one - I thought that was the second row. So what's the difference between the second Uh the second line you don't have this combo stuff so you just uh Oh. So this is like the second line but with - with the combo stuff. Yeah. And with the - all the output of the combo. Yeah. O_K. Uh Yeah. Yeah. O_K  so - alright so it looks to me - I guess the same - given that we have to take the filt ones out of the - the running because of this delay problem - Mm-hmm. so it looks to me like the ones you said I agree are - are the ones to look at but I just would add the - the - the second row one Yeah. Mmm. and then um if we can um oh yeah also when - when they're using this weighting scheme of forty  thirty-five  twenty-five is that on the percentages or on the raw errors? I guess Uh it's probably on the percentages right? I guess  yeah. Yeah O_K. I guess  yeah. Mmm. Alright. It's not clear here. O_K. Maybe - maybe they'll argue about it. Um O_K so if we can know what - how many words are in each and then um Dave uh Dave promised to get us something tomorrow which will be there Mm-hmm. as far as they've gotten Friday and then we'll operate with that and Yeah. uh how long did it I guess if we're not doing all these things - if we're only doing um um I guess since this is development data it's legitimate to do more than one  right? Yeah. I mean ordinarily if - in final test data you don't want to do several and - and take the best that's - that's - Mmm. that's not proper but if this is development data we could still look at a couple. Yeah. We can - yeah. Sure. But we have to decide - I mean we have to fix the system on this d- on this data  to choose the best and Yeah. I- Right. these But the question is when - when do we fix the system  do we fix the system uh tomorrow or do we fix the system on Tuesday? But we could it d- I - I think we fixed on Tuesday  yeah. Yeah. Mm-hmm. Yeah  O_K except that we do have to write it up. Also  so Mm-hmm. Yeah. Yeah. Um Uh yeah well. Well basically it's this with perhaps some kind of printing and some - some other @@ . Right so maybe what we do is we - we - we uh as soon as we get the data from them Yeah but Mm-hmm. we start the training and so forth but we start the write-up right away because as you say there - there's only minor differences between these. I think you - we could - we could start soon  yeah. Yeah. Write up something. Yeah  and - Um yeah. Mm-hmm. and I - I would - you know  I would - I'd kind of like to see it maybe I can - I can edit it a bit uh sure. The - my - what in this si- i- in this situation is my forte which is English. Yeah. Uh so Mmm. uh H- yeah. Have y- have you seen alt- d- do they have a format for how they want the system descriptions or anything? Uh not really. O_K. Um There is the format of the table which is quite impressive. Yeah? Uh I see. Yes  for those who are listening to this and not looking at it uh it's not really that impressive  it's just tiny. It's all these little categories set a  set b  set c  multi-condition  clean. Uh No mitigation. Wow. Do you know what no - what no mitigation means here ? Um it should be the the problem with the error - channel error or Oh that's probably the - this is probably channel error stuff huh? Oh this is i- right  it says right above here channel - channel error resilience  yeah. well  you - Yeah. Yeah. So recognition performance is just the top part  actually. Uh and they have - yes  split between seen databases and non-seen so basically between development and - and evaluation. Yeah. And so - right  it's presumed there's all sorts of tuning that's gone on on the see- what they call seen databases and there won't be tuning for the uh unseen. Multi-condition - multi-condition. So they have - looks like they have Mm-hmm. uh uh so they splitting up between the T_I-digits and everything else  I see. So the everything else is the SpeechDat-Car  that's the Yeah  so it's not divided between languages you mean or - multi- multilingual Well  it is. It is  but there's also - there's these tables over here for the - for the T_I-digits and these tables over here for the car it just Oh yeah. data which is - which is I guess all the multilingual stuff and then uh there's - Yeah. they also split up between multi-condition and clean only. For T_I-digits. Yes. Yeah  actually yeah. For the T_I-digits they want to train on clean and on noisy Yeah. and - yeah. So we're doing that also  I guess. Uh yeah. But uh we actually - do we have the features? Yeah. For the clean T_I-digits but we did not test it yet. Uh the clean training stuff. O_K. Mmm. Well anyway  sounds like there'll be a lot to do just to work with our partners to fill out the tables over the next uh Mm-hmm. Yes. next few days I guess they have to send it out - let's see the thirty-first is uh uh Wednesday and I think the - it has to be there by some hour uh European time on Wednesday so I think basically Hmm-hmm. We lost time uh Wednesday maybe because E- excuse me? that the difference in the time may be - is a long different of the time. Maybe the Thursday the twelfth of the night of the Thurs- thirty-one is - is not valid in Europe. We don't know is happening. Yeah. Yes  so I mean - I think we have to actually get it done Tuesday right because I - I think Tuesday. Yeah  well. Except if - if it's the thirty-one at midnight or I don't know - uh we can still do some work on Wednesday morning. Uh yeah well. Yeah  well. W- i- is but is - is it midni- I thought it was actually something like five P_M Yeah. Yeah. on - Mm-hmm. was like - I thought it was five P_M or something  I didn't think it was midnight. I thought they said they wanted everything by Yeah  five P_M. well  so five P_M their time is - is - if Not five P_M  three P_M. three P_M. Three P_M. Alright  that's six in the morning here. It's d- Uh no no. three - three A_- three P_M? No  we are wondering about the - the - the hour that we have to Oh yeah  yeah  yeah  yeah. eh I don't know if it's three P_M - it's Three P_M here is in Europe midnight. Yeah  it's - it's midnight but Yes  yes  but I didn't think it was midnight that it was due  I thought it was due at some hour during the day like five P_M or something. Oh O_K. Mm-hmm. Mm-hmm  maybe. In which case so I - I - uh well we should look but my assumption is that we basically have to be done Tuesday. Um Yeah. so then next Thursday we can sort of have a little aftermath but then - then we'll actually have the new data which is the German and the Danish Yeah. but that really will be much less work because uh the system will be fixed Yeah. so all we'll do is take whatever they have and - Yeah. and uh and run it through the process. Uh we won't be changing the training on anything Mm-hmm. so there'll be no new training  there'll just be new H_T_K runs  so that's means in some sense we can kind of relax from this after - after Tuesday and - and uh maybe next meeting we can start talking a little bit about where we want to go from here uh in terms of Mm-hmm. uh the research. Um you know what things uh did you think of when you were uh doing this process that uh you just didn't really have time to adequately work on uh Mm-hmm. uh Yeah. so What? Oh  Stephane always has these great ideas and - Sure. oh  but uh we don't have time. Yeah. Yeah. Yeah. I'm not sure these are great ideas. But they're ideas. Yeah? Yeah. Oh  that was good. And - and uh also it's still true that uh Yeah. But I think it's true that - that we - we at least got fairly consistent i- improved results by running uh the uh neural net transformation in parallel with the features rather than uh in sequence which was - was your suggestion and that - that - that seems to have been borne out. Mm-hmm. Mm-hmm. The fact that none of these are - are - you know  enormous is - is - is not too surprising - most improvements aren't enormous and Yeah. uh some of them are but uh I mean you have something really really wrong and you fix it you can get big and really enormous improvements but Mm-hmm. uh um Cuz our best improvements over the years that we've gotten from finding bugs  but Anyway O_K well I - I think - I see where we are and everybody knows what they're doing and is there - is there anything else we should talk about or - or - are we done? Mm-hmm. I think it's O_K um. We so basically we will - I think we'll try to - to focus on these three architectures and - and perhaps I was thinking also a fourth one with just - just a single K_L_T because we did not really test that - removing all these K_L_T's and Uh-huh. putting one single K_L_T at the end. Yeah  I mean that would be pretty low maintenance to try it. Yeah. Uh if you can fit it in. Mm-hmm. Oh I have - yeah I do have one other piece of information which uh I should tell people outside of this group too uh I don't know if we're gonna need it uh but uh Jeff up at the uh University of Washington has uh gotten a hold of a uh uh some kind of server farm of uh of ten uh uh multiprocessor uh I_B_M machines R_S six thousands Mm-hmm. and - and uh so I think each one is four processors or something or - I don't know  eight hundred megahertz or something and there's four processors in a box and there's ten boxes and there's some kind of ti- so if - you know he's got a lot of processing power and um we'd have to schedule it but if we have some big jobs and we wanna - wanna - wanna run them he's - he's offering it. Mm-hmm. So. It's uh when he was here eh uh he - he used i- not only every machine here but every machine on campus as far as I could tell  so - so in some ways he just got his payback  but Mm-hmm. uh again I - I don't know if we'll end up with - if we're gonna be C_P_U limited on anything that we're doing in this group but - but if - if we are that's an offer. O_K well uh you guys doing great stuff so that's - that - that's really neat and uh we'll uh uh g- don't think we need to uh um Oh well the other thing I guess that I will say is that uh the digits that we're gonna record momentarily is starting to get - are starting to get into a pretty good size collection and um in addition to the SpeechDat stuff we will have those to work with really pretty soon now so that's - that's another source of data. Um which is s- under somewhat better control and that we can - we can make measurements of the room the - uh that - you know if we feel there's other measurements we don't have that we'd like to have we can make them and uh Dave and I were just talking about that a little while ago Mm-hmm. so uh that's another - another possibility for this - this kind of work. K  uh if nobody has anything else maybe we should go around do - do our digits - do our digits duty. O_K. O_K I'll start. Three two three one dash three two five zero. Uh  let me say that again. Transcript number three two three one dash three two five zero. seven eight zero O_ three eight one nine zero five nine eight one seven nine four two three four five seven four seven three seven eight four four one six nine nine eight eight O_ eight three zero eight five five six one two zero zero three four two four six five two three four four O_ three six seven zero two seven six I'm reading transcript three two five one dash three two seven zero. eight nine eight five five nine O_ O_ six one six one five O_ two four three two seven eight four three five O_ four seven five six four seven nine five six seven six zero six seven zero zero zero nine O_ four - Oh. Sorry. nine one four O_ seven zero six one nine two three four zero seven two three seven four six two four seven four seven nine six zero five eight four five one Transcript three two nine one  three three one zero. O_ seven zero eight nine eight one eight seven two three zero one five five six three six seven nine eight eight eight six six eight six three nine O_ zero zero three three two nine nine two three six three six three five three four eight seven six four nine five six seven eight zero nine zero O_ three O_ six six Transcript three one seven one dash three one nine zero five nine eight zero six five nine eight one seven one seven five three nine four eight nine nine two eight three six nine O_ one one eight two five three six zero four five one five six seven zero zero nine two eight seven seven - Oh. Excuse me. nine two eight seven O_ five four three O_ five O_ zero six five eight nine one two six four three four Transcript number three one nine one dash three two one zero. six one one three seven two eight nine nine two eight nine seven nine six one zero zero O_ nine zero one zero zero one four two three three six four seven two five zero five four four six seven eight seven zero seven eight O_ four one nine zero seven zero four three four six one seven five three three two nine three four seven four six seven two five zero O_K. I guess we're done. ,The ICSI Meeting Recorder Group at Berkeley met to discuss progress on their main project  Aurora. They discussed a conference call with project partners  there have been some developments that should help speed up experiments  along with some progress made in the current area they are looking  voiced/unvoiced detection. A number of other members of the group also reported the progress they were making on their work. Me018 will mail people with details about changes to their system in order to run code on the IBM Linux machine  along with the name of the machine. Mn007 is going to try and convince OGI to use his new filters  and enquire as to the setting of a standard for the system. There was a conference call for the Aurora Project  but no one from ICSI was involved. Not only are the group unsure what if anything was decided  but project changes are being considered including changing the baseline and improvement weighting  though everyone has their own opinion on these matters. The group needs more female voices in the Meeting Recorder data  though fn002 does not consider herself very suitable. Me018 has been attempting to make experimentation faster by reducing the iterations in the HTK training. These will can be reset once the system is finalised. He then wants to try and improve accuracy may increasing the number of Gaussian mixtures in the models. Mn007 reported OGI's work on spectral subtraction  as well as his testing his new filter. The latency is reduced  and improvement increased on well matched case  though decreased on mis-matched. Along with fn002 he has been looking into voiced/unvoiced detection  and they are still looking for features. So far they are using features which approximate important details  and they seem reasonably robust on noise. Me026 has decided against using the exact method of reverberation cancellation he previously discussed  because its output does not fit the current system. Me006 has been reading on a slight tangent looking at classic work on modulation spectrum  which has inspired some ideas for input. 
"Hello? O_K. We're on. O_K  so uh had some interesting mail from uh Dan Ellis. Actually  I think he - he redirected it to everybody also so uh the P_D_A mikes uh have a big bunch of energy at - at uh five hertz uh where this came up was that uh I was showing off these wave forms that we have on the web and - and uh I just sort of hadn't noticed this  but that - the major  major component in the wave - in the second wave form in that pair of wave forms is actually the air conditioner. Huh. So. So. I I have to be more careful about using that as a - as a - as a good illustration  uh  in fact it's not  of uh - of the effects of room reverberation. It is- isn't a bad illustration of the effects of uh room noise. on - on uh some mikes uh but So. And then we had this other discussion about um whether this affects the dynamic range  cuz I know  although we start off with thirty two bits  you end up with uh sixteen bits and you know  are we getting hurt there? But uh Dan is pretty confident that we're not  that - that quantization error is not - is still not a significant factor there. So. So there was a question of whether we should change things here  whether we should change a capacitor on the input box for that or whether we should Yeah  he suggested a smaller capacitor  right? For the P_D_As? Right. But then I had some other uh thing- discussions with him and the feeling was once we start monk- monkeying with that  uh  many other problems could ha- happen. And additionally we - we already have a lot of data that's been collected with that  so. Yeah. A simple thing to do is he - he - he has a - I forget if it - this was in that mail or in the following mail  but he has a - a simple filter  a digital filter that he suggested. We just run over the data before we deal with it. Mm-hmm. um The other thing that I don't know the answer to  but when people are using Feacalc here  uh whether they're using it with the high-pass filter option or not. And I don't know if anybody knows. But. Um. I could go check. Yeah. So when we're doing all these things using our software there is - um if it's - if it's based on the RASTA-P_L_P program  which does both P_L_P and RASTA-P_L_P um then uh there is an option there which then comes up through to Feacalc which um allows you to do high-pass filtering and in general we like to do that  because of things like this and it's - it's pretty - it's not a very severe filter. Doesn't affect speech frequencies  even pretty low speech frequencies  at all  but it's What's the cut-off frequency it used? Oh. I don't know I wrote this a while ago Is it like twenty? Something like that. Yeah. I mean I think there's some effect above twenty but it's - it's - it's - it's mild. Yeah. So  I mean it probably - there's probably some effect up to a hundred hertz or something but it's - it's pretty mild. I don't know in the - in the STRUT implementation of the stuff is there a high-pass filter or a pre- pre-emphasis or something in the - Uh. I think we use a pre-emphasis. Yeah. Yeah. So. We - we - we want to go and check that in i- for anything that we're going to use the P_D_ A mike for. uh He says that there's a pretty good roll off in the P_Z_M mikes so we don't need - need to worry about them one way or the other but if we do make use of the cheap mikes  uh we want to be sure to do that - that filtering before we process it. And then again if it's uh depending on the option that the - our - our software is being run with  it's - it's quite possible that's already being taken care of. uh But I also have to pick a different picture to show the effects of reverberation. uh Did somebody notice it during your talk? uh No. Huh. Well. uh Well. If they made output they were - they were  you know - they were nice. Didn't say anything? But. I mean the thing is it was since I was talking about reverberation and showing this thing that was noise  it wasn't a good match  but it certainly was still uh an indication of the fact that you get noise with distant mikes. Mm-hmm. uh It's just not a great example because not only isn't it reverberation but it's a noise that we definitely know what to do. So  I mean  it doesn't take deep - a new - bold new methods to get rid of uh five hertz noise  so. Yeah. um uh But. So it was - it was a bad example in that way  but it's - it still is - it's the real thing that we did get out of the microphone at distance  so it wasn't it w- it w- wasn't wrong it was inappropriate. So. So uh  but uh  Yeah  someone noticed it later pointed it out to me  and I went ""oh  man. Why didn't I notice that?"" Hmm. um. So. um So I think we'll change our - our picture on the web  when we're @@ . One of the things I was - I mean  I was trying to think about what - what's the best way to show the difference an- and I had a couple of thoughts one was  that spectrogram that we show is O_ K  but the thing is the eyes uh and the the brain behind them are so good at picking out patterns from - from noise that in first glance you look at them it doesn't seem like it's that bad uh because there's many features that are still preserved. So one thing to do might be to just take a piece of the spec- uh of the spectrogram where you can see that something looks different  an- and blow it up  and have that be the part that's - just to show as well. You know. Mm-hmm. Mm-hmm. i- i- Some things are going to be hurt. um Another  I was thinking of was um taking some spectral slices  like uh - like we look at with the recognizer  and look at the spectrum or cepstrum that you get out of there  and the - the uh  um  the reverberation uh does make it - does change that. And so maybe - maybe that would be more obvious. Hmm. Spectral slices? Yeah. W- w- what d- what do you mean? Well  I mean um all the recognizers look at frames. So they - they look at - So like one instant in time. O_K. Yeah  look at a - So it's  yeah  at one point in time or uh twenty - over twenty milliseconds or something  O_K. you have a spectrum or a cepstrum. That's what I meant by a slice. Yeah. And I see. You could just - you could just throw up  you know  uh if you look at - the uh - some M_F_C_C feature vectors. You know  one from one  one from the other  and then  you know  you can look and see how different the numbers are. Right. Well  that's why I saying either Well  either spectrum or cepstrum but - but I think the thing is you wanna - I'm just kidding. I don't mean a graph. I mean the actual numbers. Oh. I see. Oh. That would be lovely  yeah. Yeah. ""See how different these sequences of numbers are?"" Yeah. Or I could just add them up and get a different total. Yeah. It's not the square. O_K. Uh. What else - wh- what's - what else is going on? Uh  yeah. Yeah  at first I had a remark why - I am wondering why the P_D_A is always so far. I mean we are always meeting at the beginning of the table and the P_D_A's there. Uh. I guess cuz we haven't wanted to move it. We - we could - we could move us  and. Yeah? That's right. O_K. Well  anyway. Um. Yeah  so. Uh. Since the last meeting we've - we've tried to put together um the clean low-pass um downsampling  upsampling  I mean  Uh the new filter that's replacing the L_D_A filters  and also the um delay issue so that - We considered th- the - the delay issue on the - for the on-line normalization. Mmm. So we've put together all this and then we have results that are not um very impressive. Well  there is no real improvement. But it's not wer- worse and it's better - better latency  right? It's not - Yeah. Yeah. Well. Actually it's better. It seems better when we look at the mismatched case but I think we are like - like cheated here by the - th- this problem that uh in some cases when you modify slight - slightly modify the initial condition you end up completely somewhere air- somewhere else in the - in the space  the parameters. So. Yeah. Well. The other system are for instance. For Italian is at seventy-eight percent recognition rate on the mismatch  and this new system has eighty-nine. But I don't think it indicates something  really. I don't - I don't think it means that the new system is more robust or - Uh-huh. It's simply the fact that - Well. Well  the test would be if you then tried it on one of the other test sets  if - Y- if it was - Right. So this was Italian  right? Yeah. Yeah. It's similar for other test sets but I mean So then if you take your changes and then - from this se- seventy-eight um percent recognition rate system  Uh-huh. I could change the transition probabilities for the - the first H_M_M and it will end up to eighty-nine also. By using point five instead of point six  point four as in the - the H_T_K script. Uh-huh. Yeah. So. Well. That's - Yeah. Yeah I looked at um - looked at the results when Stephane did that and it's - it's really wo- really happens. I mean th- the only difference is you change the self-loop transition probability by a tenth of a percent and it causes ten percent difference in the word error rate. Well. Eh uh - This really happens. Yeah. Yeah. A tenth of a per cent. Even tenth of a percent? Yeah. From point - Well  we tried - we tried point one  yeah. I - I'm sorry f- for point - from - You change at point one and n- not tenth of a percent  one tenth  alright ? Um so from point five - so from point six to point five and you get ten percent better. Hmm. Oh! Yeah. Mm-hmm. Mm-hmm. And it's - I think it's what you basically hypothesized in the last meeting about uh it just being very - and I think you mentioned this in your email too - it's just very um - you know get stuck in some local minimum and this thing throws you out of it I guess. Mmm  yeah. Mm-hmm. Well  what's - what are - according to the rules what - what are we supposed to do about the transition probabilities? Are they supposed to be point five or point six? I think you're not allowed to - Yeah. That's supposed to be point six  Yeah. for the self-loop. Point - It's supposed to be point six. Yeah. But changing it to point five I think is - which gives you much better results  but that's not allowed. But not allowed? Yeah. O_K . Yeah. Yeah  but even if you use point five  I'm not sure it will always give you the better results on other test set or it Yeah. Right. We only tested it on the - the medium mismatch  right? You said on the other cases you didn't notice - on the other training set  I mean. Yeah. But. I think  yeah. I think the reason is  yeah  I not- I - it was in my mail I think also  is the fact that the mismatch is trained only on the far microphone. Well  in - for the mismatched case everything is um using the far microphone training and testing  whereas for the highly mismatched  training is done on the close microphone so it's - it's clean speech basically so you don't have this problem of local minima probably and for the well-match  it's a mix of close microphone and distant microphone and - Well. I did notice uh something - So th- I think the mismatch is the more difficult for the training part. Somebody  I think it was Morgan  suggested at the last meeting that I actually count to see how many parameters and how many frames. Mm-hmm. Mm-hmm. And there are uh almost one point eight million frames of training data and less than forty thousand parameters in the baseline system. Hmm. Yeah. So it's very  very few parameters compared to how much training data. Mm-hmm. Well. Yes. So. And that - that says that we could have lots more parameters actually. Yeah. Yeah. I did one quick experiment just to make sure I had everything worked out and I just - Mm-hmm. uh f- for most of the um - For - for all of the digit models  they end up at three mixtures per state. And so I just did a quick experiment  where I changed it so it went to four and um it it - it didn't have a r- any significant effect at the uh medium mismatch and high mismatch cases and it had - it was just barely significant for the well-matched better. Uh so I'm r- gonna run that again but um with many more uh mixtures per state. Yeah. Cuz at forty thou- I mean you could you could have uh - Yeah  easily four times as many parameters. Mm-hmm. And I think also just seeing what we saw uh in terms of the expected duration of the silence model? when we did this tweaking of the self-loop? Yeah. The silence model expected duration was really different. And so in the case where um it had a better score  the silence model expected duration was much longer. So it was like - it was a better match. I think you know if we Yeah. make a better silence model I think that will help a lot too um for a lot of these cases so but one one thing I - I wanted to check out before I increased the um number of mixtures per state was uh in their default training script they do an initial set of three re-estimations and then they built the silence model and then they do seven iterations then the add mixtures and they do another seven then they add mixtures then they do a final set of seven and they quit. Seven seems like a lot to me and it also makes the experiments go take a really long time I mean to do one turn-around of the well matched case takes like a day. Mm-hmm. Mm-hmm. And so you know in trying to run these experiments I notice  you know  it's difficult to find machines  you know  compute the run on. And so one of the things I did was I compiled H_T_K for the Linux Mm-hmm. machines cuz we have this one from I_B_M that's got like five processors in it? Right. and so now I'm - you can run stuff on that and that really helps a lot because now we've got you know  extra machines that we can use for compute. And if - I'm do- running an experiment right now where I'm changing the number of iterations? from seven to three? just to see how it affects the baseline system. Mm-hmm. Yeah. And so if we can get away with just doing three  we can do many more experiments more quickly. And if it's not a - a huge difference from running with seven iterations  Hmm. um  you know  we should be able to get a lot more experiments done. And so. I'll let you know what - what happens with that. But if we can you know  run all of these back-ends f- with many fewer iterations and on Linux boxes we should be able to get a lot more experimenting done. Mm-hmm. So. So I wanted to experiment with cutting down the number of iterations before I increased the number of Gaussians. Right. Sorry. Um. So um  how's it going on the - So. You - you did some things. They didn't improve things in a way that convinced you you'd substantially improved anything. Yeah. But they're not making things worse and we have reduced latency  right? Yeah. But actually - um actually it seems to do a little bit worse for the well-matched case and we just noticed that - Yeah  actually the way the final score is computed is quite funny. It's not a mean of word error rate. It's not a weighted mean of word error rate  it's a weighted mean of improvements. Uh-huh. So. Which means that actually the weight on the well-matched is - Well I well what what - What happened is that if you have a small improvement or a small if on the well-matched case it will have uh huge influence on the improvement compared to the reference because the reference system is - is - is quite good for - for the well-ma- well-matched case also. So it - it weights the improvement on the well-matched case really heavily compared to the improvement on the other cases? No  but it's the weighting of the - of the improvement not of the error rate. Yeah. Yeah  and it's hard to improve on the - on the best case  cuz it's already so good  right? Yeah but what I mean is that you can have a huge improvement on the H_ - H_M_K's  uh like five percent uh absolute  and this will not affect the final score almost - Uh this will almost not affect the final score because this improvement - because the improvement uh relative to the - the baseline is small - So they do improvement in terms of uh accuracy? rather than word error rate? Uh. Uh improvement? No  it's compared to the word er- it's improvement on the word error rate  yeah. Sorry. So - O_K. So if you have uh ten percent error and you get five percent absolute uh improvement then that's fifty percent. Mm-hmm. O_K. So what you're saying then is that if it's something that has a small word error rate  Mm-hmm. then uh a - even a relatively small improvement on it  in absolute terms  will show up as quite - quite large in this. Is that what you're saying? Yes. Yeah. Yeah. O_K. But yeah that's - that's - it's the notion of relative improvement. Yeah. Word error rate. Sure  but when we think about the weighting  which is point five  point three  point two  it's on absolute on - on relative figures  not - Yeah. Yeah. So when we look at this error rate No. That's why I've been saying we should be looking at word error rate uh and - and not - not at at accuracies. It's - uh - Mmm  yeah. Mmm  yeah. Mm-hmm. I mean uh we probably should have standardized on that all the way through. It's just - Well. Mm-hmm. I mean  it's not - it's not that different  right? I mean  just subtract the accuracy. I mean - Yeah but you're - but when you look at the numbers  your sense of the relative size of things is quite different. If you had ninety percent uh correct and five percent  five over ninety doesn't look like it's a big difference  but five over ten is - is big. Oh. Oh  I see. Yeah. Mm-hmm. Mm-hmm. So just when we were looking at a lot of numbers and getting sense of what was important. I see. I see. Yeah. That makes sense. Um. Um. Mmm. Well anyway uh. So. Yeah. So it hurts a little bit on the well-match and yeah. What's a little bit? Like - Like  it's difficult to say because again um I'm not sure I have the um - Hey Morgan? Do you remember that Signif program that we used to use for testing signi- ? Is that still valid? I - I've been using that. Yeah. Yeah  it was actually updated. Uh. O_K. Oh  it was. Oh  I shoul- Jeff updated it some years ago and - and uh cleaned it up made some things better in it. So. O_K. I should find that new one. I just use my old one from ninety-two or whatever Yeah  I'm sure it's not that different but - but he - he uh - he was a little more rigorous  as I recall. O_K. Right. So it's around  like  point five. No  point six uh percent absolute on Italian - Worse. Worse  yep. Out of what? I mean. s- Uh well we start from ninety-four point sixty-four  and we go to ninety-four point O_ four. Uh-huh. So that's six - six point th- Uh. Ninety- three point six four  right? is the baseline. Oh  no  I've ninety-four. Oh  the baseline  you mean. Yeah. Well I don't - I'm not talking about the baseline here. I uh - Oh. Oh. I'm sorry. My baseline is the submitted system. Ah! O_K. Ah  ah. Hmm. Yeah. Sorry. Oh yeah. For Finnish  we start to ninety-three point eight-four and we go to ninety-three point seventy-four. And for Spanish we are - we were at ninety-five point O_ five and we go to ninety-three-s- point sixty one. O_K  so we are getting hurt somewhat. And is that wh- what - do you know what piece - you've done several changes here. Uh  do you know what pie- So. Yeah. I guess - I guess it's - it's the filter. Because nnn  well uh we don't have complete result  but the filter - So the filter with the shorter delay hurts on Italian well-matched  which - And  yeah. And the other things  like um downsampling  upsampling  don't seem to hurt and I'm - the new on-line normalization  neither. So. I'm really confused about something. If we saw that making a small change like  you know  a tenth  to the self-loop had a huge effect  can we really make any conclusions about Mm-hmm. Yeah that's th- differences in this stuff? I mean  especially when they're this small. I mean. Yeah. I think we can be completely fooled by this thing  but - I don't know. Well  yeah. So. There is first this thing  and then the - yeah  I computed the um - like  the confidence level on the different test sets. And for the well-matched they are around um point six uh percent. For the mismatched they are around like let's say one point five percent. And for the well-m- uh H_M they are also around one point five. But - O_K  so you - these - these degradations you were talking about were on the well-matched case So. Yeah. Uh. Do the - does the new filter make things uh better or worse for the other cases? But. Uh. About the same. It doesn't hurt. Yeah. Doesn't hurt  but doesn't get a little better  or something. No. No. O_K  so um I guess the argument one might make is that  ""Yeah  if you looked at one of these cases and you jiggle something and it changes then uh you're not quite sure what to make of it. But when you look across a bunch of these and there's some - some pattern  um - I mean  so eh h- here's all the - if - if in all these different cases it never gets better  and there's significant number of cases where it gets worse  then you're probably hurting things  I would say. So um I mean at the very least that would be a reasonably prediction of what would happen with - with a different test set  that you're not jiggling things with. So I guess the question is if you can do better than this. If you can - if we can approximate the old numbers while still keeping the latency down. Mmm. Yeah. Uh  so. Um. What I was asking  though  is uh - are - what's - what's the level of communication with uh the O_G_ I gang now  about this and - Well  we are exchanging mail as soon as we - Yeah. we have significant results. Um. Yeah. For the moment  they are working on integrating the um spectral subtraction Mm-hmm. apparently from Ericsson. Um. Yeah. And so. Yeah. We are working on our side on other things like uh also trying a sup- spectral subtraction but of - of our own  I mean  another Mm-hmm. spectral subtraction. Um. Yeah. So I think it's - it's O_K. It's going - Is there any further discussion about this - this idea of - of having some sort of source code control? Yeah. Well. For the moment they're - uh everybody's quite um - There is this Eurospeech deadline  so. I see. Um. And. Yeah. But yeah. As soon as we have something that's significant and that's better than - than what was submitted  we will fix - fix the system and - But we've not discussed it - it - it - this yet  yeah. Yeah. Sounds like a great idea but - but I think that - that um he's saying people are sort of scrambling for a Eurospeech deadline. But that'll be uh  uh done in a week. So  maybe after this next one. Mmm. Yeah. Wow! Already a week! Man! You're right. That's amazing. Yeah. Yeah. Anybo- anybody in the - in this group do- doing anything for Eurospeech? Or  is that what - is that - S- Yeah we are - We are trying to - to do something with the Meeting Recorder digits  Right. and - But yeah. Yeah. And the good thing is that there is this first deadline  Yeah. and  well  some people from O_G_I are working on a paper for this  but there is also the um special session about th- Aurora which is - uh which has an extended deadline. So. The deadline is in May. For uh - Oh  for Eurospeech? For th- Yeah. So f- only for the experiments on Aurora. So it - it's good  yeah. Oh! Oh  a special dispensation. That's great. Mm-hmm. Where is Eurospeech this year? Aalborg - Aalborg uh It's in Denmark. Oh. So the deadline - When's the deadline? Hmm? When's the deadline? I think it's the thirteenth of May. That's great! It's great. So we should definitely get something in for that. Yeah. But on meeting digits  maybe there's - Maybe. Maybe. Yeah. So it would be for the first deadline. Yeah. Nnn. Yeah. So  I mean  I - I think that you could certainly start looking at - at the issue uh but - but uh I think it's probably  on s- from what Stephane is saying  it's - it's unlikely to get sort of active participation from the two sides until after they've - Well I could at least - Well  I'm going to be out next week but I could try to look into like this uh C_V_S over the web. That seems to be a very popular way of people distributing changes and - over  you know  multiple sites and things so maybe Mm-hmm. if I can figure out how do that easily and then pass the information on to everybody so that it's you know  as easy to do as possible and - and people don't - it won't interfere with their regular work  then maybe that would be good. And I think we could use it for other things around here too. So. Good. That's cool. And if you're interested in using C_V_S  I've set it up here  so. Oh great. O_K. I used it a long time ago but it's been a while so maybe I can ask you some questions. um j- Oh. So. I'll be away tomorrow and Monday but I'll be back on Tuesday or Wednesday. O_K. Yeah. Dave  the other thing  actually  is - is this business about this wave form. Maybe you and I can talk a little bit at some point about coming up with a better uh demonstration of the effects of reverberation for our web page  cuz uh - the uh um I mean  actually the - the uh It made a good - good audio demonstration because when we could play that clip the - the - the really obvious difference is that you can hear two voices and - in the second one and only hear - Maybe we could just like  talk into a cup. Yeah. Some good reverb. No  I mean  it sound - it sounds pretty reverberant  but I mean you can't - when you play it back in a room with a - you know a big room  nobody can hear that difference really. They hear that it's lower amplitude and they hear there's a second voice  um but uh that - actually that makes for a perfectly good demo because that's a real obvious thing  that you hear two voices. Yeah. Yeah. Uh-huh. But not of reverberation. A boom. Well that - that - that's O_K. But for the - the visual  just  you know  I'd like to have uh uh  you know  the spectrogram again  because you're - you're - you're visual uh abilities as a human being are so good you can pick out - you know  you - you look at the good one  you look at the cru- the screwed up one  and - and you can see the features in it without trying to @@ - yeah. Yeah. I noticed that in the pictures. I thought ""hey  you know th-"" I - My initial thought was ""this is not too bad!"" Right. But you have to - you know  if you look at it closely  you see ""well  here's a place where this one has a big formant - uh uh formant - maj- major formants here are - are moving quite a bit."" And then you look in the other one and they look practically flat. Mm-hmm. So I mean you could - that's why I was thinking  in a section like that  you could take a look - look at just that part of the spectrogram and you could say "" Oh yeah. This - this really distorted it quite a bit."" Yeah. The main thing that struck me in looking at those two spectrograms was the difference in the high frequencies. It looked like for the one that was farther away  you know  it really - everything was attenuated and - Right. I mean that was the main visual thing that I noticed. Right. But it's - it's uh - So. Yeah. So there are - clearly are spectral effects. Since you're getting all this indirect energy  then a lot of it does have - have uh reduced high frequencies. But um the other thing is the temporal courses of things really are changed  and - and uh we want to show that  in some obvious way. The reason I put the wave forms in there was because uh they - they do look quite different. Uh. And so I thought ""Oh  this is good."" but I - I just uh - After - after uh they were put in there I didn't really look at them anymore  cuz I just - they were different. So I want something that has a - is a more interesting explanation for why they're different. Um. Oh. So maybe we can just substitute one of these wave forms and um then do some kind of zoom in on the spectrogram on an interesting area. Something like that. Yeah. Uh-huh. The other thing that we had in there that I didn't like was that um the most obvious characteristic of the difference uh when you listen to it is that there's a second voice  and the - the - the - the - the uh cuts that we have there actually don't correspond to the full wave form. It's just the first - I think there was something where he was having some trouble getting so much in  or. I - I forget the reason behind it. But it - it's um - it's the first six seconds or something of it and it's in the seventh or eighth second or something where @@ the second voice comes in. So we - we would like to actually see the voice coming in  too  I think  since that's the most obvious thing when you listen to it. Mm-hmm. So. Um. Uh  yeah. Yeah. I brought some - I don't know if - some @@ figures here. Well. I start - we started to work on spectral subtraction. And um the preliminary results were very bad. So the thing that we did is just to add spectral subtraction before Uh-huh. this  the Wall uh process  which contains L_D_A on-line normalization. And it hurts uh a lot. Uh-huh. And so we started to look at - at um things like this  which is  well  it's - Yeah. So you have the C_zero parameters for You can @@ . one uh Italian utterance. And I plotted this for two channels. Channel zero is the close mic- microphone  and channel one is the distant microphone. And it's perfectly synchronized  so. And the sentence contain only one word  which is ""Due"" And it can't clearly be seen. Where - where is it? Where is the word? Uh-huh. This is - this is  oh  a plot of C_zero  the energy. Hmm. So. This is a plot of C_zero  uh when we don't use spectral subtraction  and when there is no on-line normalization. So. Mm-hmm. There is just some filtering with the L_D_A and and some downsampling  upsampling. C_zero is the close talking? - uh the close channel? and s- channel one is the - So. Yeah. Yeah. Yeah. So C_zero is very clean  actually. Yeah. Uh then when we apply mean normalization it looks like the second figure  though it is not. Which is good. Well  the noise part is around zero and - Mm-hmm. And then the third figure is what happens when we apply mean normalization and variance normalization. So. What we can clearly see is that on the speech portion the two channel come - becomes very close  but also what happens on the noisy portion is that the variance of the noise is - Mm-hmm. This is still being a plot of C_zero? Yeah. This is still C_zero. O_K. Can I ask um what does variance normalization do? w- What is the effect of that? Normalizes the variance. So it - it - I mean Yeah. It normalized th- the standard deviation. So it - y- Yeah. No  I understand that  but I mean - You - you get an estimate of the standard deviation. That's um - No. No  I understand what it is  but I mean  what does it - what's - what is uh - Yeah but. What's the rationale? We- Yeah. Yeah. Why - why do it? Uh. Well  I mean  because everything uh - If you have a system based on Gaussians  everything is based on means and variances. So if there's an overall Yeah. reason - You know  it's like uh if you were doing uh image processing and in some of the pictures you were looking at  uh there was a lot of light uh and - and in some  there was low light  you know  you would want to adjust for that in order to compare things. Mm-hmm. Mm-hmm. And the variance is just sort of like the next moment  you know? So uh what if um one set of pictures was taken uh so that throughout the course it was - went through daylight and night uh um um ten times  another time it went thr- I mean i- is  you know  how - how much - Oh  O_K. how much vari- Or no. I guess a better example would be how much of the light was coming in from outside rather than artificial light. So if it was a lot - if more was coming from outside  then there'd be the bigger effect of the - of the - of the change in the - So every mean - every - all - all of the - the parameters that you have  especially the variances  are going to be affected by the overall variance. Oh  O_K. Uh-huh. I see. O_K. And so  in principle  you - if you remove that source  then  you know  you can - So would - the major effect is - that you're gonna get is by normalizing the means  but it may help - That's the first order but - thing  but then the second order is - is the variances First-order effects. And it may help to do the variance. O_K. O_K. because  again  if you - if you're trying to distinguish between E_ and B_ Mm-hmm. if it just so happens that the E_'s were a more - you know  were recorded when - when the energy was - was - was larger or something  or the variation in it was larger  Mm-hmm. Mm-hmm. Mm-hmm. uh than with the B_'s  then this will be - give you some - some bias. So the - O_K. it's removing these sources of variability in the data that have nothing to do with the linguistic component. Mmm. Gotcha. O_K . Sorry to interrupt. But the - the uh - but let me as- ask - ask you something. i- is - if - Yep. And it - and this - If you have a good voice activity detector  isn't - isn't it gonna pull that out? Yeah. Sure. If they are good. Yeah. Well what it - it shows is that  yeah  perhaps a good voice activity detector is - is good before on-line normalization and that's what uh we've already observed. But uh  yeah  voice activity detection is not an easy thing neither. But after you do this  after you do the variance normalization - I mean. Mm-hmm. I don't know  it seems like this would be a lot easier than this signal to work with. Yeah. So. What I notice is that  while I prefer to look at the second figure than at the third one  well  because you clearly see where speech is. Yeah. Yeah. But the problem is that on the speech portion  channel zero and channel one are more different than when you use variance normalization where channel zero and channel one become closer. Right. But for the purposes of finding the speech - And - Yeah  but here - Yeah. You're more interested in the difference between the speech and the nonspeech  right? Yeah. So I think  yeah. For I th- I think that it - perhaps it shows that uh the parameters that the voice activity detector should use - uh have to use should be different than the parameter that have to be used for speech recognition. Yeah. So basically you want to reduce this effect. So you can do that by doing the voi- voice activity detection. You also could do it by spect- uh spectral subtraction before the variance normalization  right? Well  y- Yeah  but it's not clear  yeah. So uh - We- So. Well. It's just to the - the number that at- that are here are recognition experiments on Italian H_M and M_M Yeah. with these two kinds of parameters. And  well  it's better with variance normalization. Yeah. Yeah. So it does get better even though it looks ugly. O_K. Uh - but does this have the voice activity detection in it? Yeah. O_K. Um. O_K. So. Where's th- But the fact is that the voice activity detector doesn't work on channel one. So. Yeah. Uh-huh. Where - at what stage is the voice activity detector applied? Is it applied here or a- after the variance normalization? or - Hmm? Spectral subtraction  I guess. It's applied before variance normalization. So it's a good thing  because Oh. Yeah. I guess voice activity detection on this should - could be worse. Is it applied all the way back here? It's applied the um on  yeah  something like this  yeah. Maybe that's why it doesn't work for channel one. Perhaps  yeah. Can I - So we could perhaps do just mean normalization before V_A_D. Mm-hmm. Mm-hmm. Can I ask a  I mean - a sort of top-level question  which is um ""if - if most of what the O_G_I folk are working with is trying to integrate this other - other uh spectral subtraction  why are we worrying about it?"" Mm-hmm. About? Spectral subtraction? Yeah. It's just uh - Well it's another - They are trying to u- to use the um - the Ericsson and we're trying to use something - something else. And. Yeah  and also to understand what happens because O_K. uh fff Well. When we do spectral subtraction  actually  I think that this is the - the two last figures. Yeah. Um. It seems that after spectral subtraction  speech is more emerging now uh Mm-hmm. Speech is more what? than - than before. Well  the difference between the energy of the speech and the energy of the n- spectral subtrac- subtracted noise portion is - is larger. Mm-hmm. Well  if you compare the first figure to this one - Actually the scale is not the same  but if you look at the - the numbers um you clearly see that the difference between the C_zero of the speech and C_zero of the noise portion is larger. Uh but what happens is that after spectral subtraction  you also increase the variance of this - of C_zero. And so if you apply variance normalization on this  it completely sc- screw Mm-hmm. everything. Well. Mm-hmm. Um. Uh. Yeah. So yeah. And what they did at O_G_I is just uh they don't use on-line normalization  for the moment  on spectral subtraction and I think - Yeah. I think as soon as they will try on-line normalization there will be a problem. So yeah  we're working on the same thing but I think uh with different - different system and - Right. I mean  i- the Intellectually it's interesting to work on things th- uh one way or the other but I'm - I'm just wondering if um - on the list of things that there are to do  if there are things that we won't do because we've got two groups doing the same thing. Um. Mm-hmm. Mm-hmm. Mm-hmm. That's - Um. Just - just asking. Uh. I mean  it's - Yeah  well  uh. There also could be - I mean. I can maybe see a reason f- for both working on it too if um you know  if - if - if you work on something else and - and you're waiting for them to give you spectral subtraction - I mean it's hard to know whether the effects that you get from the other experiments you do will carry over once you then bring in their spectral subtraction module. So it's - it's almost like everything's held up waiting for this one thing. I don't know if that's true or not  but I could see how - Mmm. Maybe that's what you were thinking. I don't know. I don't know. I mean  we still evidently have a latency reduction plan which - which isn't quite what you'd like it to be. That - that seems like one prominent thing. And then uh weren't issues of - of having a - a second stream or something? That was - Was it - There was this business that  you know  we - we could use up the full forty-eight hundred bits  and - Yeah. But I think they'r- I think we want to work on this. They also want to work on this  so. Uh. yeah. We - we will try M_S_G  but um  yeah. And they are t- I think they want to work on the second stream also  but more with some kind of multi-band or  well  what they call TRAP or generalized TRAP. Mm-hmm. Um. O_K. Do you remember when the next meeting is supposed to be? the next uh - In June. O_K. So. It's uh in June. Yeah. Yeah. Um. Yeah  the other thing is that you saw that - that mail about uh the V_A_D - V_A_Ds performing quite differently? That that uh So um. This - there was this experiment of uh ""what if we just take the baseline?"" set uh of features  just mel cepstra  Mmm. and you inc- incorporate the different V_A_Ds. And it looks like the - the French V_A_D is actually uh better - significantly better. Improves the baseline? Yeah. Yeah. Yeah but I don't know which V_A_D they use. Uh. If the use the small V_A_D I th- I think it's on - I think it's easy to do better because it doesn't work at all. So. I - I don't know which - which one. It's Pratibha that - that did this experiment. Yeah. Um. We should ask which V_A_D I don't @@ . He - Actually  I think that he say with the good V_A_D of - from OGI she used. and with the Alcatel V_A_D. And the experiment was sometime better  sometime worse. Yeah but I - it's uh - I think you were talking about the other mail that used V_A_D on the reference features. Yes. Yeah. I don't remember. And on that one  uh the French one is - was better. It was just better. Mm-hmm. I mean it was enough better that - that it would uh account for a fair amount of the difference Mm-hmm. between our performance  actually. Mm-hmm. So. Uh. So if they have a better one  we should use it. I mean. You know? it's - you can't work on everything. Uh. Yeah. Uh. Yeah  so we should find out if it's really better. I mean if it - Yeah. the - compared to the small or the big network. Mm-hmm. Yeah. And perhaps we can easily improve if - if we put like mean normalization before the - before the V_A_D. Because - Yeah. as - as you've mentioned. Mmm. H- Hynek will be back in town uh the week after next  back - back in the country. So. And start - start organizing uh more visits and connections and so forth  and - uh Mm-hmm. working towards June. Yeah. Mm-hmm. Also is Stephane was thinking that maybe it was useful to f- to think about uh voiced-unvoiced - to work uh here in voiced-unvoiced detection. Yeah. And we are looking in the uh signal. Yeah. Yeah  my feeling is that um actually when we look at all the proposals  ev- everybody is still using some kind of spectral envelope and Right. um No use of pitch uh basically. Yeah. it's - Yeah  well  not pitch  but to look at the um fine - at the - at the high re- high resolution spectrum. Yeah. So. Well  it - We don't necessarily want to find the - the pitch of the - of the sound but uh - Cuz I have a feeling that when we look - when we look at the - just at the envelope there is no way you can tell if it's voiced and unvoiced  if there is some - It's - it's easy in clean speech because voiced sound are more low frequency and. So there would be more  Yeah. uh - there is the first formant  which is the larger and then voiced sound are more high frequencies cuz it's frication and - Right. But  yeah. When you have noise there is no um - if - if you have a low frequency noise it could be taken for - for voiced speech and. Yeah  you can make these mistakes  but - but - So. Isn't there some other S- uh d- So I think that it - it would be good - Yeah  yeah  well  go - go on. Uh  I was just gonna say isn't there - aren't - aren't there lots of ideas for doing voice activity  or speech-nonspeech rather  um by looking at um  you know  uh I guess harmonics or looking across time - Well  I think he was talking about the voiced-unvoiced  though  right? So  not the speech-nonspeech. Mmm. Yeah. Well even with e- uh w- ah Yeah. you know  uh even with the voiced-non- Mmm. voiced-unvoiced um - I thought that you or somebody was talking about - Well. Uh yeah. B- We should let him finish what he w- he was gonna say  and - So. O_K. So go ahead. Um yeah  so yeah  I think if we try to develop a second stream well  there would be one stream that is the envelope and the second  it could be interesting to have that's - something that's more related to the fine structure of the spectrum. And. Yeah  so I don't know. We were thinking about like using ideas from - from Larry Saul  have a good voice detector  have a good  well  voiced-speech detector  that's working on - on the F_F_T and uh Larry Saul could be an idea. We were are thinking about just U- kind of uh taking the spectrum and computing the variance of - of the high resolution spectrum and So u- s- u- things like this. O_K. So - So many tell you something about that. Uh we had a guy here some years ago who did some work on um making use of voicing information uh to help in reducing the noise. Yeah? Mm-hmm. So what he was doing is basically y- you - you do estimate the pitch. And um you - from that you - you estimate - or you estimate fine harmonic structure  whichev- ei- either way  it's more or less the same. But uh the thing is that um you then can get rid of things that are not - i- if there is strong harmonic structure  you can throw away stuff that's - that's non- harmonic. Mm-hmm. Mm-hmm. And that - that is another way of getting rid of part of the noise Yeah. Yeah. So um that's something that is sort of finer  brings in a little more information than just spectral subtraction. Um. And he had some - I mean  he did that sort of in combination with RASTA. It was kind of like RASTA was taking care of convolutional stuff and he was - Mm-hmm. Mmm. Mm-hmm. and - and got some - some decent results doing that. So that - that's another - another way. Yeah. Mmm. But yeah  there's - there's - Right. There's all these cues. We've But - actually back when Chuck was here we did some voiced-unvoiced uh classification using a bunch of these  and - and uh works O_K. Obviously it's not perfect but um - But the thing is that you can't - Mm-hmm. given the constraints of this task  we can't  in a very nice way  feed forward to the recognizer the information - the probabilistic information that you might get about whether it's voiced or unvoiced  where w- we can't you know affect the - the uh distributions or anything. Mm-hmm. But we - what we uh - I guess we could Yeah. Didn't the head dude send around that message? Yeah  I think you sent us all a copy of the message  where he was saying that - I- I'm not sure  exactly  what the gist of what he was saying  but something having to do with the voice activity detector and that it will - that people shouldn't put their own in or something. It was gonna be a - That - But - O_K. So that's voice activity detector as opposed to voicing detector. So we're talking about something a little different. They didn't. Mmm. Oh  I'm sorry. I - I missed that. Mmm. Right? I guess what you could do  maybe this would be w- useful  if - if you have - if you view the second stream  yeah  before you - before you do K_L_T's and so forth  if you do view it as probabilities  and if it's an independent - So  if it's - if it's uh not so much envelope-based by fine-structure-based  uh looking at harmonicity or something like that  um if you get a probability from that information and then multiply it by - you know  multiply by all the voiced outputs and all the unvoiced outputs  you know  Mm-hmm. then use that as the uh - take the log of that or uh pre- pre- uh - pre-nonlinearity  Yeah. i- if - uh and do the K_L_T on the - on - on that  then that would - that would I guess Yeah. be uh a reasonable use of independent information. So maybe that's what you meant. And then that would be - Yeah  well  I was not thinking this - yeah  this could be an- yeah So you mean have some kind of probability for the v- the voicing and then R- Right. So you have a second neural net. It could be pretty small. use a tandem system and Yeah. If you have a tandem system and then you have some kind of - it can be pretty small - net - we used - we d- did some of this stuff. Uh I - I did  some years ago  and the - and - and you use - the thing is to use information primarily that's different Mm-hmm. Yeah. as you say  it's more fine-structure-based than - than envelope-based uh so then it you - you - you can pretty much guarantee it's stuff that you're not looking at very well with the other one  and uh then you only use for this one distinction. Mm-hmm. Alright. And - and so now you've got a probability of the cases  and you've got uh the probability of the finer uh categories on the other side. You multiply them where appropriate and uh um I see  yeah. Mm-hmm. if they really are from independent information sources then they should have different kinds of errors and roughly independent errors  and it's a good choice for - Mm-hmm. Mm-hmm. Mm-hmm. Yeah. Uh. Yeah  that's a good idea. Yeah. Because  yeah  well  spectral subtraction is good and we could u- we could use the fine structure to - to have a better estimate of the noise but still there is this issue with spectral subtraction that it seems to increase the variance of - of - of um Yeah. Well it's this musical noise which is Right. annoying if you d- you do some kind of on-line normalization after. So. Um. Yeah. Well. Spectral subtraction and on-line normalization don't seem to - to go together very well. I- Or if you do a spectral subtraction - do some spectral subtraction first and then do some on-line normalization then do some more spectral subtraction - I mean  maybe - maybe you can do it layers or something so it doesn't - doesn't hurt too much or something. Ah  yeah. But it - but uh  anyway I think I was sort of arguing against myself there by giving that example uh I mean cuz I was already sort of Yeah. suggesting that we should be careful about not spending too much time on exactly what they're doing In fact if you get - if you go into uh - a uh harmonics-related thing it's definitely going to be different than what they're doing and uh Mm-hmm. uh should have some interesting properties in noise. Um. I know that when have people have done um sort of the obvious thing of taking uh your feature vector and adding in some variables which are pitch related or uh that - it hasn't - my impression it hasn't particularly helped. It - Uh. Has not. it i- has not  yeah. Yeah. But I think uh Oh. that's - that's a question for this uh you know extending the feature vector versus having different streams. Was it nois- noisy condition? the example that you - And - and it may not have been noisy conditions. Yeah. I - I don't remember the example but it was - it was on some DARPA data and some years ago and so it probably wasn't  you just Yeah. Mm-hmm. actually Mm-hmm. Yeah. But we were thinking  we discussed with Barry about this  and perhaps thinking - we were thinking about some kind of sheet- cheating experiment where Uh-huh. we would use TIMIT and see if giving the d- uh  this voicing bit would help in - in terms of uh frame classification. Mmm. Why don't you - why don't you just do it with Aurora? Just any i- in - in each - in each frame Yeah  but - but - We're - B- but we cannot do the cheating  this cheating thing. Well. uh - We need labels. Why not? Cuz we don't have - Well  for Italian perhaps we have  but we don't have this labeling for Aurora. We just have a labeling with word models but I see. Not for foreigners . not for phonemes. we don't have frame - frame level Right. Um. transcriptions. Um. But you could - I mean you can - you can align so that - It's not perfect  but if you - if you know what was said and - But the problem is that their models are all word level models. So there's no phone models that you get alignments for. Yeah. Mm-hmm. Oh. You - So you could find out where the word boundaries are but that's about it. Yeah. I see. S- But we could use uh the - the noisy version that TIMIT  which Yeah. you know  is similar to the - the noises found in the T_I-digits noise  yeah. um portion of Aurora. Yeah  that's right  yep. Mmm. Well  I guess - I guess we can - Yeah. we can say that it will help  but I don't know. If this voicing bit doesn't help  uh  I think we don't have to - to work more about this because - Uh. Uh. It's just to know if it - how much i- it will help and to have an idea of Yeah. Right. how much we can gain. I mean in experiments that we did a long time ago and different ta- it was probably Resource Management or something  Mmm. um  I think you were getting something like still eight or nine percent error on the voicing  as I recall. And um  Another person's voice. so um what that said is that  sort of  left to its own devices  like without the - a strong language model and so forth  that you would - you would make significant number of errors just with your uh probabilistic machinery in deciding one oh It also - Yeah  the - though I think uh there was one problem with that in that  you know  we used canonical mapping so our truth may not have really been true to the acoustics. Uh-huh. Hmm. So. Mmm. Yeah. Well back twenty years ago when I did this voiced-unvoiced stuff  we were getting more like ninety-seven or ninety- eight percent correct in voicing. But that was speaker- dependent actually. Mm-hmm. We were doing training on a particular announcer and - and getting a Mm-hmm. very good handle on the features. And we did this complex feature selection thing where we looked at all the different possible features one could have for voicing and - and - and uh - and exhaustively searched all size subsets Wow! and - and uh - for - for that particular speaker and you'd find you know the five or six features which really did well on them. Mm-hmm. And then doing - doing all of that we could get down to two or three percent error. Mm-hmm. But that  again  was speaker- dependent with lots of feature selection and a very complex sort of thing. So I would - I would believe Mmm. that uh it was quite likely that um looking at envelope only  that we'd be significantly worse than that. Mm-hmm. Uh. And the - all the - the SpeechCorders? what's the idea behind? Cuz they - they have to - Oh  they don't even have to detect voiced spe- speech? The modern ones don't do a - a simple switch. They work on the code book excitation. Yeah they do analysis-by-synthesis. They try - they - they try every - every possible excitation they have in their code book and find the one that matches best. They just work on the code book and find out the best excitation. Yeah. Mmm. Alright. Yeah. So it would not help. Yeah. Hmm. Uh. O_ K. Can I just mention one other interesting thing? Yeah. Um. One of the ideas that we had come up with last week for things to try to improve the system - Um. Actually I - I s- we didn't - I guess I wrote this in after the meeting b- but the thought I had was um looking at the language model that's used in the H_T_K recognizer  which is basically just a big Mm-hmm. loop  right? So you - it goes ""digit"" and then that can be - either go to silence or go to another digit  which - That model would allow for the production of Mm-hmm. infinitely long sequences of digits  right? Right. So. I thought ""well I'm gonna just look at the - what actual digit strings do occur in the training data."" Right. And the interesting thing was it turns out that there are no sequences of two-long or three-long digit strings in any of the Aurora training data. So it's either one  four  five  six  uh up to eleven  and then it skips and then there's some at sixteen. But what about the testing data? Um. I don't know. I didn't look at the test data yet. So. Yeah. I mean if there's some testing data that has - has - has two or three - Yeah. But I just thought that was a little odd  that there were no two or three long - Sorry. So I - I - just for the heck of it  I made a little grammar which um  you know  had it's separate path for each length digit string you could get. So there was a one-long path and there was a four-long and a five-long and I tried that and it got way worse. There were lots of deletions. So it was - Mm-hmm. Mm-hmm. you know  I - I didn't have any weights of these paths or - I didn't have anything like that. And I played with tweaking the word transition penalties a bunch  but I couldn't go anywhere. Mm-hmm. But um. Hmm. I thought ""well if I only allow -"" Yeah  I guess I should have looked at - to see how often there was a mistake where a two-long or a three-long path was actually put out as a hypothesis. Um. But. Hmm. So to do that right you'd probably want to have - allow for them all but then have weightings and things. So. I just thought that was a interesting thing about the data. O_K. So we're gonna read some more digit strings I guess? Yeah. You want to go ahead  Morgan? Sure. Uh transcript L_ twenty. five five five four zero one eight eight seven two two four six three eight five three nine five five six one six one four zero two nine four three five nine six zero seven nine eight eight nine five six six six three four seven three three nine three one two six one one four eight five zero six eight five three seven four one one three nine two three nine one seven eight three nine seven five four six Transcript L_ one nine. eight seven four three zero nine two eight eight five nine four three four seven one O_ two O_ nine seven five O_ O_ nine six seven seven six O_ one five two five four four two three five seven five four two six nine O_ three four six two three one three eight zero zero eight zero zero seven eight three five one four seven one nine two nine one eight zero Transcript L_ sixteen. five six O_ eight four two five five six six seven three five four seven five four seven seven O_ three seven seven one five five O_ five three two five five eight one six nine three four three nine three O_ five seven O_ one nine five eight eight six two five seven six nine eight eight five O_ three three th- four three four two three O_ four six five five O_ Transcript L_ seventeen nine four three six four seven zero one three nine six seven one two six eight two zero nine seven five zero zero four six two two eight zero five two seven one seven one three three five two zero two one six one one two zero eight nine five nine zero three one five two two seven one three eight eight four two zero eight four five seven six eight eight zero four eight three five zero zero Transcript L_ eighteen. five eight five seven seven one four four three nine two four one nine one three O_ one one seven eight four five two eight three six nine eight seven three nine nine seven two four five two three nine six five seven two nine two one O_ eight six ninety-eight two two eight eight five four one seven nine four nine seven nine two four two O_ five six three five six seven nine seven eight five five eight one Transcript L_ twenty-one. zero one zero four five three three six six one two five five four five four three four four one zero one six nine six zero seven two three zero seven nine three one four one five zero four one seven zero eight three five three two four one six five six eight seven five nine four three seven seven five O_ five zero three five six eight seven four seven nine nine nine one one nine ",The ICSI Meeting Recorder Group met once more to discuss their recent progress in various projects  as well as discuss some of the issues that have arisen in the last week. There has been further work on voiced/unvoiced detection  along with spectral subtraction. The group discussed one members attendance at a conference  and another groups code  which is proving hard to follow. In trying to understand France Telecom's code  there does not appear to be a reason for a particular constant value. Speaker fn002 reported on her work with mn007  who was absent attending a conference. They have been working on voiced/unvoiced detection  and have tried two different configuration of MLP  although results have not been improved greatly. She has also begun looking at France Telecom's recogniser code. Speaker me006 has been coming up with tests for his work using acoustic events. Speaker me018 is investigating differences in another system  and feels it maybe down to bugs. Speaker me026 has made real progress on his spectral subtraction work  with an implementation and early results. 
"st- Yeah. So we're on. That's better. And  somewhere is my agenda. I think the most important thing is Morgan wanted to talk about  uh  the ARPA demo. Well  so  here's the thing. Um  why don't we s- again start off with - with  uh  -n- Yeah  I'll get it. I'll get the door. Um  I think we want to start off with the agenda. And then  given that  uh  Liz and Andreas are gonna be ten  fifteen minutes late  we can try to figure out what we can do most effectively without them here. So - O_K. So  uh - So - so  one thing is  yeah  talk about demo  uh  I_B_M transcription status  I_B_M transcription. Uh  what else? SmartKom. SmartKom. SmartKom. What's SmartKom? Uh  we wanna talk about if w- if we wanna add the data to the mar- Meeting Recorder corpus. The data. The data which we are collecting here. What - what - what are we collecting here? Data? So why don't we have that on the agenda and we'll - we'll get to it and talk about it? The SmartKom data? Yeah  right. Yeah. Uh  right. Uh. Uh  reorganization status. Reorganization status. Oh. Files and directories? Files and directories. Yep. Uh-huh. Absinthe  which is the multiprocessor UNIX - Linux. I think it was Andreas wanted to talk about segmentation and recognition  and update on S_R_I recognition experiments. Um - And then if ti- if there's time I wanted to talk about digits  but it looked like we were pretty full  so I can wait till next week. Right. O_K. Well  let's see. I think the a- certainly the segmentation and recognition we wanna maybe focus on when An- Andreas is here since that was particularly his And also the SmartKom thing. thing SmartKom also  should b- Andreas. Absinthe  I think also he has sort of been involved in a lot of those things. Yeah. At least  yeah   he'll t- he'll probably be interested. But. Yeah. Um So  I mean  I think they'll be inter- I'll be interested in all this  but - but  uh  probably  if we had to pick something that we would talk on for ten minutes or so while they're coming here. Or I guess it would be  you think  reorganization status  or - ? Yeah. I mean  I think  Chuck was the one who added out the agenda item. I don't really have anything to say other than that we still haven't done it. So. Well  I mean  I uh - just basically that - maybe I said - maybe we said this before - just that we met and we talked about it and we sort of have a plan for getting things organized and - And I - and I think a crucial part of that is the idea of - of not wanting to do it until right before the next level zero back-up so that there won't be huge number of - Right. of added  uh - Right. That - that was basically it. Not - not much @@ - Although Dave basically said Yeah. that if we wanna do it  Uh-huh. just tell him and he'll do a d- level zero then. Oh  excellent. So. Oh  so maybe we should just go ahead and get everything ready  and - Oh  good. Yep. So  I think we do need to talk a little bit about - Well  we don't need to do it during this meeting. We have a little more to discuss. Yeah. But  uh  we're - we're basically ready to do it. And  uh  I have some web pages on ts- more of the background. So  naming conventions and things like that  that I've been trying to keep actually up to date. So. And I've been sharing them with U_d- U_W folks also. O_K. I'm sorry  you've been what? Showing them? Sharing them. O_K. Sharing them with the U_W folks. O_K. O_K. Well  maybe uh  since that - that was a pretty short one  maybe we should talk about the I_B_M transcription status. Someone can fill in Liz and Andreas later. Uh O_K. So  we  uh - we did another version of the beeps  where we separated each beeps with a spoken digit. Chuck came up here and recorded some di- himself speaking some digits  and so it just goes ""beep one beep"" and then the phrase  and then ""beep two beep"" and then the phrase. And that seems pretty good. Um  I think they'll have a b- easier time keeping track of where they are in the file. And we have done that on the automatic segmentations. And we did it with the automatic segmentation  and I don't think - We ne- we didn't look at it in detail. We just sent it to I_B_M. We - we sorta spot-checked it. I listened to probably  Yeah. uh  five or ten minutes of it from the beginning. Oh  really? O_K. Yeah. And - I sorta spot-checked here and there and it sounded pretty good. So. O_K. I think it'll work. And  uh  we'll just hafta see what we get back from them. Uh - And the main thing will be if we can align what they give us with what we sent them. I mean  that's the crucial part. Right. And I think we'll be able to do that at - with this new beep format. Yep. Well  I think it's also they are much less likely to d- have errors. Mm-hmm. I mean  so the problem wi- last time is that there were errors in the transcripts where they put beeps where there weren't any  Right. or - and they put in extraneous beeps. Yeah. And with the numbers there  it's much less likely. Yeah  one interesting note is - uh  or problem - I dunno if this was just because of how I play it back  I say  uh  S_N_D-play and then the file  every once in a while  @@ uh  like a beep sounds like it's cut into two beeps. Yeah. Into two pieces. Yeah. Yeah  and Yep. I - I dunno if that's an  uh  artifact of playback - bu- uh  I don't think it's probably in the original file. Um  I recognize that  too. Yeah. but  uh - Ha. That's interesting. I didn't hear that. Yeah. But with this new format  um  that hopefully they're not hearing that  and if they are  it shouldn't throw them. Yep. So. Well  maybe we better listen to it again  make sure  but  I mean  certainly the software shouldn't do that  so. Yeah. That's what I thought. I- it's probably just  Mm-hmm. you know  mmm  somehow the audio device gets Yeah. Some latency or something. Yeah? Hiccups. hung for a second  or - Yeah. As long as they have one number  and they know that there's only one beep maximum Yeah. Yeah. that goes with that number. Yeah. The only - the only part that might be confusing is when Chuck is reading digits. Right. Yep. Right. So th- Well  you know  actually  ""Seven four eight beep seven beep eight three two"". are we having them - Yeah  but are we having them do digits? Yes. Because  uh  we don't - We - we didn't cut those out. Yeah. we didn't - They are not transcribed yet. So. Yeah. In order to cut them out we'd have to listen to it. Yeah. And we wanted to avoid doing that  so we - they are transcribing the digits. O_K. O_K. O_K. Although we could tell them - We can - we can ignore it when we get it back  huh. we could tell them  if you hear someone reading a digits string just say ""bracket digit bracket"" and don't bother actually computing the di- writing down the digits. Yeah. That'd be great. That'd be what I'm having the transcribers here do  cuz it can be extracted later. Yep. And then I wanted to talk about - but as I said I - we may not have time - what we should do about digits. We have a whole pile of digits that haven't been transcribed. Le- let's talk about it  because that's - that's something that I - I know Andreas is less interested in than Liz is  so  O_K. you know. Do we have anything else to say about transcription? It's good - About I_B_M stuff? Uh  Brian - I - I sent bresset - sent Brian a message about the meeting and I haven't heard back yet. So. O_K. I g- hope he got it and hopefully he's - Hmm. maybe he's gone  I dunno. He didn't even reply to my message. So. I should probably ping him just to make sure that he got it. Alright. @@ So  we have a whole bunch of digits  if we wanna move on to digits. Actually  maybe I - One - one relate- more related thing in transcription. So that's the I_B_M stuff. We've got that sorted out. Um  how're we doing on the - on the rest of it? We're doing well. I - I hire - I've hired two extra people already  expect to hire two more. Hmm. And  um  I've prepared  um  uh  a set of five which I'm - which I'm calling set two  which are now being edited by my head transcriber  in terms of spelling errors and all that. She's also checking through and mar- and - and monitoring  um  the transcription of another transcriber. You know  I mean  she's going through and doing these kinds of checks. Uh-huh. And  I've moved on now to what I'm calling set three. I sort of thought if I do it in sets - groups of five  then I can have  like  sort of a - a parallel processing through - through the - Uh-huh. the current. And - and you indicated to me that we have a g- a goal now  for the - for the  um  the  uh  DARPA demo  of twenty hours. So  I'm gonna go up to twenty hours  be sure that everything gets processed  and released  and - and that's - that's what my goal is. Package of twenty hours right now  and then once that's done  move on to the next. Yeah  uh  so twenty hours. But I guess the other thing is that  um  that - that's kinda twenty hours A_SAP because the longer before the demo we actually have the twenty hours  the more time it'll be for people to actually do cool things with it. Mm-hmm. So. Good. I'm - I'm hiring people who  O_K. uh  really are - They would like to do it full-time  several of these people. Mm-hmm. And - and I don't think it's possible  really  to do this full-time  but  that - what it shows is motivation to do as many hours as possible. It'll keep your accuracy up. Yep. Yeah. Yeah. And they're really excellent. Well  that's good. Yeah. Yeah  I mean  I guess the - Got a good core group now. So the difference if - if  um  if the I_B_M stuff works out  Again. the difference in the job would be that they p- primarily would be checking through things that were Mm-hmm. already done by someone else? Is that most of what it - ? And correcting. I mean - Correcting. Correcting. We'll - we'll expect that they'll have to move some time bins and do some corrections. And I - you know  I've also d- uh  discovered - So with the new transcriber I'm - um - So - Uh  lemme say that my  uh - So  um - At present  um  the people have been doing these transcriptions a channel at a time. And  that sort of  um  is useful  and t- you know  and then once in a while they'll have to refer to the other channels to clear something up. O_K. Well  I realize that  um  w- i- we- we're using the pre-segmented version  and  um  the pre-segmented version is extremely useful  and wouldn't it be  useful also to have the visual representation of those segments? And so I've - uh  I  uh  uh  I've trained the new one - uh  the new- the newest one  to  um  use the visual from the channel that is gonna be transcribed at any given time. And that's just amazingly helpful. Because what happens then  is you scan across the signal and once in a while you'll find a blip that didn't show up in the pre-segmentation. Oh  right. I see what you mean. And that'll be something like - I- it's ver- - it's interesting. A backchannel  or - Once in a while it's a backchannel. Sometimes it seems to be  Yep. um  similar to the ones that are being picked up. Mm-hmm. And they're rare events  but you can really go through a meeting very quickly. You just - you just  you know  yo- you s- you scroll from screen to screen  looking for blips. And  Yeah. I think that we're gonna end up with  uh better coverage of the backchannels  but at the same time we're benefitting tremendously from the pre-segmentation because there are huge places where there is just absolutely no activity at all. Mm-hmm. And  uh  the audio quality is so good - So they can - they can  um  scroll through that pretty quick? Yeah. Mm-hmm. That's great. Yeah. So I think that that's gonna  also eh  you know  speed the efficiency of this part of the process. Hmm. O_K. Uh  yeah. So  uh - Yeah. So let's talk about the digits  since they're not here yet. Uh  so  we have a whole bunch of digits that we've read and we have the forms and so on  um  but only a small number of that ha- well  not a small number - only a subset of that has been transcribed. And so we need to decide what we wanna do. And  uh  Liz and Andreas - actually they're not here  but  they did say at one point that they thought they could do a pretty good job of just doing a forced alignment. And  again  I don't think we'll be able to do with that alone  because  um  sometimes people correct themselves and things like that. But - so  I was just wondering what people thought about how automated can we make the process of finding where the people read the digits  doing a forced alignment  and doing the timing. Well  forced alignment would be one thing. What about just actually doing recognition? Well  we - we know what they read  because we have the forms. No  they make mistakes. Right. But  the point is that we wanna get a set of clean digits. Right. You're talking about as a pre-processing step. Right  Morgan? Um - Is that what you're - ? Yeah  I'm - I'm not quite sure what I'm talking about. I mean - I - I mean  uh  we're talking about digits now. And - and so  um  there's a bunch of stuff that hasn't been marked yet. Uh. And  um  I mean  so one option i- there's the issue that - that they - we know what - what was said  but do we? Because people make mistakes and stuff. I was just asking  just out of curiosity  if - if with  uh - uh  the S_R_I recognizer getting one percent word error  uh  would we - would we do better - ? So  if you do a forced alignment but the force- but the - but the transcription you have is wrong because they actually made mistakes  uh  or But that's pretty uncommon. false starts  it's - it's much less c- it's much less common than one percent? Um  if we could really get one percent on - Well  We should be able to. Right? I guess - yeah  I guess if we segmented it  Yeah. we could get one percent on digits. Yeah. So that's just my question. I'm not saying it should be one way or the other  but it's - If - But  Well  there - there're a couple different of doing it. We could use the tools I've already developed and transcribe it. Hire some people  or use the transcribers to do it. We could let I_B_M transcribe it. You know  they're doing it anyway  and unless we tell them different  they're gonna transcribe it. Um  or we could try some automated methods. And my - my tendency right now is  well  if I_B_M Well - comes back with this meeting and the transcript is good  just let them do it. Yeah  it's - Y- you raised a point  kind of  uh  euphemistically - but  I mean  m- maybe it is a serious problem. Ho- what will they do when they go - hear ""beep seven beep seven three five two"" - I mean  you think they'll - we'll get - ? It's pretty distinct. The beeps are pre-recorded. Yeah? It'll only be a problem for m- for mine. Yeah. Well it - it - well  it'd be preceded by ""I'm reading transcript so-and-so""? Yeah. Yes. So  I think if they're processing it at - I mean  it'll be - it will be in the midst of a digit string. So - Yeah. I mean it - sure  there - there might be a place where it's ""beep seven beep eight beep eight beep"". But  you know  they - they're - they're gonna macros for inserting the beep marks. And so  I - I don't think it'll be a problem. We'll have to see  but I don't think it's gonna be a problem. O_K. Well  I - I - I dunno  I - I think that that's - if they are in fact going to transcribe these things  uh  certainly any process that we'd have to correct them  or whatever is - needs to be much less elaborate for digits than Right. for other stuff. So  why not? Sure. That was it? That was it. Just  what do we do with digits? We have so many of them  O_K. Well  we - we - and it'd be nice to actually do something with them. we wanna have them. You mean there're more than ten? Yeah  I - Anything else? Your mike is a little low there. I- in Berkeley  yeah. So  uh You - you have to go a little early  right? Well  I can stay till about  uh  At twenty - three forty. Alright. So le- let's make sure we do the ones that - Yeah. Mm-hmm. that  uh  saved you. So there was some - Uh In - in - Adam's agenda list  he had something from you about segmentation this last recognition? Well  yeah. So this is just partly to inform everybody  Oops. um  and - and of course to get  um  input. Um  so  uh  we had a discussion - Don and Liz and I had discussion last week about how to proceed with  uh  Ch- you know  with Don's work  and - and - and  uh  one of the obvious things that occur to us was that we're - since we now have Thilo's segmenter and it works  you know  amazingly well  um  we should actually basically re-evaluate the recognition  um  results using - you know  without cheating on the segmentations. And  that should be fairly - So - And how do we find the transcripts for those so that - ? Yeah. Oh  O_K. The references for - for those segments? So  there's actually - It's not that - Why do you ask? No  actually  um  I could - Well - Hand ones. NIST has  um m- a fairly sophisticated scoring program O_K. that you can give a  um - a time  uh - You know  you basically just give two time-marked sequences of words  and it computes the um - the  uh - you know  It does all the work for you. the - the - th- it does all the work for you. Yeah. O_K. So  it - we just - and we use that actually in Hub-five to do the scoring. Um. So what we've been using so far was sort of a simplified version of the scoring. And we can - we can handle the - the - the type of problem we have here. So  we ha- So  basically you give some time constraints for - for the references and for - for the hypothesis  and - Yeah  O_K. Yeah. Right. Right. Yeah. Maybe the start of your speech and the end of it  or stuff like that. So do- O_K. Right. It does time-constrained word-alignment. O_K. So. So that should be possible. I mean that shouldn't be a problem. Uh  so that was the one thing  and the other was that  um - What was the other problem? Oh! That Thilo wanted to use the recognizer alignments to train up his  um  speech detector. Yeah. Um  so that we could use  uh - you know there wouldn't be so much hand labeling needed to  uh - to generate training data for - for the speech detector. Yeah. I'm just in progress of - of doing that. So. And I think you're in the process of doing that. So  Yeah. you can - It'll give you a lot more data  too. Won't it? you can - Yeah. So  it's basically - s- I think  eight meetings or something which - which I'm using  and  Mm-hmm. it's - before it was twenty minutes of one meeting. So - Right. should be a little bit better. That won't be perfect - the alignments aren't perfect  Great. Yeah. But - but  um  Yeah. it's probably still better to have all this extra data  Yeah. Yep. than - Yeah. We'll see that. Yeah. O_K. Actually  I had a question about that. If you find that you can lower the false alarms that you get where there's no speech  that would be useful for us to know. So  um - There were the false alarms. Yeah. So  r- right now you get f- O_K. fal- you know  false - false  uh  speech regions when it's just like  um  Yeah. breath or something like that  and I'd be interested to know the - Yep. wha- if you retrain um  Yeah. do those actually go down or not? Because of - Yeah. I'll - can make - an- can  like  make a c- comparison of - of the old system to the - to the new one  and then - Yeah. Yeah  just to see if by doing nothing in the modeling of - just having that training data wh- what happens. Yeah. Yep. Um another one that we had on Adam's agenda that definitely involved you was s- something about SmartKom? Right. So  Rob Porzel - eh  Porzel? and the  uh - Porzel. Porzel - Porzel. and the  uh  SmartKom group are collecting some dialogues. Basically they have one person sitting in here  looking at a picture  and a wizard sitting in another room somewhere. And  uh  they're doing a travel task. And  uh  it involves starting - I believe starting with a - It's - it's always the wizard  but it starts where the wizard is pretending to be a computer and it goes through a  uh  speech generation system. Yeah. Actually  it's changed to a synthesis for - for the first part now. Yeah. Synthesis system. Um  and then  it goes to a real wizard and they're evaluating that. And they wanted to use this equipment  and so the w- question came up  is - well  here's some more data. Should this be part of the corpus or not? And my attitude was yes  because there might be people who are using this corpus for acoustics  as opposed to just for language. Um  or also for dialogue of various sorts. Um  so it's not a meeting. Right? Because it's two people and they're not face to face. Wait a minute. So  Yeah. I just wanted to understand it  cuz I - I'm - uh  hadn't quite followed this process. Um. So  it's wizard in the sen- usual sense that the person who is asking the questions doesn't know that it's  uh  a machi- not a machine? Right. Actually - actually  At the beginning. w- w- the - the - We do this - I dunno who came up with it  but I think it's a really clever idea. We simulate a computer breakdown Yeah. halfway through the session  and so then after that  the person's told that they're now talking to a  uh - It's a human operator. to a human. Yeah. Yeah. But of course they don't know that it's the same person both times. So  we - we collect - we collect both human-computer and human-human data  essentially  in the same session. You might wanna try collecting it the other way around sometime  saying that th- the computer isn't up yet and then - so then you can separate it out whether it's the beginning or end kind of effects. But  yeah. Hmm. That's an idea. Yep. Yeah. That's a good idea. ""I have to go now. You can talk to the computer."" ""No!"" It's a lot more believable  too  if you tell them that they're - the computer part is running on a Windows machine. And the whole breakdown thing kinda makes sense. O- Just - just reboot it. Abort - abort  retry  fail? So did they actually save the far-field data? Cuz at first they weren't - Well  this was - this was the question. So - so they were saying they were not going to  and I said  ""well that's silly  if - if we're gonna try to do it for a corpus  Yes. Yeah. they weren't sa- O_K. Yeah. Yeah. Wow. No. there might be people who are interested in acoustics."" Or - projector Yeah. S- We were not saying we are not doing it. We wer- we just wanted to do - No  the - the question is do we save one or two far-field channels or all of them? Right. Yeah. Yeah. I - I see no reason not to do all of them. That - Um - Nnn. that if we have someone who is doing acoustic studies  Hmm. Yeah. uh  it's nice to have the same for every recording. So  what is the purpose of this recording? Mm-hmm. This is to It's to be traini- to b- training data and development data for the SmartKom system. The English system? get acoustic and language model Yeah. Yeah. training data for SmartKom. Right. Right. O_K. Where does this - ? Maybe we can have him vary the microphones  too  or they're different s- speakers. Well  B- Right. So - so - so for their usage  they don't need anything. so why not - ? Yeah. But - but I'm not sure about the legal aspect of - of that. Is - is there some contract with SmartKom or something about the data? What Right? Yeah. they - or  is - is that our data which we are collecting here  or - ? We've never signed anything that said that O_K. we couldn't use anything that we did. O_K. We weren't supposed to collect any data. So. O_K. This was all - Yeah. So. Yeah  th- th- that was the question. If - if - ? Yeah. Yeah. Basically. No that's not a problem. I - L- look  it seems to me that if we're doing it anyway and we're doing it for these - these purposes that we have  Mm-hmm. and we have these distant mikes  we definitely should re- should save it all as long as we've got disk space  and disk is pretty cheap. So should we save it? O_K. And then - Now th- Yeah. So we save it because it's - it - it's potentially useful. Right. And now  what do we do with it is - is a s- separate question. I mean  anybody who's training something up could choose to put it - Right. eh  to u- include this or not. I - I would not say it was part of the meetings corpus. It isn't. But it's some other data we have  and if somebody doing experiment wants to train up including that then they can. Right? Mm-hmm. So it's - It - it - I guess it - the - begs the question of what is the meeting corpus. So if  at U_W they start recording two-person I think it's - I - hallway conversations is that part of the meeting corpus? I think - I th- think the idea of two or more people conversing with one another is key. Well  this has two or more people conversing with each other. They're just not face to face. Nnn  well Yeah. What if we just give it a - Well this - No  it doesn't. Right? It has - a name like we give these meetings a name? I mean  that was my intention. And then later on some people will consider it a meeting and some people won't  and - Yeah. That was my intention. So - so - s- Well this - Just give it a so part of the reason that I wanted to bring this up is  title. do we wanna handle it as a special case or do we wanna fold it in  Oh. I think it is a s- we- give everyone who's involved as their own user I_D  give it session I_Ds  let all the tools that handle Meeting Recorder handle it  or do we wanna special case it? And if we were gonna special case it  Well  So. It - it - it - it - who's gonna do that? it makes sense to handle it I think - with the same infrastructure  since we don't want to duplicate things unnecessarily. But as far as distributing it  we shouldn't label it as part of this meeting corpus. We should Yeah. let it be its own corp- I don't see why not. It's just a different topic. Well it's - it - well  because - I ha- I have an extra point  which is the naturalness issue. Because we have  like  meetings that have a reason. That's one of the reasons that we were talking about this. And - and those - Yeah. and this sounds like it's more of an experimental setup. It's got a different purpose. It's scenario-based  it's - it's human-computer interface - it's really pretty different. But I- I - I have no problem with somebody folding it in Yeah. for some experiment they're gonna do  but I don't think i- it - it doesn't match anything that we've described about meetings. Mm-hmm. Whereas everything that we talked about them doing at - at U_W and so forth really does. They're actually talking - O_K. So w- so what does that mean for how we are gonna organize things? Hmm. Yeah. You can - you can - Again  as - as I think Andreas was saying  if you wanna use the same tools and the same conventions  there's no problem with that. It's just that it's  you know  different directory  it's called something different  it's - you know. It is different. You can't just fold it in as if it's - I mean  digits are different  too. Right? It might also be potentially confusing. Yeah  but those are folded in  and it's just - you just mark the transcripts differently. So - so one option is you fold it in  Right. and just simply in the file you mark somewhere that this is this type of Yeah  I th- interaction  rather than another type of interaction. Well  I don- I wouldn't call reading digits ""meetings"". Right? I mean  we - we - we were doing - Well  but - but  I put it under the same directory tree. You know  it's in ""user doctor speech data M_R"". Well - Can we just have a directory called  like  ""other stuff""? And - Other. Well - or  I dunno. I mean  I don't care what directory tree you have it under. Right? I mean that's just a - And - and just  um  store it there. I mean - O_K. My preference Yes. is to have a single procedure so that I don't have to think too much about things. And  Yeah. just have a marking. If we do it any other way that means that we need a separate procedure  O_- You - you can use whatever procedure you want and someone has to do that. that's p- convenient for you. All I'm saying is that there's no way that we're gonna tell people that reading digits is meetings. Right. And similarly we're not gonna tell them that someone talking to a computer to get travel information is meetings. Those aren't meetings. But if it makes it easier for you to pu- fold them in the same procedures and have them under the same directory tree  knock yourself out. You know? There's a couple other questions that I have too  and - and one of them is  what about  uh  consent issues? And the other one is  what about transcription? Transcription is done in Munich. Are - ? O_K. So we don't have to worry about transcribing it? Yeah. Alright. So  w- we will hafta worry about That's a - that's another argument to keep it separate  because it's gonna follow the SmartKom transcription conventions and not the format. Oh  O_K. Yeah. ICSI meeting transcription conventions. Ah. O_K. Well  I didn't realize that. That's - that's a - Good point. Good point. But I'm sure no one would have a problem with our folding it in for some acoustic modeling or - or some things. Um. Do we h- do we have  uh  um  American-born folk  uh  reading German - German  uh  pla- uh  place names and so forth? Is that - ? Yeah. Exactly. Yeah. Yep. Yeah  great. Yeah. They - they even have a reading list. It's pretty funny. I bet that sounds good  huh? Yeah. Yeah. You can do that if you want. O_K. Yeah. I dunno if you want that. Yeah. Right. So - Hmm. Heidelberg Exactly Disk might eventually be an issue so we might - Do you wanna be a subject? Yeah  I be pretty good. we - we might need to  uh  get some more disk pretty soon. We're about - We - Yeah. That was one of our concerns. Yeah. we're about half - halfway through our disk right now. Are we only half? I thought we were more than that. We're probably a little more than that because we're using up some space that we shouldn't be on. So  once everything gets converted over to the disks we're supposed to be using we'll be probably  uh  seventy-five percent. Well  when I was looking for space for Thilo  I found one disk that had  uh  I think it was nine gigs and another one had seventeen. Yep. And everything else was sorta committed. Uh - Were those backed-up or non-backed-up? Those were non-backed-up. Non-back-up. Right. So that's different. S- oh  you're talking about backed-up. I'm much more concerned about the backed-up. @@ The non-backed-up  yeah  i- I haven't looked to see how much of that we have. is cheap. I mean  if we need to we can buy a disk  hang it off a s- uh  workstation. If it's not backed-up the sysadmins don't care too much. Yeah. So  I mean  pretty much anytime we need a disk  we can get it at the rate that we're - You can - I shouldn't be saying this  but  you can just - you know  since the back-ups are every night  you can recycle the backed-up diskspace. Yeah. But that's - that's - that's risky. Yeah. You really shouldn't be saying - Mmm. Mmm. @@ I didn't say that. I didn't say that. Yeah  that's right. Beep that out. Da- we had allowed Dave to listen to these - these  Right. uh  recordings. Um - Yeah  I me- and there's been this conversation going on about getting another file server  and - Mm-hmm. and we can do that. We'll take the opportunity and get another big raft of - Yeah. of disk  I guess. Well  It's really the back-up issue rather than the file server issue. I think - I think there's an argument for having - you know  you could use our old file server for - for disks that have data that is very rarely accessed  and then have a fast new file server for data that is  um  Yeah. heavily accessed. My understanding is  the issue isn't really the file server. We could always put more disks on. It's the back-up system. Yeah. Yeah. It's the back- it's the back-up capaci- Yeah. So - which is near saturation  apparently. So. Soon. I think - I think the file server could become an issue as we get a whole bunch more new compute machines. And we've got  you know  fifty machines trying to access data off of Abbott at once. Well  we're alright for now because the network's so slow. I mean  I think - I think we've raised this before and someone said this is not a reliable way to do it  but the - What about putting the stuff on  like  C_- C_D-ROM or D_V_D or something? Yeah. That was me. I was the one who said it was not reliable. The- they - O_K. they wear out. Oh  O_K. Yeah. The - the - th- But they wear out just from sitting on the shelf? Yep. Absolutely. Or from being read and read? No. Read and write don't hurt them too much unless you scratch them. Oh  O_K. But the r- the write once  and the read-writes  don't last. So you don't wa- you don't wanna put ir- un- reproduceable data on them. Uh-huh. Wear out after what amount of time? Year or two. Would it be - ? Year or two? Yep. Wow. Hmm. But if that - then you would think you'd hear much more clamoring about Yeah. data loss and - I mean  yeah  all the L_- I - I don't know many people who do it on C_D. I mean  they're - the most - L_D_C- all the L_D_C distributions are on C_D-ROM. fo- Yeah. They're on C_D  but they're not - Like - tha- that's not the only source. They have them on disk. And they burn new ones every once in a while. But if you go - But  you know  we have - But we have like thirty you know  from ten years ago? No. if you go k- We have all sorts of C_D-ROMs from a long time ago. Yeah! Ten years ago. Ninety-one  and they're still all fine. Right. Yeah. Well  th- th- Were they burned or were they pressed? Yeah. Uh  both. I've burned them and they're still O_K. I mean  usually they're - Yeah. O_K. The - the pressed ones last for- well  not forever  they've been finding even those degrade. Oh  I see. But  uh  the burned ones - I mean  when I say two or three years what I'm saying is that That's what I - I have had disks which are gone in a year. On the average  it'll probably be three or four years. But  uh - I - I - you don't want to per- p- have your only copy on a media that fails. Mmm. And they do. So how about - ? Um  if you have them professionally pressed  y- you know  they're good for decades. So - so how about putting them on that plus  like on a - on - on DAT or some other medium that isn't I think th- um  risky ? we can already put them on tape. O_K. And the tape is hi- is very reliable. Mm-hmm. So the - the only issue is then if we need access to them. Right. So that's fine f- if we don't need access to them. Well  if - if - if you - if they last - Say  they actually last  like  five years  huh  in - in the typical case  and - and occasionally you might need to recreate one  and then you get your tape out  but otherwise you don't. Can't you just - you just put them on - ? So you just archive it on the tape  and then Yeah. put it on C_D as well? Right. Oh. So you're just saying put them on C_Ds for normal access. Right. Yeah. Yeah. I mean  you can do that but that's pretty annoying  because the C_Ds are so slow. What you - See - Yeah. Mmm. Yeah. What'd be nice is a system that re-burned the C_Ds every year. H- everytime it was a ""gonna"" - ""gonna die"". Yeah. Well  I mean  the C_Ds are - Well - It's like - like dynamic ra- D_RAM. are an op- Just before. Just before they- be- before it goes bad  it burns them in. Yeah. The - the C_D is an alternative to tape. ICSI already has a perfectly good tape system and it's more reliable. Yeah. You know - So for archiving  we'll just use tape. One - one thing I don't understand is  if you have the data - I would think - if - if you- if the meeting data is put on disk exactly once  then it's backed-up once and the back-up system should never have to Well  bother with it  uh  more than once. regardless - Well  first of all there was  um  a problem with the archive in that I was every once in a while doing a chmod on all the directories an- or recursive chmod and chown  because Mm-hmm. they weren't getting set correctly every once in a while  and I was just  doing a minus R_ star  Mm-hmm. not realizing that that Ah. caused it to be re-backed-up. But normally you're correct. But even without that  the back-up system is becoming saturated. But - but this back-up system is smart enough to figure out that something hasn't changed and doesn't need to be backed-up again. Sure  but we still have enough changed The b- that the nightly back-ups are starting to take too long. I think th- the - at least the once tha- that you put it on  it would - it would O_K. So - so then  if - So - so then  let's - Right. It has nothing to do with the meeting. It's just the general ICSI back-up system is becoming saturated. kill that. So. O_K. Right. So  what if we buy  uh - uh  what - what do they call these  um high density - ? Well  why don't you have this - have a - this conversation with Dave Johnson tha- rather than with me? No  no. Because this is maybe something that we can do without involving Dave  and - and  putting more burden on him. How about we buy  uh - uh - uh  one of these high density tape drives? And we put the data actually on non-backed-up disks. And we do our own back-up once and for all - all  and then - and we don't have to bother this @@ up? Actually  you know  we could do that just with the tape - with the current tape. I dunno what the- these tapes - uh  at some point these - I dunno. What kind of tape drive is it? Is it - is - ? I dunno but it's an automatic robot so it's very convenient. Wh- Right. You just run a program to restore them. The o- the one that we have? Yeah. But it might interfere with their back-up schedule  eh. The - I mean - But - No  we have s- we - Don't we have our own? Something wi- th- that doesn't - that isn't used by the back-up gang? Don't we have something Well they - downstairs? What kinda tape drive? Just in - ? Well - Yeah. but - no  but Andreas's point is a good one. And we don't have to do anything ourselves to do that. They're already right now on tape. Right. Right. So your - your point is  and I think it's a good one  that we could just get more disk and put it there. Mmm. On an X_H - uh  X_ - X_ whatever partition. Yeah. Yeah. That's not a bad idea. Yeah  that's basically what I was gonna say  is that a disk is - is so cheap it's es- essentially  you know  close to free. So once it's on tape - Right. And the only thing that costs is the back-up issue  Right. eh  to first order. And we can take care of that by putting it on non-back- up drives and just Mm-hmm. backing it up once onto this tape. I think that's a good idea. Right. Oh. Yeah. O_K. Good. It's good. So  who's gonna do these back-ups? The people that collect it? Uh Well  I'll talk to Dave  and - and see what th- how - what the best way of doing that is. It's probably gonna n- There's a little utility that will manually burn a tape for you  and that's probably the right way to do it. Yeah  and we should probably make that part of the procedure for recording the meetings. Well  s- Yep. Yeah. That's what I'm wondering  if - Well we're g- we're gonna automate that. My intention is to do a script that'll do everything. O_K. I mean  you don't have to physically put a tape in the drive? Or s- ? No. Oh  O_K. It's all tape robot  so you just sit down at your computer and you type a command. So it's just - Oh  O_K. Yeah  but then you're effectively using the resources of the back-up system. Or is that a different tape robot? But not at the same time. Yeah. But y- but you would be anyway. No  no. See - Right? Because - No  no  no. He's saying get a whole different drive. Yeah  just give a dedi- But there's no reason to do that. Well  I'm saying is @@ It - we already have it there and it - it's - i- if you go to Dave  and - and - and ask him ""can I use your tape robot?""  he will say  ""well that's gonna screw up our back-up operation."" No  we won't. He'll say ""if - if that means that it's not gonna be backed-up standardly  great."" He- I - Dave has - has promoted this in the past. So I don't think he's actually against it. Yeah. Yeah. It's - it's definitely no problem. Oh  O_K. Alright. Alright. Good. O_K. What about if the times overlap with the normal back-up time? Um  it's - it's just - it's just a utility which queues up. O_K. It just queues it up and - and when it's available  it will copy it. Yeah. And then you can tell it to then remove it from the disk or you can  you know  do it a- a few days later or whatever you wanna do  after you confirm that it's really backed-up. O_K. N_W - ? You saying N_W archive? N_W archive. That's what it is. Yep O_K. And if you did that during the day it would never make it to the nightly back-ups. Right. And then there wouldn't be this extra load. Well  it - if he - you have to put the data on a - on a non-backed-up disk to begin with. So that - so that - Right. Well  but you can have it N_W archive to - you can have  otherwise you don't - you - uh  a non-backed-up disk N_W archived  Right. and it'll never show up on the nightly back-ups. And then it never - Right. Right. Right. Which I'm sure would make ever- the sysadmins very happy. Right. Yeah. So  I think that's a good idea. O_K. That's what we should do. O_K. So  that means we'll probably wanna convert all - all those files - filesystems to non-backed-up media. That sounds good. Yep. Yeah. Um  another  thing on the agenda said S_R_I recognition experiments? What's that? S_R_I recognition? Oh. That wasn't me. Um. Uh. Who's that? well  we have lots of them. Uh  I dunno. Chuck  do you have any - any updates? N- I'm successfully  uh  increasing the error rate. Uh - That's good. Oh. Mmm. Lift the Herve approach. Yeah. So  I mean I'm just playing with  um  the number of Gaussians that we use in the - the recognizer  and - Well  you have to sa- you have to tell people that you're - you're doing - you're trying the tandem features. Yes  I'm using tandem features. Oh you are? Cool. A- and I'm still And - @@ tinkering with the P_L_P features. Yeah  I got confused by the results. It sai- because - uh  the meeting before  you said ""O_K  we got it down to where they're - they're within a tenth of a percent"". That was on males. Right. That was - that was before I tried it on the females. Oh. See  women are nothi- are  It's the women are the problem. O_K. trouble. Right? As we all know. So. Well  let's just say that men are simple. So - so  when - So I - I had - That was a quick response. I ha- So  we had reached the point where - I'm well rehearsed. Yeah. we had reached the point where  um  on the male portion of the development set  the  um - or one of the development sets  I should say - the  um - the male error rate with  uh  ICSI P_L_P features was pretty much identical with  uh  S_R_I features. which are M_F_C_C. So  um  then I thought  ""Oh  great. I'll j- I'll - just let's make sure everything works on the females."" And the error rate - Oh. you know  there was a three percent difference. Uh-huh. So  Is there less training data? uh - No  actually there's more training data. I mean  we don- This is on just digits? No  no. Oh  sorry. O_K. This is on - Oh  O_K. No. No. It's  uh  Swi- Hub-five. This is Hub-five. Hub-five. Yeah. Yeah. Um  and the test data is CallHome and Switchboard. So  uh - so then um - Oh  and plus the - the vocal tract length normalization didn't - actually made things worse. So something's really seriously wrong. So - Um - Aha! Hhh. So - O_K. So - So - but you see  now  between - between the males and the females  there's certainly a much bigger difference in the scaling range  than there is  say  just within the males. And what you were using before was scaling factors that were just from the - the m- the S_R_I front-end. And that worked - that worked fine. That's true. Yeah. Uh  but now you're looking over a larger range and it may not be so fine. Well  um - So - I just - d- so the one thing that I then tried was to put in the low-pass filter  which we have in the - So  most - most Hub-five systems actually band-limit the - uh  at about  uh  thirty-seven hundred  Uh-huh. um  hertz. Although  you know  normally  I mean  the channel goes to four - four thousand. Right? So  um - And that actually helped  uh - Uh-huh. uh  a little bit. Um and it didn't hurt on the males either. So  um - And I'm now  uh  trying the - Oh  and suddenly  also the v- the vocal tract length normalization only in the test se- on the test data. So  you can do vocal tract length normalization Yeah. on the test data only or on both the training and the test. And you expect it to help a little bit if you do it only on the test  and s- more if you do it on both training and test. Yeah. And so the - It now helps  if you do it only on the test  and I'm currently retraining another set of models where it's both in the training and the test  and then we'll - we'll have  hopefully  even better results. So - But there's - It looks like there will still be some difference  maybe between one and two percent  um  Huh. for the females. And so  um  you know  I'm open to suggestions. And it is true that the  uh - that the - Mm-hmm. you know  we are using the - But - it can't be just the V_T_L  Uh-huh. because if you don't do V_T_L in both systems  No - no. I - I remember that. It's much worse. uh  you know  the - Yeah. the females are considerably worse in the - with the P_L_P features. So there must be some - something else going on. Well  what's the standard - ? Yeah  so I thought the performance was actually a little better on females than males. That's what I thought  too. Um  that ye- overall  yes  but on this particular development test set  they're actually a little worse. But that's beside the point. We're looking at the discrepancy between the S_R_I system and the S_R_I system when trained with ICSI features. Right. I'm just wondering if that - What's - Are the freq- ? if - if you have any indication of your standard features  you know  if that's also different or in the same direction or not. You're - This is - lemme ask a q- Cuz - more basic que- I mean  is this  uh - uh  Mm-hmm. iterative  Baum-Welch training? Or is it Viterbi training? Or - ? It's Baum-Welch training. Baum-Welch training. And how do you determine when to - to stop iterating? Um - Well  actually  we - we just basically do a s- a fixed number of iterations. Hmm. Uh  in this case four. Um  which - Eh  we used to do only three  and then we found out we can squeeze - And it was basically  we're s- we're keeping it on the safe side. But you're d- Right. It might be that one more iteration would - would help  but it's sort of you know. Or maybe - or maybe you're doing one too many. I mean it's - it's - No  but with Baum-Welch  there shouldn't be an over-fitting issue  really. @@ Uh. Well  there can be. Sure. Um. Well  you can try each one on a cross-validation set  can't you? It d- if you - if you remember some years ago Bill Byrne did a thing where he was - he was looking at that  and he showed that you could get it. So. Yeah. But - Well  yeah. We can - but - but  um - Well  that's - that's the easy one to check  because Yeah. Do you - ? we save all the intermediate models and we can - And in each case  ho- What - ? um  I'm sorry - in each case how do you determine  you know  the - the usual fudge factors? The  uh - the  uh  language  uh  scaling  acoustic scaling  uh  Um I uh - uh - I'm actually re-optimizing them. Although that hasn't shown to make a big difference. O_K. And the pru- the question he was asking at one point about pruning  uh - Pruning - ? Remember that one? Pruning in the - ? Well  he was - he's - it looked like the probabil- at one point he was looking at the probabilities he was getting out - at the likelihoods he was getting out of P_L_P versus mel cepstrum  and they looked pretty different  Yeah  the likelihoods were lower as I recall. Oh. And so  uh  there's the question - for the P_L_P. I- you mean - did you see this in the S_R_I system? Mm-hmm. Um. Was just looking through the log files  and - Well  the likelihoods are - You can't directly compare them  because  for every set of models you compute a new normalization. And so these log probabilities  they aren't directly comparable because you have a different normalization constants for each model you train. So - Oh. Hmm. But  still it's a question - if you have some threshold somewhere in terms of beam search or something  or - ? Well  yeah. That's what I was wondering. W- yeah. I mean - Uh - I mean  if you have one threshold that works well because the range of your likelihoods is in this area - We prune very conservatively. I mean  as we saw with the meeting data  um we could probably tighten the pruning without really - So we- we basically we have a very open beam. But  you're only talking about a percent or two. Right? Here we're- we're saying that we- there - gee  there's this b- eh  there's this difference here. Yeah. And it - See cuz  i- i- there could be lots of things. Right? But - but - but - but  Right. Course. um  let's suppose just for a second that  uh  we've sort of taken out a lot of the - the major differences  uh  between the two. I mean  we're already sort of using the mel scale and we're using the same style filter integration  and Mm-hmm. Right. and  well  we're making sure that low and high - Actually  there is - the difference in that. So  for the P_L_P features we use the triangular filter shapes. And for the - in the S_R_I front-end we use the trapezoidal one. And what's the top frequency of each? Well  now it's the same. It's thirty - thirty to seven hundred and sixty hertz. Yeah. Exp- one's triangular  one's trapezoidal. So - No  no. But - Before we - i- i- th- with straight P_L_P  it's trapezoidal also. But then we had a slight difference in the - in the scale. Well - But - Uh  so. Since currently the Feacalc program doesn't allow me to change the filter shape independently of the scale. Uh-huh. And  I did the experiment on the S_R_I front-end where I tried the - y- where the standard used to be to use trapezoidal filters. You can actually continuously vary it between the two. And so I wen- I swi- I tried the trap- eh  triangular ones. And it did slightly worse  but it's really a small difference. Hmm. So - Coup- Couple tenths of a percent or something. Right. O_K. So it's not just Yeah  exactly. So  it's not - I don't think the filter shape by itself will make losing some frequency range. Yeah. a huge Right. So the oth- difference. Yeah. the other thing that - So  f- i- We've always viewed it  anyway  as the major difference between the two  is actually in the smoothing  Mm-hmm. that the - that the  um  P_L_P  and - and the reason P_L_P has been advantageous in  uh  slightly noisy situations is because  Mm-hmm. P_L_P does the smoothing at the end by an auto-regressive model  and mel cepstrum does it by just computing the lower cepstral coefficients. Mm-hmm. Um. So  um - O_K. So one thing I haven't done yet is to actually do all of this with a much larger - with our full training set. So right now  we're using a - I don't know  forty? I- i- it's - it's - eh it's a f- training set that's about  um  you know  by a factor of four smaller than what we use when we train the full system. So  Mm-hmm. some of these smoothing issues are over-fitting for that matter. And the Baum-Welch Mm-hmm. should be much less of a factor  if you go full - Could be. Yeah. whole hog. And so  w- so  just um - so the strategy is to first sort of treat things with fast turn-around on a smaller training set and then  when you've sort of  narrowed it down  you try it on a Yeah. larger training set. And so  we haven't done that yet. Now the other que- related question  though  is - is  uh  what's the boot models for these things? Th- th- the boot models are trained from scratch. So we compute  um - So  we start with a  um  alil- alignment that we computed with the b- sort of the best system we have. And - and then we train from scratch. So we com- we do a  you know  w- um - We collect the - uh  the observations from those alignments under each of the feature sets that - that we train. And then  from there we do  um - There's a lot of  actually - The way it works  you first train a phonetically-tied mixture model. Um. You do a total of - First you do a context-independent P_T_M model. Then you switch to a context - You do two iterations of that. Then you do two iterations of - of - of context- dependent phonetically-tied mixtures. And then from that you - you do the - you - you go to a state-clustered model  Yeah. and you do four iterations of that. So there's a lot of iterations overall between your original boot models and the final models. I don't think that - Hmm. We have never seen big differences. Once I thought ""oh  I can - Now I have these much better models. I'll re-generate my initial alignments. Then I'll get much better models at the end."" Made no difference whatsoever. It's - Right. I think it's - eh  i- the boot models are recur- Well  mis- for making things better. Yeah. But  this for making things worse. This it migh- Th- the thought is - is - is possible - another possible partial cause is if the boot models Mm-hmm. used a comple- used a different feature set  Mm-hmm. that - But there are no boot models  in fact. You - you're not booting from initial models. You're booting from initial alignments. Which you got from a different feature set. @@ That's correct. So  those features look at the data differently  actually. Yeah  but - I mean  you know  they - they will find boundaries a little differently  though - You know  all th- all that sort of thing is actually slightly different. I'd expect it to be a minor effect  but - But - but - but  what I'm - what I'm saying is - So  we e- w- f- w- For a long time we had used boot alignments that had been trained with a - with the same front-end but with acoustic models that were  like  fifteen percent worse than what we use now. Mm-hmm. And with a dict- different dictionary - with a considerably different dictionary  which was much less detailed and much less well-suited. Mm-hmm. Yeah. And so  then we switched to new boot alignments  which - which now had the benefit of all these improvements that we've made over two years in the system. Right. And  the result in the end was no different. Right. So  what I'm saying is  the exact nature of these boot alignments is probably not a big factor in the quality of the final models. Yeah  maybe not. But it - it - I st- still see it as - I mean  Yeah. there's - there's a history to this  too  but I - uh  I don't wanna go into  but - Mm-hmm. but I - I - I th- I think it could be Yeah. the things that it - the data is being viewed in a certain way  Right. uh  that a beginning is here rather than there and so forth  because the actual signal-processing you're doing is slightly different. Right. But  Yeah. it's - it's - that's probably not it. Anyway  I - I - I should really reserve  uh  any conclusions until we've done it on the large training set  um  and until we've seen the results with the - with the V_T_L in training. So. Yeah. At some point you also might wanna take the same thing and try it on  Yeah. uh  some Broadcast News data or something else that actually has - has some noisy - noisy components  so we can see if any conclusions we come to holds across different data. Right. Uh - And  uh  with this  I have to leave. Hmm! O_K. With this said. So  is there something quick about Absinthe that you - ? Uh. Just what we were talking about before  which is that I ported a Blass library to Absinthe  and then got - got it working with fast-forward  and got Oh. a speedup roughly proportional to the number of processors times the clock cycle. So  that's pretty good. Oh! Cool. Um  I'm in the process of doing it for Quicknet  but there's something going wrong and it's about half the speed that I was estimating it should be  and I'm not sure why. Mm-hmm. But I'll keep working on it. But the - what it means is that it's likely that for net training and forward passes  we'll - Absinthe will be a good machine. Especially if we get a few more processors and upgrade the processors. A few more processors? How many are you shooting for? There're five now. It can hold eight. Oh  O_K. Yeah  we'll just go buy them  I guess . And it's also five-fifty megahertz and you can get a gigahertz. Yeah. So. Can you mix t- uh  processors of different I don't think so. I think we'd have to do all - speed? O_K. Probably just throw away the old ones  and - Yep. Thank you for the box  and - Oh  O_K. I'll just go buy their process. Hmm! Maybe we can stick them in another system. I dunno. We'd have to get a - almost certainly have to get a  uh  Netfinity server. I see. They're pretty - pretty specialized. Yeah. O_K. O_K. Is - is Liz coming back  do you know  or - ? I don't know. I dunno. Yeah. Oh  you don't. O_K. Uh  I - I will. Alright. I - I actually have to run this stuff. Alright. O_K. See you. Um. Alright. So - Uh  they're having tea out there. So I guess the other thing that we were gonna talk about is - is  uh  demo. And  um  so  these are the demos for the uh  July  uh  meeting and  um - July what? DARPA mee- Early July? Late July? Oh  I think it's July fifteenth. Is that it? Sixteen to eighteen  I think. Yeah  sixteenth  eighteenth. Roughly. Yeah. So  we talked about getting something together for that  but maybe  uh - maybe we'll just put that off for now  given that - But I think maybe we should have a - a sub-meeting  I think  uh  probably  uh  Adam and - and  uh  Chuck and me should talk about - should get together and talk about that sometime soon. Over a cappuccino tomorrow? Yeah something like that. Um  uh  you know  maybe - maybe we'll involve Dan Ellis at some - Mm-hmm. some level as well. Um. O_K. The - the tea is - is going  so  uh  I suggest we do  uh - uh  a unison. A unison digits? O_K. Yeah. Which is gonna be a little hard for a couple people because we have different digits forms. Gets our - Oops. We have a - I found a couple of old ones. Hmm. Oh. Well  that'll be interesting. So  uh - Have you done digits before? No. I haven't done it. O_K. So  uh  the idea is just to read each line with a short pause Alright. between lines  not between - And  uh  since we're in a hurry  we were just gonna read everyone all at once. So  if you sorta plug your ears and read - O_K. So first read the transcript number  and then start reading the digits. Sure. O_K? One  two  three. L_ one fifty. Transcript L_ one four nine. Transcript L_ dash one four seven. Transcript one four three one dash one four five zero. Transcript L_ one forty eight. Transcript two two five one dash two two seven zero. Two one nine one  dash two two one zero. Two  four six nine  four eight  eight two eight  two. One eight four one  seven seven zero zero  seven zero s- two seven. Seven zero two seven. One three one  three seven four  nine three nine three. Two one  five four  eight five  six two  five six. Eight three one. nine seven one Nine six four. Three O_ four  two eight t- O_  O_ seven one. Zero  six six four  two four  three nine seven  eight. O_ O_. zero Zero nine nine zero. One eight  three nine  seven  six three four. Six. one zero two nine seven nine nine Eight one nine. One three seven one  O_  five one two. Six five one - Lemme do that again. One zero. Two. Nine three zero. three one two eight seven four four. One eight four one  seven seven zero zero  seven zero two seven. Four one two. one nine five three  five two zero nine  four seven nine one Six eight eight eight  nine  seven eight zero. Eight five six  three one nine  eight eight seven. Five three one nine four. O_ six two one four. four six six Six six four. five three Six five one  seven three  six two six zero. Zero five five  one four nine  nine five five three. Zero nine five one two zero eight. Seven five. Two O_ five  seven two four  nine one six. six eight seven Nine  eight eight zero  three one  nine one one  eight. Eight. Nine O_ O_. seven O_ five two Three three eight nine  seven  four three one. Zero  eight eight two  one six  eight six seven  one. O_ O_ nine nine eight eight eight. Five two nine eight  eight eight four five  five six two four. eight One. nine zero Two. Eight five eight two  seven  five three two. zero three Three O_ O_. One five three one  one seven one five  three zero five two. One four. one two Two six six one three eight nine. Five zero nine one  nine five three four  two six five six. Five one zero six one zero one. two seven one Nine one  eight four  three six  nine eight  seven two. Three two three eight  seven five four five  seven seven zero zero. three eight O_ five five O_ three Nine nine three  five eight three  six seven three. Three five four four zero six zero. Six five three three. Two one two  four five nine  three two five. four Seven  two six two  one two  six five nine  eight. five Eight zero  five six  zero  eight one eight. Seven seven. Four eight. six O_ nine O_ Five  nine four one  six one  nine five six  seven. Five. Seven three three  seven five  nine zero four one. Eight. eight two four O_ one seven eight Six. Seven O_ nine six nine. Six nine eight  four two  two nine zero zero. Nine. nine O_. two O_ four six. Four four eight  six seven one  four five six. Eight zero  three two  three seven  seven six  zero one. Zero. Two two five eight. Three one. Four four six nine. Five. Six. O_K we're done. And - ","The discussion concerned the revised semantic specification and the construction formalism. The different levels of the latter focus on what construction types are encountered and what bindings there are between them. The notation maintains properties of both dependency and constituent-based grammars. The encoding of features is still incomplete: frame profiles  focus  adjectives  nominal expressions are phenomena in the process of being integrated. Similarly  ways to handle mental spaces will have to be added on top. On the other hand  the semantic specification structures information in terms of ""scenario""  ""referent"" and ""discourse segment"". Each category comprises a number of slots filled in by information derived from the utterance. It is  essentially  a toolkit with which to create semantic constructions  as well as the bindings between them and with the ontology. Among the issues still being defined  mental spaces and context (eg pronoun references) present similarities that can be echoed in the specification. Work on both of these formalisms will continue with circumscription of the construction space that will be studied in more detail. Work on construal will use Bayes-nets  which will be fed information from other modules and implement general rules to infer how utterances are construed. The semantic specification requires some adjustments. Amongst other things ""cause"" has to be added as another X-schema. Linguistic hedging will also be encoded as a demarcation of evidentiality or speaker confidence. Mental spaces can be tackled with mechanisms that can also deal with context issues (time  space etc.): creating a base space and rules of interaction with other interconnected spaces. However  the complexity of these mechanisms has to be bound as well: it is necessary to define the range of constructions to be studied. Given the domains currently used (tourist  child language learning)  some features  like speaker attitude  are not of equal importance at this stage. On the other hand  it was decided for the inheritance between constructions to be left out for now  as the notation can be rendered more elegant later on. Finally  a preliminary presentation on the idea of how to use Bayes-nets for construal will take place in the next meeting. The construction formalism is not yet complete as to the semantic constraints -the terminology has also been met with objections- and does not deal with mental spaces. On their own  constructions can only give limited information regarding mental spaces: forms can provide cues to create a different mental space  but the semantic nuances are defined by context. It is not decided at this stage whether the necessary values should be coded within the construction or as part of construal. Other issues concern focus and stress: focus is seen as an information structure device  but there has been no suggestion as to how to predict its effects or break it down in possible focused elements; as to stress  it may not be useful as a form value  as it shows the focus exponent  but not what the focus is on. Moving to the semantic specification  the analysis still needs mechanisms to deal with causality  as well as mental spaces and bindings between them. Additionally  how referring expressions are linked to referents or even how mental spaces affect this linking are still to lay down in detail. The revised semantic specification and construction formalism are more stable than the previous versions. In the latter  we find both construction types and meaning types along with formal considerations like verb subcategorisation  or the ones a ""directed motion"" construction would dictate. Semantic constraints also come into play. The semantic specification  on the other hand  is split into three levels: ""scenario"" is a list of schemas and bindings between them  which describes the current event in terms of Source-Path-Goal  Container  etc.; ""referent"" is about the entities in the discourse and includes grammatical information and pointers to the ontology; ""discourse segment"" comprises utterance-specific things. Apart from the presentation  JavaBayes can now run through the modified web page of the project. "
"Eight  eight? Three. O_K  we're going. This is three. Yep. Yep. Test. Hmm. Let's see. Move it bit. Test? Test? O_K  I guess it's alright. So  let's see. Yeah  Barry's not here and Dave's not here. Um  I can say about - just q- just quickly to get through it  that Dave and I submitted this A_S_R_U. This is for A_S_R_U. Yeah. So. Um. Yeah  it's - it's interesting. I mean  basically we're dealing with rever- reverberation  and  um  when we deal with pure reverberation  the technique he's using works really  really well. Uh  and when they had the reverberation here  uh  we'll measure the signal-to-noise ratio and it's  uh  about nine D_B. So  um  Hmm. a fair amount of - You mean  from the actual  uh  recordings? k- Yeah. It's nine D_B? Yeah. Um - And actually it brought up a question which may be relevant to the Aurora stuff too. Um  I know that when you figured out the filters that we're using for the Mel scale  there was some experimentation that went on at - at  uh - at O_G_I. Um  but one of the differences that we found between the two systems that we were using  the - the Aurora H_T_K system baseline system and the system that we were - the - the uh  other system we were using  the uh  the S_R_I system  was that the S_R_I system had maybe a  um  hundred hertz high-pass. Yep. And the  uh  Aurora H_T_K  it was like twenty. S- sixty-four. Uh. S- sixty-four. Sixty-four? Uh. Yeah  if you're using the baseline. Is that the ba- band center? No  the edge. The edge is really  uh  sixty-four? For some reason  uh  Yeah . @@ So the  uh  center would be somewhere around Dave thought it was twenty  but. like hundred and - hundred and - hundred - hundred and - maybe - it's like - fi- hundred hertz. But do you know  for instance  h- how far down it would be at twenty hertz? What the - how much rejection would there be at twenty hertz  let's say? At twenty hertz. Yeah  any idea what the curve looks like? Twenty hertz frequency - Oh  it's - it's zero at twenty hertz  right? The filter? Yea- actually  the left edge of the first filter is at sixty-four. So - Sixt- s- sixty-four. So anything less than sixty-four is zero. Mmm. It's actually set to zero? Yeah. What kind of filter is that? Yeah. Is this - oh  from the - from - It - This is the filter bank in the frequency domain that starts at sixty-four. Yeah. Oh  so you  uh - so you really set it to zero  the F_F_T? Yeah  yeah. So it's - it's a weight on the ball spectrum. Triangular weighting. Right. O_K. Um - O_K. So that's - that's a little different than Dave thought  I think. But - but  um  still  it's possible that we're getting in some more noise. So I wonder  is it - @@ Was there - their experimentation with  uh  say  throwing away that filter or something? And  uh - Uh  throwing away the first? Yeah. Um  yeah  we - we've tried including the full - full bank. Right? From zero to four K_. And that's always worse than using sixty-four hertz. Mm-hmm. Right  but the question is  whether sixty-four hertz is - is  uh  too  uh  low. Yeah  I mean  make it a hundred or so? Yeah. I t- I think I've tried a hundred and it was more or less the same  or slightly worse. On what test set? On the same  uh  SpeechDat-Car  Aurora. Um  it was on the SpeechDat-Car. Yeah. So I tried a hundred to four K_. Yeah. Um  So it was - and on - and on the  um  um  T_I-digits also? No  no  no. I think I just tried it on SpeechDat-Car. Mmm. That'd be something to look at sometime because what  um  eh  he was looking at was performance in this room. Mm-hmm. Would that be more like - Well  you'd think that'd be more like SpeechDat-Car  I guess  in terms of the noise. The SpeechDat-Car is more  uh  sort of roughly stationary  a lot of it. And - Yeah. and T_I-digits maybe is not so much as - Yeah. Mm-hmm. Yeah. Mm-hmm. O_K. Well  maybe it's not a big deal. But  um - Anyway  that was just something we wondered about. But  um  uh  certainly a lot of the noise  uh  is  uh  below a hundred hertz. Uh  the Yeah. signal-to-noise ratio  you know  looks a fair amount better if you - if you high-pass filter it from this room. But  um - but it's still pretty noisy. Even - even for a hundred hertz up  it's - it's still fairly noisy. The signal-to-noise ratio is - is - Mm-hmm. is actually still pretty bad. So  um  I mean  the main - the - the - Hmm. So that's on th- that's on the f- the far field ones though  right? Yeah. Yeah  that's on the far field. Yeah  the near field's pretty good. So wha- what is  uh - what's causing that? Well  we got a - a video projector in here  uh  and  uh - which we keep on during every - every session we record  which  you know  I - I - Yeah. w- we were aware of but - but we thought it wasn't a bad thing. I mean  that's a Uh-huh. Yeah. nice noise source. Uh  and there's also the  uh - uh  air conditioning. Hmm. Which  uh  you know  is a pretty low frequency kind of thing. But - but  uh - Mm-hmm. So  those are - those are major components  I think  I see. uh  for the stationary kind of stuff. Mmm. Um  but  um  it  uh - I guess  I - maybe I said this last week too but it - it - it really became apparent to us that we need to - to take account of noise. And  uh  so I think when - when he gets done with his prelim study I think one of the next things we'd want to do is to take this  uh - uh  noise  uh  processing stuff and - and  uh - uh  synthesize some speech from it. And then - When are his prelims? Um  I think in about  um  a little less than two weeks. Oh. Wow. Yeah. Yeah. So. Uh  it might even be sooner. Uh  let's see  this is the sixteenth  seventeenth? Yeah  I don't know if he's before - It might even be in a week. A week  week and a half. So  I- Huh. I - I guessed that they were gonna do it some time during the semester but they'll do it any time  huh? They seem to be - Well  the semester actually is starting up. Is it already? Yeah  the semester's late - late August they start here. Yikes. So they do it right at the beginning of the semester. Yeah. Yeah. So  uh - Yep. I mean  that - that was sort of one - I mean  the overall results seemed to be first place in - in - in the case of either  um  artificial reverberation or a modest sized training set. Uh  either way  uh  i- uh  it helped a lot. And - But if you had a - a really big training set  a recognizer  uh  system that was capable of taking advantage of a really large training set - I thought that - One thing with the H_T_K is that is has the - as we're using - the configuration we're using is w- s- is - being bound by the terms of Aurora  we have all those parameters just set as they are. So even if we had a hundred times as much data  we wouldn't go out to  you know  ten or t- or a hundred times as many Gaussians or anything. So  um  it's kind of hard to take advantage of - of - of big chunks of data. Mmm  yeah. Mm-hmm. Uh  whereas the other one does sort of expand as you have more training data. It does it automatically  actually. And so  um  uh  that one really benefited from the larger set. And it was also a diverse set with different noises and so forth. Uh  so  um  that  uh - that seemed to be - So  if you have that - that better recognizer that can - that can build up more parameters  and if you  um  have the natural room  which in this case has a p- a pretty bad signal-to-noise ratio  then in that case  um  the right thing to do is just do - u- use speaker adaptation. And - and not bother with - with this acoustic  uh  processing. But I think that that would not be true if we did some explicit noise-processing as well as  uh  Mm-hmm. the convolutional kind of things we were doing. So. That's sort of what we found. Hmm. I  um - uh  started working on the uh - Mississippi State recognizer. Oh  O_K. So  I got in touch with Joe and - and  uh  from your email and things like that. And  uh  they added me to the list - uh  the mailing list. And he gave me all of the O_K  great. pointers and everything that I needed. And so I downloaded the  um - There were two things  uh  that they had to download. One was the  uh  I guess the software. And another wad - was a  um  sort of like a sample - a sample run. So I downloaded the software and compiled all of that. And it Eight. compiled fine. No problems. Oh  eh  great. And  um  I grabbed the sample stuff but I haven't  uh  That sample was released only yesterday or the day before  right? compiled it. No - Well  I haven't grabbed that one yet. So there's two. Oh  there is another short sample set - o- o- sample. O_K. There was another short one  yeah. And so I haven't grabbed the latest one that he just  uh  put out yet. Oh  O_K. F- Yeah  O_K. So. Um  but  the software seemed to compile fine and everything  so. And  um  So. Is there any word yet about the issues about  um  adjustments for different feature sets or anything? No  I - I d- You asked me to write to him and I think I forgot to ask him about that. Yeah. Or if I did ask him  he didn't reply. I - I don't remember yet . Uh  I'll - I'll d- I'll double check that and ask him again. Yeah. Yeah  it's like that - that could r- turn out to be an important issue for us. Yeah. Hmm. Mmm. Yeah. Yeah. Cuz they have it - Maybe I'll send it to the list. Yeah. Cuz they have  uh  already frozen those in i- insertion penalties and all those stuff is what - I feel. Because they have this document Uh-huh. explaining the recognizer. And they have these tables with  uh  various language model weights  insertion penalties. u- O_K  I haven't seen that one yet. Uh  it's th- it's there on that web. And  uh  on that  I mean  they have run some experiments using various So. O_K. insertion penalties and all those - And so they've picked - Yeah  I think they pi- p- yeah  they picked the values from - the values. Oh  O_K. O_K. For r- w- what test set? Uh  p- the one that they have reported is a NIST evaluation  Wall Street Journal. But that has nothing to do with what we're testing on  right? You know. No. So they're  like - um - Mm-hmm. So they are actually trying to  uh  fix that - those values using the clean  uh  training part of the Wall Street Journal. Which is - I mean  the Aurora. Aurora has a clean subset. I mean  they want to train it and then this - they're going to run some evaluations. Right. So they're set- they're setting it based on that? Yeah. O_K. So now  we may come back to the situation where we may be looking for a modification of the features to account for the fact that we can't modify these parameters. But  um  Yeah. Yeah. uh - but it's still worth  I think  just - since - you know  just chatting with Joe about the issue. Yeah  O_K. Do you think that's something I should just send to him or do you think I should send it to this - there's an - a m- a mailing list. Um - Well  it's not a secret. I mean  we're  you know  certainly willing to talk about it with everybody  but I think - I think that  um - um  it's probably best to start talking with him just to - O_K. Uh @@ you know  it's a dialogue between two of you about what - you know  what does he think about this and what - what - you know - what could be done about it. Um  Yeah. O_K. if you get ten people in - involved in it there'll be a lot of perspectives based on  you know  how - Yeah. you know. Uh - But  I mean  I think it all should come up eventually  but if - if - Right. O_K. if there is any  uh  uh  way to move in - a way that would - that would  you know  be more open to different kinds of features. But if - if  uh - if there isn't  and it's just kind of shut down and - and then also there's probably not worthwhile bringing it into a larger forum where - where political issues will come in. Yeah. O_K. Oh. @@ So this is now - it's - it's compiled under Solaris? Yeah. Yeah  O_K. Because he - there was some mail r- saying that it's - may not be stable for Linux and all those. Yep. Yeah. Yeah  i- that was a particular version. SUSI yeah. Yeah  SUSI or whatever it was but we don't have that. So. Yeah  yeah. Yeah  O_K. O_K  that's fine. Yeah. Should be O_K. Yeah  it compiled fine actually. No - no errors. Nothing. So. That's good. Uh  this is slightly off topic but  uh  I noticed  just glancing at the  uh  Hopkins workshop  uh  web site that  uh  um - one of the thing- I don't know - Well  we'll see how much they accomplish  but one of the things that they were trying to do in the graphical models thing was to put together a - a  uh  tool kit for doing  uh r- um  arbitrary graphical models for  uh  speech recognition. Hmm. So - And Jeff  uh - the two Jeffs were Who's the second Jeff? Uh - Oh  uh  do you know Geoff Zweig? No. Oh. Uh  he - he  uh - he was here for a couple years and he  uh - got his P_H_D. He - Oh  O_K. And he's  uh  been at I_B_M for the last couple years. Oh  O_K. So. Uh  so he did - he did his P_H_D on dynamic Bayes-nets  uh  Wow. That would be neat. for - for speech recognition. He had some continuity built into the model  presumably to handle some  um  inertia in the - in the production system  and  um - Hmm. So. Hmm. Um  I've been playing with  first  the  um  V_A_D. Um  so it's exactly the same approach  but the features that the V_A_D neural network use are  uh  M_F_C_C after noise compensation. Oh  I think I have the results. What was it using before? Before it was just @@ P_L_Ps. So. Yeah  it was actually - No. Not - I mean  it was just the noisy features I guess. Yeah  yeah  yeah  not compensated . Yeah  noisy - noisy features. Um - This is what we get after - This - So  actually  we  yeah  here the features are noise compensated and there is also the L_D_A filter. Um  and then it's a pretty small neural network which use  um  nine frames of - of six features from C_zero to C_fives  plus the first derivatives. And it has one hundred hidden units. Is that nine frames u- s- uh  centered around the current frame? Or - Yeah. Mm-hmm. S- so  I'm - I'm sorry  there's - there's - there's how many - how many inputs? So it's twelve times nine. Twelve times nine inputs  and a hundred  uh  hidden. Hidden and Two outputs. two outputs. Two outputs. O_K. So I guess about eleven thousand parameters  Mm-hmm. which - actually shouldn't be a problem  even in - in small phones. Yeah. So  I'm - I'm - s- so what is different between this and - It should be O_K. So the previous syst- It's based on the system that has a fifty-three point sixty-six percent improvement. and what you - It's the same system. The only thing that changed is the n- a p- eh - a es- the estimation of the silence probabilities. Ah. O_K. Which now is based on  uh  cleaned features. And  it's a l- it's a lot better. Wow. Yeah. Um - That's great. So it's - it's not bad  but the problem is still that the latency is too large. What's the latency? Because - um - the - the latency of the V_A_D is two hundred and twenty milliseconds. And  uh  the V_A_D is used uh  i- for on-line normalization  and it's used before the delta computation. So if you add these components it goes t- to a hundred and seventy  right? I - I'm confused. You started off with two-twenty and you ended up with one-seventy? With two an- two hundred and seventy. If - Yeah  if you add the c- delta comp- delta computation which is done afterwards. Two-seventy. Oh. Um - So it's two-twenty. I- the- is this - are these twenty-millisecond frames? Is that why? Is it after downsampling? or - The two-twenty is one hundred milliseconds for the um - No  it's forty milliseconds for t- for the  uh  uh  cleaning of the speech. Um - then there is  um  the neural network which use nine frames. So it adds forty milliseconds. a- O_K. Um  after that  um  you have the um  filtering of the silence probabilities. Which is a million filter it   and it creates a one hundred milliseconds delay. So  um - @@ Plus there is a delta at the input. Yeah  and there is the delta at the input which is  One hundred um - milliseconds for smoothing. So it's - Uh  median. @@ - It's like forty plus - forty - plus - Mmm. Forty - And then forty - This forty plus twenty  plus one hundred. forty p- @@ So it's two hundred actually. Uh - Yeah  there are twenty that comes from - There is ten that comes from the L_D_A filters also. Right? Oh  O_K. Uh  so it's two hundred and ten  yeah. If you are using - Uh - Plus the frame  so it's two-twenty. t- If you are using three frames - If you are phrasing f- using three frames  it is thirty here for delta. Yeah  I think it's - it's five frames  but. So five frames  that's twenty. O_K  so it's who un- two hundred and ten. Uh  p- Wait a minute. It's forty - forty for the - for the cleaning of the speech  forty for the I_N_ - A_N_N  a hundred for the smoothing. So. Forty cleaning. Yeah. Well  but at ten -   Twenty for the delta. At th- At the input. I mean  that's at the input to the net. Twenty for delta. Yeah. And there i- Delta at input to net? Yeah. Yeah. So it's like s- five  six cepstrum plus delta at nine - nine frames of - And then ten milliseconds for - Fi- There's an L_D_A filter. ten milliseconds for L_D_A filter  and t- and ten - another ten milliseconds you said for the frame? For the frame I guess. I computed two-twenty - Yeah  well  it's - I guess it's for the fr - the - O_K. And then there's delta besides that? So this is the features that are used by our network and then afterwards  you have to compute the delta on the  uh  main feature stream  which is O_K. um  delta and double-deltas  which is fifty milliseconds. Yeah. No  I mean  the - after the noise part  the forty - the - the other hundred and eighty - Well  I mean  hhh  Wait a minute. Some of this is  uh - is  uh - is in parallel  isn't it? I mean  the L_D_A - Oh  you have the L_D_A as part of the V_D_- uh  V_A_D? Or - The V_A_D use  uh  L_D_A filtered features also. Oh  it does? Mm-hmm. Ah. So in that case there isn't too much in parallel. Uh - No. There is  um  just downsampling  upsampling  and the L_D_A. Um  so the delta at the end is how much? It's - It's fifty. Fifty. Alright. So - But well  we could probably put the delta  um  before on-line normalization. It should not that make a big difference  because - What if you used a smaller window for the delta? Could that help a little bit? I mean  I guess there's a lot of things you could do to - Yeah. Yeah. Yeah  but  nnn - So- Yeah. So if you - if you put the delta before the  uh  ana- on-line - If - Yeah - uh - then - then it could go in parallel. And then y- then you don't have that additive - Mm-hmm. Cuz i- Yep. Yeah  cuz the time constant of the on-line normalization is pretty long compared to the O_K. delta window  so. It should not make - O_K. And you ought to be able to shove tw-   uh - sh- uh - pull off twenty milliseconds from somewhere else to get it under two hundred  right? I mean - Mm-hmm. Is two hundred the d- The hundred milla- mill- a hundred milliseconds for smoothing is sort of an arbitrary amount. It could be eighty and - and probably do @@ - Yeah  yeah. i- a hun- uh - Wh- what's the baseline you need to be under? Well  we don't know. They're still arguing about it. I mean  if it's two - if - if it's  uh - Two hundred? @@ Oh. if it's two-fifty  then we could keep the delta where it is if we shaved off twenty. If it's two hundred  if we shaved off twenty  we could - we could  uh  meet it by moving the delta back. So  how do you know that what you have is too much if they're still deciding? Uh  we don't  but it's just - I mean  the main thing is that since that we got burned last time  and - you know  by not worrying about it very much  we're just staying conscious of it. Uh-huh. Oh  O_K  I see. And so  th- I mean  if - if - if a week before we have to be done someone says  ""Well  you have to have fifty milliseconds less than you have now""  it would be pretty frantic around here. So - Ah  O_K. Uh - But still  that's - that's a pretty big  uh  win. And it doesn't seem like you're - in terms of your delay  you're  uh  that - He added a bit on  I guess  because before we were - we were - had - were able to have the noise  Hmm. uh  stuff  uh  and the L_V_A be in parallel. And now he's - he's requiring it to be done first. Well  but- I think the main thing  maybe  is the cleaning of the speech  which takes forty milliseconds or so. And - Right. Well  so you say - let's say ten milliseconds - seconds for the L_D_A. and - but - the L_D_A is  well  pretty short right now. Yeah. Well  ten. And then forty for the other. Yeah  the L_D_A - L_D_A - we don't know  is  like - is it very crucial for the features  right? No. I just - This is the first try. I mean  I - maybe the L_D_A's not very useful then. Yeah. S- s- h- Right  so you could start pulling back  but - Yeah  l- But I think you have - I mean  you have twenty for delta computation which y- now you're sort of doing twice  right? But yo- w- were you doing that before? Mmm. On the - in the - Mm-hmm. Well  in the proposal  um  the input of the V_A_D network were Just - just three frames  I think. Yeah  just the static  no delta. Uh  static features. Right. So  what you have now is fort- uh  forty for the - the noise  twenty for the delta  and ten for the L_D_A. That's seventy milliseconds @@ of stuff which was formerly in parallel  right? So I think  Mm-hmm. you know  that's - that's the difference as far as the timing  right? Yeah. Um  and you could experiment with cutting various pieces of these back a bit  but - I mean  we're s- we're not - we're not in terrible shape. Yeah  that's what it seems like to me. It's pretty good. Mm-hmm. Yeah. It's - it's not like it's adding up to four hundred milliseconds or something. Where - where is this - where is this fifty-seven point O_ two in - in comparison to the last evaluation? Well  it's - I think it's better than anything  uh  anybody got. Yeah. Oh  is that right? The best was fifty-four Yeah. point five. Point s- Oh. Yeah. Uh- And our system was forty-nine  but with the neural network. Wow. So this is almost ten percent. With the f- with the neural net. Yeah  and r- and - Yeah  so this is - this is like the first proposal. The proposal- one. It was forty-four  actually. It would- Yeah. Yeah. And we still don't have the neural net in. So - so it's - You know. So it's - Wow. We're - we're doing better. I mean  we're getting This is - this is really good. better recognition. I mean  I'm sure other people working on this are not sitting still either  but - but - Yeah. but  uh - Uh  I mean  the important thing is that we learn how to do this better  and  you know. So. Um  Yeah. So  our  um - Yeah  you can see the kind of - kind of numbers that we're having  say  on SpeechDat-Car which is a hard task  cuz it's really  um - I think it's just sort of - sort of reasonable numbers  starting to be. Mm-hmm. I mean  it's still terri- Yeah  even for a well-matched case it's sixty percent error rate reduction  which is - Yeah. Yeah. Probably half. Good! Um  Yeah. So actually  this is in between what we had with the previous V_A_D and what Sunil did with an I_D_L V_A_D. Which gave sixty-two percent improvement  right? Yeah  it's almost that. It's almost an average somewhere around - Yeah. So - Yeah. What was that? Say that last part again? So  if you use  like  an I_D_L V_A_D  o- o- uh  for dropping the frames  Or the best we can get. the best that we can get - i- That means that we estimate the silence probability on the clean version of the utterances. Then you can go up to sixty-two percent error rate reduction  globally. Mmm. Mmm - Yeah. So that would be even - That wouldn't change this number down here to sixty-two? Yeah. Yeah. So you - you were get- If you add a g- good v- very good V_A_D  Yeah. that works as well as a V_A_D working on clean speech  Yeah. then you wou- you would go - So that's sort of the best you could hope for. Mm-hmm. I see. Probably. Yeah. So fi- si- fifty-three is what you were getting with the old V_A_D. Yeah. And  uh - and sixty-two with the - the  you know  quote  unquote  cheating V_A_D. And fifty-seven is what you got with the real V_A_D. Mm-hmm. That's great. Uh  yeah  the next thing is  I started to play - Well  I don't want to worry too much about the delay  no. Maybe it's better to wait O_K. for the decision Yeah. from the committee. Uh  but I started to play with the  um  uh  tandem neural network. Mmm I just did the configuration that's very similar to what we did for the February proposal. And - Um. So. There is a f- a first feature stream that use uh straight M_F_C_C features. Mm-hmm. Well  these features actually. And the other stream is the output of a neural network  using as input  also  these  um  cleaned M_F_C_C. Um - I don't have the comp- Mmm? Those are th- those are th- what is going into the tandem net? Those two? So there is just this feature stream  the fifteen M_F_C_C plus delta and double-delta. No. Yeah? Um  so it's - makes forty-five features that are used as input to the H_T_K. And then  there is - there are more inputs that comes from the tandem M_L_P. Oh  oh. O_K. I see. Yeah  h- he likes to use them both  cuz then it has one part that's discriminative  one part that's not. Uh- huh. Yeah. Um - Right. O_K. So  um  uh  yeah. Right now it seems that - i- I just tested on SpeechDat-Car while the experiment are running on your - on T_I-digits. Well  it improves on the well-matched and the mismatched conditions  but it get worse on the highly mismatched. Um  Compared to these numbers? Compared to these numbers  yeah. Um  like  on the well-match and medium mismatch  the gain is around five percent relative  y- but it goes down a lot more  like fifteen percent on the H_M case. You're just using the full ninety features? @@ The - Y- you have ninety features? i- I have  um - From the networks  it's twenty-eight. So - And from the other side it's forty-five. So it's - you have seventy-three features  So  d- i- It's forty-five. Yeah. Yeah. and you're just feeding them like that. Mm-hmm. There isn't any K_L_T or anything? There's a K_L_T after the neural network  as - as before. That's how you get down to twenty-eight? Yeah. Why twenty-eight? I don't know. Uh. It's - Oh. i- i- i- It's because it's what we did for the first proposal. We tested  Ah. uh  trying to go down and Yeah. It's a multiple of seven. Yeah. Yeah. Yeah. So - Um. I wanted to do something very similar to the proposal as a first - I see. Yeah. Yeah. That makes sense. first try. But we have to - for sure  we have to go down  because the limit is now sixty features. So  Yeah. uh  we have to find a way to decrease the number of features. Um - So  it seems funny that - I don't know  maybe I don't u- quite understand everything  but that adding features - I guess - I guess if you're keeping the back-end fixed. Maybe that's it. Because it seems like just adding information shouldn't give worse results. But I guess if you're keeping the number of Gaussians fixed in the recognizer  then - Well  yeah. But  I mean  just in general  adding information - Mmm. Suppose the information you added  well   was a really terrible feature and all it brought in was noise. Yeah. Right? So - so  um - Or - or suppose it wasn't completely terrible  but it was completely equivalent to another one feature that you had  except it was noisier. Uh-huh. Right? In that case you wouldn't necessarily expect it to be better at all. Oh  yeah  I wasn't necessarily saying it should be better. I'm just surprised that you're getting fifteen percent relative worse Uh-huh. But it's worse. on the wel- On the highly mismatch. Yeah. On the highly mismatched condition. Yeah  I - So  ""highly mismatched condition"" means that in fact your training is a bad estimate of your test. Uh-huh. So having - having  uh  a g- a l- a greater number of features  if they aren't maybe the right features that you use  certainly can e- can easily  uh  make things worse. I mean  you're right. If you have - if you have  uh  lots and lots of data  and you have - and your - your - your training is representative of your test  then getting more sources of information should just help. But - but it's - It doesn't necessarily work that way. Huh. Mm-hmm. So I wonder  um  Well  what's your - what's your thought about what to do next with it? Um  I don't know. I'm surprised  because I expected the neural net to help more when there is more mismatch  as it was the case for the - So  was the training set same as the p- the February proposal? Mm-hmm. @@ Yeah  it's the same training set  so it's TIMIT with O_K. the T_I-digits'  uh  noises  Mm-hmm. uh  added. Um - Well  we might - uh  we might have to experiment with  uh better training sets. Again. But  I - The other thing is  I mean  before you found that was the best configuration  but you might have to retest those things now that we have different - Mm-hmm. The rest of it is different  right? So  um  uh  For instance  what's the effect of just putting the neural net on without the o- other - other path? Mm-hmm. Yeah. I mean  you know what the straight features do. That gives you this. Mm-hmm. You know what it does in combination. You don't necessarily know what - What if you did the - Would it make sense to do the K_L_T on the full set of combined features? Instead of just on the - Yeah. I g- I guess. Um. The reason I did it this ways is that in February  it - we - we tested different things like that  so  having two K_L_T  having just a K_L_T for a network  or having a global K_L_T. Oh  I see. And - So you tried the global K_L_T before and it didn't really - Well - Yeah. And  uh  th- Yeah. The differences between these configurations were not huge  but - I see. it was marginally better with this configuration. Uh-huh. Uh-huh. But  yeah  that's obviously another thing to try  Um. since things are - things are different. And I guess if the - Mm-hmm. Mm-hmm. These are all - so all of these seventy-three features are going into  um  the  uh - the H_M_M. Yeah. And is - are - i- i- are - are any deltas being computed of tha- of them? Of the straight features  yeah. So. n- Not of the - But n- th- the  um  tandem features are u- Are not. used as they are. So  yeah  maybe we can add some context from these features also as - Could. i- Dan did in - in his last work. Yeah  but the other thing I was thinking was  um - Uh  now I lost track of what I was thinking. But. What is the - You said there was a limit of sixty features or something? Mm-hmm. What's the relation between that limit and the  um  forty-eight - Oh  I know what I was gonna say. uh  forty eight hundred bits per second? Um  not - no relation. The f- the forty-eight No relation. So I - I - I don't understand  because i- hundred bits is for transmission of some features. I mean  if you're only using h- And generally  i- it - s- allows you to transmit like  fifteen  uh  cepstrum. The issue was that  um  this is supposed to be a standard that's then gonna be fed to somebody's recognizer somewhere which might be  you know  it - it might be a concern how many parameters are use - u- used and so forth. And so  uh  they felt they wanted to set a limit. So they chose sixty. Some people wanted to use hundreds of parameters and - and that bothered some other people. u- And so Uh-huh. they just chose that. I - I - I think it's kind of r- arbitrary too. But - but that's - that's kind of what was chosen. I - I remembered what I was going to say. What I was going to say is that  um  maybe - maybe with the noise removal  uh  these things are now more correlated. So you have two sets of things that are kind of uncorrelated  uh  within themselves  but they're pretty correlated with one another. Mm-hmm. And  um  they're being fed into these  uh  variants  only Gaussians and so forth  and - and  uh  Mm-hmm. so maybe it would be a better idea now than it was before to  uh  have  uh  one K_L_T over everything  Mm-hmm. to de-correlate it. Yeah  I see. Maybe. You know . What are the S_N_Rs in the training set  TIMIT? It's  uh  ranging from zero to clean? Mm-hmm. Yeah. From zero to clean. Yeah. So we found this - this  uh - this Macrophone data  and so forth  that we were using for these other experiments  to be pretty good. So that's - i- after you explore these other alternatives  that might be another way to start looking  is - is just improving the training set. Mm-hmm. Mm-hmm. I mean  we were getting  uh  lots better recognition using that  than - Of course  you do have the problem that  um  u- i- we are not able to increase the number of Gaussians  uh  or anything to  uh  uh  to match anything. So we're only improving the training of our feature set  but that's still probably something. So you're saying  add the Macrophone data to the training of the neural net? The tandem net? Yeah  that's the only place that we can train. We can't train the other stuff with anything other than the standard amount  so. Yeah. Right. Um  um - What - what was it trained on again? The one that you used? It's TIMIT with noise. Uh-huh. So  yeah  it's rather a small - Yeah. How big is the net  by the way? Um  Uh  it's  uh  five hundred hidden units. And - And again  you did experiments back then where you made it bigger and it - and that was - that was sort of the threshold point. Much less than that  it was worse  and Yeah. Yeah. much more than that  it wasn't much better. Hmm. So is it - is it though the performance  Yeah. @@ ? big relation in the high ma- high mismatch has something to do with the  uh  cleaning up that you - that is done on the TIMIT after adding noise? So - it's - i- All the noises are from the T_I-digits  right? Yeah. So you - i- Um - Well  it- it's like the high mismatch of the SpeechDat-Car They - k- uh - after cleaning up  maybe having more noise than the - the training set of TIMIT after clean - s- after you do the noise clean-up. Mmm. I mean  earlier you never had any compensation  you just trained it straight away. Mm-hmm. So it had like all these different conditions of S_N_Rs  Mm-hmm. actually in their training set of neural net. Mm-hmm. But after cleaning up you have now a different set of S_N_Rs  right? For the training of the neural net. Yeah. Mm-hmm. And - is it something to do with the mismatch that - that's created after the cleaning up  like the high mismatch - You mean the - the most noisy occurrences on SpeechDat-Car might be Mm-hmm. a lot more noisy than - Of - that - I mean  the S_N_R after the noise compensation of the SpeechDat-Car. Oh  so - Right. So the training - the - the neural net is being trained with noise compensated Maybe. @@ Yeah. Yeah  yeah. stuff. Which makes sense  Yeah. but  uh  you're saying - Yeah  the noisier ones are still going to be  Yeah. even after our noise compensation  are still gonna be pretty noisy. Mm-hmm. Yeah  so now the after-noise compensation the neural net is seeing a different set of S_N_Rs than that was originally there in the training set. Of TIMIT. Because in the TIMIT it was zero to some clean. Right. So the net saw all the S_N_R @@ Yes. conditions. Now after cleaning up it's a different set of S_N_R. Right. Right. And that S_N_R may not be  like  com- covering the whole set of S_N_Rs that you're getting in the SpeechDat-Car. Right  but the SpeechDat-Car data that you're seeing is also reduced in noise by the noise compensation. Yeah  yeah  yeah  yeah  it is. But  I'm saying  there could be some - Yeah. So. Mm-hmm. some issues of - Yeah. Well  if the initial range of S_N_R is different  we - the problem was already there before. And - Yeah. Because - Mmm - Yeah  I mean  it depends on whether you believe that the noise compensation is equally reducing the noise on the test set and the training set. Uh - Hmm. On the test set  yeah. @@ Right? I mean  you're saying there's a mismatch in noise Hmm. Mm-hmm. that wasn't there before  but if they were both the same before  then if they were both reduic- reduced equally  Mm-hmm. then  there would not be a mismatch. So  I mean  this may be - Heaven forbid  this noise compensation process may be imperfect  but. Uh  so maybe it's treating some things differently. Well  I - Yeah  uh - I don't know. I - I just - that could be seen from the T_I-digits  uh  testing condition because  um  the noises are from the T_I-digits  right? Noise - Yeah. So - So cleaning up the T_I-digits and if the performance goes down in the T_I-digits mismatch - high mismatch like this - Clean training  yeah. on a clean training  or zero D_B testing. Yeah  we'll - so we'll see. Uh. Maybe. Yeah. Then it's something to do. Mm-hmm. Yeah. I mean  one of the things about - I mean  the Macrophone data  um  I think  you know  it was recorded over many different telephones. Mm-hmm. And  um  so  there's lots of different kinds of acoustic conditions. I mean  it's not artificially added noise or anything. So it's not the same. I don't think there's anybody recording over a car from a car  but - I think it's - it's varied enough that if - if doing this adjustments  uh  and playing around with it doesn't  uh  make it better  the most - uh  it seems like the most obvious thing to do is to improve the training set. Um - I mean  what we were - uh - the condition - It - it gave us an enormous amount of improvement in what we were doing with Meeting Recorder digits  even though there  again  these m- Macrophone digits were very  very different from  uh  what we were going on here. I mean  we weren't talking over a telephone here. But it was just - I think just having a - a nice variation in acoustic conditions was just a good thing. Mm-hmm. Mmm. Yep. Yeah  actually to s- eh  what I observed in the H_M case is that the number of deletion dramatically increases. It - it doubles. Number of deletions. When I added the num- the neural network it doubles the number of deletions. Yeah  so I don't you know how to interpret that  but  mmm - Yeah. Me either. t- And - and did - an- other numbers stay the same? Insertion substitutions stay the same? They p- stayed the same  they - maybe they are a little bit Roughly? uh  lower. Uh-huh. They are a little bit better. Yeah. But - Mm-hmm. Did they increase the number of deletions even for the cases that got better? Say  for the - I mean  it - So it's only the highly mismatched? No  it doesn't. No. And it - Remind me again  the ""highly mismatched"" means that the - Clean training and - Uh  sorry? It's clean training - Well  close microphone training and Close mike training - distant microphone  um  high speed  I think. Well - The most noisy cases are the distant microphone for testing. Right. So - Well  maybe the noise subtraction is subtracting off speech. Wh- Separating . Yeah. But - Yeah. I mean  but without the neural network it's - well  it's better. It's just when we add the neural networks. The feature are the same except that - Yeah  right. Uh  that's right  that's right. Um - Well that - that says that  you know  the  um - the models in - in  uh  the recognizer are really paying attention to the neural net features. Yeah. Mm-hmm. Uh. But  yeah  actually - the TIMIT noises are sort of a range of noises and they're not so much the stationary driving kind of noises  right? It's - it's pretty different. Isn't it? Uh  there is a car noise. So there are f- just four noises. Um  uh  ""Car""  I think  ""Babble."" ""Babble""  ""Subway""  right? and - ""Street"" or ""Airport"" or something. and - ""Street"" isn't - "" Train station""  yeah. Or ""Train station"". Yeah. So - it's mostly - Well  ""Car"" is stationary  Mm-hmm. ""Babble""  it's a stationary background plus some voices  Mm-hmm. some speech over it. And the other two are rather stationary also. Well  I - I think that if you run it - Actually  you - maybe you remember this. When you - in - in the old experiments when you ran with the neural net only  and didn't have this side path  um  uh  with the - the pure features as well  Mm-hmm. did it make things better to have the neural net? Was it about the same? Uh  w- i- It was - b- a little bit worse. Than - ? Than just the features  yeah. So  until you put the second path in with the pure features  the neural net wasn't helping at all. Mm-hmm. Well  that's interesting. It was helping  uh  if the features are b- were bad  I mean. Yeah. Just plain P_L_Ps or M_F_C_Cs. Yeah. But as soon as we added L_D_A on-line normalization  and all these things  then - They were doing similar enough things. Well  I still think it would be k- sort of interesting to see what would happen if you just had the neural net without the side thing. And - and the thing I - I have in mind is  Yeah  mm-hmm. uh  maybe you'll see that the results are not just a little bit worse. Maybe that they're a lot worse. You know? And  um - But if on the ha- other hand  uh  it's  say  somewhere in between what you're seeing now and - and - and  uh  what you'd have with just the pure features  then maybe there is some problem of a - of a  uh  combination of these things  or correlation between them somehow. Mm-hmm. If it really is that the net is hurting you at the moment  then I think the issue is to focus on - on  uh  improving the - the net. Yeah  mm-hmm. Um. So what's the overall effe- I mean  you haven't done all the experiments but you said it was i- somewhat better  say  five percent better  for the first two conditions  and fifteen percent worse for the other one? But it's - but of course that one's weighted lower  so I wonder what the net effect is. Y- yeah  oh. Yeah. I d- I - I think it's - it was one or two percent. That's not that bad  but it was l- like two percent relative worse on SpeechDat-Car. I have to - to check that. Well  I have - I will. Well  it will - overall it will be still better even if it is fifteen percent worse  because the fifteen percent worse is given like f- w- twenty-five - Right. point two five eight. Mm-hmm. Hmm. Right. So the - so the worst it could be  if the others were exactly the same  is four  Is it like - Yeah  so it's four. and - and  uh  in fact since the others are somewhat better - Is i- So either it'll get cancelled out  or you'll get  like  almost the same. Yeah  it was - it was slightly worse. Um  Uh. Slightly bad. Yeah. Yeah  it should be pretty close to cancelled out. Yeah. Mm-hmm. You know  I've been wondering about something. In the  um - a lot of the  um - the Hub-five systems  um  recently have been using L_D_A. and - and they  um - They run L_D_A on the features right before they train the models. So there's the - the L_D_A is - is right there before the H_M_Ms. Yeah. So  you guys are using L_D_A but it seems like it's pretty far back in the process. Uh  this L_D_A is different from the L_D_A that you are talking about. The L_D_A that you - saying is  like  you take a block of features  like nine frames or something  Yeah. Uh-huh. and then do an L_D_A on it  and then reduce the dimensionality to something like twenty-four or something like that. And then feed it to H_M_M. Yeah  you c- you c- you can. I mean  it's - you know  you're just basically i- Yeah  so this is like a two d- two dimensional tile . You're shifting the feature space. Yeah. So this is a two dimensional tile . And the L_D_A that we are f- applying is only in time  not in frequency - high cost frequency. So it's like - more like a filtering in time  rather than Ah. O_K. doing a r- So what i- what about  um - i- u- what i- w- I mean  I don't know if this is a good idea or not  but what if you put - ran the other kind of L_D_A  uh  on your features right before they go into Uh  it - the H_M_M? m- Mm-hmm. No  actually  I think - i- Well. What do we do with the A_N_N is - is something like that except that it's not linear. But Yeah. it's - it's like a nonlinear discriminant analysis. But. Right  it's the - It's - Right. The - So - Yeah  so it's sort of like - The tandem stuff is kind of like i- Yeah. It's - nonlinear L_D_A. I g- Yeah. Yeah. Yeah. Uh. But I mean  w- but the other features that you have  um  th- the non- tandem ones  Mm-hmm. Yeah  I know. That - that - Yeah. Well  in the proposal  they were transformed u- using P_C_A  but - Uh-huh. Yeah  it might be that L_D_A The a- the argument i- is kind of i- in - and it's not like we really know  but the argument anyway is that  um  could be better. uh  we always have the prob- I mean  discriminative things are good. L_D_A  neural nets  they're good. Yeah. Uh  they're good because you - you - you learn to distinguish between these categories that you want to be good at distinguishing between. And P_C_A doesn't do that. It - P_A_C- P_C_A - low-order P_C_A throws away pieces that are uh  maybe not - not gonna be helpful just because they're small  basically. Right. But  uh  the problem is  training sets aren't perfect and testing sets are different. So you f- you - you face the potential problem with discriminative stuff  be it L_D_A or neural nets  that you are training to discriminate between categories in one space but what you're really gonna be g- getting is - is something else. Uh-huh. And so  uh  Stephane's idea was  uh  let's feed  uh  both this discriminatively trained thing and something that's not. So you have a good set of features that everybody's worked really hard to make  Yeah. and then  uh  you - you discriminately train it  but you also take the path that - that doesn't have that  and putting those in together. Uh-huh. And that - that seem- So it's kind of like a combination of the - uh  what  uh  Dan has been calling  you know  a feature - uh  you know  a feature combination versus posterior combination or something. It's - it's  you know  you have the posterior combination but then you get the features from that and use them as a feature combination with these - these other things. And that seemed  at least in the last one  as he was just saying  he - he - when he only did discriminative stuff  Yeah. i- it actually was - was - it didn't help at all in this particular case. There was enough of a difference  I guess  between the testing and training. But by having them both there - The fact is some of the time  the discriminative stuff is gonna help you. Mm-hmm. And some of the time it's going to hurt you  and by combining two information sources if  you know - if - if - Right. So you wouldn't necessarily then want to do L_D_A on the non-tandem features because That i- i- now you're doing something to them that - I think that's counter to that idea. Now  again  it's - we're just trying these different things. We don't really know what's gonna work best. But Yeah  right. if that's the hypothesis  at least it would be counter to that hypothesis to do that. Right. Um  and in principle you would think that the neural net would do better at the discriminant part than L_D_A. Right. Yeah. Well - y- Though  maybe not. Yeah. Exactly. I mean  we  uh - we were getting ready to do the tandem  uh  stuff for the Hub- five system  and  um  Andreas and I talked about it  and the idea w- the thought was  ""Well  uh  yeah  that i- you know - th- the neural net should be better  but we should at least have uh  a number  you know  to show that we did try the L_D_A in place of the neural net  so that we can Right. you know  show a clear path. You know  that you have it without it  then you have the L_D_A  then you have the neural net  and you can see  theoretically. So. I was just wondering - I - I - Well  I think that's a good idea. Yeah. Did - did you do that or - tha- that's a - Um. No. That's what - that's what we're gonna do next as soon as I finish this other thing. So. Yeah. Yeah. No  well  that's a good idea. I - I - i- Yeah. We just want to show. I mean  it - everybody believes it  but you know  we just - Oh  no it's a g- No  no  but it might not - not even be true. I mean  it's - it's - it's - it's - it's a great idea. I mean  Yeah. one of the things that always disturbed me  uh  in the - the resurgence of neural nets that happened in the eighties was that  um  a lot of people - Because neural nets were pretty easy to - to use - Yeah. a lot of people were just using them for all sorts of things without  uh  looking at all into the linear  uh - uh  versions of them. And  Mm-hmm. Yeah. uh  people were doing recurrent nets but not looking at I_I_R filters  and - You know  I mean  uh  so I think  yeah  it's definitely a good idea to try it. Yeah  and everybody's putting that on their systems now  and so  I- that's what made me wonder about Well  they've been putting them in their systems off and on for ten years  but - but - but  uh  this  but. Yeah  what I mean is it's - it's like in the Hub-five evaluations  you know  and you read the system descriptions and And now they all have that. everybody's got  you know  L_D_A on their features. And so. Uh. I see. Yeah. It's the transformation they're estimating on - Well  they are trained on the same data as the final H_M_M are. Yeah  so it's different. Yeah  exactly. Cuz they don't have these  you know  mismatches that - that you guys have. So that's why I was wondering if maybe it's not even a good idea. I don't know. Mm-hmm. Mm-hmm. I - I don't know enough about it  but - Um. Mm-hmm. I mean  part of why - I - I think part of why you were getting into the K_L_T - Y- you were describing to me at one point that you wanted to see if  uh  you know  getting good orthogonal features was - and combining the - the different temporal ranges - was the key thing that was happening or whether it was this discriminant thing  right? So you were just trying - I think you r- I mean  this is - it doesn't have the L_D_A aspect but th- as far as the orthogonalizing transformation  you were trying that at one point  right? Mm-hmm. Mm-hmm. I think you were. Yeah. Does something. It doesn't work as well. Yeah. Yeah. So  yeah  I've been exploring a parallel V_A_D without neural network with  like  less latency using S_N_R and energy  um  after the cleaning up. So what I'd been trying was  um  uh - After the b- after the noise compensation  n- I was trying t- to f- find a f- feature based on the ratio of the energies  that is  cl- after clean and before clean. So that if - if they are  like  pretty c- close to one  which means it's speech. And if it is n- if it is close to zero  which is - So it's like a scale @@ probability value. So I was trying  uh  with full band and multiple bands  m- ps- uh - separating them to different frequency bands and deriving separate decisions on each bands  and trying to combine them. Uh  the advantage being like it doesn't have the latency of the neural net if it - if it can g- And it gave me like  uh  one point - Mm-hmm. One - more than one percent relative improvement. So  from fifty-three point six it went to fifty f- four point eight. So it's  like  only slightly more than a percent improvement  just like - Mm-hmm. Which means that it's - it's doing a slightly better job than the previous V_A_D  Mm-hmm. uh  at a l- lower delay. Mm-hmm. Um  so  um - so - u- But - i- d- I'm sorry  does it still have the median filter stuff? It still has the median filter. So - So it still has most of the delay  it just doesn't - Yeah  so d- with the delay  that's gone is the input  which is the sixty millisecond. The forty plus twenty. At the input of the neural net you have this  uh  f- nine frames of context plus the delta. Well  w- i- Mm-hmm. Oh  plus the delta  right. O_K. Yeah. So that delay  plus the L_D_A. Mm-hmm. Uh  so the delay is only the forty millisecond of the noise cleaning  plus the hundred millisecond smoothing at the output. Mm-hmm. Mm-hmm. Um. So. Yeah. So the - the - di- the biggest - The problem f- for me was to find a consistent threshold that works well across the different databases  because I t- I try to make it work on tr- SpeechDat-Car and it fails on T_I-digits  or if I try to make it work on that it's just the Italian or something  it doesn't work on the Finnish. Mm-hmm. Mm-hmm. So  um. So there are - there was  like  some problem in balancing the deletions and insertions when I try different thresholds. So - Mm-hmm. The - I'm still trying to make it better by using some other features from the - after the p- clean up - maybe  some  uh  correlation - auto-correlation or some s- additional features of - to mainly the improvement of the VAD . I've been trying. Now this - this - this  uh  ""before and after clean""  it sounds like you think that's a good feature. That - that  it - you th- think that the  uh - the - i- it appears to be a good feature  right? Mm-hmm. Yeah. What about using it in the neural net? Yeah  eventually we could - could just Yeah  so - Yeah  so that's the - Yeah. So we've been thinking about putting it into the neural net also. Yeah. Because they did - that itself - Then you don't have to worry about the thresholds and - There's a threshold and - Yeah. Yeah. So that - that's  uh - Yeah. but just - Yeah. So if we - if we can live with the latency or cut the latencies elsewhere  then - then that would be a  Yeah. Yeah. uh  good thing. Um  anybody - has anybody - you guys or - or Naren   uh  somebody  tried the  uh  um  second th- second stream thing? Uh. Oh  I just - I just h- put the second stream in place and  uh ran one experiment  but just like - just to know that everything is fine. Uh-huh. So it was like  uh  forty-five cepstrum plus twenty-three mel - log mel. Yeah. And - and   just  like  it gave me the baseline performance of the Aurora  which is like zero improvement. Yeah. Yeah. So I just tried it on Italian just to know that everything is - But I - I didn't export anything out of it because it was  like  a weird feature set. Yeah. So. Yeah. Well  what I think  you know  would be more what you'd want to do is - is - is  uh  put it into another neural net. Yeah  yeah  yeah  yeah. Mm-hmm. Right? And then - But  yeah  we're - we're not quite there yet. So we have to figure out the neural nets  I guess. Yeah. The uh  other thing I was wondering was  um  if the neural net  um  has any - because of the different noise con- unseen noise conditions for the neural net  where  like  you train it on those four noise conditions  Mm-hmm. while you are feeding it with  like  a- additional - some four plus some - f- few more conditions which it hasn't seen  actually  from the - f- f- while testing. Um - Yeah  yeah. Right. instead of just h- having c- uh  those cleaned up t- cepstrum  sh- should we feed some additional information  like - The - the - We have the V_A_D flag. I mean  should we f- feed the V_A_D flag  also  at the input so that it - it has some additional discriminating information at the input? Hmm- hmm! Um - Wh- uh  the - the V_A_D what? We have the V_A_D information also available at the back-end. Uh-huh. So if it is something the neural net is not able to discriminate the classes - Yeah. I mean - Because most of it is sil- I mean  we have dropped some silence f- We have dropped so- silence frames? No  we haven't dropped silence frames still. Mm-hmm. Uh  still not. Yeah. Yeah. So - the b- b- biggest classification would be the speech and silence. Th- So  by having an additional  uh  feature which says ""this is speech and this is nonspeech ""  I mean  it certainly helps in some unseen noise conditions for the neural net. What - Do y- do you have that feature available for the test data? Well  I mean  we have - we are transferring the V_A_D to the back-end - feature to the back-end. Because we are dropping it at the back-end after everything - all the features are computed. So - Oh  oh  I see. I see. so the neural - so that is coming from a separate neural net or some V_A_D. O_K. O_K. Which is - which is certainly giving a So you're saying  feed that  also  into @@ to - Yeah. So it- it's an - additional discriminating information. the neural net. Yeah. Yeah. Right. You could feed it into the neural net. The other thing you could do So that - is just  um  p- modify the  uh  output probabilities of the - of the  uh  uh  um  neural net  tandem neural net  based on the fact that you have a silence probability. Mm-hmm. Right? Mm-hmm. So you have an independent estimator of what the silence probability is  and you could multiply the two things  and renormalize. Uh  I mean  you'd have to do the Yeah. nonlinearity part and deal with that. Uh  I mean  go backwards from what the nonlinearity would  you know - would be. But - but  uh - Through - t- to the soft max . Yeah  so - maybe  yeah  when - But in principle wouldn't it be better to feed it in? And let the net do that? Well  u- Not sure. I mean  let's put it this way. I mean  y- you - you have this complicated system with thousands and thousand parameters Hmm. Yeah. and you can tell it  uh  "" Learn this thing. "" Or you can say  ""It's silence! Go away! "" I mean  I mean  i- Doesn't - ? I think - I think the second one sounds a lot more direct. Uh. What - what if you - Right. So  what if you then  uh - since you know this  what if you only use the neural net on the speech portions? Well  uh  That's what - Well  I guess that's the same. Uh  that's similar. Yeah  I mean  y- you'd have to actually run it continuously  but it's - @@ - But I mean - I mean  train the net only on - Well  no  you want to train on - on the nonspeech also  because that's part of what you're learning in it  to - to - to generate  that it's - it has to distinguish between. Speech. But I mean  if you're gonna - if you're going to multiply the output of the net by this other decision  uh  would - then you don't care about whether the net makes that distinction  right? Well  yeah. But this other thing isn't perfect. Ah. So that you bring in some information from the net itself. Right  O_K. That's a good point. Yeah. Now the only thing that - that bothers me about all this is that I - I - I - The - the fact - i- i- It's sort of bothersome that you're getting more deletions. Yeah. But - So I might maybe look at  is it due to the fact that um  the probability of the silence at the output of the network  is  uh  Is too high. too - too high or - If it's the case  then multiplying it again by - Yeah. So maybe - So - It may not be - it - i- by something? Mm-hmm. Yeah  it - it may be too - it's too high in a sense  like  everything is more like a  um  Yeah. flat probability. Yeah. So  like  it's not really doing any distinction between speech and nonspeech - or  I mean  different - among classes. Oh-eee-hhh. Uh  yeah. Yeah. Mm-hmm. Be interesting to look at the - Yeah  for the - I wonder if you could do this. But if you look at the  um  highly mism- high mismat- the output of the net on the high mismatch case and just look at  you know  the distribution versus the - the other ones  do you - do you see more peaks or something? Yeah. Yeah  like the entropy of the - the output  or - Yeah. Yeah  for instance. It - it seems that the V_A_D network doesn't - Well  But I - bu- it doesn't drop  uh  too many frames because the dele- the number of deletion is reasonable. But it's just when we add the tandem  the final M_L_P  and then - Yeah. Now the only problem is you don't want to ta- I guess wait for the output of the V_A_D u- before you can put something into the other system  cuz that'll shoot up the latency a lot  right? Am I missing something here? Mm-hmm. But - Yeah. Right. Yeah. So that's maybe a problem with what I was just saying. But - but - I- I guess - But if you were gonna put it in as a feature it means you already have it by the time you get to the tandem net  right? Um  well. We - w- we don't have it  actually  because it's - it has a high rate energy - the V_A_D has a - No. Ah. Yeah. O_K. It's kind of done in - I mean  some of the things are  not in parallel  but certainly  it would be in parallel with the - with a tandem net. Right. In time. So maybe  if that doesn't work  um - But it would be interesting to see if that was the problem  anyway. And - and - and then I guess another alternative would be to take the feature that you're feeding into the V_A_D  Mm-hmm. and feeding it into the other one as well. Mm-hmm. And then maybe it would just learn - learn it better. Um - But that's - Yeah  that's an interesting thing to try to see  if what's going on is that in the highly mismatched condition  it's  um  causing deletions by having this silence probability up - up too high  Mm-hmm. at some point where the V_A_D is saying it's actually speech. Yeah. So  m- Which is probably true. Cuz - Well  the V_A_- if the V_A_D said - since the V_A_D is - is - is right a lot  uh - Yeah. Hmm. Anyway. Might be. Mm-hmm. Yeah. Well  we just started working with it. But these are - these are some good ideas I think. Mm-hmm. Yeah  and the other thing - Well  there are other issues maybe for the tandem  like  uh  well  do we want to  w- uh n- Do we want to work on the targets? Or  like  instead of using phonemes  using more context dependent units? For the tandem net you mean? Hmm. Well  I'm - Yeah. I'm thinking  also  a w- about Dan's work where he - he trained a network  not on phoneme targets but on the H_M_M state targets. And - it was giving s- slightly better results. Problem is  if you are going to run this on different m- Yeah. test sets  including large vocabulary  Yeah. um  Uh - Mmm. I was just thinking maybe about  I think - like  generalized diphones  and - come up with a - a reasonable  not too large  set of context dependent units  and - and - Yeah. And then anyway we would have to reduce this with the K_L_T. So. But - I don't know. Yeah. Yeah. Well  maybe. Mm-hmm. But I d- I d- it - it - i- it's all worth looking at  but it sounds to me like  uh  looking at the relationship between this and the - speech noise stuff is - is - Mm-hmm. is probably a key thing. That and the correlation between stuff. So if  uh - if the  uh  high mismatch case had been more like the  uh  the other two cases in terms of giving you just a better performance  Mm-hmm. how would this number have changed? Oh  it would be - Yeah. Around five percent better  I guess. If - y- Like sixty? if - i- Well  we don't know what's it's gonna be the T_I-digits yet. He hasn't got the results back yet. Yeah. If you extrapolate the SpeechDat-Car well-matched and medium-mismatch  Uh-huh. Yeah. it's around  yeah  maybe five. So this would be sixty-two? Sixty- two. Sixty-two  yeah. Yeah. Somewhere around sixty  must be. Which is - Right? Yeah. Well  it's around five percent  because it's - s- Right? If everything is five percent. Yeah. Yeah. Mm-hmm. All the other ones were five percent  the - I d- I d- I just have the SpeechDat-Car right now  so - Yeah. Yeah. It's running - it shou- we should have the results today during the afternoon  but - Hmm. Well. Hmm. Well - Um - So I won't be here for - When - When do you leave? Uh  I'm leaving next Wednesday. May or may not be in in the morning. I leave in the afternoon. Um  so I - But you're - are you - you're not gonna be around this afternoon? Yeah. Oh  well. I'm talking about next week. I'm leaving - leaving next Wednesday. Oh. This afternoon - uh - Oh  right  for the Meeting meeting? Yeah  that's just cuz of something on campus. Uh-huh. Ah  O_K  O_K. Yeah. But  um  yeah  so next week I won't  and the week after I won't  cuz I'll be in Finland. And the week after that I won't. By that time you'll be - Uh  you'll both be gone from here. So there'll be no - definitely no meeting on - on September sixth. Uh  and - What's September sixth? Uh  that's during Eurospeech. Oh  oh  right. O_K. So  uh  Sunil will be in Oregon. Uh  Stephane and I will be in Denmark. Uh - Right? So it'll be a few weeks  really  before we have a meeting of the same cast of characters. Um  but  uh - I guess  just - I mean  you guys should probably meet. And maybe Barry - Barry will be around. And - and then uh  uh  we'll start up again with Dave and - Dave and Barry and Stephane and us on the  uh  twentieth. No. Thirteenth? About a month? So  uh  you're gonna be gone for the next three weeks or something? I'm gone for two and a half weeks starting - starting next Wed- late next Wednesday. So that's - you won't be at the next three of these meetings. Is that right? Uh  I won't - it's probably four because of - is it three? Let's see  twenty-third  thirtieth  sixth. That's right  next three. And the - the third one won't - probably won't be a meeting  cuz - cuz  uh  Su- Sunil  Stephane  and I will all not be here. Oh  right. Right. Um - Mmm. So it's just  uh  the next two where there will be - there  you know  may as well be meetings  but I just won't be at them. O_K. And then starting up on the thirteenth  uh  we'll have meetings again but we'll have to do without Sunil here somehow. So. When do you go back? Thirty-first  August. Yeah. Yeah. So. Cool . When is the evaluation? November  or something? Yeah  it was supposed to be November fifteenth. Has anybody heard anything different? I don't know. The meeting in - is the five and six of December. p- s- It's like - Yeah  it's tentatively all full . Yeah. So - Mm-hmm. Uh  that's a proposed date  I guess. Yeah  um - so the evaluation should be on a week before or - Yeah. Yep. But  no  this is good progress. So. Uh - O_K. Guess we're done. Should we do digits? Digits? Yep. Transcript L_ dash three five two. Five seven six four  five six seven zero  four six nine three. Six eight five zero  nine one three nine  four six four eight. Three four four two  seven  one eight two. One eight seven four  nine nine eight four  five eight nine seven. One eight three nine  zero one four five  three six two nine. Five four three  six two  six six seven three. Seven one five one  six zero seven two  five nine four two. Nine eight eight  eight one  nine eight one eight. Transcript L_ dash three five three. Seven nine one  one two six  five four two. Eight seven three  nine eight four  nine six four six. Three five  seven four  two two  five nine  six one. Five nine  nine seven  nine eight  five one  eight two. Seven five four five  six six five three  zero one one two. Nine nine zero seven  three  nine two six. Zero one nine  three nine eight  zero three five zero. Two eight six  two zero two  one eight one. Transcript L_ dash three five four. Two nine six  eight six three  seven six zero five. Seven one five six  one three seven zero  four two five six. Nine five three seven  zero  two one eight. One eight six three  nine eight seven one  one zero two nine. Three five three three  four nine three nine  zero three one five. Four zero eight six  four eight  nine five zero  three. Eight zero two eight  nine  seven nine one. Eight nine  nine one  three five  one eight  zero four. Transcr- transcript L_ dash three five zero. Eight four four  two three two  two six one seven. One two  eight three  one nine  nine one  one three. Four five  two four  five nine  six two  three three. Three  eight four six  five five  two zero two  five. Four  six nine three  one three  three six four  six. Two  eight four six  four one  four four six  four. Two nine nine four  three two eight seven  eight seven four two. Four two eight  two zero  seven four eight zero. O_K. It's a wrap. ",The Berkeley Meeting Recorder group discussed recording equipment issues  including the purchase of two additional headsets and the prospect of getting a new base station and a set of wireless microphones to replace those wired microphones currently in use. Speaker fe008 presented the current status on transcriptions  and explained procedures for cleaning up transcripts and ensuring they conform with set conventions. Speaker mn014 briefly described his efforts to normalize loudness levels across speech channels to distinguish between foreground and background speech. Finally  the group discussed legal and procedural issues concerning the provision of transcripts to meeting participants for 'bleeping out' any sections of speech they want excluded from the Meeting Recorder database. The consent form issued to subjects prior to meetings will be revised to make more explicit details concerning access to transcripts and the ability of subjects to mark sections of meetings for exclusion from the database. The group decided to replace wired microphones with a wireless setup  i.e. a new base station and set of wireless microphones. Efforts will be made to ensure that recording conventions are consistent across ICSI  the University of Washington  and SRI. Some of the meeting recordings contain spikes. It was surmised that such disturbances are probably due to the connectors attached to a set of wired microphones in use. With respect to editing bleeps  should participants be allowed to edit out others' speech? It was suggested that making transcripts available to all of the subjects involved might make it too easy for them to edit out sections of meetings  whereas the bias should be to ensure that editing bleeps occur only rarely in the database. Two additional wireless headsets will soon be made available. Approximately 32-35 hours of meeting data have been recorded  roughly 30 hours of which comprise non-digits recordings. The transcribers have begun performing digit extraction (see abstract for Bmr013) and should be finished within a few days. Approximately 11 hours of speech have been transcribed. Efforts by speaker fe008 are in progress to ensure that transcripts are clean (i.e. spell checked)  channelized  and conform to set conventions regarding the coding of numbers  acronyms  and explicit comments (e.g. door slams  coughs  and laughter). Subsequent efforts by speaker fe008 will be to tighten up boundaries on the time bins. Inter-annotator agreement was reported to be very good. Speaker mn014's multi-channel speech/non-speech segmenter is in use. The SRI recognizer will be fed with pre-segmented output to eliminate difficulties with processing overly long speech segments. Efforts by speaker mn014 to normalize loudness and distinguish between foreground and background speech have been largely successful. The group discussed procedural issues concerning the provision of transcripts to meeting participants for 'bleeping out' any sections of speech they want excluded from the Meeting Recorder database. 
"How about channel O_K. We're recording. Yeah  go ahead. Alright. Alright  and no crash. Hmm. I pre-crashed it. Yeah. Pre-crashed! It never crashes on me. I think it's actually - it depends on if the temp files are there or not  What is - what is that? that - at least that's my current working hypothesis  Ah. that I think what happens is it tries to clear the temp files and if they're too big  it crashes. Ah. When the power went out the other day and I restarted it  it crashed the first time. Oh  that's right. After the power out- So then there would be no temp files. Yeah. O_K. Hmm. Uh  no  it doesn't - it doesn't clear those necessarily  so. Oh wait - It - it doesn't clear them  O_K. Hmm  no connection. It's - i- they're called temp files  but they're not actually in the temp directory they're in the scratch  so. They're not backed up  but they're not erased either on power failure. But that's usually the meeting that I recorded  and it neve- it doesn't crash on me. Oh well. Well this wasn't - Actually  this wasn't a- before your meeting  this was  um  Tuesday afternoon when  um  Oh - uh  Robert just wanted to do a little recording  and the power had gone out earlier in the day. Oh  right. O_K. Huh  O_K. I don't know when would be a good excuse for it  but I just can't wait to be giving a talk t- and - and - and use the example from last week with everybody t- doing the digits at once. Yeah. That was fun. I'd love to play somebody that. That was fun. It was quick. It was. It was really efficient. Talk about a good noise shield. You know? You wanted to pe- keep people from listening in  you could like have that playing outside the room. Nobody could listen in. Yeah. Well  I had this idea we could make our whole meeting faster that way. Yeah. Everybody give the reports about what they were doing at exactly the same time  yeah. And we'll just all leave  and - And then we'll - we'll go back later and review the individual channels  right? If you wanna know what - Yep  and then everyone can listen to it later. Yes. Absolutely. Actually isn't that what we have been doing? Yeah. It's what it sounds like. Practically  huh. With all the overlaps. Yeah. What are we doing? I - Since I've been gone all week  I didn't send out a reminder for an agenda  so. Yeah  and I'm just - Do we have anything to talk about or should we just read digits and go? I wouldn't mind hearing how the conference was. What conference? Uh  Yeah  really. I had one question about - Aren't the U_W folks coming this weekend? It's all a blur. Yep. No. The next  right? Next weekend  week from - Next weekend? That is right. The next weekend. Sorry  not - not - not the days coming up  but - It's like the - A week from Saturday. Yeah  within ten days. So  are we - do we have like an agenda or anything that we should be - That's when they're coming. That's correct. No  but that would be a good idea. O_K. Why don't we w- So - so the deal is that I can  um  uh  I can be available after  uh  like ten thirty or something. I don't know how s- how early you wanted to - They're not even gonna be here until eleven or so. Oh  O_K. That's good. So - Wait  this is on - on Sunday? Or Saturday? Cuz they're flying up that day. Saturday. Saturday. Saturday. O_K. Well  y- S- Saturday. Mm-hmm. Yeah. Eurospeech is due on Friday and then I'm going down to San - uh  San Jose Friday night  so  if - you know  if we start nice and late Saturday that's a good thing. No  I mean  they're flying up from - from - Seattle. down from Seattle. They're flying from somewhere to somewhere  Yeah  yeah. and they'll end up here. So b- and also Brian Kingsbury is actually flying from  uh  the east coast on that - that morning. Excellent. Oh. So  i- I - I will be - I mean  he's taking a very early flight and we do have the time work difference running the right way  but I still think that there's no way we could start before eleven. It might end up really being twelve. So when we get closer we'll find people's plane schedules  and let everybody know. Uh  So. That's good. But  uh  yeah maybe an agenda  or at least some things to talk about would be a good idea. Well we can start gathering those - those ideas  but then we - we should firm it up by next - next Thursday's meeting. Will we have time to  um  to prepare something that we - in the format we were planning for the I_B_M transcribers by then  or - ? Oh yeah. Absolutely. O_K. So have you heard back from Brian about that  Chuck? Yes  um  he's - I - I'm sorry  I should have forwarded that along. Uh  oh I - I think I mentioned at the last meeting  he said that  um  he talked to them and it was fine - with the beeps they would be - That's easy for them to do. Great. O_K. So  uh  oh  though Thi- Thilo isn't here  um  but  uh  I - I have the program to insert the beeps. What I don't have is something to parse the output of the channelized transcripts to find out where to put the beeps  but that should be really easy to do. So do we have a meeting that that's been done with  He's - he's - that we've tightened it up to the point where we can actually give it to I_B_M and have them try it out? He generated  um  a channel-wise presegmented version of a meeting  but it was Robustness rather than E_D_U so I guess depends on whether we're willing to use Robustness? Mm-hmm. Well for this experiment I think we can use pre- pretty much anything. This experiment of just - O_K. Well we had - we had talked about doing maybe E_D_U as a good choice  though. Well  whatever we have. Well we've talked about that as being the next ones we wanted to transcribe. Right. But for the purpose of sending him a sample one to - f- I - I don't think it matte- O_K. Yeah  maybe it doesn't matter. Great. I'll - I'll - I'll  um  get - make that available. O_K  and has it been corrected? Oh  well  wait. Um - Hand-checked? Cuz that was one of the processes we were talking about as well. Right  so we need to run That's right. Thilo's thing on it  and then we go in and adjust the boundaries. Yeah that's right. Yeah  we haven't done that. I - I could set someone on that tomorrow. And time how long it takes. Right. O_K. And we probably don't have to do necessarily a whole meeting for that if we just wanna send them a sample to try. I think they're coming - O_K. What would be a good number of minutes? I don't know  maybe we can figure out how long it'll take @@ to - to do. Um  I don't know  it seems to me w- we probably should go ahead and do a whole meeting because we'll have to transcribe the whole meeting anyway sometime. Yes except that if they had - if there was a choice between having fifteen minutes that was fully the way you wanted it  and having a whole meeting that didn't get at what you wanted for them - It's just dependent of how much - Like I - I mean I guess if we have to do it again anyway  but  uh Yeah. I guess  the only thing I'm not sure about is  um  how quickly can the transcribers scan over and fix the boundaries  and - Mm-hmm. I mean  is it pretty easy? I think it's gonna be one or two times real time at - Wow  excuse me  two or more times real time  right? Cuz they have to at least listen to it. Can we pipeline it so that say there's  uh  the transcriber gets done with a quarter of the meeting and then we - you run it through this other - other stuff? Well the other stuff is I_B_ M. Uh  I'm just thinking that from a data - O_K  so. keeping-track-of-the- data point of view  it may be best to send them whole meetings at a time and not try to send them bits and pieces. Oh  that's right. So the first thing is the automatic thing  Right. and then it's - then it's - then it's the transcribers tightening stuff up  and then it's I_B_M. Mm-hmm. Mm-hmm  mm-hmm. Right. O_K  so you might as well ha- run the automatic thing over the entire meeting  and then - and then  uh  you would give I_B_M whatever was fixed. And have them fix it over the entire meeting too? Right. Well  yeah  but start from the beginning and go to the end  right? So if they were only half way through then that's what you'd give I_B_M. O_K. Right? As of what point? I mean. The - I guess the question on my mind is do we wait for the transcribers to adjust the marks for the whole meeting before we give anything to I_B_M  or do we go ahead and send them a sample? Why wouldn't we s- @@ w- i- if they were going sequentially through it  why wouldn't Let their - we give them - I mean i- are we trying to get something done by the time Brian comes? Well I - I - I mean  I don't know. That was the question. Though. So if we - if we were  Yeah. then it seems like giving them something  whatever they had gotten up to  would be better than nothing. Uh. That - I agree. I agree. Well  I don't think - I mean  h- they - they typically work for what  four hours  something like that? Hmm  I gue- hmm. I think the- they should be able to get through a whole meeting in one sitting. I would think  unless it's a lot harder than we think it is  which it could be  certainly. If it's got like for speakers then I guess - I mean if - hhh Or seven or eight. We're just doing the individual channels  right? Individual channels. Yeah. So it's gonna be  depending on the number of people in the meeting  I guess there is this issue of  you know  if - if the segmenter thought there was no speech on - on a particular stretch  on a particular channel  and there really was  um  Well - then  if it didn't show up in a mixed signal to verify  then it might be overlooked  so  I mean  the question is ""should - should a transcriber listen to the entire thing or can it g- can it be based on the mixed signal?"" And I th- eh so far as I'm concerned it's fine to base it on the mixed signal at this point  and - That's what it seems to me too  in that if they need to  just like in the other cases  they can listen to the individual  And that cuts down the time. if they need to. Yeah. But they don't have to for most of it. Yeah  that's good. So. Yeah. Good  good  good. I don't see how that will work  though. What - what aspect? So you're talking about tightening up time boundaries? So how do you - Yeah. So  they have the normal channeltrans interface where they have each individual speaker has their own line  Yeah. but you're listening to the mixed signal and you're tightening the boundaries  correcting the boundaries. You shouldn't have to tighten them too much because Thilo's program does that. Should be pretty good  yeah. Except for it doesn't do well on short things  remember. It will miss them. It will miss most of the really short things. Right  so - so you'll have to I - Uh-huh. Like that. But those would be - those would be - Uh-huh. Uh-huh! It will - it will miss - Yeah  you have to say ""uh-huh"" more slowly to - Sorry. to get c- No  I'm s- I'm actually serious. So it will miss stuff like that which - I'll work on that. Well  so - so that's something that the transcribers will have to - have to do. I - Yeah  but presumably  most of those they should be able to hear from the mixed signal unless they're embedded in the heavil- heavy overlap section when - in which case they'd be listening to the channels anyway. Right  and that's what I'm not sure about. That's - that's what I'm - I'm concerned about the part. Yeah  I am too. And I think it's an empirical question. Can't we - uh couldn't we just have  um  I don't know  maybe this just doesn't fit with the software  but I guess if I didn't know anything about Transcriber and I was gonna make something to let them adjust boundaries  I would just show them one channel at a time  with the marks  and let them adju- Oh they can - Well  but then they have to do - but then they - for this meeting they would have to do seven times real time  Yeah  that's it. and it would probably be more than that. Yeah. Right? Because they'd have to at least listen to each channel all the way through. But i- but it's very quick  right? I mean  you scan - I mean  if you have a display of the waveform. And if - Uh-huh. Oh  you're talking about visually. I just don't think - Yeah. w- Well  the other problem is the breaths cuz you also see the breaths on the waveform. I've - I've looked at the int- uh  s- I've tried to do that with a single channel  and - and you do see all sorts of other stuff besides just the voice. Uh-huh. Yeah  and I - I think that they're going much more on acoustics than they are on visuals. Well that - that I'm not sure. What you - the digital - what the digital task that you had your interface? So. Um  I know for a fact that one of those - sh- she could really well - she could judge what th- what the number was based on the - on the waveform. Yeah  that's actually true. Yeah  you're right. You're absolutely right. Yeah  I found the same thing that when I was scanning through the wave form I could see when someone started to read digits just by the shapes. Yeah  she could tell which one was seven. Um  maybe. Yeah. So I don't - I'm - I'm now entirely confused about what they do. So  they're - they're looking at But - a mixed signal  or they're looking - what - what are they looking at visually? Well  they have a choice. They could choose any signal to look at. I've tried lookin- but usually they look at the mixed. But I've - I've tried looking at the single signal and - and in order to judge when it - when it was speech and when it wasn't  but the problem is then you have breaths which - which show up on the signal. Oh. But the procedure that you're imagining  I mean  people vary from this  is that they have the mixed signal wave form in front of them  Yes. @@ Yes. and they have multiple  uh  well  let's see  there isn't - we don't have transcription yet. So - but there's markers of some sort that have been happening automatically  and those show up on the mixed signal? There's a @@ clicks? Right. Yes. Oh  they show up on the separate ribbons. So you have a separate ribbon for each channel  N- the t- Right. There're separate ribbons. and - and i- i- it'll be - because it's being segmented as channel at a time with his - with Thilo's new procedure  then you don't have the correspondence of the times across the bins - uh across the ribbons uh you could have - And is there a line moving across the waveform as it goes? O_K  so Yes. Yes. The way you're imaging is they kind of play it  and they see oh this happened  then this happened  then - and if it's about right  they just sort of let it slide  Right. Yeah. Right. and if it - if it - there's a question on something  they stop and maybe look at the individual wave form. Right. Oh  well not - not ""look"". Well  they wouldn't look at it at this point. They would just listen. They - they might look at it  right? Well  the problem is that the - the interface doesn't really allow you to switch visuals. Not very quickly. You can but it takes time. The problem is that - that - the Tcl-T_K interface with the visuals  it's very slow to load waveforms. That's it. Uh-huh. And so when I tried - that - that was the first thing I tried when I first started it  Oh  oh. Visually. right? You can - you can switch quickly between the audio  but you just can't get the visual display to show quickly. So you have to - It takes  I don't know  three  four minutes to - Well  I mean  it takes - it takes long enough - Yeah  it's very slow It takes long enough cuz it has to reload the I - I don't know exactly what it's doing frankly cuz - but it t- it takes long enough that it's just not a practical alternative. to do that. That w- Well it - it does some sort of shape pre-computation so that it can then scroll it quickly  yeah. But then you can't But you can cancel that. Yeah. change the resolution or scroll quickly. Oh  really? So. Huh! Now you could set up multiple windows  each one with a different signal showing  and then look between the windows. Maybe that's the solution. I mean  we - we could do different interfaces  right? What if you preload them all? I mean  so - so we could use like X_Waves instead of Transcriber  Yeah. and it loads faster  certainly. What if you were to preload all the channels or - or initially - like doesn't - Well that's what I tried originally. So I - I actually before  uh  Dave Gelbart did this  I did an interface which showed each waveform and ea- a ribbon for each waveform  Mm-hmm. but the problem with it is even with just three waveforms it was just painfully slow to scroll. Oh  O_K. So you just scroll a screen and it would  you know go ""kur-chunk!"" Mm-hmm. And so it just was not doable with the current interface. You know  I am thinking if we have a meeting with only four speakers and  you know  you could fire up a Transcriber interface for  y- you know  in different windows  multiple ones  one for each channel. And it's sort of a - a hack but I mean it would be one way of seeing the visual form. I think that if we decide that we need - that they need to see the visuals  we need to change the interface so that they can do that. Yeah. Yeah. That's actually what I thought of  loading the chopped up waveforms  I mean  An- So - you know  that - that would make it faster - Hmm. But isn't - The chopped up waveforms. Isn't that - So. The problem is if - if anything's cut off  you can't expand it from the chopped up - Right. Right  but if you And wouldn't that be the same as the mixed signal? a- at some point - No  I mean the individual channels that were chopped up that - it'd be nice to be able to go back and forth between those short segments. Cuz you don't really nee- like Mm-hmm. nine tenths of the time you're throwing most of them out  but what you need are tho- that particular channel  or that particular location  and  Yeah. um  Yeah. might be nice  cuz we save those out already  um  to be able to do that. But it won't work for I_B_M of course  it only works here cuz they're not saving out the individual channels. Well  I - I do think that this - this will be a doable procedure  and have them starting with mixed and  um  Yeah. O_K. then when they get into overlaps  just have them systematically check all the channels to be sure that there isn't something hidden from - from audio view. Yeah. Yeah  hopefully  I mean - The mixed signal  the overlaps are pretty audible because it is volume equalized. So I think they should be able to hear. The only problem is - is  you know  counting how many and if they're really correct or not. So   I don't know. I don't know that you can locate them very well from the mixed signal  Right but - but once - once you know that they happen  you can at least listen but you would know that they were there  and then you would switch. Right. And then you would switch into the other - to the close talking  so. But right now  to do this limitation  the switching is going to be switching of the audio? Is what she's saying. Right. Yeah. So - Right  so - so did Dave - so they're using their ears to do these markings anyway. Yes. Yes. Did Dave do that change where you can actually just click rather than having to go up to the menu Yeah. Click - to listen to the individual channels? Um  I had suggested it before. I just don't know whether he did it or not. I'm not sure what - click what - click on the ribbon? Yeah  you can get that - oh  oh  get - you can get the  uh - you can get it to switch audio? Yeah. Yeah. Uh  not last I tried  but  um  maybe he's changed it again. We should get him to do that because  uh  I think that would be much  much faster than going to the menu. I disagree. There's a reason I disagree  and that is that  uh  you - it's very good to have a dissociation between the visual and the audio. There're times when I wanna hear the mixed signal  bu- but I want to transcribe on the single channel. So right now - Then maybe just buttons down at the bottom Maybe  I just don't - I don't see that it's a - next to it. Just something so that it's not in the menu option so that you can do it much faster. Well  I mean  that's the i- I - I think that might be a personal style thing. I find it really convenient the way - the way it's set up right now. Well it just seems to me that if you wanna quickly - ""well was that Jane  no  was that Chuck  no  was that Morgan""  right now  you have to go up to the menu  and each time  go up to the menu  select it  listen to that channel then click below  and then go back to the menu  select the next one  and then click below. That's fine. Yeah  it's true. So you can definitely streamline that with the i- with the interface. Yeah  it could be faster  but  you know  I mean  th- in the ideal world - Yeah. No I - I agree that'd be nice. Yeah. O_K. What? O_K. So  um  Done with that? @@ Does any - I forget  does anybody  uh  working on any - any Eurospeech submission related to this? Hhh! I would like to try to do something on digits but I just don't know if we have time. I mean  it's due next Friday so we have to do the experiments and write the paper. So  I'm gonna try  but  uh  we'll just have to see. So actually I wanna get together with both Andreas and  uh  uh  Stephane with their respective systems. Yeah. Yeah there was that - we- that's right  we had that one conversation about  uh  what - what - what did it mean for  uh  one of those speakers to be pathological  was it a - Right  and I haven't had s- chance to sit down and listen. I was going to do that this afternoon. Oh  I haven't - I haven't listened to them either  but there must be something wrong  I mean  unless our - Well  Morgan and I were - were having a debate about that. Whereas I think it- it's probably something pathologic and actually Stephane's results  I think confirm that. He s- he did the Aurora system also got very lousy average error  like fifteen or - or  uh  fifteen to twenty percent average? But then he ran it just on the lapel  and got about five or six percent word error? So that - that means to me that somewhere in the other recordings there are some pathological cases. But  you know  we - th- that may not be true. It may be just some of the segments they're just doing a lousy job on. So I'll - I'll listen to it and find out since you'd actually split it up by Right. segment. So I can actually listen to it. Yeah. Did you run the - Andreas - the r- S_R_I recognizer on the digits? Yeah. Oh  I thought he had sent that around to everyone  did you just sent that to me? No  I d- I didn't. Oh. Since I considered those preliminary  I didn't. But  yeah  if you take - I- it wasn't - It was bimodal. So if you - Yeah  it's actually  um  it - uh - it was trimodal  actually - Oh  was it trimodal  O_K. Yeah. There's zero  a little bit  and a lot. trimodal  so there were - t- there was - there was one h- one bump at ze- around zero  which were the native speakers  Yeah. Yeah. the non-pathological native speakers. Zero percent error? Y- yeah. Then there was another bump at  um  oh  like fifteen or something. Oh was it fifteen? This is error you're talking about? O_K. whe- Yeah. Yeah. Those were the non-natives. Yeah. And then there was another distinct bump at  like  a hundred  Oh  wow! which must have been some problem. I can't imagine that - Oh  O_K. What is patho- what do you mean by pathological? I'm sorry  I don't - Just - just something really wrong with - In the recording Oh. A bug is what I mean  so that it's like - Oh  O_K. I see. And there was this one meeting  I forget which one it was  where like  uh  six out of the eight channels were all  like - had a hundred percent error. Which probably means like there was a - th- the recording Right. interface crashed  or there was a short - you know  someone was jiggling with a cord or  But - - uh  I extracted it incorrectly  it was labeled - it was transcribed incorrectly  something But - Mm-hmm. really bad happened  and I just haven't listened to it yet to find out what it was. O_K. So  if I excluded the pathological ones  What we're calling. by definition  those that had like over ninety-five percent error rate  and the non-natives  then the average error rate was like one point four or something  Oh. Oh. Hmm! which - which seemed reasonable given that  you know  the models weren't tuned for - Yeah. for it. And the grammar wasn't tuned either. It was just a @@ . And it didn't matter whether it was the lapel or whether it was the - I haven't split it up that way  but it would be - But there's no overlap during the digit readings  so it shouldn't really matter. Right. Right. Yeah. So it should - No  but there's a little difference  and we haven't looked at it for digits  right? And so  There's a lot. Yeah. Yeah  so I was curious about that. cuz - because what he was - what I was saying when I looked at those things is it - it - I was almost gonna call it quadrimodal because - because there was a whole lot of cases where it was zero percent. Mm-hmm. They just plain got it all right. Yeah. And then there - and then there was another bunch that were couple percent or something. But if you p- if you actually histogrammed it  and - it was a nice - Yeah. uh  you know  it - it was - zero was the most of them  but then A normal. Yeah. there were - the others were sort of decaying from there. Yeah  yeah. And then there was the bump for the non-natives and then the pathological ones  so. I see. I see. @@ @@ Yeah  cuz some of our non-natives are pretty non-native. So. Yeah. You - did you have  uh  something in the report about  uh  - about  uh  for- f- uh  forced alignment? Have you - have you started on that? Oh  well  yeah  so I've been struggling with the forced alignments. Um. So the scheme that I drew on the board last time where we tried to  um allow reject models for the s- speech from other speakers  um  most of the time it doesn't work very well. So  um  Hmm. and the - I haven't done - I mean  the only way to check this right now was for me to actually load these into X_Waves and  you know  plus the alignments  and s- play them and see where the - And it looks - And so I looked at all of the utterances from you  Chuck  in that one conversation  I don't know which - You probably know which one I mean  it's where you were on the lapel and Morgan was sitting next to you and we can hear everything Morgan says. Hmm. But - and - and some of what you - I mean  you also appear quite a bit in that cross-talk. So  I actually went through all of those  there were I think fifty-five segments  um  in - in X_Waves   and - and sort of did a crude check  and more often than not  it - it gets it wrong. So there's either the beginning  mostly the beginning word  where th- you  um  you know  Chuck talks somewhere into the segment  but the first  um  word of what he says  often ""I"" but it's very reduced ""I "" that's just aligned to the beginning of someone else's speech  uh in that segment  which is cross-talk. So  um  I'm still tinkering with it  but it might well be that we can't get clean alignments out of this - out of those  uh  channels  so. Unless maybe we do this  uh  um  cancellation business. Right  but that's - I mean  that was our plan  but it's clear from Dan that this is not something you can do in a short amount of time. Yeah  right. Oh  the short amount of time thing  right. So - so we - you know  we had spent a lot of time  um  writing up the H_L_T paper and we wanted to use that  uh  kind of analysis  but the H_L_T paper has  Yeah. you know  it's a very crude measure of overlap. It's not really something you could scientifically say is overlap  it's just whether or not the  um  c- High correlation. the segments that were all synchronized  whether there was some overlap somewhere. And  you know  that pointed out some differences  so he thought well if we can do something quick and dirty because Dan said the cross-cancellation  it's not straight-forward. If it were straight-forward then we would try it  but - so  it's sort of good to hear that it was not straight-forward  thinking if we can get decent forced alignments  then at least we can do sort of a overall report of what happens with actual overlap in time  but  um - I didn't think that his message said it wasn't straight- forward. Well if we'd just - Well Um-hmm. I thought he's just saying you have to look over a longer time window when you do it. and the - but there are some issues of this timing  um  in the recordings and - Yeah. Right. So you just have to look over longer time when you're trying to align the things  you can't - you can't just look - Well. are you talking about the fact that the recording software doesn't do time-synchronous? Is that what you're referring to? @@ That seems to me you can do that over the entire file and get a very accurate - I don't thi- I d- I don't think that was the issue. The issue was that you have - I - yeah  that was sort of a side issue. I didn't think so either. to - you have have - you first have to have a pretty good speech detection on the individual channels. And it's dynamic  so I guess it was more dynamic than some simple models would be able t- to - so - so there are some things available  and I don't know too much about this area where if people aren't moving around much than you could apply them  and it should work pretty well if you took care of this recording time difference. Right  which should be pretty straight forward. Which a- at least is well defined  and Yeah. um  but then if you add the dynamic aspect of adapting distances  then it wasn't - I guess it just wasn't something that he could do quickly and not - in time for us to be able to do something by two weeks from now  so. Well less than a week. So - um  so I don't know what we can do if anything  that's sort of worth  you know  a Eurospeech paper at this point. Well  Andreas  how well did it work on the non-lapel stuff? Yeah. That's what I was gonna say. C- I haven't checked those yet. It's very tedious to check these. Mmm. Um  we would really need  ideally  a transcriber to time mark the - you know  the be- at least the beginning and s- ends of contiguous speech. Um  and  you know  then with the time marks  you can do an automatic comparison of your - of your forced alignments. Oh  M_N_C_M . Because - really the - the - at least in terms of how we were gonna use this in our system was Mm-hmm. to get an ideal - an idea  uh  for each channel about the start and end boundaries. We don't really care about like intermediate word boundaries  so - No  that's how I've been looking at it. I mean  I don't care that the individual words are aligned correctly  but Right. Yeah. Yeah. you don't wanna  uh  infer from the alignment that someone spoke who didn't. @@ so  so - Right  exactly. So that's why I was wondering if it - I mean  maybe if it doesn't work for lapel stuff  we can just not use that and - Yeah. I haven't - I ha- just haven't had the time to  um  do the same procedure on one of the - so I would need a k- I would need a channel that has a speaker whose - who has a lot of overlap but s- you know  is a non-lapel mike. And  um  where preferably  also there's someone sitting next to them who talks a lot. Hmm! So  @@ So a meeting with me in it. I - maybe someone can help me find a good candidate and then I would be willing to We c- you know what? Maybe the best way to find that would be to look through these. you know  hand- Cuz you can see the seat numbers  and then you can see what type of mike they were using. And so we just look for  you know  somebody sitting next to Adam at one of the meetings - Actually y- we can tell from the data that we have  um  yeah  there's a way to tell. It might not be a single person who's always overlapping that person but any number of people  and  From the insertions  maybe? fr- fr- from the - Right. um  if you align the two hypothesis files across the channels  you know  just word alignment  you'd be able to find that. So - so I guess that's sort of a last - ther- there're sort of a few things we could do. One is just do like non-lapels if we can get good enough alignments. Another one was to try to get - somehow align Thilo's energy segmentations with what we have. But then you have the problem of not knowing where the words are because these meetings were done before that segmentation. But maybe there's something that could be done. What - what is - why do you need the  um  the forced alignment for the H_L_T - I mean for the Eurospeech paper? Well  I guess I - I wanted to just do something not on recognition experiments because that's ju- way too early  but to be able to report  you know  actual numbers. Like if we - if we had hand-transcribed pe- good alignments or hand-checked alignments  then we could do this paper. It's not that we need it to be automatic. But without knowing where the real words are  in time - So it was to get - it was to get more data and better - to - to squeeze the boundaries in. To - to know what an overlap really - if it's really an overlap  or if it's just a - Ah  O_K. Yeah. a - a segment correlated with an overlap  and I guess that's the difference to me between like a real paper and a sort of  promissory paper. So  um  if we d- it might be possible to take Thilo's output and like if you have  um  like right now these meetings are all  Ugh! I forgot the digital camera again. Every meeting! um  you know  they're time-aligned  so if these are two different channels and somebody's talking here and somebody else is talking here  just that word  Mm-hmm. if Thilo can tell us that there're boundaries here  we should be able to figure that out because the only thing transcribed in this channel is this word. But  um  you know  if there are things - Two words. Yeah  if you have two and they're at the edges  it's like here and here  and there's speech here  then it doesn't really help you  so  um - Thilo's won't put down two separate marks in that case - Thilo's will. But. Well it w- it would  but  um  we don't know exactly where the words are because the transcriber gave us two words in this time bin and we don't really know  I mean  yeah it's - Well it's a merging problem. If you had a - if you had a s- if you had a script which would - I've thought about this  um  and I've discussed - I've discussed it with Thilo  I mean  if you have any ideas. I would - um  the  I mean  I - I - in principle I could imagine writing a script which would approximate it to some degree  but there is this problem of slippage  yeah. Well maybe - Maybe that will get enough of the cases to be useful. Right. I mean  that - that would be really helpful. That was sort of another possibility. You know s- cuz it seemed like most of the cases are in fact the single word sorts  or at least a single phrase Mmm. Well they - they can be stretched. I wouldn't make that generalization cuz sometimes people will say  ""And then I"" and there's a long pause in most of the bins. Yeah. and finish the sentence and - and sometimes it looks coherent and - and the - I mean it's - it's not a simple problem. But it's really - And then it's coupled with the problem that sometimes  you know  with - with a fricative you might get the beginning of the word cut off and so it's coupled with the problem that Thilo's isn't perfect either. I mean  we've i- th- it's like you have a merging problem plus - so merging plus this problem of  uh  not - Right. Hmm! y- i- i- if the speech-nonspeech were perfect to begin with  the detector  that would already be an improvement  but that's impossible  you know  i- that's too much to ask. Right. Yes. And so i- and may- you know  I mean  it's - I think that there always - th- there would have to be some hand-tweaking  but it's possible that a script could be written to merge those two types of things. I've - I've discussed it with Thilo and I mean - in terms of not him doing it  but we - we discussed some of the parameters of that and how hard it would be to - in principle - to write something that would do that. I mean  I guess in the future it won't be as much as an issue if transcribers are using the tightened boundaries to start with  then we have a good idea of where the forced alignment is constrained to. So I'm no- I don't know if this- Well  it's just  you know  a matter of we had the revolution - we had the revolution of improved  uh  interface  um  one month too late  but it's like  Oh. Tools. Oh it's - it's a - yeah. you know  it's wonderful to have the revolution  so it's just a matter of - of  you know  from now on we'll be able to have things channelized to begin with. Right. And we'll just have to see how hard that is. Yeah  that's right. That's right. So - so whether the corrections take too much time. I was just thinking about the fact that if Thilo's missed these short segments  Yeah. that might be quite time-consuming for them to insert them. Good point. But he - he also can adjust this minimum time duration constraint and then what you get is Yeah. Spurious. noises mostly  but that might be O_K  an- It might be easier to delete something that's wrong than to insert something that's missing. What do you think  Jane? Right. And you can also see in the waveform - exac- yeah. If you can feel confident that what the - yeah  that there's actually something - that you're not gonna miss something  yeah. Yeah. Yeah. Cuz then - then you just delete it  and you don't have to pick a time. I think it's - Well the problem is I - you know - I - I - it's a - it's a really good question  and I really find it a pain in the neck to delete things because you have to get the mouse up there on the t- on the text line and i- and otherwise you just use an arrow to get down - I mean  i- it depends on how lar- @@ th- there's so many extra things that would make it one of them harder than the other  or - or vice versa. It's not a simple question. But  you know  I mean  in principle  like  you know  if one of them is easier then to bias it towards whichever one's easier. Yeah  I guess the semantics aren't clear when you delete a segment  right? Because you would say - You would have to determine what the surroundings were. You could just say it's a noise  though  and write  you know  a post-processor will just - all you have to do is just - If it's really a noise. or just say it's - just put ""X_ "" you know  like ""not speech"" or something  and then you can get - I think it's easier to add than delete  frankly  because you have to  uh  maneuver around on the - on both windows then. Yeah  or- To add or to delete? To delete. O_K. Anyways  so I - I guess - That - Maybe that's an interface issue that might be addressable. But I think it's the semantics that are - that are questionable to me  that you delete something - So let's say someone is talking to here  It's possible. and then you have a little segment here. Well  is that part of the speech? Is it part of the nonspeech? I mean  w- what do you embed it in? There's something nice  though  about keeping  and this is probably another discussion  keeping the stuff that Thilo's detector detected as possible speech and just marking it as not speech than deleting it. Because then when you align it  then the alignment can - you can put a reject model or whatever  and you're consistent with th- the automatic system  whereas if you delete it - Oh  I see. So then they could just like put - Oh that's what you meant by just put an ""X_"" there. Uh  that's an interesting idea. So - so all they - So that all they would have to do is put like an ""X_"" there. Yeah  or some  you know  dummy reject mod- So blank for - blank for silence  ""S_"" for speech  ""X_"" for whatever  yeah. That's actually a better way to do it cuz the a- the forced alignment will probably be more consistent than - something else. Well  like  I think there's a complication which is that - that you can have speech and noise in s- I mean if it's just as easy  but - uh  you know  on the same channel  the same speaker  so now sometimes you get a ni- microphone pop and  uh  I mean  there're these fuzzy hybrid cases  and then the problem with the boundaries that have to be shifted around. It's not a simple - not a simple problem. Anyway  quick question  though  at a high level do people think  let's just say that we're moving to this new era of like using the  um  pre-segmented t- you know  non-synchronous conversations  does it make sense to try to take what we have now  which are the ones that  you know  we have recognition on which are synchronous and not time-tightened  and try to get something out of those for sort of purposes of illustrating the structure and the nature of the meetings  or is it better to just  you know  forget that and tr- I mean  it's - Well  I think we'll have to  eventually. And my hope was that we would be able to use the forced alignment to get it. But if we can't - Right. That was everybody's hope. And maybe we can for the non-lapel  but is it worth - if we can't then we can fake it even if we're - we report  you know  we're wrong twenty percent of the time or ten percent of the time. But if we can't  then maybe we just have to - Well  I'm thinking - are you talking about for a paper  or are talking about for the corpus. Uh - uh  that's a good question actually. I mean cuz for the corpus it would be nice if everything were - Actually that's a good question because we'd have to completely redo those meetings  and we have like ten of them now. We wouldn't have to re- do them  we would just have to edit them. Well  and also  I mean  I still haven't - I still haven't given up on forced alignment. I think that when Brian comes  this'll be uh an interesting aspect to ask him as well b- No  you're right  actually - When - when Brian Kingsbury comes. Oh  Brian. You s- I thought you said Ryan. And it's like  ""Who's Ryan?"" O_K. Yeah  good question. Well  Ryan could come. Uh  no  that's a good point  though  because for feature extraction like for prosody or something  I mean  the meetings we have now  it's a good chunk of data - Yep. we need to get a decent f- O_K. So we should at least try it even if we can't  right? That's what my hope has been  and that's what - that's what - you know  ever since the - the February meeting that I transcribed from last year  forced alignment has been on the - on the table as a way of cleaning them up later. And - and so I'm hopeful that that's possible. I know that there's complication in the overlap sections and with the lapel mikes  but - On the table  right? There's - Yeah. I mean  we might be able  at the very worst  we can get transcribers to correct the cases where - I mean  you sort of have a good estimate where these places are because the recognition's so poor. Right? And so you're - Yeah  we were never just gonna go with these as the final alignments. We were always gonna run them past somebody. I agree. I agree. Yeah. So we need some way to push these first chunk of meetings into a state where we get good alignments. Absolutely. I'm probably going to spend another day or so trying to improve things by  um  by using  um  acoustic adaptation. Um  the - Right now I'm using the unadapted models for the forced alignments  and it's possible that you get considerably better results if you  uh  manage to adapt the  uh  phone models to the speaker and the reject model to the - to - to all the other speech. Um  so Could you - could you at the same time adapt the reject model to the speech from all the other channels? That's what he just said. That's what he was saying. That's what I just said. Yeah. Oh  not just the speech from that - of the other people from that channel  Right. Right. but the speech from the a- actual other channels. Oh  oh  I see. Um  I don't think so. I don't think that would work  right? Because you'd - A lot of it's dominated by channel properties. Oh. No  it - th- Exactly. So you want to u- But what you do wanna do is take the  even if it's klugey  take the segments - the synchronous segments  the ones from the H_L_T paper  where only that speaker was talking. Use those for adaptation  cuz if you - if you use everything  then you get all the cross-talk in the adaptation  and it's just sort of blurred. And that we know  I mean  we have that. And it's about That's a good point. Yep. If you - Yeah. roughly two-thirds  I mean  very roughly averaged. That's not completely negligible. Like a third of it is bad for adaptation or so. Mm-hmm. Cool. I thought it was higher than that  that's pr- It really - it depends a lot. This is just sort of an overall - So. Well I know what we're not turning in to Eurospeech  a redo of the H_L_T paper. Right. That - I don't wanna do that  but. Yeah  I'm doing that for AVIOS. Yeah. But I think we're - oh  Morgan's talk went very well  I think. I think Morgan's talk went very well it woke - you know  it was really a well presented - and got people laughing - Bleep. Uh  ""bleep"". Yeah  really. Excellent. Some good jokes in it? Yeah. Especially the batteried meter popping up  that was hilarious. Right when you were talking about that. Yeah. You know  that wa- that was the battery meter saying that it was fully charged  yeah. It's full. Yeah. You said  ""Speaking about energy""  or something. That was very nice. But that was funny. He - he - he was onto the bullet points about talking about Yeah. Po- low power and - the - you know - the little hand-held  and trying to get lower power and so on  and Microsoft pops up a little window saying ""Your batteries are now fully charged."" Yeah  yeah  yeah. That's great. I'm thinking about scripting that for my talk  you know  put - put a little script in there to say ""Your batteries are low"" right when I'm saying that. Yeah. Yeah. No I mean  i- in - in your case  I mean  you were joking about it  but  I mean  your case the fact that your talking about similar things at a couple of conferences  it's not - these are conferences that have d- really different emphases. Whereas H_L_T and - and Eurospeech  pretty - Are too close  yeah. pretty - pretty similar  so I - I - I can't see really just putting in the same thing  but - No  I d- I don't think that paper is really - the H_L_T paper is really more of a introduction-to-the-project paper  and  um - Yeah. Yeah  for Eurospeech we want some results if we can get them. Well  yeah  it - it's - probably wouldn't make sense  but - Or some - or some - I mean  I would see Eurospeech - if we have some Eurospeech papers  these will be paper p- p- uh  submissions. These will be things that are particular things  aspects of it that we're looking at  rather than  you know  attempt at a global paper about it. Detail  yeah. Overall. Right  right. I did go through one of these meetings. I had  uh  one of the transcribers go through and tighten up the bins on one of the  uh  N_S_A meetings  and then I went through afterwards and double-checked it so that one is really very - Oh. very accurate. I men- I mentioned the link. I sent - You know that one? Oh  so - The - which one? I'm sorry. Um  I'm trying to remember - I don't remember the number off hand. It's one of the N_S_A's. I sent email before the conference  before last week. Those are all - Oh  O_K. That might - might have been the one - one of the ones that we did. Bef- What I mean is Wednesday  Thursday. Mm-hmm. I'm sure that that one's accurate  I've been through it myself. O_K. So that might actually be useful but they're all non-native speakers. So we could compare before and after and see - Yeah. Yeah  that's what I was gonna say. The problem with those  they're all German. So. Yeah  that's the problem with the N_S_A speakers. oh  And e- and e- and extremely hard to follow  like word-wise  I bet the transcri- I mean  I have no idea what they're talking about  so  um  Darn! Yeah. I corrected it for a number of the words. I'm sure that  um  they're - they're accurate now. Uh  actually I have to - I mean  this is tough for a language model probably - to go. Right. but - but that might be useful just for Well. O_K  Andreas is leaving - leaving the building. speech. Mm-hmm. Yeah. See ya. See ya. Um  oh  before you l- go - I don't think we'll go much longer. I guess it's alright for you to talk a little without the mike - I noticed you adjusting the mike a lot  did it not fit you well? Oh. Well I won- I noticed when you turned your head  it would - it would tilt. Maybe it wasn't just tightened enough  or - Maybe the - yeah  the s- thing that you have tightened @@   oh. Actually if - if you have a larger head  that mike's gotta go farther away which means the - the balance is gonna make it wanna tip down. O_K. Anyway. Yeah. O_K  see ya. Cuz  I'm just thinking  you know  we were - we're - we've been talking about changing the mikes  @@ Yeah. uh  for a while  and if these aren't - acoustically they seem really good  but if they're not comfortable  we have the same problems we have with these stupid things. I think it's com- This is the first time I've worn this  I find it very comfortable. I find it very comfortable too  but  uh  it looked like Andreas was having problems  and I think Morgan was saying it - Well  but I had it on - I had it on this morning and it was fine. Can I see that? Oh  oh you did wear it this morning? O_K  it's off  so you can put it on. Yeah. I - yeah  I don't want it on  I just - I just want to  um  say what I think is a problem with this. If you are wearing this over your ears and you've got it all the way out here  then the balance is gonna want to pull it this way. Where as if somebody with a smaller head has it back here  Yeah. Right. It's more balanced. So we have to right? Yeah. Then it - then it falls back this way so it's - Oh! Well wh- what it's supposed to do is the backstrap is supposed to be under your crown  and so that should be - should be - if it's right against your head there  which is what it's supposed to be  that balances it Ah. so it doesn't slide up. Yep  right - right below - if you feel the back of your head  you feel a little lump  So this is supposed to be under that little protuberance. um  and so it's supposed to be right under that. Yeah. So it's really supposed to go more like this than like this. But then isn't that going to - Well  I guess you can control that. Yes  exactly. That - that - that tilts  right? In lots and lots of different ways. So I'm not saying anything about bias towards small headsize  but About heads? It would be an advantage. does seem  uh - Well  wonder if it's - if - if he was wearing it over his hair instead of under his hair. Well  we should - I think probably it was - We shou- we should work on compressing the heads  and - Yeah. It probably just wasn't tight enough to the back of his head. I mean  so the directions do talk about bending it to your size  The other thing that would do it would be to hang a five pound weight off the back. which is not really what we want. Yeah that's good! Right. What did you say? wh- A little  um  Hang a five pound weight off the - off the back. Weight. We did that - Hang a five pound weight off the back. We - at Boeing I used - I was doing augmented reality so they had head-mounts on  and we - we had a little jury-rigged one with a welder's helmet  and we had just a bag Counter-balance. with a bunch of marbles in it as a counter-balance. Or maybe this could be helpful just for evening the conversation between people. If people - those who talk a lot have to wear heavier weights or something  and - and - Yeah! um  Anyway. um  so  uh  what was I gonna say? Oh  yeah  I was gonna say  uh  I had these  uh  conversations with NIST folks also while I was there and - and  uh  Yep. um  so they - they have their - their plan for a room  uh  with  um  mikes in the middle of the table  and  uh  close-mounted mikes  and they're talking about close-mounted and lapels  just cuz sort of - and the array. Yeah  so they were - And arrays  which is the i- interesting - and video  right. And arrays  yep. And cameras. And yeah  like multiple - multiple video cameras coverin- covering every - everybody - every place in the room  uh  the - yeah - the - the mikes in the middle  the head-mounted mikes  the lapel mikes  the array  uh  with - well  there's some discussion of fifty-nine  they might go down to fifty-seven Fifty-nine elements. Because  uh  there is  uh  some pressure from a couple people at the meeting for them to use a KEMAR head. I forget what KEMAR  uh  stands for  but what it is is it's dummy head that is very specially designed  and - Mm-hmm. Oh  that's right. Yep. Right. That's a great idea. and - and  so what they're actually doing is they're really - there's really two recording systems. So they may not be precisely synchronous  but the- but there's two - two recording systems  one with  I think  twenty-four channels  and one with sixty-four channels. And the sixty-four channel one is for the array  but they've got some empty channels there  and anyway they - like they're saying they may give up a couple or something if - for - for the KEMAR head if they go - go with that. Right. Yeah  it is a good idea. Yeah  h- uh  J- Jonathan Fiscus did say that  So. uh  they have lots of software for doing calibration for skew and offset between channels and that they've found that's just not a big deal. Mm-hmm Yeah. So. Yeah  I'm not too worried about that. I was thinking - But they're still planning to do like fake - they have to do something like that  right. Scenario-based. Y- right. Their - their legal issues won't allow them to do otherwise. Yeah. But it sounded like they were pretty well thought out and they're - they're gonna be real meetings  it's just that they're with str- with people who would not be meeting otherwise. So. Yeah  th- that's true. Mm-hmm. Mm-hmm. Did - did they give a talk on this or was this informal? No. It's just informal. No. No  we just had some discussions  various discussions Mm-hmm. Mm-hmm. Yeah. Yeah  I also sat and chatted with several of the NIST folks. They seemed like a good group. with them. What was the  um - the paper by  um  Lori Lamel that you mentioned? Um  Mmm  yeah. yeah  we sh- we should just have you - have you read it  but  I mea- ba- i- i- uh  we've all got these little proceedings  but  um  basically  it was about  um  uh  going to a new task where you have insufficient data and using - using data from something else  and adapting  and how well that works. Uh  so in - in fact it was pretty related to what Liz and Andreas did  uh  except that this was not with meeting stuff  it was with Right. uh  like I think they s- didn't they start off with Broadcast News system? And then they went to - The- their Broadcast News was their acoustic models and then all the other tasks were much simpler. Yeah. So they were command and control and that sort of thing. T_I-digits was one of them  Yep. and  uh  Wall Street Journal. What was their Yeah  read Wall Street Journal. rough - what was their conclusion? It works. Yeah. Well  it's - it's a good paper  I mean - Yeah. Yeah  yeah. Yeah  that was one of the ones that I liked. That - It not only works  in some cases it was better  which I thought was pretty interesting  but that's cuz they didn't control for parameters. Bring the - So. You know  the Broadcast News nets were - not nets  acoustic models were a lot more complex. Probably. Right. Did they ever try going - going the other direction from simpler task to more complicated tasks  or - ? n- Not in that paper. That might be hard. Yeah  well  one of the big problems with that is - is often the simpler task isn't fully - doesn't have all the phones in it  and that - that makes it very hard. Yeah. Mm-hmm. Yeah. But I've done the same thing. I've been using Broadcast News nets for digits  Yeah. like for the spr- speech proxy thing that I did? That's what I did. Yeah  sure. So. It works. Yeah. Yeah  and they have - I mean - they have better adaptation than we had than that - that system  so they - Yep. um  You mean they have some. yeah  we should probably what would - actually what we should do  uh  I haven't said anything about this  but probably the five of us should pick out a paper or two that - that  uh  you know  got our interest  and we should go around the room at one of the Tuesday lunch meetings and say  you know  what - what was good about the conference  yeah. Present. Yep. Do a trip report. Well  the summarization stuff was interesting  I mean  I don't know anything about that field  but for this proposal on meeting summarization  um  I mean  it's sort of a far cry because they weren't working with meeting type data  but Right. he got sort of an overview on some of the different approaches  Do you remember who the groups were that we're doing? so. Well there're - this was the last day  but  I mean  there's - that's a huge field and probably the groups there may not be representative of the field  I - I don't know exactly A lot of different ones. R- I think - Mm-hmm. that everyone submits to this particular conference  but Was - were there folks from B_B_N presenting? yet there was  let's see  this was on the last day  Mitre  B_B_N  Mitre  B_B_N  I_B_M. and  um  Prager - Uh  Maryland. um  I wo- it was - no it was - this was Wednesday morning. Columbia have anything? No. Wasn't - Who - who - who did the order one? The sentence ordering one  was that Barselou   and these guys? Ugh! I'm just so bad at that. Oh. Anyway  I - I - it's in the program  I should have read it to remind myself  but that's sort of useful and I think like when Mari and Katrin and Jeff are here it'd be good to figure out some kinds of things that we can start doing maybe just on the transcripts cuz we already have - Mm-hmm. Yeah  we do have word transcripts. So. you know  yeah. Well  I like the idea that Adam had of - of  um  z- maybe generating minutes based on some of these things that we have because it would be easy to - to - to do that just  Right. you know  and - and it has to be  though  someone from this group because of the technical nature of the thing. Someone who actually does take notes  um  I'm very bad at note-taking. But I think what's interesting is there's all these different evaluations  like - I always write down the wrong things. just  you know  how do you evaluate whether the summary is good or not  and that's what's - was sort of interesting to me is that there's different ways to do it  and - I do take notes. A judge. Yep. Was S_R_A one of the groups talking about summarization  no? Hm-umm . No. It was an interesting session. One of those w- And as I said  I like the Microsoft talk on scaling issues in  uh  word sense disambiguation  that was interesting. Yeah. Yeah  that was an interesting discussion  uh  The - I It - it - it was the only one - The data issue comes up all the ti- It was the only one that had any sort of real disagreement about. So. Well  I didn't have as much disagreement as I would have liked  but I didn't wanna - I wouldn- I didn't wanna get into it because  uh  you know  it was the application was one I didn't know anything about  uh  it just would have been  you know  me getting up to be argumentative  but - Yep. but  uh  I mean  the missing thi- so - so what they were saying - it's one of these things - is - you know  all you need is more data  sort of - But I mea- i- wh- it - @@ that's - that's dissing it  uh  improperly  I mean  it was a nice study. Uh  they were doing this - it wasn't word-sense disambiguation  it was - was it w- was it word-sense? Yes. Well  it sort of was. Yeah - yeah - yeah - But it was - it was a very simple case of ""to"" versus ""too"" versus ""two"" and ""there""  ""their""  ""they're"" - And there and their and - and that you could do better with more data  I mean  that's clearly statistically - Yeah  yeah. O_K. Right. Yeah. And so  what they did was they had these different kinds of learning machines  and they had different amounts of data  and so they did like  you know  eight different methods that everybody  you know  uh  argues about - about  ""Oh my - my kind of learning machine is better than your kind of learning machine."" And  uh  they were - started off with a million words that they used  which was evidently a number that a lot of people doing that particular kind of task had been using. So they went up  being Microsoft  they went up to a billion. And then they had this log scale showing a - @@ you know  and - and naturally everything gets - Them being beep  they went off to a billion. they - well  it's a big company  @@ I didn't - I didn't mean it as a ne- anything negative  but i- i- i- Yeah. You mean the bigger the company the more words they use for training? Well  I think the reason they can do that  is that they assumed that text that they get off the web  like from Wall Street Journal  is correct  and edit it. So that's what they used as training data. It's just saying if it's in this corpus it's correct. Yeah. O_K. But  I mean  yes. Of course there was the kind of effect that  you know  one would expect that - uh - that you got better and better performance with more and more data. Um  but the - the real point was that the - the different learning machines are sort of all over the place  and - and by - by going up significantly in data you can have much bigger effect then by switching learning machines and furthermore which learning machine was on top kind of depended on where you were in this picture  so  uh  This was my concern about the recognizer in Aurora. That - That the differences we're seeing in the front-end Yeah. Are irrelevant. is b- are irrelevant once you get a real recognizer at the back-end. Yeah. If you add more data? Or - Yeah. You know? Huh. Yeah  could well be. So - so  I mean  that was - that was kind of  you know  it's a good point  but the problem I had with it was that the implications out of this was that  uh  the kind of choices you make about learning machines were therefore irrelevant which is not at - n- t- as for as I know in - in tasks I'm more familiar with @@ is not at all true. What i- what is - is true is that different learning machines have different properties  and you wanna know what those properties are. And someone else sort of implied that well we s- you know  a- all the study of learning machine we still don't know what those properties are. We don't know them perfectly  but we know that some kinds use more memory and - and some other kinds use more computation and some are - are hav- have limited kind of discrimination  but are just easy to use  and others are - But doesn't their conclusion just sort of - you could have guessed that before they even started? Because if you assume that these learning things get better and better and better  You would guess - then as you approach - there's a point where you can't get any better  right? You get everything right. So they're all approaching. Yeah. It's just no - But - No  but there was still a spread. They weren't all up- They weren't converging. They were all still spread. It w- But what I'm saying is that th- they have to  as they all get better  they have to get closer together. But they - Right  right. Sure. But they hadn't even come close to that point. All the tasks were still improving when they hit a billion. Yeah. But they're all going the same way  right? So you have to get closer. Eventually. O- one would thi- But they didn't get closer. Oh they didn't? They just switched position. Well - well that's getting cl- I mean  yeah  the spread was still pretty wide that's th- that's true  but - but  uh  Yep. I think it would be irntu- intu- intuition that this would be the case  but  uh  to really see it and to have the intuition is quite different  I mean  I think somebody w- w- let's see who was talking about earlier that the effect of having a lot more data is quite different in Switchboard than it is in - in Broadcast News  yeah. Well it's different for different tasks. So it depends a lot on whether  you know  it - disambiguation is exactly the case where more data is better  right? You're - Yeah. It was Liz. Yeah. Yeah. you're - you can assume similar distributions  but if you wanted to do disambiguation on a different type of  uh  test data then your training data  then that extra data wouldn't generalize  so. Right. But  I think one of their p- Right. They - they had a couple points. w- Uh  I think one of them was that ""Well  maybe simpler algorithms and more data are - is better"". Less memory  faster operation  simpler. Right? Because their simplest  most brain-dead algorithm did pretty darn well Mm-hmm. when you got - gave it a lot more data. And then also they were saying  ""Well  m- You have access to a lot more data. Why are you sticking with a million words?"" I mean  their point was that this million-word corpus that everyone uses is apparently ten or fifteen years old. And everyone is still using it  so. Yeah. But anyway  I - I - I think it's - it's just the - the i- it's - it's - it's not really the conclusion they came to so much  But we could talk about this stuff  I think this would be fun to do. Ha! as the conclusion that some of the  uh  uh  commenters in the crowd came up with that  uh  you know  this therefore is further evidence that  you know  more data is really all you should care about  and that I thought was just kind of Right. going too far the other way  and - and the - the  uh  Machine-learning. one - one person ga- g- g- got up and made a - a brief defense  uh  but it was a different kind of grounds  it was that - that  uh  i- w- the reason people were not using so much data before was not because they were stupid or didn't realize data was important  but in fact th- they didn't have it available. Um  but the other point to make a- again is that  uh  machine learning still does matter  but it - it matters more in some situations than in others  and it - and also there's - there's not just mattering or not mattering  but there's mattering in different ways. I mean  you might be in some situation where you care how much memory you're using  Right. or you care  you know  what recall time is  or you care  you know  and - and - Or you only have a million words Yeah  or - or  uh - for your - some new task. Or done another language  or - I mean  you - so there's papers on portability and rapid prototyping and blah-blah-blah  and then there's people saying  ""Oh  just add more data."" So  these are like two different religions  basically. Yep. Yeah. Right. Yeah. And there's cost! Mm-hmm. Cost. Yeah. That's a big one. There's just plain cost  you know  so - so these  I mean th- the - in the - in the speech side  the thing that @@ always occurs to me is that if you - if you - uh - one person has a system that requires ten thousand hours to train on  and the other only requires a hundred  and they both do about the same because the hundred hour one was smarter  that's - that's gonna be better. Yep. because people  I mean  there isn't gonna be just one system that people train on and then that's it for the r- for all of time. I mean  people are gonna be doing other different things  and so it - these - these things matters - matter. Yeah  that's it. So  I mean  this was a very provocative slide. She put this up  and it was like this is - Yeah  so that's one of the slides they put up. Yeah. this p- people kept saying  ""Can I see that slide again?"" and then they'd make a comment  and one person said  Yeah  yeah. well-known person said  um  you know  ""Before you dismiss forty-five years including my work -"" Forty-five years of research. Yeah. Yeah. But th- you know  the same thing has happened in computational linguistics  right? You look at the A_C_L papers coming out  and now there's sort of a turn back towards  O_K we've learned statistic - you know  we're basically getting what we expect out of some statistical methods  and  you know  the- there's arguments on both sides  so - Yep. I think the matters is the thing that - that was misleading. That was very offending  very offending. Yeah  yeah. Is that - all - all of them are based on all the others  right? Just  you - you can't say - Right. Maybe they should have said ""focus"" or something. Yeah. I mean  so. - And I'm saying the same thing happened with speech recognition  right? For a long time people were hand-c- coding linguistic rules and then they discovered machine-learning worked better. And now they're throwing more and more data and worrying - perhaps worrying less and less about  uh  the exact details of the algorithms. And - and then you hit this - Except when they have a Eurospeech paper. Yeah. Yeah. Anyway. Anyway  tea is - tea is  uh  starting. Shall we read some digits? Are we gonna do one at a time? Or should we read them all agai- at once again. Let's do it all at once. We - @@ - let's try that again. Yes! Yeah  that's good. O_K. So  and maybe we won't laugh this time also. So remember to read the transcript number so that  uh  everyone knows that - what it is. And ready? Three  two  one. Yeah. Transcript L_ twenty-three Transcript L_ twenty-four. L_ twenty-two. Transcript L_ two five. Transcript L_ twenty-eight. Transcript L_ dash two seven. two two four five two three three nine two seven four three five five one O_ four two three two zero two one seven four one nine three nine three zero six nine one three three four three seven seven nine four three zero three four eight six nine five three six five five five three three three eight seven eight four three two nine four eight seven five three nine five six four three eight five five zero zero seven two seven one six nine nine five nine three O_ eight two one six two two one nine seven nine three two one eight nine nine nine six seven nine four three nine five seven five six three six seven six two two eight four five eight four eight nine one three three eight seven five two four O_ two four eight O_ eight six nine four seven three one six two two eight seven one zero eight nine three zero seven nine four eight eight nine one O_ eight one eight eight four four three three four zero four nine one eight five one five one O_ two four four six nine eight nine five eight six eight eight eight eight four seven three five one one six seven seven six four three three five six seven O_ three nine nine one two seven zero two three eight seven eight three three six nine nine three one nine two three O_ nine seven seven five nine nine six seven eight six eight two five eight six three eight two one five O_ eight nine seven nine eight one seven four six nine one nine four zero three six one eight zero seven one six three six seven two five O_ two nine eight five zero zero zero two four eight four two zero zero four two nine four six four four two nine one eight eight two three six two five seven zero three two five nine eight two O_ eight eight eight O_ five nine seven five nine two six three seven eight eight two seven nine two one eight six six three O_ seven one five one six six two two zero three eight three two zero nine five five two two nine one five zero two one zero three eight two zero three four three two two zero nine nine three four O_ eight three O_ five seven seven nine six five four five three five five two seven one one six two nine two four zero two zero one three O_ five six one nine O_ one one seven O_ five three six nine one two nine one nine five seven three eight seven seven seven seven five eight O_ nine six eight one nine six four seven four nine eight one four three three one six one two Boy  is that ever efficient. Yep. That's really fast. Yeah. Yeah. ",This meeting of the Berkley Meeting Recorder group charts the progress of their project and covers ongoing issues as well as some new ones. The most pressing issue concerns the demos which the group are preparing for the DARPA meeting next month. Here they discuss the querying and indexing tool which is progressing well albeit with a few front-end issues  and also the transcriber tool. Transcription is progressing well  with new people hired  and double checking almost complete. Work is also going on in parallel with IBM. Additionally  the group have progressed further with data storage issues  with backing-up their data now regarded as a priority  and more disk space required. Tools for accessing key file information have been developed which should ensure all meeting information is present. The collection of CrossPad note-taking data will be pursued in future meetings. Finally  other progress made includes getting the ChannelTrans interface working  ordering more wireless microphones  and analysing recognition runs. DARPA demo illustrating prosody of meeting events will be completed by the end of the (next) month  so as to allow a week or so for transferring data across to laptops. After consulting others  the group decide that not backing up their meeting data is a bad idea  since this cannot be recovered. Therefore  meeting recorder data will be backed up  and also stored manually using the NW archive. More disk space will be purchased  however for now  data which is easily re-creatable will not be backed up. After evaluating the use of the CrossPads  the group decide to return one  and also keep one which will be left out in meetings to encourage its use. Use of PDAs and buttons for note-taking will be investigated  along with using the data for a student research project. Because of time constraints  the group decide to do simultaneous digits. Improving the THISL front-end user interface may be difficult if it is to run on Windows as a web server Tcl-TK may need to be used instead. More information on XML transcription formats is required for the creation of the tools. Information for key files  such as Microphone type  seating  and participant information  is not being properly recorded  and in some cases the microphones aren't working properly. However this should be rectified by the use of appropriate XML tools. The CrossPads have not been used to their full potential  which is because of a number of problems regarding their use: First  they need to be synchronised with the time of the meeting recording  and secondly  they use a non standard data format which needs converting. Additionally  some people tend not to make notes in meetings. Several transcribers have now been hired  and double-checking is almost finished. Information retrieval will progress using uncorrected transcripts. In parallel to the transcription  the first five EDU meetings are chunked and ready to go to IBM  once the IBM transcript is checked and satisfactory. For the DARPA demo: Tools have been written to convert everything into the correct file formats  an now the THISTL back-end querying and indexing tool is now working on the command line. Work is also underway to improve the front-end THISTL interface. The demo of Transcriber interface was discussed  and may include graphical illustration of prosodic features indicating sentence or turn boundaries. Work is continuing to create tools for handling XML data  which extract data from the XML key files. Tools will also be provided to ensure that appropriate meeting information is present  and that microphones and recording is working. The ChannelTrans interface is now working on Windows machines. More wireless microphones have been ordered  and the room is to be re-wired as suggested by Jane  using the wireless close-talking microphones first  and then the far-field microphones. Analysis of the recognition runs to identify features and their characteristics is on-going. 
"O_K  we're on. O_K  what are we talking about today? I don't know. Do you have news from the conference talk? Uh  that was Uh - Yesterday programmed for yesterday - I guess. Uh - Yesterday morning on video conference. Uh  oh  I'm sorry. I know - now I know what you're talking about. No  nobody's told me anything. Well Oh. Conference call. Alright. Oh  this was the  uh  talk where they were supposed to try to decide - To - to decide what to do  yeah. Yeah. Ah  right. Yeah. No  that would have been a good thing to find out before this meeting  that's. No  I have no - I have no idea. Um  Uh  so I mean  let's - let's assume for right now that we're just kind of plugging on ahead  because even if they Yeah. tell us that  uh  the rules are different  uh  we're still interested in doing what we're doing. So what are you doing? Mm-hmm. Uh  well  we've - a little bit worked on trying to see  To improve - uh  what were the bugs and the problem with the latencies. So  We took - first we took the L_D_A filters and  uh  we designed new filters  using uh recursive filters actually. So when you say ""we""  is that something Sunil is doing or is that - ? I'm sorry? Who is doing that? Uh  us. Yeah. Oh  oh. Oh  O_K. But - So we took the filters - the FIR filters and we designed  uh  I_I_R filters that have the same frequency response. Mm-hmm. Well  similar  but that have shorter delays. Mm-hmm. So they had two filters  one for the low frequency bands and another for the high frequency bands. And so we redesigned two filters. And the low frequency band has sixty-four milliseconds of delay  and the high frequency band filter has something like eleven milliseconds compared to the two hundred milliseconds of the I_I_R filters. But it's not yet test. So we have the filters but we still have to implement a routine that does recursive filtering and - O_K. You - you had a discussion with Sunil about this though? No. No. Uh- huh. Yeah  you should talk with him. Yeah  yeah. Yeah. No  I mean  because the - the - the - the whole problem that happened before was coordination  right? So - so you need to Mm-hmm. discuss with him what we're doing  uh  cuz they could be doing the same thing and - or something. Yeah. Mm-hmm. Uh  I - yeah  I don't know if th- that's what they were trying to - Right. They were trying to do something different like taking  uh - well  using filter that takes only a past and this is just a little bit different. But I will- I will send him an email and tell him exactly what we are doing  so. Yeah  yeah. Um  I mean - Um  We just - we just have to be in contact more. I think that - the - the fact that we - Mm-hmm. we did that with - had that thing with the latencies was indicative of the fact that there wasn't enough communication. So. Alright. Um  O_K. Yeah. Well  there is w- one  um  remark about these filters  that they don't have a linear phase. So  Right. Well  I don't know  perhaps it - perhaps it doesn't hurt because the phase is almost linear but. Um  and so  yeah  for the delay I gave you here  it's - it's  uh  computed on the five hertz modulation frequency  which is the - mmm  well  the most important for speech so . Uh  this is the first thing. The low f- f- So that would be  uh  a reduction of a hundred and thirty-six milliseconds  which  uh - Yeah. What was the total we ended up with through the whole system? Three hundred and thirty. So that would be within - ? Yeah  but there are other points actually  uh  which will perhaps add some more delay. Is that some other - other stuff in the process were perhaps not very - um perf- well  not very correct  like the downsampling which w- was simply dropping frames. Yeah. Um  so we will try also to add a nice downsampling having a filter Uh-huh. that - that - well  a low-pass filter at - at twenty-five hertz. Uh  because wh- when - when we look at the L_D_A filters  well  they are basically low-pass but they leave a lot of what's above twenty-five hertz. Yeah. Um  and so  yeah  this will be another filter which would add ten milliseconds again. Yeah. Um  yeah  and then there's a third thing  is that  um  basically the way on-line normalization was done uh  is just using this recursion Yeah. on - on the um  um  on the feature stream  and - but this is a filter  so it has also a delay. Uh  and when we look at this filter actually it has a delay of eighty-five milliseconds. So if we - Eighty - five . Yeah. If we want to be very correct  so if we want to - the estimation of the mean t- t- to - to be - well  the right estimation of the mean  we have to t- to take eighty-five milliseconds in the future. Mmm. Hmm! That's a little bit of a problem. Yeah. Um  But  well  when we add up everything it's - it will be alright. We would be at six- so  sixty-five  plus ten  plus - for the downsampling  plus eighty-five for the on-line normalization. So it's Uh  yeah  but then there's - plus - plus eighty for the neural net and P_C_A. Oh. So it would be around two hundred and forty - so  well  Just - just barely in there. plus - plus the frames  but it's O_K. What's the allowable? Two-fifty  unless they changed the rules. Hmm. Which there is - there's some discussion of. But - Yeah. What were they thinking of changing it to? Uh  well the people who had very low latency want it to be low - uh  very - very- very narrow  uh  latency bound. And the people who have longer latency don't. So. Huh. So  yeah. Unfortunately we're the main ones with long latency  but Ah! But  uh  you know  it's - Yeah  and basically the best proposal had something like thirty or forty milliseconds of latency. So. Yeah. Well. Yeah  so they were basically - I mean  they were more or less trading computation for performance and we were  uh  trading latency for performance. And they were dealing with noise explicitly and we weren't  and so I think of it as complementary  that if we can put the - Think of it as what? Complementary. Hmm. I think the best systems - so  uh  everything that we did in- in a way it was - it was just adamantly insisting on going in with a brain damaged system  which is something - actually  we've done a lot over the last thirteen years. Uh  which is we say  well this is the way we should do it. And then we do it. And then someone else does something that's straight forward. So  w- th- w- this was a test that largely had additive noise and we did - we adde- did absolutely nothing explicitly to handle ad- additive noise. Right. We just  uh  you know  trained up systems to be more discriminant. And  uh  we did this  uh  RASTA-like filtering which was done in the log domain and was tending to handle convolutional noise. We did - we actually did nothing about additive noise. So  um  the  uh  spectral sub- subtraction schemes a couple places did seem to- seem to do a nice job. And so  uh  we're talking about putting - putting some of that in while still keeping some of our stuff. I think you should be able to end up with a system that's better than both but clearly the way that we're operating for this other stuff does involved some latency to - to get rid of most of that latency. To get down to forty or fifty milliseconds we'd have to throw out most of what we're doing. And - and  uh  I don't think there's any good reason for it in the application actually. I mean  you're - you're - you're speaking to a recognizer on a remote server and  uh  having a - a - a quarter second for some processing to clean it up. It doesn't seem like it's that big a deal. These aren't large vocabulary things so the decoder shouldn't take a really long time  and. Mm-hmm. So. And I don't think anybody's gonna notice the difference between a quarter of a second of latency and thirty milliseconds of latency. No. What - what does - wa- was your experience when you were doing this stuff with  uh  the - the - the surgical  uh  uh  microscopes and so forth. Um  how long was it from when somebody  uh  finished an utterance to when  uh  something started happening? Um  we had a silence detector  so we would look for the end of an utterance based on the silence detector. Mm-hmm. And I - I can't remember now off the top of my head how many frames of silence we had to detect before we would declare it to be Mm-hmm. Mm-hmm. the end of an utterance. Um  but it was  uh  I would say it was probably around the order of two hundred and fifty milliseconds. Yeah  and that's when you'd start doing things. Yeah  we did the back trace at that point to get the answer. Yeah. Of course that didn't take too long at that point. Yeah. No  no it was pretty quick. Yeah  so you - you - so you had a So - this w- so you had a - a quarter second delay before  uh  plus some little processing time  and then Right. the - the microscope would start moving or something. Right. Yeah. Right. And there's physical inertia there  so probably the - the motion itself was all - And it felt to  uh  the users that it was instantaneous. I mean  as fast as talking to a person. It - th- I don't think anybody ever complained about the delay. Yeah  so you would think as long as it's under half a second or something. Uh  I'm not an expert on that but. Yeah. Yeah. I don't remember the exact numbers but Yeah. it was something like that. I don't think you can really tell. A person - I don't think a person can tell the difference between  uh  you know  a quarter of a second and a hundred milliseconds  and - Yeah. I'm not even sure if we can tell the difference between a quarter of a second and half a second. Yeah. I mean it just - it feels so quick. I mean  basically if you - yeah  if you said  uh  um  ""what's the  uh  uh - what's the shortest route to the opera?"" and it took half a second to get back to you  I mean  it would be f- I mean  it might even be too abrupt. You might have to put in a s- a s- a delay. Yeah. Yeah. Yeah. I mean  it may feel different than talking to a person because when we talk to each other we tend to step on each other's utterances. So like if I'm asking you a question  you may start answering before I'm even done. Yeah. So it - it would probably feel different but I don't think it would feel Right. slow. Right. Well  anyway  I mean  I think - we could cut - we know what else  we could cut down on the neural net time by - by  uh  playing around a little bit  going more into the past  or something like that. We t- we talked about that. So is the latency from the neural net caused by how far ahead you're looking? Mm-hmm. Mm-hmm. And there's also - well  there's the neural net and there's also this  uh  uh  multi-frame  uh  uh  K_L_T. Wasn't there - Was it in the  uh  recurrent neural nets where they weren't looking ahead at all? They weren't looking ahead much. They p- they looked ahead a little bit. A little bit. O_K. Yeah. Yeah  I mean  you could do this with a recurrent net. And - and then - But you also could just  um  I mean  we haven't experimented with this but I imagine you could  um  uh  predict a  uh - um  a label  uh  from more in the past than in - than - than in the future. I mean  we've d- we've done some stuff with that before. I think it - it works O_K. Mm-hmm. So. We've always had - usually we used the symmetric windows but I don't think - Yeah  but we've - but we played a little bit with - with asymmetric  guys. You can do it. Yeah. So. So  that's what - that's what you're busy with  s- messing around with this  yeah. Uh  yeah. And  uh  Also we were thinking to - to  uh  apply the eh  spectral subtraction from Ericsson and to - to change the contextual K_L_T for L_D_A. Yeah. Uh-huh. Change the what? The contextual K_L_T. I'm missing that last word. Context- K_ - K_L_T. Oh. K_L_T. K_L_T - K_L_T  I'm sorry. K_L_T. Oh  K_L_T. Uh-huh. Mm-hmm. Uh  to change and use L_D_A discriminative. Yeah. But - Uh- huh. I don't know. Uh  What is the advantage of that? Uh - Well  it's that by the- for the moment we have  uh  something that's discriminant and nonlinear. And the other is linear but it's not discriminant at all. Well  it's it's a linear transformation  that - Uh - So at least just to understand maybe what the difference was between how much you were getting from just putting the frames together and how much you're getting from the discriminative  what the nonlinearity does for you or doesn't do for you. Just to understand it a little better I guess. Mmm. Well - uh - yeah. Actually what we want to do  perhaps it's to replace - to - to have something that's discriminant but linear  also. And to see if it - if it improves ov- over - over the non-discriminant linear transformation. And if the neural net is better than this or  well. Hmm. Yeah  well  that's what I meant  is to see whether - So. Ye- whether it - having the neural net really buys you anything. Mmm. Uh  I mean  it doe- did look like it buys you something over just the K_L_T. But maybe it's just the discrimination and - and maybe - yeah  maybe the nonlinear discrimination isn't necessary. Yeah. S- maybe. Yeah. Mm-hmm. Could be. Maybe. Good - good to know. But the other part you were saying was the spectral subtraction  so you just kind of  uh - Yeah. At what stage do you do that? Do you - you're doing that  um - ? So it would be on the um - We was think- on - on the mel frequency bands  so. Yeah  be- before everything. Yeah  we - no - nnn O_K  so just do that on the mel f- We - we was thinking to do before after V_A_D or- Oh  we don't know exactly when it's better. Before after V_A_D or - Yeah  um - and then So - so you know that - that - that the way that Um. they're - uh  one thing that would be no - good to find out about from this conference call is that what they were talking about  what they're proposing doing  was having a third party  um  run a good V_A_D  and - and determine boundaries. Yeah. And then given those boundaries  Begin to work. then have everybody do the recognition. The reason for that was that  um  uh - if some- one p- one group put in the V_A_D and another didn't  uh  or one had a better V_A_D than the other since that - they're not viewing that as being part of the - the task  and that any - any manufacturer would put a bunch of effort into having some s- kind of good speech-silence detection. It still wouldn't be perfect but I mean  e- the argument was ""let's not have that be part of this test."" ""Let's - let's separate that out."" And so  uh  I guess they argued about that yesterday and  yeah  I'm sorry  I don't - don't know the answer but we should find out. I'm sure we'll find out soon what they  uh - what they decided. So  uh - Yeah  so there's the question of the V_A_D but otherwise it's - it's on the - the  uh - the mel fil- filter bank  uh  energies I guess? You do - doing the - ? Mm-hmm. Mmm  yeah. Mm-hmm. And you're - you're subtracting in the - in the - in the - I guess it's power - power domain  uh  or - or magnitude domain. Probably power domain  right? why I guess it's power domain  yeah. I don't remember exactly. I don't remember. Yeah  yep. But - yeah  so it's before everything else  and - I mean  if you look at the theory  it's - it should be in the power domain but - but  uh  I've seen implementations where people do it in the magnitude domain and - Yeah. Mmm. I have asked people why and they shrug their shoulders and say  ""oh  it works."" So. Yeah. Uh  and there's this - I guess there's this mysterious - I mean people who do this a lot I guess have developed little tricks of the trade. I mean  there's - there's this  um - you don't just subtract the - the estimate of the noise spectrum. You subtract th- that times - A little bit more and - Or - or less  or - Yeah. Yeah. Really? Huh! Yeah. And generated this - this  um  Uh. so you have the estimation of the power spectra of the noise  and you multiply this by a factor which is depend- dependent on the S_N_R. Hmm  maybe . So. Well. Hmm! When the speech lev- when the signal level is more important  compared to this noise level  the coefficient is small  and around one. But when the power le- the s- signal level is uh small compared to the noise level  the coefficient is more important. And this reduce actually the music- musical noise  uh Oh! which is more important during silence portions  when Uh-huh. the s- the energy's small. Hmm! So there are tricks like this but  mmm. Hmm! Yeah. So. Yeah. Is the estimate of the noise spectrum a running estimate? Yeah. Yeah. Or - Yeah. Well  that's - I mean  that's what differs from different - different tasks and different s- uh  spectral subtraction methods. I mean  if - Hmm! if you have  uh  fair assurance that  uh  the noise is - is quite stationary  then the smartest thing to do is use as much data as possible to estimate the noise  get a much better estimate  and subtract it off. Mm-hmm. But if it's varying at all  which is gonna be the case for almost any real situation  you have to do it on-line  uh  with some forgetting factor or something. So do you - is there some long window that extends into the past over which you calculate the average? Well  there's a lot of different ways of computing the noise spectrum. So one of the things that  uh  Hans-Guenter Hirsch did  uh - and pas- and other people - actually  he's - he wasn't the only one I guess  was to  uh  take some period of - of - of speech and in each band  uh  develop a histogram. So  to get a decent histogram of these energies takes at least a few seconds really. But  uh - I mean you can do it with a smaller amount but it's pretty rough. And  um  in fact I think the NIST standard method of determining signal-to-noise ratio is based on this. So - A couple seconds? No  no  it's based on this kind of method  this histogram method. So you have a histogram. Hmm. Now  if you have signal and you have noise  you basically have these two bumps in the histogram  which you could approximate as two Gaussians. But wh- don't they overlap sometimes? Oh  yeah. O_K. So you have a mixture of two Gaussians. Yeah. Right? And you can use E_M to figure out what it is. You know. So - so basically now you have this mixture of two Gaussians  you - you n- know what they are  and  uh - I mean  sorry  you estimate what they are  Yeah. and  uh  so this gives you what the signal is and what the noise e- energy is in that band in the spectrum. And then you look over the whole thing and now you have a noise spectrum. So  uh  Hans-Guenter Hirsch and others have used that kind of method. And the other thing to do is - which is sort of more trivial and obvious - is to  uh  uh  determine through magical means that - that  uh  there's no speech in some period  and then see what the spectrum is. Mm-hmm. Uh  but  you know  it's - that - that - that's tricky to do. It has mistakes. Uh  and if you've got enough time  uh  this other method appears to be somewhat more reliable. Uh  a variant on that for just determining signal-to-noise ratio is to just  uh - you can do a w- a uh - an iterative thing  E_M-like thing  to determine means only. I guess it is E_M still  but just - just determine the means only. Don't worry about the variances. And then you just use those mean values as being the - the  uh Mm-hmm. uh signal-to-noise ratio in that band. But what is the - it seems like this kind of thing could add to the latency. I mean  depending on where the window was that you used to calculate the signal-to-noise ratio. Yeah  sure. But - Mmm. Not necessarily. Cuz if you don't look into the future  right? if you just - yeah - I mean  if you just - if you - O_K  well that - I guess that was my question  yeah. you  uh - a- at the beginning you have some - Guess. esti- some guess and - and  uh  uh - Yeah  but it - It's an interesting question. I wonder how they did Actually  it's a mmm - do it? If- if you want to have a good estimation on non-stationary noise you have to look in the - in the future. I mean  if you take your window and build your histogram in this window  um  what you can expect is to have an estimation of th- of the noise in - in the middle of the window  not at the end. So - Mm-hmm. Well  yeah  but what does - what - what - what does Alcatel do? And - and France Telecom. the - but - but people - The- They just look in the past. I guess it works because the noise are  uh pret- uh  almost stationary but  Pretty stationary. Pretty stationary  yeah. Well  the thing  e- e- e- e- um - Yeah  y- I mean  you're talking about non-stationary noise but I think that spectral subtraction is rarely - is - is not gonna work really well for - for non-stationary noise  you know? Well  if y- if you have a good estimation of the noise  yeah  because well it- it has to work. i- But it's hard to - but that's hard to do. Yeah  that's hard to do. Yeah. Yeah. So - so I think that - that what - what is - But - wh- what's more common is that you're going to be helped with r- slowly varying or stationary noise. Mm-hmm. That's what spectral subtraction will help with  practically speaking. Mm-hmm. Mm-hmm. If it varies a lot  to get a- If - if - to get a good estimate you need a few seconds of speech  even if it's centered  right? if you need a few seconds to get a decent estimate but it's changed a lot in a few seconds  Mm-hmm. then it  you know  i- it's kind of a problem. I mean  imagine e- five hertz is the middle of the - of the speech modulation spectrum  right? Yeah. Mmm. So imagine a jack hammer going at five hertz. Yeah  that's - I mean  good - good luck. So  So in this case  yeah  sure  you cannot - Yeah. But I think y- um  Hirsch does experiment with windows of like between five hundred milliseconds and one second. And well  five hundred wa- was not so bad. I mean and he worked on non-stationary noises  like noise modulated with well  wi- with amplitude modulations and things like that  and - Were his  uh  windows centered around the - But - Um  yeah. Well  I think - Yeah. Well  in - in the paper he showed that actually the estimation of the noise is - is delayed. Well  it's - there is - you - you have to center the window  yeah. Yeah. Mmm. No  I understand it's better to do but I just think that - that  uh  Mmm. for real noises wh- what - what's most likely to happen is that there'll be some things that are relatively stationary where you can use one or another spectral subtraction thing and other things where it's not so stationary and - Yeah. I mean  you can always pick something that - that falls between your methods  uh  uh  but I don't know if  you know  if sinusoidally  uh  Hmm. modul- amplitude modulated noise is - is sort of a big problem in - in in - practice. I think that it's uh - Yeah. We could probably get a really good estimate of the noise if we just went to the noise files  and built the averages from them. Yeah. Well. What - What do you mean? Just cheat - You're saying  cheat. But if the - if the noise is stationary Yeah. Yeah. perhaps you don't even need some kind of noise estimation algorithm. We just take th- th- th- the beginning of the utterance and Oh  yeah  sure. I- I know p- I don't know if people tried this for Aurora. Well  everybody seems to use some kind of adaptive  well  It's the same. Yeah. But - but - scheme but  A dictionary. you know  stationary - is it very useful and is the c- Very slow adaptation. Right  the word ""stationary"" is - has a very precise statistical meaning. But  you know  in - in signal-processing really what we're talking about I think is things that change slowly  th- uh  compared with our - our processing techniques. So Mm-hmm. if you're driving along in a car I - I would think that most of the time the nature of the noise is going to change relatively slowly. It's not gonna stay absolute the same. If you - if you check it out  uh  five minutes later you may be in a different part of the road or Mm-hmm. whatever. But it's - it's - i- i- i- using the local characteristics in time  is probably going to work pretty well. Mm-hmm. But you could get hurt a lot if you just took some- something from the beginning of all the speech  of  you know  an hour of speech and then later - Yeah. Uh  so they may be - you know  may be overly  uh  complicated for - for this test but - but - but  uh  I don't know. But what you're saying  you know  makes sense  though. I mean  if possible you shouldn't - you should - you should make it  uh  the center of the - center of the window. But - uh  we're already having problems with these delay  uh - delay issues. So  uh  we'll have to figure ways without it. Yeah  so. Um  If they're going to provide a  uh  voice activity detector that will tell you the boundaries of the speech  then  couldn't you just go outside those boundaries and do your estimate there? Oh  yeah. You bet. Yeah. So I - I imagine that's what they're doing  right? Is they're - they're probably looking in nonspeech sections and getting some  uh - Yeah  they have some kind of threshold on - on the previous estimate  and -  So. @@ Yeah. I think. Yeah  I think Ericsson used this kind of threshold. Yeah  so  they h- they have an estimate of the noise level and they put a threshold like six or ten D_B above  and what's under this threshold is used to update the estimate. Is - is that right or - ? Yeah. I think so. I have not here the proposal. So it's - it's - Yeah. Does France Telecom do this - It's like saying what's under the threshold is silence  and - Hmm. Does France Telecom do th- do the same thing? More or less? I d- I - Y- you know  perhaps? No. I do- I have not here the proposal. O_K. Um  O_K  if we're - we're done - done with that  uh  let's see. Uh  maybe we can talk about a couple other things briefly  just  uh  things that - that we've been chatting about but haven't made it into these meetings yet. So you're coming up with your quals proposal  and  uh - Wanna just give a two three minute summary of what you're planning on doing? Oh  um  two  three  it can be shorter than that. Um. Yeah. Well  I've - I've talked to some of you already. Um  but I'm  uh  looking into extending the work done by Larry Saul and John Allen and uh Mazin Rahim. Um  they - they have a system that's  uh  a multi-band  um  system but their multi-band is - is a little different than the way that we've been doing multi-band in the past  where um - Where we've been @@ uh taking um sub-band features and i- training up these neural nets and - on - on phonetic targets  and then combining them some- somehow down the line  um  they're - they're taking sub-band features and  um  training up a detector that detects for  um  these phonetic features for example  um  he presents um  uh  a detector to detect sonorance. And so what - what it basically is - is  um - it's - there's - at the lowest level  there - it's - it's an OR ga- I mean  it's an AND gate. So  uh  on each sub-band you have several independent tests  to test whether um  there's the existence of sonorance in a sub-band. And then  um  it c- it's combined by a soft AND gate. And at the - at the higher level  for every - if  um - The higher level there's a soft OR gate. Uh  so if - if this detector detects um  the presence of - of sonorance in any of the sub-bands  then the detect- uh  the OR gate at the top says  ""O_K  well this frame has evidence of sonorance."" And these are all - What are - what are some of the low level detectors that they use? Oh  O_K. Well  the low level detectors are logistic regressions. Um  and the  uh - the one o- So that  by the way  basically is a - is one of the units in our - in our - our neural network. So that's all it is. It's a sig- it's a sigmoid  Yeah. uh  with weighted sum at the input  Hmm. Right. which you train by gradient descent. Yeah  so he uses  um  an E_M algorithm to - to um train up these um parameters for the logistic regression. The - Well  actually  yeah  so I was using E_M to get the targets. So - so you have this - this - this AND gate - what we were calling an AND gate  but it's a product - product rule thing at the output. And then he uses  uh  i- u- and then feeding into that are - I'm sorry  there's - it's an OR at the output  isn't it? Yeah  so that's the product. And then  um  Mm-hmm. then he has each of these AND things. And  um  but - so they're little neural - neural units. Um  and  um  they have to have targets. And so the targets come from E_M. And so are each of these  low level detectors - are they  uh - are these something that you decide ahead of time  like ""I'm going to look for this particular feature or I'm going to look at this frequency "" or - What - what - what are they looking at? What are their inputs? Um - Uh- Right  so the - O_K  so at each- for each sub-band there are basically  uh  several measures of S_N_R and - and correlation. Ah  O_K  O_K. Um  um and he said there's like twenty of these per - per sub-band. Um  and for - for every s- every sub-band  e- you - you just pick ahead of time  um  ""I'm going to have like five i- independent logistic tests."" Mm-hmm. And you initialize these parameters  um  in some - some way and use E_M to come up with your training targets for a - for the - the low-level detectors. Mm-hmm. And then  once you get that done  you - you - you train the whole - whole thing on maximum likelihood. Um  and h- he shows that using this - this method to detect sonorance is- it's very robust compared to  um - to typical  uh  full-band Gaussian mixtures um estimations of - of sonorance. Mm-hmm. Mm-hmm. And  uh so - so that's just - that's just one detector. So you can imagine building many of these detectors on different features. You get enough of these detectors together  um  then you have enough information to do  um  higher level discrimination  for example  discriminating between phones Mm-hmm. and then you keep working your way up until you - you build a full recognizer. Mm-hmm. So  um  that's - that's the direction which I'm - I'm thinking about going in my quals. Cool. You know  it has a number of properties that I really liked. I mean  one is the going towards  um  using narrow band information for  uh  ph- phonetic features of some sort rather than just  uh  immediately going for the - the typical sound units. Right. Another thing I like about it is that you t- this thing is going to be trained - explicitly trained for a product of errors rule  which is what  uh  Allen keeps pointing out that Fletcher observed in the twenties  Mm-hmm. uh  for people listening to narrow band stuff. That's Friday's talk  by the way. And then  um  Uh  the third thing I like about it is  uh  and we've played around with this in a different kind of way a little bit but it hasn't been our dominant way of - of operating anything  um  this issue of where the targets come from. So in our case when we've been training it multi-band things  the way we get the targets for the individual bands is  uh  that we get the phonetic label - for the sound there and we say  ""O_K  we train every -"" What this is saying is  O_K  that's maybe what our ultimate goal is - or not ultimate but Mm-hmm. penultimate goal is getting these - these small sound units. But - but  um  along the way how much should we  uh - uh  what should we be training these intermediate things for? I mean  because  uh  we don't know uh  that this is a particularly good feature. I mean  there's no way  uh - someone in the audience yesterday was asking  ""well couldn't you have people go through and mark the individual bands and say where the - where it was sonorant or not?"" Mm-hmm. But  you know  I think having a bunch of people listening to critical band wide  uh  chunks of speech trying to determine whether - I think it'd be impossible. It's all gonna sound like - like sine waves to you  more or less. I mean - Well not- I mean  it's g- all g- narrow band Ouch. Mm-hmm. uh  i- I m- I think it's very hard for someone to - to - a person to make that determination. So  um  um  we don't really know how those should be labeled. It could sh- be that you should  um  not be paying that much attention to  uh  certain bands for certain sounds  uh  in order to get the Mm-hmm. best result. So  um  what we have been doing there  just sort of mixing it all together  is certainly much - much cruder than that. We trained these things up on the - on the  uh- the final label. Now we have I guess done experiments - you've probably done stuff where you have  um  done separate  uh  Viterbis on the different - Yeah. Forced alignment on the sub-band labels? Yeah. Yeah. You've done that. Did - did that help at all? Um  it helps for one or t- one iteration but um  anything after that it doesn't help. So - so that may or may t- it - that aspect of what he's doing may or may not be helpful because in a sense that's the same sort of thing. You're taking global information and determining what you - how you should - But this is - this is  uh  I th- I think a little more direct. And - How did they measure the performance of their detector? Well  he's look- he's just actually looking at  uh  the confusions between sonorant and non-sonorant. Mm-hmm. So he hasn't applied it to recognition or if he did he didn't talk about it. It's - it's just - And one of the concerns in the audience  actually  was that - that  um  the  uh  uh - he - he did a comparison to  uh  you know  our old foil  the - the nasty old standard recognizer with mel - mel filter bank at the front  and H_M_Ms  and - and so forth. And  um  it didn't do nearly as well  especially in - in noise. But the - one of the good questions in the audience was  well  yeah  but that wasn't trained for that. I mean  this use of a very smooth  uh  spectral envelope is something that  you know  has evolved as being generally a good thing for speech recognition but if you knew that what you were gonna do is detect sonorants or not - So sonorants and non-sonorants is - is - is almost like voiced-unvoiced  except I guess that the voiced stops are - are also called ""obstruents"". Uh  so it's - it's - uh  but with the exception of the stops I guess it's pretty much the same as voiced-unvoiced  right? So - so - Mm-hmm. Um. So  um  if you knew you were doing that  if you were doing something say for a - a  uh - a - a Vocoder  you wouldn't use the same kind of features. You would use something that was sensitive to the periodicity and - and not just the envelope. Uh  and so in that sense it was an unfair test. Um  so I think that the questioner was right. It - it was in that sense an unfair test. Nonetheless  it was one that was interesting because  uh  this is what we are actually using for speech recognition  these smooth envelopes. And this says that perhaps even  you know  trying to use them in the best way that we can  that - that - that we ordinarily do  with  you know  Gaussian mixtures and H_M_Ms and so forth  you - you don't  uh  actually do that well on determining whether something is sonorant or not. Which means you're gonna make errors between similar sounds that are son- sonorant or obstruent. Didn't they - Didn't they also do some kind of an oracle experiment where they said ""if we could detect the sonorants perfectly and then show how it would improve speech recognition? I thought I remember hearing about an experiment like that. The- these same people? I don't remember that. Mm-hmm. Hmm. That would - that's - you're right  that's exactly the question to follow up this discussion  is suppose you did that  uh  got that right. Um  Yeah. Hmm. What could be the other low level detectors  I mean  for - Other kind of features  or - ? in addition to detecting sonorants or - ? Um - Th- that's what you want to - to - to go for also or - ? What t- Oh  build other - other detectors on different phonetic features? Um  Other low level detectors? Yeah. uh Let's see  um  Yeah  I d- I don't know. e- Um  um  I mean  w- easiest thing would be to go - go do some voicing stuff but that's very similar to sonorance. Mm-hmm. Um  When we - when we talked with John Ohala the other day we made a list of some of the things that w- Yeah. Oh! O_K. like frication  Mm-hmm. abrupt closure  Mm-hmm. R_coloring  nasality  voicing - Yeah  so there's a half dozen like that that are - Now this was coming at it from a different angle but maybe it's a good Uh . Yeah  nasality. way to start. Uh  these are things which  uh  John felt that a - a  uh - a human annotator would be able to reliably mark. Oh  O_K. So the sort of things he felt would be difficult for a human annotator to reliably mark would be tongue position kinds of things. Yeah. Placing stuff  yeah. Mm-hmm. Uh - There's also things like stress. You can look at stress. Mm-hmm. But stress doesn't  uh  fit in this thing of coming up with features that will distinguish words from one another  right? It's a - it's a good thing to mark and will probably help us ultimate with recognition but - Yeah  there's a few cases where it can like permit and permit. But - that's not very common in English. In other languages it's more uh  important. Well  yeah  but i- either case you'd write P_E_R_M_I_T  right? So you'd get the word right. No  I'm saying  i- i- e- I thought you were saying that stress doesn't help you distinguish between words. Um  Oh  I see what you're saying. As long as you get - We're g- if we're doing - if we're talking about transcription as opposed to something else - The sequence  right? Yeah. Yeah  yeah  yeah. Yeah. Yeah. Right. So where it could help is maybe at a higher level. Yeah. Understanding  yeah. Exactly. Like a understanding application. Yeah. Right. Yeah. But that's this afternoon's meeting. Yeah. We don't understand anything in this meeting. Yeah  so that's - yeah  that's  you know  a neat - neat thing and - and  uh - S- so  um  Ohala's going to help do these  uh transcriptions of the meeting data? So. Uh  well I don't know. We d- we sort of didn't get that far. Um  we just talked about some possible features that could be marked by humans and  um  Hmm. because of having maybe some extra transcriber time we thought we could go through and mark some portion of the data for that. And  uh - Hmm. Yeah  I mean  that's not an immediate problem  that we don't immediately have a lot of extra transcriber time. But - but  uh  in the long term I guess Chuck is gonna continue the dialogue with John and - and  uh  Yeah  right. and  we'll - we'll end up doing some I think. I'm definitely interested in this area  too  f- uh  acoustic feature Uh-huh. O_K. stuff. So. Yeah  I think it's an interesting - interesting way to go. Um  Cool. I say it like ""said-int"" . I think it has a number of good things. Um  so  uh  y- you want to talk maybe a c- two or three minutes about what we've been talking about today and other days? Ri- Yeah  O_K  so  um  we're interested in  um  methods for far mike speech recognition  um  mainly  uh  methods that deal with the reverberation in the far mike signal. So  um  one approach would be  um  say M_S_G and P_L_P  like was used in Aurora one and  um  there are other approaches which actually attempt to remove the reverberation  instead of being robust to it like M_S_G. And so we're interested in  um  comparing the performance of um  a robust approach like M_S_G with these  um  speech enhancement or de-reverber- de-reverberation approaches. Mm-hmm. @@ And  um  it looks like we're gonna use the Meeting Recorder digits data for that. And the de-reverberation algorithm  do you have - can you give some more details on this or - ? Does it use one microphone? o- o- Several microphones? Does it - ? O_K  well  um  there was something that was done by  um  a guy named Carlos  I forget his last name  who worked with Hynek  who  um  Avendano. O_K. Who  um  Yeah. Mm-hmm. um  it was like RASTA in the sense that of it was  um  de-convolution by filtering um  except he used a longer time window  Mm-hmm. like a second maybe. And the reason for that is RASTA's time window is too short to  um include the whole  um  reverberation - um  I don't know what you call it - the reverberation response. @@ I- if you see wh- if you see what I mean. The reverberation filter from my mouth to that mike is like - it's t- got- it's too long in the - in the time domain for the um - for the RASTA filtering to take care of it. And  um  then there are a couple of other speech enhancement approaches which haven't been tried for speech recognition yet but have just been tried for enhancement  which  um  have the assumption that um  you can do L_P_C um analysis of th- of the signal you get at the far microphone and the  um  all pole filter that you get out of that should be good. It's just the  um  excitation signal that is going to be distorted by the reverberation and so you can try and reconstruct a better excitation signal and  um  feed that through the i- um  all pole filter and get enhanced speech with reverberation reduced. Mm-hmm. Mm-hmm. There's also this  uh  um  uh  echo cancellation stuff that we've sort of been chasing  so  uh we have  uh - and when we're saying these digits now we do have a close microphone signal and then there's the distant microphone signal. And you could as a kind of baseline say  ""O_K  given that we have both of these  uh  we should be able to do  uh  a cancellation. "" So that  uh  um  we - we  uh  essentially identify the system in between - the linear time invariant system between the microphones and - and - and - and re- and invert it  uh  or - or cancel it out to - to some - some reasonable approximation Mm-hmm. through one method or another. Uh  that's not a practical thing  uh  if you have a distant mike  you don't have a close mike ordinarily  but we thought that might make - also might make a good baseline. Uh  it still won't be perfect because there's noise. Uh  but - And then there are s- uh  there are single microphone methods that I think people have done for  uh - for this kind of de-reverberation. Do y- do you know any references to any? Cuz I - I w- I was - w- w- I - I lead him down a - a bad Uh  path on that. But. I g- I guess - I guess when people are working with single microphones  they are more trying to do - well  not - not very - Well  there is the Avendano work  Right. but also trying to mmm  uh - trying to f- t- find the de-convolution filter but in the um - not in the time domain but in the uh the stream of features uh I guess . Yeah  O_K. Well  @@ there - there's someone working on this on i- in Mons So perhaps  yeah  we should try t- to - He's working on this  on trying to - Yeah. on re- reverberation  um - The first paper on this is gonna have great references  I can tell already. Mm-hmm. It's always good to have references  especially when reviewers read it or - or one of the authors and  feel they'll ""You're O_K  you've r- You cited me."" So  yeah. Well  he did echo cancellation and he did some fancier things like  uh  uh  training different network on different reverberation conditions and then trying to find the best one  but. Well. Yeah. Yeah. The oth- the other thing  uh  that Dave was talking about earlier was  uh  uh  multiple mike things  uh  where they're all distant. So  um  I mean  there's - there's all this work on arrays  but the other thing is  uh  what can we do that's cleverer that can take some advantage of only two mikes  uh  particularly if there's an obstruction between them  as we - as we have over there. If there is - ? An obstruction between them. Ah  yeah. It creates a shadow which is - is helpful. It's part of why you have such good directionality with  with two ears even though they're not several feet apart. Mm-hmm. For most - for most people's heads. That could help though. So that - Yeah  the - the head  in the way  is really - that's what it's for. That's what the head's for? It's basically  Yeah  it's to separate the ears. To separate the ears? That's right  yeah. Yeah. Uh  so. Anyway  O_ K. Uh  I think that's - that's all we have this week. And  uh  I think it's digit time. Oh. Actually the  um - For some reason the digit forms are blank. Yeah? Uh  I think th- that may be due to the fact that Oh! Adam ran out of digits  uh  and didn't have time to regenerate any. Oh! I guess it's - Well there's no real reason to write our names on here then  is there? Yeah  if you want to put your credit card numbers and  uh - Oh  no - ? Or do - did any - do we need the names for the other stuff  or - ? Uh  yeah  I do need your names and - and the time  and all that  cuz we put that into the ""key"" files. Um. Oh  O_K. Oh  O_K. But w- O_K. That's why we have the forms  uh  even if there are no digits. O_K  yeah  I didn't notice this. I'm sitting here and I was - I was about to read them too. It's a  uh  blank sheet of paper. So I guess we're - we're done. Yeah  yeah  I'll do my credit card number later. O_K. ",The Berkeley Meeting Recorder group discussed the collection status for a set of connected digits recordings that are nearly complete and ready to be trained on a recognizer. Anticipated results were discussed in reference to results obtained for other digits corpora  i.e. Aurora and TI-digits. The group also considered the prospect of performing fine-grained acoustic-phonetic analyses on a subset of Meeting Recorder digits or Switchboard data. Pre-segmentation manipulations that allow for the segmentation of channel-specific speech/non-speech portions of the signal and the distinction of foreground versus background speech were discussed. Finally  speaker fe008 and fe016 reported on new efforts to adapt transcriptions to the needs of the SRI recognizer  including conventions for encoding acronyms  numbers  ambient noise  and unidentified inbreaths. The group decided to delegate the extraction of digits to the transcriber pool. A tentative decision was also made to delegate transcribers with the task of labelling a subset of digits or Switchboard data for fine-grained acoustic-phonetic features. Speaker fe008 will run selected Meeting Recorder data through channelize and determine whether the resulting units are of a sufficient length. With respect to encoding more fine-grained acoustic information in transcriptions  the question was posed: which features should be marked? Speaker mn014 reported problems pre-segmenting speech recorded via the lapel microphones. Normalization of the energy measured across and within channels is problematic when performed for speakers who say little or nothing during meetings. The evaluation of pre-segmented data is difficult without tightly transcribed time references to the individual channels from which the speech was derived. The SRI recognizer requires that multi-channel format units not be too large  indicating that some additional pre-processing of unit lengths may be necessary. A test set of Meeting Recorder digits is nearly complete. Future work will include training this data on a recognizer  and feeding the recognizer with corresponding far-field microphone data. It was noted that the results of experiments testing similar digits corpora have yielded high error rates  indicating that similar problems may be expected for the set of Meeting Recoreder digits. The group discussed the prospect of performing fine-grained acoustic-phonetic analyses on a subset of digits or Switchboard data. It was suggested that prior to the use of data-driven methods  knowledge-driven approaches should be used to 'seed' the data with sub-phonemic features  either manually  or using a rich pronunciation dictionary. A new version of the pre-segmentation tool that segments channel-specific speech/non-speech portions of the signal has been developed and tested. Future pre-segmentation work will include normalizing other features  such as loudness  enabling the distinction of foreground versus background speech. Speaker mn014 will also look at cross-correlations for removing false overlaps. New efforts were reported to adapt transcriptions to the needs of the SRI recognizer  including conventions for encoding acronyms  numbers  ambient noise  and unidentified inbreaths. With the arrival of the SRI recognizer  12 hours of forced aligned  recognized data can be expected. 
"Channel one. Test. Hello. Channel three. Test. Uh-oh. So you think we're going now  yes? O_K  good. Alright Going again Uh - So we're gonna go around as before  and uh do - do our digits. Uh transcript one three one one dash one three three zero. three two three four seven six five five three one six two four one six seven seven eight nine zero nine four zero zero three zero one five eight one seven three five three two six eight zero three six two four three zero seven four five zero six nine four seven four eight five seven nine six one five O_ seven eight O_ two zero nine six zero four zero zero one two I am reading transcript one three nine one one four one zero six seven eight nine O_ six nine eight zero one three one nine one six two three seven three four five six eight four seven nine two zero seven five O_ three six nine six zero nine three one zero zero three one two O_ three zero nine seven five two six seven nine eight three six seven zero six O_K I've got transcript one two seven one one two nine zero. one seven one O_ two eight one two zero seven one three four five zero nine six zero eight zero seven three eight six eight one three nine three four zero three nine four O_ four two zero one two zero three eight two three two four two eight one six five three five eight three O_ eight six five nine one zero seven six seven six six two eight seven nine O_ O_ five seven eight O_ four one four one Transcript number one two nine one dash one three one zero. two three nine zero two three eight one five two four seven four six seven five six seven zero nine four O_ one O_ O_ zero seven two six zero one six zero four six four five two three four zero seven two six four four zero seven four one eight eight eight eight five six three nine eight nine O_ zero zero three nine one Transcript number one one seven one dash one one nine zero. eight three four three five nine four three nine O_ three zero nine six nine one one two O_ O_ three five five six O_ eight six three zero four zero one one seven eight eight two eight nine eight five five O_ zero zero two one four three six seven O_ two four seven nine five nine zero five seven three five six seven six nine four six five seven O_K  this is Barry Chen and I am reading transcript one three five one dash one three seven zero  um four five O_ seven eight nine two eight seven one four eight nine eight five nine three six nine seven one five nine O_ four seven three O_- one O_ O_ zero one two zero six two four three four six seven five four four six seven eight nine nine eight nine O_ O_ one one two four seven three four nine two five three five zero zero eight four eight Transcript one three seven one one three nine O_. five six seven O_ O_ five nine two six three three O_ five five zero four nine one six two three O_ O_ two one nine two four six six seven two O_ eight five O_ three eight six nine O_ eight eight seven four zero one zero eight three two four five four five five six nine two three Uh - Yeah  you don't actually n- need to say the name. That'll probably be bleeped out. O_K. So. That's if these are anonymized  but Oh. O_K. Yeah - uh - I mean - not that there's anything defamatory about uh - eight five seven or or anything  but Uh  anyway. O_K. Uh - so here's what I have for - I - I was just jotting down things I think th- w- that we should do today. Uh - This is what I have for an agenda so far Um  We should talk a little bit about the plans for the uh - the field trip next week. Uh - a number of us are doing a field trip to uh Uh - O_G_I And uh - mostly uh- First though about the logistics for it. Then maybe later on in the meeting we should talk about what we actually you know  might accomplish. Uh - O_K. Uh  in and kind of go around - see what people have been doing - talk about that  a r- progress report. Um  Essentially. Um - And then uh - Another topic I had was that uh - uh - Uh - Dave here had uh said uh ""Give me something to do."" And I - I have - I have uh - failed so far in doing that. And so maybe we can discuss that a little bit. If we find some holes in some things that - that - someone could use some help with  he's - he's volunteering to help. I've got to move a bunch of furniture. O_K  always count on a serious comment from that corner. So  um  uh  and uh  then uh  talk a little bit about - about disks and resource - resource issues that - that's starting to get worked out. And then  anything else anybody has that isn't in that list? Uh - I was just wondering  does this mean the battery's dying and I should change it? Uh I think that means the battery's O_ K. - d - do you Let me see. Oh O_K  so th- Yeah  that's good. Yeah. You're alright? Cuz it's full. Yeah. Yeah. Alright. Yeah. It looks full of electrons. O_K. Plenty of electrons left there. O_K  so  um  uh. O_K  so  uh  I wanted to start this with this mundane thing. Um - Uh - I - I - it was - it was kind of my bright idea to have us take a plane that leaves at seven twenty in the morning. Um. Oh  yeah  that's right. Uh this is uh - The reason I did it uh was because otherwise for those of us who have to come back the same day it is really not much of a - of a visit. Uh - So um the issue is how - how - how would we ever accomplish that? Uh - what - what - what part of town do you live in? Um  I live in  um  the corner of campus. The  um  southeast corner. O_K. O_K  so would it be easier - those of you who are not  you know  used to this area  it can be very tricky to get to the airport at - at uh  you know  six thirty. Um. So. Would it be easier for you if you came here and I drove you? Yeah? Yeah  perhaps  yeah. Yeah  yeah  O_K. Yeah. Mm-hmm. Yeah. Sure. O_K  so if - if everybody can get here at six. At six. Yeah  I'm afraid we need to do that to get there on time. Yeah  so. Six  O_K. Oh boy. Anyway  so. Will that be enough time? Yeah. Yeah  so I'll just pull up in front at six and just be out front. And  uh  and yeah  that'll be plenty of time. It'll take - it - it - it won't be bad traffic that time of day and - and uh I guess once you get past the bridge that that would be the worst. Going to Oakland. Yeah   Oakland. Oakland. Yeah. Bridge Once you get past the turnoff to the Bay Bridge. oh  the turnoff to the bridge Yeah. Won't even do that. I mean  just go down Martin Luther King. Yeah. Yeah. O_K. Mm-hmm. And then Martin Luther King to nine-eighty to eight-eighty  and it's - it'd take us  Yeah. tops uh thirty minutes to get there. Oh  I - So that leaves us fifty minutes before the plane - it'll just - yeah. So Great  O_K so that'll It's - I mean  it's still not going to be really easy but - well Particularly for - for uh - for Barry and me  we're not - we're not staying overnight so we don't need to bring anything particularly except for uh - a pad of paper and - So  and  uh you  two O_K. have to bring a little bit but uh - you know  don't - don't bring a footlocker and we'll be O_K So. s- W- you're staying overnight. I figured you wouldn't need a great big suitcase  yeah. That's sort of So just - Oh yeah. Yeah. one night. So. Anyway. So  s- six A_M  in front. O_K. Six A_M in front. O_K. Uh  I'll be here. Uh - I'll - I'll - I'll - I'll give you my phone number  If I'm not here for a few m- after a few minutes then Wake you up. Nah  I'll be fine. I just  uh - it - for me it just means getting up a half an hour earlier than I usually do. Not - not - not a lot  so O_K. Wednesday. O_K  that was the real real important stuff. Um  I - I - I figured maybe wait on the potential goals for the meeting uh - until we talk about wh- what's been going on. So  uh  what's been going on? Why don't we start - start over here. Um. Well  preparation of the French test data actually. O_K. So  it means that um  well  it is  uh  a digit French database of microphone speech  downsampled to eight kilohertz and I've added noise to one part  with the - actually the Aurora-two noises. And  @@ so this is a training part. And then the remaining part  I use for testing and - with other kind of noises. So we can - So this is almost ready. I'm preparing the - the H_T_K baseline for this task. And  yeah. O_K Uh  So the H_T_K base lines - so this is using mel cepstra and so on  or - ? Yeah. Yeah. O_K. And again  I guess the p- the plan is  uh  to uh - then given this - What's the plan again? The plan with these data? With - So - So - Does i- Just remind me of what - what you were going to do with the - what - what - what - what's - y- You just described what you've been doing. Yeah. So if you could remind me of what you're going to be doing. Oh  this is - yeah  yeah. Uh  yeah. @@ Tell him about the cube. Well. The cube? I should tell him about the cube? Yeah. Yeah. Oh! Cube. Yeah. Uh we - actually we want to  mmm  Fill in the cube. Uh  uh  analyze three dimensions  the feature dimension  the training data dimension  and the test data dimension. Um. Well  what we want to do is first we have number for each uh task. So we have the um  T_I-digit task  the Italian task  the French task and the Finnish task. Yeah? So we have numbers with uh - systems - I mean - I mean neural networks trained on the task data. And then to have systems with neural networks trained on  uh  data from the same language  if possible  with  well  using a more generic database  which is phonetically - phonetically balanced  and. Um. Yeah. So. So- so we had talked - I guess we had talked at one point about maybe  the language I_D corpus? Is that a possibility for that? Ye- uh - Yeah  but  uh these corpus  w- w- there is a CallHome and a CallFriend also  The CallFriend is for language ind- identification. Well  anyway  these corpus are all telephone speech. So  um. This could be a - a problem for - Why? Because uh  uh  the - the SpeechDat databases are not telephone speech. They are downsampled to eight kilohertz but - but they are not uh with telephone bandwidth. Yeah. That's really funny isn't it? I mean cuz th- this whole thing is for developing new standards for the telephone. Telephone. Yeah. Yeah  but the - the idea is to compute the feature before the - before sending them to the - Well  you don't - do not send speech  you send features  computed on th- the - the device  or - Well. Mm-hmm. Yeah  I know  but the reason - Oh I see  so your point is that it's - it's - So you - it's uh - the features are computed locally  and so they aren't necessarily telephone bandwidth  uh Yeah. Yeah. or telephone Did you happen to find out anything about the O_G_I multilingual database? distortions. Yeah  that's wh- that's wh- that's what I meant. Yeah  it's - I said - @@   there's - there's - there's an O_G_I language I_D  not the - not the  uh - the CallFriend is a - is a  uh  L_D_C w- thing  right? Yea- Yeah  there are also two other databases. One they call the multi-language database  and another one is a twenty-two language  something like that. But it's also telephone speech. Oh  they are? O_K. Uh. Well  nnn. But I'm not sure - So - I mean  we'r- e- e- The bandwidth shouldn't be such an issue right? Because e- e- this is downsampled and - and filtered  right? So it's just the fact that it's not telephone. And there are so many other differences between these different databases. I mean some of this stuff's recorded in the car  and some of it's - I mean there's - there's many different acoustic differences. So I'm not sure Yeah. if - . I mean  unless we're going to include a bunch of car recordings in the - in the training database  I'm not sure if it's - completely rules it out if our - if we - if our major goal is to have phonetic context Mmm. and you figure that there's gonna be a mismatch in acoustic conditions does it make it much worse f- to sort of add another mismatch  if you will. Uh  i- i- I - I guess the question is how important is it to - for us to get multiple languages uh  in there. Yeah  but - Mm-hmm. Um. Yeah. Well  actually  for the moment if we w- do not want to use these phone databases  we - we already have uh - English  Spanish and French uh  with microphone speech. Mm-hmm. Yeah. So. So that's what you're thinking of using is sort of the multi- the equivalent of the multiple? Well. Yeah  for the multilingual part we were thinking of using these three databases. And for the difference in phonetic context that you - ? Well  this - Uh  actually  these three databases are um generic databases. Provide that. Yeah. So w- f- for - for uh Italian  which is close to Spanish  French and  i- i- uh  T_I-digits we have both uh  digits training data and also more general training data. So. Mmm. Well  we also have this Broadcast News that we were talking about taking off the disk  which is - is microphone data for - for English. Yeah. Yeah  perhaps - yeah  there is also TIMIT. We could use TIMIT. Yeah. Right. Um. Yeah  so there's plenty of stuff around. O_K  so anyway  th- the basic plan is to  uh  test this cube. @@ Yeah. Yes. To fill i- fill it in  yeah. To fill in the cube. Uh. O_K. Yeah  and perhaps  um - We were thinking that perhaps the cross-language issue is not  uh  so big of a issue. Well  w- w- we - perhaps we should not focus too much on that cross-language stuff. I mean  uh  training - training a net on a language and testing a- for another language. Uh-huh. Mmm. Perhaps the most important is to have neural networks trained on the target languages. But  uh  with a general database - general databases. But that's - u- So that th- Well  the - the guy who has to develop an application with one language can use the net trained o- on that language  or a generic net  but not trained on a - Uh  depen- it depen- it depends how you mean ""using the net"". So  if you're talking about for producing these discriminative features that we're talking about you can't do that. Mmm. Because - because the - what they're asking for is - is a feature set. Right? And so  uh  we're the ones who have been weird by - by doing this training. Yeah. But if we say  ""No  you have to have a different feature set for each language "" I think this is ver- gonna be very bad. Oh. You think so. Mmm. So - Oh yeah. Yeah. I mean  Oh. That's - in principle  I mean conceptually  it's sort of like they want a re- @@ Mmm. well  they want a replacement for mel cepstra. So  we say ""O_K  this is the year two thousand  we've got something much better than mel cepstra. It's  you know  gobbledy-gook."" O_K? And so we give them these gobbledy-gook features but these gobbledy-gook features are supposed to be good for any language. Hmm. Cuz you don't know who's gonna call  and you know  I mean so it's - it's - it's  uh  uh - how do you know what language it is? Somebody picks up the phone. So thi- this is their image. Well  I chh - Someone picks up the phone  right? And - and he - he picks up the ph- Yeah  but the - the application is - there is a target language for the application. So  if a - Yeah. y- y- y- Well. Well. But  no but  y- you - you pick up the phone  Yeah? you talk on the phone  and it sends features out. O_K  so the phone doesn't know what a - what - what your language is. Yeah  if - Yeah. If it's th- in the phone  but - well  it - that - that could be th- at the server's side  and  well. But that's the image that they have. It could be  but that's the image they have  right? Mmm  yeah. So that's - that's - I mean  one could argue all over the place about how things really will be in ten years. But the particular image that the cellular industry has right now is that it's distributed speech recognition  where the  uh  uh  probabilistic part  and - and s- semantics and so forth are all on the servers  and you compute features of the - uh  on the phone. So that's - that's what we're involved in. We might - might or might not agree that that's the way it will be in ten years  but that's - that's - that's what they're asking for. So - so I think that - th- th- it is an important issue whether it works cross-language. Now  it's the O_G_I  uh  folks' perspective right now that probably that's not the biggest deal. And that the biggest deal is the  um envir- acoustic-environment mismatch. And they may very well be right  but I - I was hoping we could just do a test and determine if that was true. If that's true  we don't need to worry so much. Maybe - maybe we have a couple languages in the training set and that gives us enough breadth uh  uh  that - that - that the rest doesn't matter. Um  the other thing is  uh  this notion of training to uh - which I - I guess they're starting to look at up there  training to something more like articulatory features. Uh  and if you have something that's just good for distinguishing different articulatory features that should just be good across  you know  a wide range of languages. Mmm. Yeah. Yeah. Uh  but - Yeah  so I don't th- I know - unfortunately I don't - I see what you're comi- where you're coming from  I think  but I don't think we can ignore it. So we - we really have to do test with a real cross-language. I mean  tr- for instance training on English and testing on Italian  or - Or we can train - or else  uh  can we train a net on  uh  a range of languages and - Test on an unseen. which can include the test - the test @@ the target language  or - Yeah  so  um  there's - there's  uh - This is complex. So  ultimately  uh  as I was saying  I think it doesn't fit within their image that you switch nets based on language. Yeah. Now  can you include  uh  the - the target language? Um  from a purist's standpoint it'd be nice not to because then you can say when - because surely someone is going to say at some point  ""O_K  so you put in the German and the Finnish. Uh  now  what do you do  uh  when somebody has Portuguese?"" you know? Mmm. Um  and - Uh  however  you aren't - it isn't actually a constraint in this evaluation. So I would say if it looks like there's a big difference to put it in  then we'd make note of it  and then we probably put in the other  because we have so many other problems in trying to get things to work well here that - Mmm? that  you know  it's not so bad as long as we - we note it and say  ""Look  we did do this"". And so  ideally  what you'd wanna do is you'd wanna Uh. run it with and without the target language and the training set for a wide range of languages. Yeah. Yeah  perhaps. Yeah. Yeah. And that way you can say  ""Well "" Yeah. you know  ""we're gonna build it for what we think are the most common ones""  but if that - somebody uses it with a different language  you know  ""here's what's you're l- here's what's likely to happen."" Yeah  cuz the truth is  is that it's - it's not like there are - I mean  al- although there are thousands of languages  uh  from uh  uh  the point of view of cellular companies  there aren't. There's - you know  there's fifty or something  you know? So  Right. uh  an- and they aren't - you know  with the exception of Finnish  which I guess it's pretty different from most - most things. uh  it's - it's  uh - most of them are like at least some of the others. And so  our guess that Spanish is like Italian  and - and so on. I guess Finnish is a - is - is a little bit like Hungarian  supposedly  right? Or is - I think - well  I kn- oh  well I know that I don't know anything about Finnish. H- uh  H- I mean  I'm not a linguist  but I guess Hungarian and Finnish and one of the - one of the languages from the former Soviet Union are in this sort of same family. But they're just these  you know  Hmm. uh - countries that are pretty far apart from one another  have - Hmm. Mmm. I guess  people rode in on horses and brought their - O_K. The - Yeah. Your turn. Oh  my turn. Oh  O_K. Um  Let's see  I - I spent the last week  uh  looking over Stephane's shoulder. And - and understanding some of the data. I re-installed  um  um  H_T_K  the free version  so  um  everybody's now using three point O_  which is the same version that  uh  O_G_I is using. Oh  good. Yeah. So  without - without any licensing big deals  or anything like that. And  um  so we've been talking about this - this  uh  cube thing  and it's beginning more and more looking like the  uh  the Borge cube thing. It's really gargantuan. Um  but I- So are - are you going to be assimilated? I'm - Am I - Resistance is futile. Exactly. Um  yeah  so I- I've been looking at  uh  uh  TIMIT stuff. Um  the - the stuff that we've been working on with TIMIT  trying to get a  um - a labels file so we can  uh  train up a - train up a net on TIMIT and test  um  the difference between this net trained on TIMIT and a net trained on digits alone. Mm-hmm. Um  and seeing if - if it hurts or helps. And again  when y- just to clarify  when you're talking about training up a net  you're talking about training up a net for a tandem Anyway. Yeah  yeah. Um. approach? And - and the inputs are P_L_P and delta and that sort of thing  or - ? Mm-hmm. Well  the inputs are one dimension of the cube  which  um  we've talked about it being  uh  P_L_P  um  M_F_C_Cs  um  J_- J_RASTA  J_RASTA-L_D_A - Hmm. Yeah  but your initial things you're making one choice there  right? Which is P_L_P  or something? Yeah. Yeah  right. Um  I - I haven't - I haven't decided on - on the initial thing. Probably - probably something like P_L_P. Yeah. Yeah. Hmm. Um  so - so you take P_L_P and you - you  uh  do it - uh  you - you  uh  use H_T_K with it with the transformed features using a neural net that's trained. And the training could either be from Digits itself or from TIMIT. And that's the - and  and th- and then the testing would be these other things which - which - which might be foreign language. Right. Right. I see. Right. I - I - I get in the picture about the cube. O_K. Yeah. Maybe - O_K. O_K. Um  I mean  those listening to this will not have a picture either  so  um  I guess I'm - I'm not any worse off. Uh-huh. But but at some point - somebody should just show me the cube. It sounds s- I - I get - I think I get the general idea of it  yeah. Yeah  yeah  b- May- So  when you said that you were getting the labels for TIMIT  Mm-hmm. um  are y- what do you mean by that? Oh  I'm just - I'm just  uh  transforming them from the  um  the standard TIMIT transcriptions into - into a nice long huge P_file to do training. Mmm. Were the digits  um  hand-labeled for phones? Um  the - the digits - Or were they - those labels automatically derived? Oh yeah  those were - those were automatically derived by - by Dan Mmm. using  um  embedded - embedded training and alignment. Ah  but which Dan? Uh  Ellis. Right? O_K. O_K. Yeah. So. I was just wondering because that test you're t- I - I think you're doing this test because you want to determine whether or not  Uh-huh. uh  having s- general speech performs as well as having specific speech. That's right. Well  especially when you go over the different languages again  because you'd - the different languages have different words for the different digits  so it's - Mm-hmm. And I was - yeah  so I was just wondering if the fact that TIMIT - you're using the hand-labeled stuff from TIMIT might be - confuse the results that you get. I - I think it would  but - but on the other hand it might be better. Right  but if it's better  it may be better because it was hand-labeled. Oh yeah  but still @@ probably use it. Yeah. I mean  you know  I - I - I guess I'm sounding cavalier  but I mean  I think O_K. the point is you have  uh  a bunch of labels and - and they're han- hand uh - hand-marked. Uh  I guess  actually  TIMIT was not entirely hand-marked. It was automatically first  and then hand - hand-corrected. Oh  O_K. But - but  um  uh  it - it  um  it might be a better source. So  i- it's - you're right. It would be another interesting scientific question to ask  ""Is it because it's a broad source or because it was  you know  carefully?"" uh. Mm-hmm. And that's something you could ask  but given limited time  I think the main thing is if it's a better thing for going across languages on this training tandem system  then it's probably - Yeah. Right. What about the differences in the phone sets? Uh  between languages? No  between TIMIT and the - the digits. Oh  um  right. Well  there's a mapping from the sixty-one phonemes in TIMIT to - Sixty-one. to fifty-six  the ICSI fifty-six. Oh  O_K. I see. And then the digits phonemes  um  there's about twenty- twenty-two or twenty-four of them? Is that right? Yep. Out of that fifty-six? Oh  O_K. Out of that fifty-six. Yeah. So  it's - it's definitely broader  yeah. But  actually  the issue of phoneti- phon- uh phone- phoneme mappings will arise when we will do severa- use several languages because Yeah. you - Well  some phonemes are not  uh  in every languages  and - So we plan to develop a subset of the phonemes  uh  that includes  uh  all the phonemes of our training languages  and Mm-hmm. Mm-hmm. use a network with kind of one hundred outputs or something like that. You mean a superset  sort of. Uh  yeah  superset  yeah. Yeah. Yeah. Yeah. I th- I looks the SAMPA- SAMPA phone. Mmm. Yeah. SAMPA phone? For English - uh American English  and the - the - the language who have more phone are the English. Mmm. Of the - these language. But n- for example  in Spain  the Spanish have several phone that d- doesn't appear in the E- English and we thought to complete. But for that  it needs - we must r- h- do a lot of work because we need to generate new tran- transcription for the database that we have. Mm-hmm. Mm-hmm. Other than the language  is there a reason not to use the TIMIT phone set? Cuz it's larger? As opposed to the ICSI phone set? Oh  you mean why map the sixty-one to the fifty-six? Yeah. I don't know. I have - Um  I forget if that happened starting with you  or was it - o- or if it was Eric  afterwards who did that. But I think  basically  there were several of the phones that were just hardly ever there. Yeah  and I think some of them  they were making distinctions between silence at the end and silence at the beginning  when really they're both silence. Oh. I th- I think it was things like that that got it mapped down to fifty-six. O_K. Yeah  especially in a system like ours  which is a discriminative system. You know  you're really asking this net to learn. @@ It's - it's kind of hard. So. Yeah. Yeah. There's not much difference  really. And the ones that are gone  I think are - I think there was - they also in TIMIT had like a glottal stop  which Mm-hmm. was basically a short period of silence  and so. Well  we have that now  too  right? I don't know. Yeah. i- It's actually pretty common that a lot of the recognition systems people use have things like - like  say thirty-nine  So. phone symbols  right? Uh  and then they get the variety by - by bringing in the context  the phonetic context. Uh. So we actually have an unusually large number in - in what we tend to use here. Um. So  a- a- actually - maybe - now you've got me sort of intrigued. What - there's - Can you describe what - what's on the cube? I mean - Yeah  w- I th- I think that's a good idea to - to talk about the whole cube and maybe we could sections in the cube for people to work on. Yeah  yeah. Yeah. Yeah. Yeah. Yeah. Um  O_K. O_K  so even - even though the meeting recorder doesn't - doesn't  uh - and since you're not running a video camera we won't get this  but if you use a board it'll help us anyway. Uh  do you wanna do it? Uh  point out one of the limitations of this medium  but you've got the wireless on  right? Yeah  so you can walk around. O_K. O_K. Yeah  I have the wireless. O_K. Can y- can you walk around too? No. O_K  well  um  s- Uh  he can't  actually  but - He's tethered. basically  the - the cube will have three dimensions. The first dimension is the - the features that we're going to use. And the second dimension  um  is the training corpus. And that's the training on the discriminant neural net. Um and the last dimension Yeah and again - Yeah. So the - the training for H_T_K is always - that's always set up for the individual test  right? That there's some training data and some test data. So that's different than this. Yeah. happens to be - Right  right. This is - this is for - for A_N_N only. And  yeah  the training for the H_T_K models is always  uh  fixed for whatever language you're testing on. Right. And then  there's the testing corpus. So  then I think it's probably instructive to go and - and - and show you the features that we were talking about. Um  so  let's see. Help me out with - With what? @@ P_L_P. P_L_P. P_L_P? O_K. M_S_G. M_S_G. Uh  J_RASTA. J_RASTA. And J_RASTA-L_D_A. J_RASTA-L_D_A. Um  multi-band. Multi-band. So there would be multi-band before  um - before our network  I mean. Yeah  just the multi-band features  right? And - Yeah. Yeah. Uh-huh. Ah. Ah. So  something like  uh  s- T_C_T within bands and - Well. And then multi-band after networks. Meaning that we would have  uh  neural networks  uh  discriminant neural networks for each band. Uh  yeah. And using the - the outputs of these networks or the linear outputs or something like that. Uh  yeah. What about mel cepstrum? Oh  um - Or is that - you don't include that because it's part of the base or something ? Well  y- you do have a baseline Yeah databases. Yeah. system that's m- that's mel cepstra  right? So. Mm-hmm. But  uh  well  not for the - the A_N_N. I mean - O_K. So  yeah  we could - we could add M_F_C_C also. We could add - Probably should. I mean at least - at least conceptually  you know  it doesn't meant you actually have to do it  but conceptually it makes sense as a - as a base line. Yeah . Yeah. It'd be an interesting test just to have - just to do M_F_C_C with the neural net and everything else the same. Compare that with just M_- M_F_C_C without the - the net. Without the - Yeah. Yeah. Mm-hmm. I think - I think Dan did some of that. Oh. Um  in his previous Aurora experiments. And with the net it's - it's wonderful. Without the net it's just baseline. Um  I think O_G_I folks have been doing that  too. D- Because I think that for a bunch of their experiments they used  uh  mel cepstra  actually. Yeah. Yeah. Um  of course that's there and this is here and so on. O_K? O_K. Um  for the training corpus - corpus  um  we have  um  the - the d- digits from the various languages. Um  English Spanish um  French What else do we have? And the Finnish. Finnish. And Italian. Uh  no  Italian no. Italian no. I- Italian yes. Italian? Where did th- where did that come from? Digits? Oh. Oh. Italian. Italian. Is that - Was that distributed with Aurora  or - ? Where did that - ? One L_ or two L_'s? The newer one. Yeah. So English  uh  Finnish and Italian are Aurora. And Spanish and French is something that we can use in addition to Aurora. Uh  well. Yeah  so Carmen brought the Spanish  and Stephane brought the French. O_K. And  um  oh yeah  and - Is it French French or Belgian French? There's a - It's  uh  French French. French French. Like Mexican Spain and Spain. Yeah. Or Swiss. Swiss-German. I think that is more important  Mexican Spain. Because more people - Yeah. Yeah. Yeah  probably so. Yeah. Yeah  Herve always insists that Belgian is - i- is absolutely pure French  has nothing to do with - but he says those - those - those Parisians talk funny. Yeah  yeah  yeah. They have an accent. Yeah they - they do  yeah. Yeah. But then he likes Belgian fries too  so. O_K. And then we have  uh  um  broader - broader corpus  um  like TIMIT. TIMIT so far  right? And Spanish too. Spanish - Oh  Spanish stories? Albayzin is the name. What about T_I-digits? Um  T_I-digits - uh all these Aurora f- d- data p- Uh-huh. data is from - is derived from T_I-digits. Oh. Oh O_K. Um  basically  they - they corrupted it with  uh  different kinds of noises at different S_N_R levels. Ah. I see. Yeah. y- And I think Stephane was saying there's - there's some broader s- material in the French also? Yeah  we cou- we could use - Yeah. O_K. The French data. Spanish stories? No. No. Sp- Not Spanish stories? No. Spanish - No. Spanish something. O_K. Albayz- Yeah. Did the Aurora people actually corrupt it themselves  or just specify the signal and the signal-to-noise ratio? They - they corrupted it  um  themselves  but they also included the - the noise files for us  right? O_K. Yeah. Or - so we can go ahead and corrupt other things. I'm just curious  Carmen - I mean  I couldn't tell if you were joking or - i- Is it - is it Mexican Spanish  or is it - No no no no. No no no no. Spanish from Spain. Oh  no  no. It's - it's Spanish from Spain  Spanish. Yeah  O_K. From Spain. Alright. Spanish from Spain. Yeah  we're really covered there now. O_K. And the French from France. O_K. Yeah  the - No  the French is f- yeah  from  uh  Paris  also. Oh  from Paris  O_K. Yeah. O_K. And TIMIT's from lots of different places. Yeah. Yeah. From T_I. From - i- It's from Texas. So may- maybe it's - From the deep South. So- s- so it's not really from the U_S either. Is that - ? O_K. O_K. And  um  with- within the training corporas um  we're  uh  thinking about  um  training with noise. So  incorporating the same kinds of noises that  um  Aurora is in- incorporating in their  um - in their training corpus. Um  I don't think we- we're given the  uh - the unseen noise conditions  though  right? I think what they were saying was that  um  for this next test there's gonna be some of the cases where they have the same type of noise as you were given before hand and some cases where you're not. Like - Mm-hmm. Mm-hmm. O_K. So  presumably  that'll be part of the topic of analysis of the - the test results  is how well you do when it's matching noise and how well you do where it's not. Right. I think that's right. So  I guess we can't train on - on the - the unseen noise conditions. Well  not if it's not seen  yeah. Right. If - Not if it's unseen. Yeah. O_K. I mean  i- i- i- i- it does seem to me that a lot of times when you train with something that's at least a little bit noisy it can - it can help you out in other kinds of noise even if it's not matching just because there's some more variance that you've built into things. But  but  uh  uh  exactly how well it will work will Mm-hmm. depend on how near it is to what you had ahead of time. So. Mm-hmm. O_K  so that's your training corpus  and then your testing corpus - ? Um  the testing corporas are  um  just  um  the same ones as Aurora testing. And  that includes  um  the English Finnish. Spa- um  Italian. Finnish. Uh  we'r- we're gonna get German  right? Ge- Well  so  yeah  the final test  on a guess  is supposed to be German and Danish  right? At the final test will have German. Uh  yeah. Right. The s- yeah  the Spanish  perhaps  Spanish. Oh yeah  we can - we can test on s- Spanish. we will have. Yeah. But the - the Aurora Spanish  I mean. Oh yeah. Mm-hmm. Oh  there's a - there's Spanish testing in the Aurora? Uh  not yet  but  uh  yeah  Yeah  it's preparing. uh  e- pre- they are preparing it  and  well  according to Hynek it will be - we will have this at the end of November  or - They are preparing. Um. O_K  so  uh  something like seven things in each  uh - each column. Yeah - So that's  uh  three hundred and forty-three  uh  different systems that are going to be developed. There's three of you. Uh  so that's hundred and - hundred and fourteen each. What? Yeah. One hundred each  about. What a- what about noise conditions? w- Don't we need to put in the column for noise conditions? Are you just trying to be difficult? No  I just don't understand. Well  th- uh  when - when I put these testings on there  I'm assumi- I'm just kidding. Yeah. There- there's three - three tests. Um  type-A_  type-B_  and type-C_. And they're all - they're all gonna be test- tested  um  with one training of the H_T_K system. Um  there's a script that tests all three different types of noise conditions. Test-A_ is like a matched noise. Test-B_ is a - is a slightly mismatched. And test-C_ is a  um  mismatched channel. And do we do all our training on clean data? Um  no  no  we're - we're gonna be  um  training on the noise files that we do have. Also  we can clean that. No. So  um - Yeah  so I guess the question is how long does it take to do a - a training? I mean  it's not totally crazy t- I mean  these are - a lot of these are built-in things and we know - we have programs that compute P_L_P  we have M_S_G  we have J_RA- you know  a lot of these things will just kind of happen  won't take uh a huge amount of development  it's just trying it out. So  we actually can do quite a few experiments. But how - how long does it take  do we think  for one of these Mm-hmm. trainings? That's a good question. What about combinations of things? Oh yeah  that's right. I mean  cuz  so  for instance  I think the major advantage of M_S_G - Yeah  good point. Oh! Och! A major advantage of M_S_G  I see  th- that we've seen in the past is combined with P_L_P. Yeah. Um. Now  this is turning into a four-dimensional cube? Or you just add it to the features. No. Here. Well  you just select multiple things on the one dimension. Just - Oh  yeah. O_K. Yeah  so  I mean  you don't wanna  uh - Let's see  seven choose two would - - be  uh  twenty-one different combinations. Um. It's not a complete set of combinations  though  right? Probably - What? It's not a complete set of combinations  though  right? No. Yeah  I hope not. Yeah  there's - That would be - Uh  yeah  so P_L_P and M_S_G I think we definitely wanna try cuz we've had a lot of good experience with putting those together. Mm-hmm. Um. Yeah. When you do that  you're increasing the size of the inputs to the net. Do you have to reduce the hidden layer  or something? Well  so - I mean  so i- it doesn't increase the number of trainings. No  no  I'm - I'm just wondering about number of parameters in the net. Do you have to worry about keeping that the same  or - ? But - Uh  I don't think so. There's a computation limit  though  isn't there? Yeah  I mean  it's just more compu- Excuse me? Isn't there like a limit on the computation load  or d- latency  or something like that for Aurora task? Oh yeah  we haven't talked about any of that at all  have we? No. Yeah  so  there's not really a limit. What it is is that there's - there's  uh - it's just penalty  you know? That - that if you're using  uh  a megabyte  then they'll say that's very nice  but  of course  it will never go on a cheap cell phone. O_K. Um. And  u- uh  I think the computation isn't so much of a problem. I think it's more the memory. Uh  and  expensive cell phones  exa- expensive hand-helds  and so forth  are gonna have lots of memory. So it's just that  uh  these people see the - the cheap cell phones as being still the biggest market  so. Mm-hmm. Um. But  yeah  I was just realizing that  actually  it doesn't explode out  um - It's not really two to the seventh. But it's - but - but - i- i- it doesn't really explode out the number of trainings cuz these were all trained individually. Right? So  uh  if you have all of these nets trained some place  then  uh  you can combine their outputs and do the K_L transformation and so forth and - Mm-hmm. and  uh - So  what it - it blows out is the number of uh testings. And  you know - and the number of times you do that last part. But that last part  I think  is so - has gotta be pretty quick  so. Uh. Right? I mean  it's just running the data through - Oh. Well  you gotta do the K_L transformation  but - But wh- what about a net that's trained on multiple languages  though? Eight - y- Is that just separate nets for each language then combined  or is that actually one net trained on? Necessary to put in . Uh  probably one net. Well. Good question. Uh. One would think one net  So. but we've - I don't think we've tested that. Right? So  in the broader training corpus we can - we can use  uh  the three  or  a combination of - of two - two languages. Database three. In one net. Yeah. Mm-hmm. Yeah  so  I guess the first thing is if w- if we know how much a - how long a - a training takes  if we can train up all these - these combinations  uh  then we can start working on testing of them individually  and in combination. Right? Mm-hmm. Because the putting them in combination  I think  is not as much computationally as the r- training of the nets in the first place. Yeah. Right? So y- you do have to compute the K_L transformation. Uh  which is a little bit  but it's not too much. It's not too much  no. Yeah. So it's - But - Yeah. But there is the testing also  which implies training  uh  the H_T_K models and  well  The - the model - the H_T_K model. Uh  right. it's - yeah. But it's - it's - it's not so long. It @@ - Yeah. Right. So if you do have lots of combinations  it's - How long does it take for an  uh  H_T_K training? It's around six hours  I think. For training and testing  yeah. It depends on the - More than six hours. For the Italian  yes. More. Maybe one day. One day? For H_T_K? Really? Yeah. Well. Running on what? Uh  M_ - M_F_C_C. No  I'm sorry  ru- running on what machine? Uh  Ravioli. Uh  I don't know what Ravioli is. Is it - is it an Ultra-five  or is it a - ? mmm Um. I don't know. I don't know. Who is that? I don't know. I don't know. I don't know what a Ravioli is. I don't know. We can check really quickly  I guess. Yeah  I- I think it's- it's- it's not so long because  well  the T_I-digits test data is about  uh how many hours? Uh  th- uh  thirty hours of speech  I think  something like that. It's a few hours. Hmm. Yeah. And it p- Well. Right  so  I mean  clearly  there - there's no way we can even begin to do an- any significant amount here unless we use multiple machines. It's six hours. Right? So - so - w- we - I mean there's plenty of machines here and they're n- they're often not in - in a great - great deal of use. So  I mean  I think it's - it's key that - that the - that you look at  uh  you know  what machines are fast  what machines are used a lot - Uh  are we still using P_make? Is that - ? Oh  I don't know how w- how we would P_make this  though. Um. Well  you have a - I mean  once you get the basic thing set up  you have just all the - uh  a- all these combinations  right? Yeah. Mm-hmm. Um. It's - it's - let's say it's six hours or eight hours  or something for the training of H_T_K. How long is it for training of - of  uh  the neural net? The neural net? Um. I would say two days. Depends on the corpuses  right? It depends. Yeah. It s- also depends on the net. Depends on the corpus. Yeah. How big is the net? For Albayzin I trained on neural network  uh  was  um  one day also. Uh  but on what machine? On a SPERT board. Uh. I - I think the neural net SPERT. Yes. Y- you did a - you did it on a SPERT board. O_K  again  we do have a bunch of SPERT boards. Yeah. And I think there - there - there's - I think you folks are probably go- the ones using them right now. Is it faster to do it on the SPERT  or - ? Uh  don't know. Used to be. It's - it's still a little faster on the SPERTs. Yeah  yeah. Is it? Ad- Adam - Adam did some testing. Or either Adam or - or Dan did some testing and they found that the SPERT board's still - still faster. And Mm-hmm. Mm-hmm. the benefits is that  you know  you run out of SPERT and then you can do other things on your - your computer  and you don't - Mm-hmm. Yeah. So you could be - we have quite a few SPERT boards. You could set up  uh  you know  ten different jobs  or something  to run on SPERT - different SPERT boards and - and have ten other jobs running on different computers. So  it's got to take that sort of thing  or - or we're not going to get through any significant number of these. Yeah. Um. So this is - Yeah  I mean  I kind of like this because what it - No - uh  no  what I like about it is we - we - we do have a problem that we have very limited time. O_K. You know  so  with very limited time  we actually have really quite a - quite a bit of computational resource available if you  you know  get a look across the institute and how little things are being used. And uh  on the other hand  almost anything that really i- you know  is - is new  where we're saying  Yeah. ""Well  let's look at  like we were talking before about  uh  uh  voiced-unvoiced-silence detection features and all those sort -"" that's - I think it's a great thing to go to. But if it's new  then we have this development and - and - and learning process t- to - to go through on top of - just the - the - all the - all the work. So  I - I - I don't see how we'd do it. So what I like about this is you basically have listed all the things that we already know how to do. And - and all the kinds of data that we  at this point  already have. Yeah. And  uh  you're just saying let's look at the outer product of all of these things and see if we can calculate them. a- a- Am I - am I interpreting this correctly? Is this sort of what - what you're thinking of doing in the short term? Mmm. O_K. Yeah. So - so then I think it's just the - the missing piece is that you need to  uh  you know - you know  talk to - talk to  uh  Chuck  talk to  uh  Adam  uh  sort out about  uh  what's the best way to really  you know  attack this as a - as a - as a mass problem in terms of using many machines. Uh  and uh  then  you know  set it up in terms of scripts and so forth  and - uh  in - in kind o- some kind of structured way. Uh. Um  and  you know  when we go to  uh  O_G_I next week  uh  we can then present to them  you know  what it is that we're doing. And  uh  we can pull things out of this list that we think they are doing sufficiently  that  you know  we're not - we won't be contributing that much. Mmm. Mm-hmm. Um. And  uh - Then  uh  like  we're there. Yeah? How big are the nets you're using? Um  for the - for nets trained on digits  um  we have been using  uh  four hundred order hidden units. And  um  for the broader class nets we're - we're going to increase that because the  um  the digits nets only correspond to about twenty phonemes. Uh-huh. So. Broader class? Um  the broader - broader training corpus nets like TIMIT. Um  w- we're gonna - Oh  it's not actually broader class  it's actually finer class  but you mean - Right. y- You mean Right. Yeah. more classes. More classes. Right  right. More classes. That's what I mean. Mm-hmm. Yeah. Yeah. Yeah. And. Yeah. Carmen  did you - do you have something else to add? We - you haven't talked too much  and - D- I begin to work with the Italian database to - nnn  to - with the f- front-end and with the H_T_K program and the @@ . And I trained eh  with the Spanish two neural network with P_L_P and with LogRASTA P_L_P. I don't know exactly what is better if - if LogRASTA or J_RASTA. Well  um  J_RASTA has the potential to do better  but it doesn't always. It's - i- i- J_RASTA is more complicated. It's - it's  uh - instead of doing RASTA with a log  you're doing RASTA with a log-like function that varies depending on a J_ parameter  uh  which is supposed to be sensitive to the amount of noise there is. So  it's sort of like the right transformation to do the filtering in  is dependent on how much noise there is. Hm-hmm. And so in J_RASTA you attempt to do that. It's a little complicated because once you do that  you end up in some funny domain and you end up having to do a transformation afterwards  which requires some tables. And  uh  so it's - it's - it's a little messier  uh  there's more ways that it can go wrong  uh  but if - if - if you're careful with it  it can do better. So  it's - So. Hm-hmm. It's a bit - I'll do better. Um  and I think to - to - to recognize the Italian digits with the neural netw- Spanish neural network  and also to train another neural network with the Spanish digits  the database of Spanish digits. And I working that. Yeah. But prepa- to prepare the - the database are difficult. Was for me  n- it was a difficult work last week with the labels because the - the program with the label obtained that I have  the Albayzin  is different w- to the label to train the neural network. And that is another work that we must to do  to - to change . I - I didn't understand. Uh  for example Albayzin database was labeled automatically with H_T_K. It's not hand - it's not labels by hand. Labels. I'm sorry  I'm sorry. The labels. I'm sorry. Oh  ""l- labeled "". I'm sorry  I have a p- I had a problem with the pronunciation. The labels. Yeah  O_K. So  O_K  so let's start over. So  TI- TIMI- TIMIT's hand-labeled  and - and you're saying about the Spanish? Oh  also that - Yes. The Spanish labels? That was in different format  that the format for the em - the program to train the neural network. Oh  I see. I necessary to convert. And someti- well - It's - it's - Yeah. So you're just having a problem converting the labels. Yeah  but n- yes  because they have one program  Feacalc  but no  l- LabeCut  but don't - doesn't  eh  include the H_T_K format to convert. Mm-hmm. And  Hmm. I don't know what. I ask - e- even I ask to Dan Ellis what I can do that  and h- they - he say me that h- he does- doesn't any - any s- any form to - to do that. And at the end  I think that with LabeCut I can transfer to ASCII format  and H_T_K is an ASCII format. And I m- do another  uh  one program to put ASCII format of H_T_K to ase- ay- ac- ASCII format to Mm-hmm. Exceed and they used LabCut to - to pass . O_K  yeah. Actually that was complicated  but well  I know how we can did that - do that. So you- Sure. So it's just usual kind of uh - sometimes say housekeeping  right? To get these - get these things sorted out. Yeah. So it seems like there's - there's some peculiarities of the  uh - of each of these dimensions that are getting sorted out. And then  um  if - if you work on getting the  uh  assembly lines together  and then the - the pieces sort of get ready to go into the assembly line and gradually can start  you know  start turning the crank  more or less. And  uh  uh  we have a lot more computational capability here than they do at O_G_I  so I think that i- if - What's - what's great about this is it sets it up in a very systematic way  so that  uh  once these - all of these  you know  mundane but real problems get sorted out  we can just start turning the crank and - and Mm-hmm. push all of us through  and then finally figure out what's best. Yeah. Um  I - I was thinking two things. Uh  the first thing was  um - we - we actually had thought of this as sort of like  um - not - not in stages  but more along the - the time axis. Just kind of like one stream at a time  je-je-je-je-je check out the results and - and go that way. Mm-hmm. Oh  yeah  yeah  sure. No  I'm just saying  I'm just thinking of it like loops  right? And so  y- y- y- if you had three nested loops  that you have a choice for this  a choice for this  and a choice for that  right? And you're going through them all. That - that's what I meant. Uh-huh. Yeah. Mm-hmm. Right  right. And  uh  the thing is that once you get a better handle on how much you can realistically do  uh  um  concurrently on different machines  different SPERTs  and so forth  uh  and you see how long it takes on what machine and so forth  you can stand back from it and say  ""O_K  if we look at all these combinations we're talking about  and combinations of combinations  and so forth "" you'll probably find you can't do it all. Mm-hmm. O_K. O_K  so then at that point  uh  we should sort out which ones do we throw away. Which of the combinations across - you know  what are the most likely ones  and - Mm-hmm. And  uh  I still think we could do a lot of them. I mean  it wouldn't surprise me if we could do a hundred of them or something. But  probably when you include all the combinations  you're actually talking about a thousand of them or something  and that's probably more than we can do. Uh  but a hundred is a lot. And - and  uh  um - O_K. Yeah. Yeah  and the - the second thing was about scratch space. And I think you sent an email about  um  e- scratch space for - for people to work on. And I know that  uh  Stephane's working from an N_T machine  so his - his home directory exists somewhere else. His - his stuff is somewhere else  yeah. Yeah  I mean  my point I - I want to - Yeah  thanks for bring it back to that. My - th- I want to clarify my point about that - that - that Chuck repeated in his note. Um. We're - over the next year or two  we're gonna be upgrading the networks in this place  but right now they're still all te- pretty much all ten megabit lines. Mm-hmm. And we have reached the - this - the machines are getting faster and faster. So  it actually has reached the point where it's a significant drag on the time for something to move the data from one place to another. Mm-hmm. So  you - you don't w- especially in something with repetitive computation where you're going over it multiple times  you do - don't want to have the - the data that you're working on distant from where it's being - where the computation's being done if you can help it. Mm-hmm. Uh. Now  we are getting more disk for the central file server  which  since it's not a computational server  would seem to be a contradiction to what I just said. But the idea is that  uh  suppose you're working with  uh  this big bunch of multi- multilingual databases. Um  you put them all in the central ser- at the cen- central file server. Mm-hmm. Then  when you're working with something and accessing it many times  you copy the piece of it that you're working with over to some place that's close to where the computation is and then do all the work there. And then that way you - you won't have the - the network - you won't be clogging the network for yourself and others. That's the idea. Mmm. So  uh  it's gonna take us - It may be too late for this  uh  p- precise crunch we're in now  but  uh  we're  uh - It's gonna take us a couple weeks at least to get the  uh  uh  the amount of disk we're gonna be getting. We're actually gonna get  uh  I think four more  uh  thirty-six gigabyte drives and  uh  put them on another - another disk rack. We ran out of space on the disk rack that we had  so we're getting another disk rack and four more drives to share between  uh - primarily between this project and the Meetings - Meetings Project. Um. But  uh  we've put another - I guess there's another eighteen gigabytes that's - that's in there now to help us with the immediate crunch. But  uh  are you saying - So I don't know where you're - Stephane  where you're doing your computations. If - i- so  you're on an N_T machine  so you're using some external machine to - Yeah  it  uh - Well  to - It's Nutmeg and Mustard  I think  the - Do you know these yet? I don't know what kind. Nuh-uh. Yeah  O_K. Uh  are these - are these  uh  computational servers  or something? I'm - I've been kind of out of it. Yeah  I think  yeah. I think so. Mmm. Unfortunately  these days my idea of running comput- of computa- doing computation is running a spread sheet. So. Mmm. Uh  haven't been - haven't been doing much computing personally  so. Um. Yeah  so those are computational servers. So I guess the other question is what disk there i- space there is there on the computational servers. Right. Yeah  I'm not sure what's available on - is it - you said Nutmeg and what was the other one? Mustard. Mustard. O_K. Huh. Yeah  Well  you're the - you're the disk czar now. So Right  right. Well   I'll check on that. Yeah. Yeah  so basically  uh  Chuck will be the one who will be sorting out what disk needs to be where  and so on  and I'll be the one who says  ""O_K  spend the money."" So. Which  I mean  n- these days  uh  if you're talking about scratch space  it doesn't increase the  uh  need for backup  and  uh  I think it's not that big a d- and the - the disks themselves are not that expensive. Right now it's - What you can do  when you're on that machine  is  uh  just go to the slash-scratch directory  and do a D_F minus K_  and it'll tell you if there's space available. Yeah. Uh  and if there is then  uh - But wasn't it  uh - I think Dave was saying that he preferred that people didn't put stuff in slash-scratch. It's more putting in d- s- X_A or X_B or  right? Well  there's different - there  um  there's - Right. So there's the slash-X_-whatever disks  and then there's slash-scratch. And both of those two kinds are not backed up. And if it's called ""slash-scratch""  it means it's probably an internal disk to the machine. Um. And so that's the kind of thing where  like if - um  O_K  if you don't have an N_T  but you have a - a - a Unix workstation  and they attach an external disk  it'll be called ""slash-X_-something"" uh  if it's not backed up and it'll be ""slash-D_-something"" if it is backed up. And if it's inside the machine on the desk  it's called ""slash-scratch"". But the problem is  if you ever get a new machine  they take your machine away. It's easy to unhook the external disks  put them back on the new machine  but then your slash-scratch is gone. So  you don't wanna put anything in slash-scratch that you wanna keep around for a long period of time. But if it's a copy of  say  some data that's on a server  you can put it on slash-scratch because  um  first of all it's not backed up  and second it doesn't matter if that machine disappears and you get a new machine because you just recopy it to slash-scratch. So tha- that's why I was saying you could check slash-scratch on those - on - on  um  Mustard and - and Nutmeg to see if - if there's space that you could use there. I see. You could also use slash-X_-whatever disks on Mustard and Nutmeg. Yeah  yeah. Um. Yeah  and we do have - I mean  yeah  so - so you - yeah  it's better to have things local if you're gonna run over them lots of times so you don't have to go to the network. Right  so es- so especially if you're - right  if you're - if you're taking some piece of the training corpus  which usually resides in where Chuck is putting it all on the - on the  uh  file server  uh  then  yeah  it's fine if it's not backed up because if it g- g- gets wiped out or something  y- I mean it is backed up on the other disk. So  yeah  O_K. Mm-hmm. Yeah  so  one of the things that I need to - I've started looking at - Uh  is this the appropriate time to talk about the disk space stuff? Sure. I've started looking at  um  disk space. Dan - David  um  put a new  um  drive onto Abbott  that's an X_ disk  which means it's not backed up. So  um  I've been going through and copying data that is  you know  some kind of corpus stuff usually  that - that we've got on a C_D-ROM or something  onto that new disk to free up space on other disks. And  um  so far  um  I've copied a couple of Carmen's  um  databases over there. We haven't deleted them off of the slash-D_C disk that they're on right now in Abbott  um  uh  but we - I would like to go through - sit down with you about some of these other ones and see if we can move them onto  um  this new disk also. Yeah  O_K. There's - there's a lot more space there  and it'll free up more space for doing the experiments and things. So  anything that - that you don't need backed up  we can put on this new disk. Um  but if it's experiments and you're creating files and things that you're gonna need  you probably wanna have those on a disk that's backed up  just in case something goes wrong. So. Um So far I've - I've copied a couple of things  but I haven't deleted anything off of the old disk to make room yet. Um  and I haven't looked at the - any of the Aurora stuff  except for the Spanish. So I - I guess I'll need to get together with you and see what data we can move onto the new disk. Yeah  O_K. Um  yeah  I - I just - an- another question occurred to me is - is what were you folks planning to do about normalization? Uh. Um. Well  we were thinking about using this systematically for all the experiments. Um. This being - ? So  but - Uh. So that this could be another dimension  but we think perhaps we can use the - the best  uh  um  uh  normalization scheme as O_G_I is using  so  with parameters that they use there  or - and - Yeah  I think that's a good idea. I mean it's i- i- we - we seem to have enough dimensions as it is. So probably if we u- u- Yeah  yeah  yeah. sort of take their - probably the on-line - line normalization because then it - it's - if we do anything else  we're gonna end up having to do on-line normalization too  so we may as well just do on-line normalization. Mm-hmm. So. Um. So that it's plausible for the final thing. Good. Um. So  I guess  yeah  th- the other topic - I - maybe we're already there  or almost there  is goals for the - for next week's meeting. Uh. i- i- i- it seems to me that we wanna do is flush out what you put on the board here. Uh. You know  maybe  have it be somewhat visual  a little bit. Uh  so w- we can say what we're doing  yeah. O_K. Like a s- like a slide? O_K. And  um  also  if you have sorted out  um  this information about how long i- roughly how long it takes to do on what and  you know  what we can - how many of these trainings  uh  uh  and testings and so forth that we can realistically do  uh  then one of the big goals of going there next week would be to - to actually settle on which of them we're gonna do. And  uh  when we come back we can charge in and do it. Um. Anything else that - I- a- a- Actually - started out this - this field trip started off with - with  uh  Stephane talking to Hynek  so you may have - you may have had other goals  uh  for going up  and any- anything else you can think of would be - we should think about accomplishing? I mean  I'm just saying this because maybe there's things we need to do in preparation. Oh   I think basically  this is - this is  uh  yeah. O_K. O_K. Uh. Alright. And uh - and the other - the - the last topic I had here was  um  uh d- Dave's fine offer to - to  uh  do something on this. I mean he's doing - - he's working on other things  but to - to do something on this project. So the question is  ""Where - where could we  uh  uh  most use Dave's help?"" Um  yeah  I was thinking perhaps if  um  additionally to all these experiments  which is not really research  well I mean it's  uh  running programs and  um  Yeah. trying to have a closer look at the - perhaps the  um  speech  uh  noise detection or  uh  voiced-sound-unvoiced-sound detection and - Which could be important in - i- for noise - noise - I think that would be a - I think that's a big - big deal. Because the - you know  the thing that Sunil was talking about  uh  with the labels  uh  labeling the database when it got to the noisy stuff? The - That - that really throws things off. You know  having the noise all of a sudden  your - your  um  speech detector  I mean the - the  um - What was it? What was happening with his thing ? He was running through these models very quickly. He was getting lots of  uh  Mmm. uh insertions  is what it was  in his recognitions. The only problem - I mean  maybe that's the right thing - the only problem I have with it is exactly the same reason why you thought it'd be a good thing to do. Um  I - I think that - Let's fall back to that. But I think the first responsibility is sort of to figure out if there's something that  uh  an - an additional - Uh  that's a good thing you - remove the mike. Go ahead  good. Uh  uh. What an additional clever person could help with when we're really in a crunch for time. Right? Cuz Dave's gonna be around for a long time  right? He's - he's gonna be here for years. Yeah. Yeah. And so  um  over years  if he's - if he's interested in  you know  voiced-unvoiced-silence  he could do a lot. But if there - if in fact there's something else that he could be doing  that would help us when we're - we're sort of uh strapped for time - We have - we - we've  you know  only  uh  another - another month or two to - you know  with the holidays in the middle of it  um  to - to get a lot done. If we can think of something - some piece of this that's going to be - The very fact that it is sort of just work  and i- and it's running programs and so forth  is exactly why it's possible that it - some piece of could be handed to someone to do  because it's not - Uh  yeah  so that - that's the question. And we don't have to solve it right this s- second  but if we could think of some - some piece that's - that's well defined  that he could help with  he's expressing a will- willingness to do that. What about training up a  um  Uh. a multilingual net? Yeah. Yes  maybe to  mmm  put together the - the label - the labels between TIMIT and Spanish or something like that. Yeah  so defining the superset  and  uh  joining the data and - Yes. Yeah. Mmm. Yeah. Uh. Yeah  that's something that needs to be done in any event. Yeah. So what we were just saying is that - that  um - I was arguing for  if possible  coming up with something that - that really was development and wasn't research because we - we're - we have a time crunch. And so  uh  if there's something that would - would save some time that someone else could do on some other piece  then we should think of that first. See the thing with voiced-unvoiced-silence is I really think that - that it's - to do - to do a - a - a - a poor job is - is pretty quick  uh  or  you know  a so-so job. You can - you can - you can throw in a couple fea- we know what - what kinds of features help with it. Hmm. You can throw something in. You can do pretty well. But I remember  in fact  when you were working on that  and you worked on for few months  as I recall  and you got to  say ninety-three percent  and Mm-hmm. getting to ninety-four really really hard. Another year. Yeah  yeah. So  um - And th- th- the other tricky thing is  since we are  uh  even though we're not - we don't have a strict prohibition on memory size  and - and computational complexity  uh  clearly there's some limitation to it. So if we have to - if we say we have to have a pitch detector  say  if we - if we're trying to incorporate pitch information  or at least some kind of harmonic - harmonicity  or something  this is another whole thing  take a while to develop. Anyway  it's a very very interesting topic. I mean  one - I think one of the - a lot of people would say  and I think Dan would also  uh  that one of the things wrong with current speech recognition is that we - we really do throw away all the harmonicity information. Uh  we try to get spectral envelopes. Reason for doing that is that most of the information about the phonetic identity is in the spectral envelopes are not in the harmonic detail. But the harmonic detail does tell you something. Like the fact that there is harmonic detail is - is real important. So. Um. So  uh. So I think - Yeah. So - wh- that - so the - the other suggestion that just came up was  well what about having him work on the  uh  multilingual super f- superset kind of thing. Uh  coming up with that and then  you know  training it - training a net on that  say  um  from - from  uh - from TIMIT or something. Is that - or uh  for multiple databases. What - what would you - what would you think it would - wh- what would this task consist of? Yeah  it would consist in  uh  well  um  creating the - the superset  and  uh  modifying the lab- labels for matching the superset. Uh. Uh  creating a superset from looking at the multiple languages  and then creating i- m- changing labels on TIMIT? Well  creating the mappings  actually. Yeah. Or on - or on multiple language - multiple languages? No. Yeah  yeah  with the @@ three languages  and - The multiple language. Maybe for the other language because TIMIT have more phone. Yeah. Uh. So you'd have to create a mapping from each language to the superset. Yeah. Mm-hmm. From each language to the superset  yeah. Mmm. Yeah. There's  um - Carmen was talking about this SAMPA thing  and it's  um  it's an effort by linguists to come up with  um  a machine readable I_P_A  um  sort of thing  right? And  um  they - they have a web site that Stephane was showing us that has  um - Yeah. has all the English phonemes and their SAMPA correspondent  um  phoneme  and then  um  they have Spanish  they have German  they have all - all sorts of languages  um  mapping - mapping to the SAMPA phonemes  which - Yeah  the tr- the transcription  though  for Albayzin is n- the transcription are of SAMPA the same  uh  how you say  symbol that SAMPA appear. SAMPA? What does ""SAMPA"" mean? Mm-hmm. Hmm. But I don't know if TIMIT o- how is TIMIT. So  I mean - What - I'm sorry. Go ahead. I was gonna say  does that mean I_P_A is not really international? No  it's - it's saying - y- It uses special diacritics and stuff  which you can't do with ASCII characters. Yeah. can't print on ASCII. Oh  I see. So the SAMPA's just mapping those. Got it. What  uh - Has O_G_I done anything about this issue? Do they have - Do they have any kind of superset that they already have? I don't think so. Well  they - they - they're going actually the - the other way  defining uh  phoneme clusters  apparently. Well. Aha. That's right. Uh  and that's an interesting way to go too. So they just throw the speech from all different languages together  then cluster it into sixty or fifty or whatever clusters? I think they've not done it  uh  doing  uh  multiple language yet  but what they did is to training  uh  English nets with all the phonemes  and then training it in English nets with  uh  kind of seventeen  I think it was - seventeen  uh  broad classes. Automatically derived - Mm-hmm. Yeah. Automatically derived broad classes  or - ? Uh-huh. Yeah  I think so. Uh  and  yeah. And the result was that apparently  when testing on cross-language it was better. I think so. But Hynek didn't add - didn't have all the results when he showed me that  so  well. But - So that does make an interesting question  though. Is there's some way that we should tie into that with this. Um. Right? I mean  if - if in fact that is a better thing to do  should we leverage that  rather than doing  um  our own. Right? So  if i- if - if they s- I mean  we have - i- we have the - the trainings with our own categories. And now we're saying  ""Well  how do we handle cross-language?"" And one way is to come up with a superset  but they are als- they're trying coming up with clustered  and do we think there's something wrong with that? I think that there's something wrong or - Well  because - O_K. What w- Well  for the moment we are testing on digits  and e- i- perhaps u- using broad phoneme classes  it's - it's O_K for um  uh classifying the digits  but as soon as you will have more words  well  words can differ with only a single phoneme  and - which could be the same  uh  class. Well. I see. So. Right. Although  you are not using this for the - You're using this for the feature generation  though  not the - So  I'm afraid - Yeah  but you will ask the net to put one for th- th- the phoneme class and - s- Yeah. So. Yeah. So you're saying that there may not be enough information coming out of the net to help you discriminate the words? Well. Yeah  yeah. Mmm. Fact  most confusions are within the phone - phone classes  right? I think  uh  Larry was saying like obstruents are only confused with other obstruents  et cetera  et cetera. Yeah. Hmm. Yeah. Yeah. Yeah  this is another p- yeah  another point. Yeah. So - so  maybe we could look at articulatory type stuff  right? But that's what I thought they were gonna - Did they not do that  or - ? I don't think so. Well  they were talking about  perhaps  but they d- I d- w- Yeah. So - They're talking about it  but that's sort of a question whether they did because that's - that's the other route to go. Instead of this  you know - Mm-hmm. Superclass. Instead of the - the - the - the superclass thing  which is to take - So suppose y- you don't really mark arti- To really mark articulatory features  you really wanna look at the acoustics and - and see where everything is  and we're not gonna do that. So  uh  the second class way of doing it is to look at the  uh  phones that are labeled and translate them into acoustic - uh  uh - articulatory  uh  uh  features. So it won't really be right. You won't really have these overlapping things and so forth  but - So the targets of the net - are these - ? Articulatory feature. Articulatory features. Right. But that implies that you can have more than one on at a time? That's right. Ah. O_K. You either do that or you have multiple nets. I see. Um. And  um I don't know if our software - this - if the qu- versions of the Quicknet that we're using allows for that. Do you know? Allows for - ? Multiple targets being one? Oh  um  we have gotten soft targets to - to work. O_K. So that - that'll work  yeah. Yeah. O_K. So  um  that's another thing that could be done - is that we could - we could  uh  just translate - instead of translating to a superset  Um. just translate to articulatory features  some set of articulatory features and train with that. Now the fact - even though it's a smaller number  it's still fine because you have the - the  uh  combinations. So  in fact  it has every  you know - it had - has - has every distinction in it that you would have the other way. Yeah. But it should go across languages better. We could do an interesting cheating experiment with that too. We could - I don't know  if you had uh the phone labels  you could replace them by their articulatory features and then feed in a vector with those uh  things turned on based on what they're supposed to be for each phone to see if it - if you get a big win. Do you know what I'm saying? No. So  um  I mean  if your net is gonna be outputting  uh  a vector of - basically of - well  it's gonna have probabilities  but let's say that they were ones and zeros  then y- and you know for each  um  I don't know if you know this for your testing data  but if you know for your test data  you know  what the string of phones is and - and you have them aligned  then you can just - instead of going through the net  just create the vector for each phone and feed that in to see if that data helps. Eh  eh  what made me think about this is  I was talking with Hynek and he said that there was a guy at A_T_and_T who spent eighteen months working on a single feature. And because they had done some cheating experiments - This was the guy that we were just talking a- that we saw on campus. So  this was Larry Saul who did this - did this. He used sonorants. Oh  O_K. Right  O_K  right. Was what he was doing. Yeah. And they - they had done a cheating experiment or something  right? and determined that - He - he di- he didn't mention that part. But. Well  Hynek said that - that  I guess before they had him work on this  they had done some experiment where if they could get that one feature right  I see. O_K. it dramatically improved the result. So I was thinking  you know - it made me think about this  that if - it'd be an interesting experiment just to see  you know  if you did get all of those right. Should be. Because if you get all of them in there  that defines all of the phones. So that's - that's equivalent to saying that you've got - Right. got all the phones right. So  if that doesn't help  there's - Yeah. Although  yeah  it would be - make an interesting cheating experiment because we are using it in this funny way  where we're converting it into features. Yeah. And then you also don't know what error they've got on the H_T_K side. You know? Yeah. It sort of gives you your - the best you could hope for  kind of. Mmm. Mmm  I see. The soft training of the nets still requires the vector to sum to one  though  right? To sum up to one. So you can't really feed it  like  two articulatory features that are on at the same time with ones cuz it'll kind of normalize them down to one half or something like that  for instance. But perhaps you have the choice of the final nonl- uh  nonlinearity  yeah. Right. Nonlinearity? Um  it's sig- No  it's actually sigmoid-X_ Is it always softmax or - ? Yeah. for the - So if you choose sigmoid it's o- it's O_K? You  um - I think - I think apparently  the  uh - Did we just run out of disk  or - ? Why don't you just choose linear? What's that? Right? Linear outputs? Linear outputs? Isn't that what you'll want? Um. If you're gonna do a K_L Transform on it. Right  right. Right  but during the training  we would train on sigmoid-X_ and then Oh  you - Yeah? at the end just chop off the final nonlinearity. Hmm. So  we're - we're - we're off the air  or - ? Um  about to be About to be off the air. ","Two main options were discussed as to the organisation of the collected data. On the one hand  a bespoke XML structure that connects transcriptions and annotations (down to the word-level) to a common timeline. Its advantages are that it is easier to read  parse  map onto the Transcriber format and to expand with extra features. Phone-level analysis can be included in the same structure  or in a separate  linked file. The respective frame-level representation can be handled by P-files  a technology developed at ICSI  which also comes with a library of tools. Separation of levels of analysis makes files more compact and manageable. XML standards offer libraries that can be used for the development of search tools. On the other hand  the ATLAS (NIST) technology offers a very similar  but more generic organisational scheme based on nodes and links. These are labeled with domain specific types  like ""utterance"" or ""speaker"". This option offer well-developed infrastructure and flexibility as to the type of data storage (flat XML files or relational database). In either case  it is important for the chosen format to allow for fast searches  flexible updates and  if possible  be reusable in future work. In order to confirm the suitability of the data format provided by the ATLAS project  its current state of development will be investigated. More specifically  the issues that have to be ascertained are  firstly  whether the external file representation offers a format that would be appropriate for speech data  and  secondly  how the linking between the different annotations (eg  between word-level representations and prosodic-feature structures) can be achieved. Regardless of the actual format  however  there was consensus that keeping levels of analysis (words  phones  frames  etc) on separate  inter-linked files can make their management easier. Choosing a project-specific format for the representation of the data might not be optimal for future work. On the other hand  it is not yet clear whether a more standardised  but generic technology  like that of the ATLAS project  can accommodate all the requirements of speech analysis. Regardless of the particular format  including all annotations (sentences  words  phones  frames  etc) in one file could result in unmanageable file sizes. Searching  updating or simply parsing a file for a simple task can become an unwieldy process. Even P-files  which are only for frame-level annotation  may be too verbose for the amount of data resulting from hour-long recordings. The actual mapping of word-level transcriptions to frame-level representations is expected to be problematic anyway. Likewise  problems will arise if  in the future  slightly different transcripts of the same data are annotated in formats that do not include time-marks. Trying to merge such annotations later will not be easy  because of the combination of transcription discrepancies with the loss of the underlying connection offered by the time-marks. An XML scheme to build representations of the data is already available. It incorporates information regarding utterances  sentences  speakers  words  etc. All these features are linked together via time-marks that slot into a single  common timeline. This format also allows for linking to different levels of representation of the same data. For the frame-level representation  P-files is a readily available technology  developed at ICSI. Besides the appropriate format  P-files come with a library of tools and the respective documentation. "
"-st. Am I on? I guess so. Radio two. Hmm. Radio two. Hello? Video killed the radio star. Wow. Mm-hmm. Hi? Blow into it  it works really well. Channel B_. People say the strangest things when their microphones are on. Channel four. Test. O_K. Uh-oh. Radio four. Hello? Today's So everybody- everybody's on? Yeah. So y- you guys had a - a meeting with uh - with Hynek which I unfortunately had to miss. Um Mmm. and uh somebody eh e- and uh I guess Chuck you weren't there either  so the- uh I was there. Oh you were there? With Hynek? Yeah. Yeah. So everybody knows what happened except me. O_K. Maybe somebody should tell me. Oh yeah. Alright. Well. Uh first we discussed about some of the points that I was addressing in the mail I sent last week. Uh-huh. So. Yeah. About the um  well - the downsampling problem. Yeah. Uh and about the f- the length of the filters and - Yeah. What was the - w- what was the downsampling problem again? I forget. So we had - So the fact that there - there is no uh low-pass filtering before the downsampling. Well. Uh-huh. There is because there is L_D_A filtering but that's perhaps not uh the best w- m- Well. Depends what it's frequency characteristic is  yeah. Mm-hmm. System on So you could do a - you could do a stricter one. Maybe. Yeah. Yeah. So we discussed about this  about the um - Was there any conclusion about that? Uh ""try it"". Yeah. I see. I guess. Uh. Yeah. So again this is th- this is the downsampling uh of the uh - the feature vector stream and um Yeah I guess the - the uh L_D_A filters they were doing do have um uh let's see  so the - the - the feature vectors are calculated every ten milliseconds so uh the question is how far down they are at fifty - fifty hertz. Uh. Yeah. Mm-hmm. Um. Sorry at twenty- five hertz since they're downsampling by two. So. Does anybody know what the frequency characteristic is? We don't have yet Oh O_K. um So  yeah. O_K. We should have a look first at  perhaps  Yeah. the modulation spectrum. Um. So there is this  there is the um length of the filters. Um. So the i- this idea of trying to find filters with shorter delays. Um. Hmm-hmm. We started to work with this. Mmm. And the third point um was the um  yeah  the on-line normalization where  well  the recursion f- recursion for the mean estimation is a filter with some kind of delay Yeah. and that's not taken into account right now. Um. Yeah. And there again  yeah. For this  the conclusion of Hynek was  well  ""we can try it but -"" Uh-huh. Um. Try - try what? So try to um um take into account the delay of the recursion for the mean estimation. O_K. Mmm. And this - we've not uh worked on this yet. Um  yeah. And so while discussing about these - these L_D_A filters  some i- issues appeared  like well  the fact that if we look at the frequency response of these filters it's uh  well  we don't know really what's the important part in the frequency response and there is the fact that in the very low frequency  these filters don't - don't really remove a lot. compared to the - to the uh standard RASTA filter. Uh and that's probably a reason why  yeah  on-line normalization helps because Right. it - it  yeah  it removed this mean. Um. Yeah  but perhaps everything could - should be - could be in the filter  I mean  uh the - the mean normalization and - Yeah. So. Yeah. So basically that was - that's all we discussed about. We discussed about good things to do also uh well  generally good stuff Mm-hmm. to do for the research. And this was this L_D_A uh tuning perhaps and Hynek proposed again to his uh TRAPS  so. O_K. Yeah  um. I mean I g- I guess the key thing for me is - is figuring out how to better coordinate between the two sides cuz - because um Mm-hmm. uh I was talking with Hynek about it later and the - the - sort of had the sense sort of that - that neither group of people wanted to - to bother the other group too much. And - and I don't think anybody is  you know  closed in in their thinking or are unwilling to talk about things but I think that you were sort of waiting for them to tell you that they had something for you and - and that - and expected that they would do certain things and they were sor- they didn't wanna bother you and Mm-hmm. they were sort of waiting for you and - and - and uh we ended up with this thing where they - they were filling up all of the possible latency themselves  and they just had- hadn't thought of that. So. Uh. Yeah. Well  but. Yeah. Yeah. I mean it's true that maybe - maybe no one really thought about that - that this latency thing would be such a - a strict issue Well - in - in uh - the other - Yeah I don't know what happened really  but Yeah. I guess it's - it's also so uh the time constraints. Because  well  we discussed about that - about this problem and they told us ""well  we will do all that's possible to have enough space for a network"" but then  yeah  Then they couldn't. perhaps they were too short with the time and I see. uh yeah. But there was also problem - perhaps a problem of communication. So  yeah. Now we will try to - Just talk more. Yeah  slikes and send mails. u- s- o- o- Yeah. Yeah. Yeah. Uh. O_K . So there's um - Alright. Well maybe we should just uh I mean you're - you're bus- other than that you folks are busy doing all the - all the things that you're trying that we talked about before right? And this - machines are busy and you're busy and Yeah. Basically. Yeah. O_K. Um. Oh. Let's - let's  I mean  I think that as - as we said before that one of the things that we're imagining is that uh there - there will be uh in the system we end up with there'll be something to explicitly uh uh do something about noise in addition to the Mm-hmm. uh other things that we're talking about and that's probably the best thing to do. And there was that one email that said that it sounded like uh uh things looked very promising up there in terms of uh I think they were using Ericsson's approach or something and in addition to - They're doing some noise removal thing  right? Yeah  yeah. So yeah we're - will start to do this also. Yeah. Uh so Carmen is just looking at the Ericsson - Ericsson code. Yeah. We modif- Mm-hmm. Yeah  I modified it - well  modifying - And I studied Barry's sim code   more or less. to take @@ the first step the spectral subtraction. and we have some - the feature for Italian database and we will try with this feature with the filter Mm-hmm. Mm-hmm. to find the result. But we haven't result until this moment. Yeah  sure. But well  we are working in this also and maybe try another type of spectral subtraction  I don't - Yeah. When you say you don't have a result yet you mean it's - it's just that it's in process or that you - it finished and it didn't get a good result? No. No  no n- we have n- we have do the experiment only have the feature - the feature but Yeah. the experiment have we have not make the experiment and Oh. O_K. maybe will be good result or bad result  we don't know. Yeah. Yeah. Yeah. O_K. So um I suggest actually now we - we - we sorta move on and - and hear what's - what's - what's happening in - in other areas like what's - what's happening with your investigations Oh um about echos and so on. Well um I haven't started writing the test yet  I'm meeting with Adam today Mm-hmm. um and he's going t- show me the scripts he has for um running recognition on mee- Meeting Recorder digits. Mm-hmm. Uh I also um haven't got the code yet  I haven't asked Hynek for - for the - for his code yet. Cuz I looked at uh Avendano's thesis and I don't really understand what he's doing yet but it - it - it sounded like um the channel normalization part um of his thesis um was done in a - a bit of I don't know what the word is  a - a bit of a rough way um it sounded like he um he - he - it - it wasn't really fleshed out and maybe he did something that was interesting for the test situation but I - I'm not sure if it's what I'd wanna use so I have to - I have to read it more  I don't really understand what he's doing yet. O_K. It's my Yeah I haven't read it in a while so I'm not gonna be too much help unless I read it again  so. Oh yeah? I know this is mine here. O_K. Um. The um - so you  and then you're also gonna be doing this echo cancelling between the - the close mounted and the - and the - the - the - what we're calling a cheating experiment uh of sorts Uh I- between the distant - I'm ho- Right. Well - or I'm hoping - I'm hoping Espen will do it. Um Ah! O_K. u- F- um Delegate. That's good. It's good to delegate. I - I think he's at least planning to do it for the cl- close-mike cross-talk and so maybe I can just take whatever setup he has and use it. Great. Great. Yeah actually um he should uh I wonder who else is I think maybe it's Dan Ellis is going to be doing uh a different cancellation. Um. One of the things that people working in the meeting task wanna get at is they would like to have cleaner close-miked recordings. So uh this is especially true for the lapel but even for the close - close-miked uh cases um we'd like to be able to have um other sounds from other people and so forth removed from - So when someone isn't speaking you'd like the part where they're not speaking to actually be - So what they're talking about doing is using ec- uh echo cancellation-like techniques. It's not really echo but uh just um uh taking the input from other mikes and using uh uh a uh - an adaptive filtering approach to remove the effect of that uh other speech. So. Um what was it  there was - there was some - some - some point where eh uh Eric or somebody was - was speaking and he had lots of silence in his channel and I was saying something to somebody else uh which was in the background and it was not - it was recognizing my words  which were the background speech Hmm. on the close - close mike. Oh the - What we talked about yesterday? Yeah that was actually my - I was wearing the - Yes. Oh you - it was you I was I was wearing the lapel and you were sitting next to me  Yeah. and I only said one thing but you were talking and it was picking up all your words. Yeah. Yeah. So they would like clean channels. Uh and for that - mmm uh - that purpose uh they'd like to pull it out. So I think - I think Dan Ellis or somebody who was working with him was going to uh work on that. So. O_K. Right? Um. And uh I don't know if we've talked lately about the - the plans you're developing that we talked about this morning uh I don't remember if we talked about that last week or not  but maybe just a quick reprise of - of what we Yeah. O_K. were saying this morning. Uh. Um. So continuing to um extend uh Larry Saul's work um just reading - reading how - how we can take that as a front-end cuz it - it detects these features and they plug it into um back-end so I've been looking at a lot of um back-end stuff people have been doing articulatory features and seeing - seeing what I can - what I can pull off the shelf and plug into um Larry Saul's work. What about the stuff that um Mirjam has been doing? Oh yeah  sh- And - And Shawn? and S- Shawn  yeah. Yeah. They're - they're doing uh neural nets  just - just training up a whole bunch of neural nets and Oh. I - I think they're trying to understand um what's good about neural nets in - in terms of  you know  their patterns of errors and So they're training up nets to try to recognize these acoustic features? Yeah. I see. Yeah. But that's uh uh all - that's - is a - a certainly relevant uh study and  you know  what are the features that they're finding. We have this problem with the overloading of the term ""feature"" so Yeah. uh what are the variables  what we're calling this one  what are the variables that they're found - finding useful Hmm. um for - And their - their targets are based on canonical mappings of phones to acoustic f- Right. And that's certainly one thing to do and we're gonna try and do something more f- more fine than that features. but uh um so um So I guess you know what  I was trying to remember some of the things we were saying  do you ha- still have that - ? Oh yeah. Yeah. There's those that uh yeah  some of - some of the issues we were talking about was in j- just getting a good handle on - on uh what ""good features"" are and - What does - what did um Larry Saul use for - it was the sonorant uh detector  right? He di- he did uh yeah. How did he - H- how did he do that? Wh- what was his detector? We- oh. Um yeah  it was uh sonorance and he also had a paper on voicing too. Mm-hmm. Um and basically um in his variables that he used um or measures of S_N_R at - at sub-bands. Actually critical bands like Mm-hmm. um the um measures of correlation and covariance Oh  O_K. um within the sub-bands and um and at the upper level detecting uh sonorance and voicing. Mm-hmm. So how did he combine all these features? What - what r- mmm Oh. classifier did he u- Um he used uh um uh a - a belief-net where the lower levels of the belief-net are - correspond to individual tests of whether there is sonorance within this critical band Hmm. and then at an upper-level um there's like this soft ""OR"" gate so if - Oh right. You were talking about that  yeah. so if yeah. Yeah. I see. And the other thing you were talking about is - is - is where we get the targets from. So I mean  there's these issues of what are the - what are the variables that you use and do you combine them using the soft ""AND-OR"" or you do something  you know  more complicated um and then the other thing was so where do you get the targets from? The initial thing is just the obvious that we're discussing is starting up with phone labels from somewhere and then uh doing the transformation. But then the other thing is to do something better and eh w- why don't you tell us again about this - this database? This is the - Oh O_K. Um Yeah  so there's uh a group at um Edinburgh is working on um this MOCHA database where um they have measurements of um articulatory positions. So you - you put some - some pellets on people's tongues and lips Hmm! and - and they can tell And then tell them to talk naturally? Yeah  yeah. and they Well I guess if you got people who had like um you know  tongue rings - Pierced tongues and Pierced tongues  or - Yeah. You could just mount it to that and they wouldn't even notice. Yeah it doesn't matter . Yeah. Weld it. Zzz. But I - I don't - I don't think they're doing that though. Maybe you could go to these parlors and - and you could  you know - you know have - have  you know  reduced rates if you - Yeah. I- if you can do the measurements. That's right. You could - what you could do is you could sell little rings and stuff with embedded Yeah. you know  transmitters in them and things and Yeah  be cool and help science. Yeah. Ye- cool. Yeah. O_K. Yeah  so they - they - they have this - they're working on the database  it's still - it's still being - being uh transcribed and produced. Um where either you have um acoustic features at the same or - or just uh the acoustic waveform's being recorded for frame and then at each frame you have a measurement of - of the different positions of um uh articulators. Hmm! Yeah. There's a bunch of data that l- around  that - people have done studies like that w- way way back right? I mean I can't remember where - uh Wisconsin or someplace that used to have a big database of - Yeah they have a X_ - X_ray - Yeah. X_ray database. Yeah. It's I remember there was this guy at A_T_and_T  Randolph? or r- What was his name? Do you remember that guy? Um  researcher at A_T_and_T a while back that was studying  trying to do speech recognition from these kinds of features. Hmm. I can't remember what his name was. Dang. Now I'll think of it. Hmm. Do you mean eh - but you - I mean - Mar- you mean That's interesting. Well he was the guy - the guy that was using - when was - was Mark Randolph there  or - ? Mark Randolph. Yeah he's - he's - he's at Motorola now. Oh is he? Oh O_K. Yeah. Yeah. Yeah. Is it the guy that was using the pattern of pressure on the tongue or - ? I can't remember exactly what he was using  now. But I know - I just remember it had to do with you know What - Yeah. uh positional parameters and trying to m- you know Mm-hmm. do speech recognition based on them. Yeah. So the only - the only uh hesitation I had about it since  I mean I haven't see the data is it sounds like it's - it's continuous variables and a bunch of them. Hmm. And so I don't know how complicated it is to go from there - What you really want are these binary labels  and just a few of them. And maybe there's a trivial mapping if you wanna do it and it's e- but it - I - I - I worry a little bit that this is a research project in itself  whereas um if you did something instead that - like um having some manual annotation by uh you know  linguistics students  this would - there'd be a limited s- set of things that you could do a- as per our discussions with - with John before Mm-hmm. but the things that you could do  like nasality and voicing and a couple other things you probably could do reasonably well. Mm-hmm. And then there would - it would really be uh this uh uh binary variable. Course then  that's the other question is do you want binary variables. So. I mean the other thing you could do is boot trying to - to uh get those binary variables and take the continuous variables from uh the uh uh the data itself there  but I - I'm not sure - Could you cluster the - just do some kind of clustering? Guess you could  yeah. Bin them up into different categories and - Yeah. So anyway that's - that's uh - that's another whole direction that cou- could be looked at. Mm-hmm. Um. Um. I mean in general it's gonna be - for new data that you look at  it's gonna be hidden variable because we're not gonna get everybody sitting in these meetings to wear the pellets and - Right. Um. Right. So. So you're talking about using that data to get uh instead of using canonical mappings Right. of phones. So you'd use that data to give you sort of what the - the true mappings are for each phone? Mm-hmm. I see. Mm-hmm. Yeah. So wh- yeah  where this fits into the rest in - in my mind  I guess  is that um we're looking at different ways that we can combine uh different kinds of - of rep- front-end representations um in order to get robustness under difficult or even  you know  typical conditions. And part of it  this robustness  seems to come from uh multi-stream or multi-band sorts of things and Saul seems to have a reasonable way of looking at it  at least for one - one um articulatory feature. The question is is can we learn from that to change some of the other methods we have  since - I mean  one of the things that's nice about what he had I thought was that - that it - it um - the decision about how strongly to train the different pieces is based on uh a - a reasonable criterion with hidden variables rather than um just assuming that you should train e- e- every detector uh with equal strength towards uh it being this phone or that phone. Hmm. Right? So it - so um he's got these um uh uh he ""AND's"" between these different features. It's a soft ""AND""  I guess but in - in principle you - you wanna get a strong concurrence of all the different things that indicate something and then he ""OR's"" across the different - soft-""OR's"" across the different uh multi-band channels. And um the weight yeah  the target for the training of the ""AND"" - ""AND'ed"" things is something that's kept uh as a hidden variable  and is learned with E_M. So he doesn't have - Whereas what we were doing is - is uh taking the phone target and then just back propagating from that which means that it's - it's uh i- It could be for instance that for a particular point in the data you don't want to um uh train a particular band - train the detectors for a particular band. You - you wanna ignore that band  cuz that's a - Ban- band is a noisy - noisy measure. Mm-hmm. And we don't - We're - we're still gonna try to train it up. In our scheme we're gonna try to train it up to do as well - well as it can at predicting. Uh. Maybe that's not the right thing to do. So he doesn't have to have truth marks or - Ho- F- right  and uh he doesn't have to have hard labels. Well at the - at the tail end  yeah  he has to know what's - where it's sonorant. Right. For the full band. But he's - but what he's- but what he's not training up - uh what he doesn't depend on as truth is um I guess one way of describing would be if - if a sound is sonorant is it sonorant in this band? Is it sonorant in that band? Is it sonorant in that band? Right. i- It's hard to even answer that what you really mean is that the whole sound is sonorant. So Mm-hmm. O_K. then it comes down to  you know  to what extent should you make use of information from particular band towards making your decision. I see. And um uh we're making in a sense sort of this hard decision that you should - you should use everything uh with - with uh equal strength. And uh because in the ideal case we would be going for posterior probabilities  if we had uh enough data to really get posterior probabilities and if the - if we also had enough data so that it was representative of the test data then we would in fact be doing the right thing to train everything as hard as we can. But um this is something that's more built up along an idea of robustness from - from the beginning and so you don't necessarily want to train everything up towards the - So where did he get his - uh his tar- his uh high-level targets about what's sonorant and what's not? From uh canonical mappings O_K. um at first and then Yeah. it's unclear um eh using TIMIT right  right. Using TIMIT? or using - Uh-huh. And then uh Yeah. he does some fine tuning um for um special cases. Yeah. Yeah. I mean we ha- we have a kind of iterative training because we do this embedded Viterbi  uh so there is some- something that's suggested  based on the data but it's - it's not - I think it s- doesn't seem like it's quite the same  cuz of this - cuz then whatever that alignment is  it's that for all - Mm-hmm. all bands. Well no  that's not quite right  we did actually do them separate - tried to do them separately so that would be a little more like what he did. Um. But it's still not quite the same because then it's - it's um setting targets based on where you would say the sound begins in a particular band. Where he's s- this is not a labeling per se. Mm-hmm. Might be closer I guess if we did a soft - soft target uh uh embedded neural net training like we've done a few times uh f- the forward um - do the forward calculations to get the gammas and train on those. Mmm. Uh what's next? I could say a little bit about w- stuff I've been playing with. Oh. I um You're playing? Huh? You're playing? Yes  I'm playing. Um so I wanted to do this experiment to see um uh what happens if we try to uh improve the performance of the back-end recognizer for the Aurora task and see how that affects things. And so I had this um - I think I sent around last week a - this plan I had for an experiment  this matrix where I would take the um - the original um the original system. So there's the original system trained on the mel cepstral features and then com- and then uh optimize the b- H_T_K system and run that again. So look at the difference there and then uh do the same thing for the I_C_S_I-O_G_I front-end. What - which test set was this? This is - that I looked at? Mm-hmm. Uh I'm looking at the Italian Mm-hmm. right now. So as far as I've gotten is I've uh been able to go through from beginning to end the um full H_T_K system for the Italian data and got the same results that um - that uh Stephane had. So um I started looking - to - and now I'm - I'm sort of lookin- at the point where I wanna know what should I change in the H_T_K back-end in order to try to - uh to improve it. So. One of the first things I thought of was the fact that they use the same number of states for all of the models Mm-hmm. and so I went on-line and I uh found a pronunciation dictionary for Italian digits Mm-hmm. and just looked at  you know  the number of phones in each one of the digits. Um you know  sort of the canonical way of setting up a - an H_M_M system is that you use um three states per phone and um so then the - the total number of states for a word would just be  you know  the number of phones times three. And so when I did that for the Italian digits  I got a number of states  ranging on the low end from nine to the high end  eighteen. Um. Now you have to really add two to that because in H_T_K there's an initial null and a final null so when they use uh models that have eighteen states  there're really sixteen states. They've got those initial and final null states. And so um their guess of eighteen states seems to be pretty well matched to the two longest words of the Italian digits  the four and five Mm-hmm. which um  according to my  you know  sort of off the cuff calculation  should have eighteen states each. And so they had sixteen. So that's pretty close. Um but for the - most of the words are sh- much shorter. So the majority of them wanna have nine states. And so theirs are s- sort of twice as long. So my guess - uh And then if you - I - I printed out a confusion matrix um uh for the well-matched case  and it turns out that the longest words are actually the ones that do the best. So my guess about what's happening is that you know  if you assume a fixed - the same amount of training data for each of these digits and a fixed length model for all of them but the actual words for some of them are half as long you really um have  you know  half as much training data for those models. Because if you have a long word and you're training it to eighteen states  uh you've got - Mm-hmm. you know  you've got the same number of Gaussians  you've gotta train in each case  but for the shorter words  you know  the total number of frames is actually half as many. Mm-hmm. So it could be that  you know  for the short words there's - because you have so many states  you just don't have enough data to train all those Gaussians. So um I'm going to try to um create more word-specific um uh prototype H_M_Ms to start training from. Yeah  I mean  it's not at all uncommon you do worse on long word- on short words than long words anyway just because you're accumulating more evidence for the - Mm-hmm. for the longer word  but. Yeah so I'll - I'll  the next experiment I'm gonna try is to just um you know create uh models that seem to be more w- matched to Mm-hmm. my guess about how long they should be. And as part of that um I wanted to see sort of how the um - how these models were coming out  you know  what w- when we train up uh th- you know  the model for "" one ""  which wants to have nine states  you know  what is the - uh what do the transition probabilities look like - in the self-loops  look like in - in those models? And so I talked to Andreas and he explained to me how you can calculate the expected duration of an H_M_M just Mm-hmm. by looking at the transition matrix and so I wrote a little Matlab script that calculates that and so I'm gonna sort of print those out for each of the words Mm-hmm. to see what's happening  you know  how these models are training up  you know  the long ones versus the short ones. Mm-hmm. I d- I did - quickly  I did the silence model and - and um that's coming out with about one point two seconds as its average duration and the silence model's the one that's used at the beginning and the end of each of the Wow. string of digits. Lots of silence. Yeah  yeah. And so the S_ P model  which is what they put in between digits  I - I haven't calculated that for that one yet  but um. So they basically - their - their model for a whole digit string is silence digit  S_P  digit  S_P blah-blah-blah and then silence at the end. And so. Are the S_P's optional? I mean skip them? I have to look at that  but I'm not sure that they are. Now the one thing about the S_ P model is really it only has a single s- emitting state to it. Mm-hmm. So if it's not optional  you know  it's - it's not gonna hurt a whole lot I see. and it's tied to the center state of the silence model so it's not its own - um Mm-hmm. It doesn't require its own training data  it just shares that state. Mm-hmm. So it  I mean  it's pretty good the way that they have it set up  but um i- So I wanna play with that a little bit more. I'm curious about looking at  you know how these models have trained and looking at the expected durations of the models and I wanna compare that in the - the well-matched case f- to the unmatched case  and see if you can get an idea of - just from looking at the durations of these models  you know  what- what's happening. Yeah  I mean  I think that uh  as much as you can  it's good to d- sort of not do anything really tricky. Mm-hmm. Not do anything that's really finely tuned  but just sort of Yeah. eh you know you t- you i- z- The premise is kind of you have a - a good person look at this for a few weeks and what do you come up with? Mm-hmm. Mm-hmm. And uh And Hynek  when I wa- told him about this  he had an interesting point  and that was th- um the - the final models that they end up training up have I think probably something on the order of six Gaussians per state. So they're fairly  you know  hefty models. And Hynek was saying that well  probably in a real application  you wouldn't have enough compute to handle models that are very big or complicated. So in fact what we may want are simpler models. Could be. And compare how they perform to that. But you know  it depends on what the actual application is and it's really hard to know what your limits are in terms of how many Gaussians you can have. Right. And that  I mean  at the moment that's not the limitation  so. Mm-hmm. I mean  I - I - I - what I thought you were gonna say i- but which I was thinking was um where did six come from? Probably came from the same place eighteen came from. You know  so. Yeah. Right. Uh that's another parameter  right? that - Yeah  yeah. that maybe  you know  uh - you really want three or nine or - Well one thing - I mean  if I - if - if I start um reducing the number of states for some of these shorter models that's gonna reduce the total number of Gaussians. So in a sense it'll be a simpler Right. system. Yeah. Yeah. But I think right now again the idea is doing just very simple things Yeah. how much better can you make it? And um Mm-hmm. since they're only simple things there's nothing that you're gonna do that is going to blow up the amount of computation um so Right. Right. if you found that nine was better than six that would be O_ K  I think  actually. Mm-hmm. Yeah. Doesn't have to go down. I really wasn't even gonna play with that part of the system yet  I was just gonna Mm-hmm  O_K. Yeah  just work with the models  yeah. change the - the t- yeah  just look at the length of the models and just see what happens. Yeah. So. Cool. O_K. So uh what's uh I guess your plan for - You - you - you guys' plan for the next - next week is just continue on these - these same things we've been talking about for Aurora and Yeah  I guess we can try to have some kind of new baseline for next week perhaps. with all these minor things modified. And then do other things  play with the spectral subtraction  and retry the M_S_G and things like that. Yeah. Yeah. Yeah we - we have a big list. Big list? You have a big list of - of things to do. So. Well that's good. I think that after all of this uh um confusion settles down in another - some point a little later next year there will be some sort of standard and it'll get out there and hopefully it'll have some effect from something that - that has uh been done by our group of people but uh e- even if it doesn't there's - there's go- there'll be standards after that. So. Does anybody know how to um run Matlab sort of in batch mode like you c- send it s- a bunch of commands to run and it gives you the output. Is it possible to do that? I - I think uh Mike tried it Yeah? and he says it's impossible so he went to Octave. Octave is the um UNIX clone of - of Matlab which you can batch. Octave. Ah! O_K . Great. Thanks. Yeah. I was going crazy trying to do that. Huh. Yeah. What is Octave so ? It's What's that? a free software? Uh  Octave? Yeah it's - it's - it's free. I think we have it here Yeah. r- running somewhere. Great! Yeah. And it does the same syntax and everything eh Um like Matlab  or - ? i- it's a little behind  it's the same syntax but it's a little behind in that Matlab went to these like um you can have cells and you can - you can uh implement object-oriented type things with Matlab. Uh Octave doesn't do that yet  so I think you  Octave is kinda like Matlab um four point something or. If it'll do like a lot of the basic The basic stuff  right. matrix and vector stuff that's perfect. Yeah. Great! O_K  guess we're done. O_K. Well   although by the way. ",The Meeting Recorder group at Berkeley met to discuss recent progress. Of greatest interest was the progress on improving the latency and performance of their recogniser. There was also concern over overlap of work with partners OGI  and a lack of a good example of room reverberation for demonstrations. Everyone must be sure and use the high-pass filtering option on the groups software  to deal with irregularities between mics. In order to coordinate better with OGI  some sort of source code control is required and me018 has offered to investigate  but only minimal progress can be made until after the upcoming deadline for Eurospeech. When he returns me026 will help. Also  in two weeks one of the OGI members will return  and meetings should be arranged with him before the next big project meeting. OGI seem to be having some good results with voice activation detection  so the group need to find out which is the best VAD and start using it. The is a waveform example of room reverberation on the groups website that was used in a presentation. It turns out that it is a good example of many things  but not the reverb it is supposed to contain. Need to find a better example  maybe by just looking at a closer section of waveform. Minor experimenting found that by dropping the self-loop transition in the HMMs by just 0.1% can increase performance by 10%  but the rules of the task forbid this change. There is some confusion over what the results produced mean  since it appears they are weighted  which biases improvements in some cases quite heavily. Speaker me013 is worried that his groups work on spectral subtraction overlaps with that of OGI  and that it may be time better spent on other tasks. Speaker mn007 and fn002 have been working on improving the recogniser performance as well as reducing it's latency. Work on new filters has reduced latency  but made no improvement  though a slight reduction in performance occurred in the well matched case. Also tried adding some spectral subtraction  but it doesn't work will with on-line normalization and at this stage is just hurting results. They have also been considering the possibility of using a second stream of data looking at the voicedness of the data  which would draw some ideas from previous work. Me018 has been looking at the baseline system and feels it may be possible to decrease the run time of experiments by decreasing the iteration  and he has also got the five processor Linux machine capable of running HTKs. 
"O_K  we're going. Damn . And uh Hans- uh  Hans-Guenter will be here  um  I think by next - next Tuesday or so. Mm-hmm. Oh  O_K. So he's - he's going to be here for about three weeks  and  uh - Oh! That's nice. Just for a visit? Uh  we'll see. Huh. We might - might end up with some longer collaboration or something. So he's gonna look in on everything we're doing and Cool. Mm-hmm. give us his - his thoughts. And so it'll be another - another good person looking at things. Oh. Hmm. Th- that's his spectral subtraction group? Is that right? Yeah  yeah. Oh  O_K. So I guess I should probably talk to him a bit too? Oh  yeah. Yeah. Yeah. No  he'll be around for three weeks. He's  uh  um  very  very  easygoing  easy to talk to  and  uh  very interested in everything. Really nice guy. Yeah  yeah. Yeah  we met him in Amsterdam. Yeah  yeah  he's been here before. I mean  he's - he's - he's - he's - Oh  O_K. I haven't noticed him. Wh- Back when I was a grad student he was here for a  uh  uh - a year or n- six months. Something like that. N- nine months. Something like that. Yeah. Yeah. Yeah. He's - he's done a couple stays here. Yeah. Hmm. So  um  I guess we got lots to catch up on. And we haven't met for a couple of weeks. We didn't meet last week  Morgan. Um  I went around and talked to everybody  and it seemed like they - they had some new results but rather than them coming up and telling me I figured we should just wait a week and they can tell both - you know  all of us. So  um  why don't we - why don't we start with you  Dave  and then  um  we can go on. So. Oh  O_K. So  um  since we're looking at putting this  um - mean log m- magnitude spectral subtraction  um  into the SmartKom system  I- I did a test seeing if  um  it would work using past only and plus the present to calculate the mean. So  I did a test  um  where I used twelve seconds from the past and the present frame to  um  calculate the mean. And - Twelve seconds - Twelve - twelve seconds back from the current frame  is that what you mean? Uh - Twelve seconds  um  counting back from the end of the current frame  yeah. So it was  um  twen- I think it was twenty-one frames and that worked out to about twelve seconds. O_K  O_K. Mm-hmm. And compared to  um  do- using a twelve second centered window  I think there was a drop in performance but it was just a slight drop. Mm-hmm. Hmm! Is - is that right? Um  yeah  I mean  it was pretty - it was pretty tiny. Yeah. Uh-huh. So that was encouraging. And  um  that - that - um  that's encouraging for - for the idea of using it in an interactive system like SmartKom. And  um  another issue I'm - I'm thinking about is in the SmartKom system. So say twe- twelve seconds in the earlier test seemed like a good length of time  but what happens if you have less than twelve seconds? And  um - So I w- bef- before  um - Back in May  I did some experiments using  say  two seconds  or four seconds  or six seconds. In those I trained the models using mean subtraction with the means calculated over two seconds  or four seconds  or six seconds. And  um  here  I was curious  what if I trained the models using twelve seconds but I f- I gave it a situation where the test set I was - subtracted using two seconds  or four seconds  or six seconds. And  um - So I did that for about three different conditions. And  um - I mean  I th- I think it was  um  four se- I think - I think it was  um  something like four seconds and  um  six seconds  and eight seconds. Something like that. And it seems like it - it - it hurts compared to if you actually train the models using th- that same length of time but it - it doesn't hurt that much. Um  u- usually less than point five percent  although I think I did see one where it was a point eight percent or so rise in word error rate. But this is  um  w- where  um  even if I train on the  uh  model  and mean subtracted it with the same length of time as in the test  it - the word error rate is around  um  ten percent or nine percent. So it doesn't seem like that big a d- a difference. But it - but looking at it the other way  isn't it - what you're saying that it didn't help you to have the longer time for training  if you were going to have a short time for - That - that's true. Um  I mean  why would you do it  if you knew that you were going to have short windows in testing. Wa- Yeah  it seems like for your - I mean   in normal situations you would never get twelve seconds of speech  right? You need twelve seconds in the past to estimate  right? I'm not - e- u- Um  t- twelve s- Yeah. Or l- or you're looking at six sec - seconds in future and six in - N- n- uh - For the test it's just twelve seconds in the past. No  total. No  it's all - Oh  O_K. Is this twelve seconds of - uh  regardless of speech or silence? Or twelve seconds of speech? Of - of speech. O_K. Mm-hmm. The other thing  um  which maybe relates a little bit to something else we've talked about in terms of windowing and so on is  that  um  I wonder if you trained with twelve seconds  and then when you were two seconds in you used two seconds  and when you were four seconds in  you used four seconds  and when you were six - and you basically build up to the twelve seconds. So that if you have very long utterances you have the best  Yeah. but if you have shorter utterances you use what you can. Right. And that's actually what we're planning to do in SmartKom. O_K. But - s- so I g- So I guess the que- the question I was trying to get at with those experiments is  Yeah. ""does it matter what models you use? Does it matter how much time y- you use to calculate the mean when you were  um  tra- doing the training data?"" Right. But I mean the other thing is that that's - I mean  the other way of looking at this  going back to  uh  mean cepstral subtraction versus RASTA kind of things  is that you could look at mean cepstral subtraction  especially the way you're doing it  uh  as being a kind of filter. And so  the other thing is just to design a filter. You know  basically you're - you're - you're doing a high-pass filter or a band-pass filter of some sort and - and just design a filter. And then  you know  a filter will have a certain behavior and you loo- can look at the start up behavior Mm-hmm. when you start up with nothing. And - and  you know  it will  uh  if you have an I_I_R filter for instance  it will  um  uh  not behave in the steady-state way that you would like it to behave until you get a long enough period  but  um  uh  by just constraining yourself to have your filter be only a subtraction of the mean  you're kind of  you know  tying your hands behind your back because there's - filters have all sorts of be- temporal and spectral behaviors. Mm-hmm. And the only thing  you know  consistent that we know about is that you want to get rid of the very low frequency component. Hmm. But do you really want to calculate the mean? And you neglect all the silence regions or you just use everything that's twelve seconds  and - Um  you - do you mean in my tests so far? Ye- yeah. Most of the silence has been cut out. O_K. Just - There's just inter-word silences. Mm-hmm. And they are  like  pretty short. Shor- Yeah  O_K. Yeah. Pretty short. Yeah. Mm-hmm. So you really need a lot of speech to estimate the mean of it. Well  if I only use six seconds  it still works pretty well. Yeah. Yeah. Uh-huh. I saw in my test before. I was trying twelve seconds cuz that was the best in my test before and that increasing past twelve seconds didn't seem to help. O_K. Hmm. Huh. th- um  yeah  I guess it's something I need to play with more to decide how to set that up for the SmartKom system. Like  may- maybe if I trained on six seconds it would work better when I only had two seconds or four seconds  and - Yeah. Yeah. And  um - O_K. Yeah  and again  if you take this filtering perspective and if you essentially have it build up over time. I mean  if you computed means over two and then over four  and over six  essentially what you're getting at is a kind of  uh  ramp up of a filter anyway. And so you may - may just want to think of it as a filter. But  uh  if you do that  then  um  in practice somebody using the SmartKom system  one would think - if they're using it for a while  it means that their first utterance  instead of  you know  getting  uh  a forty percent error rate reduction  they'll get a - uh  over what  uh  you'd get without this  uh  um  policy  uh  you get thirty percent. And then the second utterance that you give  they get the full - you know  uh  full benefit of it if it's this ongoing thing. Oh  so you - you cache the utterances? That's how you get your  uh - M- Well  I'm saying in practice  yeah  that's - If somebody's using a system to ask for directions or something  Ah. O_K. O_K. you know  they'll say something first. And - and to begin with if it doesn't get them quite right  ma- m- maybe they'll come back and say  ""excuse me?"" uh  or some - I mean it should have some policy like that anyway. Mm-hmm. Mm-hmm. And - and  uh  uh  in any event they might ask a second question. And it's not like what he's doing doesn't  uh  improve things. It does improve things  just not as much as he would like. And so  uh  there's a higher probability of it making an error  uh  in the first utterance. What would be really cool is if you could have - uh  this probably - users would never like this - but if you had - could have a system where  before they began to use it they had to introduce themselves  Mm-hmm. verbally. You know. ""Hi  my name is so-and-so  I'm from blah-blah-blah."" And you could use that initial speech to do all these Yeah. adaptations and - Mm-hmm. Right. Oh  the other thing I guess which - which  uh  I don't know much about - as much as I should about the rest of the system but - but  um  couldn't you  uh  if you - if you sort of did a first pass - I don't know what kind of  uh  uh  capability we have at the moment for - for doing second passes on - on  uh  uh  some kind of little - small lattice  or a graph  or confusion network  or something. But if you did first pass with  um  the - with - either without the mean sub- subtraction or with a - a very short time one  and then  um  once you  uh  actually had the whole utterance in  if you did  um  the  uh  uh  longer time version then  based on everything that you had  um  and then at that point only used it to distinguish between  you know  top N_  um  possible utterances or something  you - you might - it might not take very much time. I mean  I know in the large vocabulary stu- uh  uh  systems  people were evaluating on in the past  some people really pushed everything in to make it in one pass but other people didn't and had multiple passes. And  um  the argument  um  against multiple passes was u- u- has often been ""but we want to this to be r- you know - have a nice interactive response"". And the counterargument to that which  say  uh  B_B_N I think had  was ""yeah  but our second responses are - second  uh  passes and third passes are really  really fast"". Mm-hmm. So  um  if - if your second pass takes a millisecond who cares? Um. S- so  um  the - the idea of the second pass would be waiting till you have more recorded speech? Or - ? Yeah  so if it turned out to be a problem  that you didn't have enough speech because you need a longer - longer window to do this processing  Mm-hmm. then  uh  one tactic is - you know  looking at the larger system and not just at the front-end stuff - is to take in  um  the speech with some simpler mechanism or shorter time mechanism  um  do the best you can  and come up with some al- possible alternates of what might have been said. And  uh  either in the form of an N_best list or in the form of a lattice  or - or confusion network  or whatever. Mm-hmm. And then the decoding of that is much  much faster or can be much  much faster if it isn't a big bushy network. And you can decode that now with speech that you've actually processed using this longer time  uh  subtraction. Mmm. So I mean  it's - it's common that people do this sort of thing where they do more things that are more complex or require looking over more time  whatever  in some kind of second pass. Mm-hmm. O_K. um  and again  if the second pass is really  really fast - Uh  another one I've heard of is - is in - in connected digit stuff  um  going back and l- and through backtrace and finding regions that are considered to be a d- a digit  but  uh  which have very low energy. Mm-hmm. O_K. So  uh - I mean  there's lots of things you can do in second passes  at all sorts of levels. Anyway  I'm throwing too many things out. But. So is that  uh - that it? I guess that's it. O_K  uh  do you wanna go  Sunil? Yep. Um  so  the last two weeks was  like - So I've been working on that Wiener filtering. And  uh  found that  uh  s- single - like  I just do a s- normal Wiener filtering  like the standard method of Wiener filtering. And that doesn't actually give me any improvement over like - I mean  uh  b- it actually improves over the baseline but it's not like - it doesn't meet something like fifty percent or something. So  I've been playing with the v- Improves over the base line M_F_C_C system? Yeah. Yeah. Yeah. Yeah. So  um - So that's - The improvement is somewhere around  like  thirty percent over the baseline. Is that using - in combination with something else? With - with a - No  just - just one stage Wiener filter which is a standard Wiener filter. No  no  but I mean in combination with our on-line normalization or with the L_D_A? Oh  O_K. Yeah  yeah  yeah  yeah. So I just plug in the Wiener filtering. I mean  in the s- in our system  where - So  I di- i- di- Oh  O_K. So  does it g- does that mean it gets worse? Or - ? No. It actually improves over the baseline of not having a Wiener filter in the whole system. Like I have an L_D_A f- L_D_A plus on-line normalization  Yeah? and then I plug in the Wiener filter in that  so it improves over not having the Wiener filter. So it improves but it - it doesn't take it like be- beyond like thirty percent over the baseline. So - But that's what I'm confused about  cuz I think - I thought that our system was more like forty percent without the Wiener filtering. No  it's like  uh  Mmm. well  these are not - Is this with the v- new V_A_D? No  it's the old V_A_D. So my baseline was  uh  nine - This is like - w- the baseline is ninety-five point six eight  and eighty-nine  and - So I mean  if you can do all these in word errors it's a lot - a lot easier actually. What was that? Sorry? If you do all these in word error rates it's a lot easier  right? Oh  O_K  O_K  O_K. Errors  right  I don't have. It's all accuracies. O_K  cuz then you can figure out the percentages. Yeah. The baseline is something similar to a w- I mean  the t- the - the baseline that you are talking about is the M_F_C_C baseline  right? The t- yeah  there are two baselines. O_K. So the baseline - One baseline is M_F_C_C baseline that - When I said thirty percent improvement it's like M_F_C_C baseline. Or - ? Mm-hmm. So - so - so what's it start on? The M_F_C_C baseline is - is what? Is at what level? It's the - it's just the mel frequency and that's it. No  what's - what's the number? Uh  so I- I don't have that number here. O_K  O_K  O_K  I have it here. Uh  it's the V_A_D plus the baseline actually. I'm talking about the - the M_F_C_C plus I do a frame dropping on it. So that's like - the word error rate is like four point three. Four point three. Like - Ten point seven. What's ten point seven? It's a medium misma- O_K  sorry. There's a well ma- well matched  medium mismatched  and a high matched. So I don't have the - like the - Ah. Yeah. O_K  four point three  ten point seven  and - So - And forty- forty. Forty percent is the high mismatch. O_K. And that becomes like four point three - Not changed. Yeah  it's like ten point one. Still the same. And the high mismatch is like eighteen point five. Eighteen point five. And what were you just describing? Five . Oh  the one is - this one is just the baseline plus the  uh  Wiener filter plugged into it. But where's the  uh  on-line normalization and so on? Oh  O_K. So - Sorry. So  with the - with the on-line normalization  the performance was  um  ten - O_K  so it's like four point three. Uh  and again  that's the ba- the ten point  uh  four and twenty point one. That was with on-line normalization and L_D_A. So the h- well matched has like literally not changed by adding on-line or L_D_A on it. But the - I mean  even the medium mismatch is pretty much the same. And the high mismatch was improved by twenty percent absolute. O_K  and what kind of number - an- and what are we talking about here? Is this T_I-digits or - It's the It- it's Italian. I'm talking about Italian  yeah. Italian? And what did - So  what was the  um  uh  corresponding number  say  for  um  uh  the Alcatel system for instance? Do you know? Mmm. @@ Yeah  so it looks to be  um - You have it? Yep  it's three point four  uh  eight point  uh  seven  and  uh  thirteen point seven. O_K. O_K. Yep. So - Thanks. Mm-hmm. O_K. So  uh  this is the single stage Wiener filter  with - The noise estimation was based on Mm-hmm. first ten frames. Actually I started with - using the V_A_D to estimate the noise and then I found that it works - it doesn't work for Finnish and Spanish because the V_A_D endpoints are not good to estimate the noise because it cuts into the speech sometimes  so I end up overestimating the noise and getting a worse result. Mm-hmm. So it works only for Italian by u- for - using a V_A_D to estimate noise. It works for Italian because the VAD was trained on Italian. Mm-hmm. So  uh - so this was  uh - And so this was giving - um  this - this was like not improving a lot on this baseline of not having the Wiener filter on it. And  so  uh  I ran this stuff with one more stage of Wiener filtering on it but the second time  what I did was I - estimated the new Wiener filter based on the cleaned up speech  Mm-hmm. and did  uh  smoothing in the frequency to - to reduce the variance - I mean  I have - I've - I've observed there are  like  a lot of bumps in the frequency when I do this Wiener filtering which is more like a musical noise or something. And so by adding another stage of Wiener filtering  the results on the SpeechDat-Car was like  um - So  I still don't have the word error rate. I'm sorry about it. But the overall improvement was like fifty-six point four six. This was again using ten frames of noise estimate and two stage of Wiener filtering. And the rest is like the L_D_A plu- and the on-line normalization all remaining the same. Uh  so this was  like  compared to  uh  uh - Fifty-seven is what you got by using the French Telecom system  right? No  I don't think so. Is it on Italian? Y- i- No  this is over the whole SpeechDat-Car. So - Oh  yeah  fifty-seven - Right. point - Yeah  so the new - the new Wiener filtering schema is like - some fifty-six point four six which is like Uh-huh. one percent still less than what you got using the French Telecom system. Mm-hmm. But it's a pretty similar number in any event. It's very similar. Yeah. But again  you're - you're more or less doing what they were doing  right? It's - it's different in a sense like I'm actually cleaning up the cleaned up spectrum which they're not doing. They're d- what they're doing is  they have two stage - stages of estimating the Wiener filter  Yeah. but - the final filter  what they do is they - they take it to their time domain by doing an inverse Fourier transform. Uh-huh. And they filter the original signal using that fil- filter  which is like final filter is acting on the input noisy speech rather than on the cleaned up. So this is more like I'm doing Wiener filter twice  but the only thing is that the second time I'm actually smoothing the filter and then cleaning up the cleaned up spectrum first level. O_K. And so that - that's - that's what the difference is. And actually I tried it on s- the original clean - I mean  the original spectrum where  like  I - the second time I estimate the filter but actually clean up the noisy speech rather the c- s- first - output of the first stage and that doesn't - seems to be a - giving  I mean  that much improvement. I - I didn- didn't run it for the whole case. And - and what I t- what I tried was  by using the same thing but - Uh  so we actually found that the VAD is very  like  crucial. I mean  just by changing the VAD itself gives you the - a lot of improvement by instead of using Mm-hmm. the current VAD  if you just take up the VAD output from the channel zero  when - instead of using channel zero and channel one  because that was the p- that was the reason why I was not getting a lot of improvement for estimating the noise. So I just used the channel zero VAD to estimate the noise so that it gives me some reliable mar- markers for this noise estimation. What's a channel zero VAD? I'm - I'm confused about that. Um  so  it's like - So it's the close-talking microphone. Yeah  the close-talking without - So because the channel zero and channel one are like the same Oh  oh  oh  oh. @@ speech  but only w- I mean  the same endpoints. But the only thing is that the speech is very noisy for channel one  so you can actually use the output of the channel zero for channel one for the VAD. I mean  that's like a cheating method. Right. I mean  so a- are they going to pro- What are they doing to do  do we know yet? about - as far as what they're - what the rules are going to be and what we can use? Yeah  so actually I received a - a new document  describing this. And what they did finally is to  mmm  Yeah  that's - uh  not to align the utterances but to perform recognition  um  only on the close-talking microphone  and to take the result of the recognition to get the boundaries Which is the channel zero. uh  of speech. So it's not like that's being done in one place or one time. That's - that's just a rule and we'd - you - you were permitted to do that. Is - is that it? And - Uh  I think they will send  um  files but we - we don't - Well  apparently - Oh  so they will send files so everybody will have the same boundaries to work with? Yeah. Yeah. But actually their alignment actually is not seems to be improving in like on all cases. O_K. Oh  i- Yeah  so what happened here is that  um  the overall improvement that they have with this method - So - Well  to be more precise  what they have is  they have these alignments and then they drop the beginning silence and - and the end silence but they keep  uh  two hundred milliseconds before speech and two hundred after speech. And they keep the speech pauses also. Um  and the overall improvement over the M_F_C_C baseline - So  when they just  uh  add this frame dropping in addition it's r- uh  forty percent  right? Fourteen percent  I mean. Mm-hmm. Mm-hmm. Yeah  which is - Um  which is  um  t- which is the overall improvement. But in some cases it doesn't improve at all. Like  uh  y- do you remember which case? Mm-hmm. It gives like negative - Well  in - in like some Italian and T_I-digits  right? Yeah  some @@ . Right. Yeah. So by using the endpointed speech  actually it's worse than the baseline in some instances  which could be due to the Mmm. Yeah. And - Yeah  the other thing also is that fourteen percent is less than what you obtain using a real V_A_D. Yeah  but that word pattern. Yeah  our neural net - So with- without cheating like this. So - Uh - So I think this shows that there is still work - Yeah  yeah. Yeah. Uh  well  working on the V_A_D is still - still important I think. Yeah  c- Uh - Can I ask just a - a high level question? Can you just say like one or two sentences about Wiener filtering and why - Hmm. why are people doing that? What's - what's the deal with that? O_K  so the Wiener filter  it's - it's like - it's like you try to minimize - I mean  so the basic principle of Wiener filter is like you try to minimize the  uh  d- uh  difference between the noisy signal and the clean signal if you have two channels. Like let's say you have a clean t- signal and you have an additional channel where you know what is the noisy signal. And then you try to minimize the error between these two. Mm-hmm. Mm-hmm. So that's the basic principle. And you get - you can do that - I mean  if - if you have only a c- noisy signal  at a level which you  you w- try to estimate the noise from the w- assuming that the first few frames are noise or if you have a w- voice activity detector  uh  you estimate the noise spectrum. And then you - Mm-hmm. Yeah. Do you assume the noise is the same? in - yeah  after the speech starts. So - Uh-huh. but that's not the case in  uh  many - many of our cases but it works reasonably well. I see. And - and then you What you do is you  uh b- fff. So again  I can write down some of these eq- Oh  O_K. Yeah. And then you do this - uh  this is the transfer function of the Wiener filter  so ""S_F"" is a clean speech spectrum  power spectrum Mm-hmm. And ""N_"" is the noisy power spectrum. And so this is the transfer function. Right actually  I guess - And  Yeah. Yeah. And then you multiply your noisy power spectrum with this. You get an estimate of the clean power spectrum. I see. O_K. So - but the thing is that you have to estimate the S_F from the noisy spectrum  what you have. So you estimate the N_F from the initial noise portions and then you subtract that from the current noisy spectrum to get an estimate of the S_F. So sometimes that becomes zero because you do- you don't have a true estimate of the noise. So the f- filter will have like sometimes zeros Mm-hmm. in it because some frequency values will be zeroed out because of that. And that creates a lot of discontinuities across the spectrum because @@ the filter. So  uh  so - that's what - that was just the first stage of Wiener filtering that I tried. So is this  um  basically s- uh  similar to just regular spectral subtraction? It's all pretty related  yeah. It's - it's - there's a di- there's a whole class of techniques where you try in some sense to minimize the noise. It - Yeah. Uh-huh. And it's typically a mean square sense  uh - uh - uh  i- in - in - in some way. And  uh - uh  spectral subtraction is - is  uh - uh  one approach to it. Do people use the Wiener filtering in combination with the spectral subtraction typically  or is i- are they sort of Not seen. competing techniques? They are very s- similar techniques. So it's like I haven't seen anybody using s- Wiener filter with spectral subtraction. Yeah. O- oh  O_K. Mm-hmm. I see  I see. I mean  in the long run you're doing the same thing but y- but there you make different approximations  and - Mm-hmm. Yeah. Mmm. in spectral subtraction  for instance  there's a - a - an estimation factor. You sometimes will figure out what the noise is and you'll multiply that noise spectrum times some constant and subtract that rather than - and sometimes people - even though this really should be in the power domain  sometimes people s- work in the magnitude domain because it - it - it works better. And  uh  Mm-hmm. uh  you know. So why did you choose  uh  Wiener filtering over some other - one of these other techniques? Uh  the reason was  like  we had this choice of using spectral subtraction  Wiener filtering  and there was one more thing which I- which I'm trying  is this sub space approach. So  Stephane is working on spectral subtraction. So I picked up - Oh  O_K. So you're sort of trying @@ them all. Y- Yeah  @@ we just wanted to have a few noise production - compensation techniques and then pick some from that - pick one. Ah  I see. Oh  O_K. I m- I mean - yeah  I mean  there's Car- Carmen's working on another  on the vector Taylor series. So they were just kind of trying to cover a bunch of different things Mm-hmm. VA- Yeah  V_A_D. w- Yeah. Yeah. Ah  O_K. That makes sense. with this task and see  you know  what are - what are the issues for each of them. Um. Yeah. Mm-hmm. Mm-hmm. Cool  thanks. So - so one of - one of the things that I tried  like I said  was to remove those zeros in the fri- filter by doing some smoothing of the filter. Yeah. Mm-hmm. Like  you estimate the edge of square and then you do a f- smoothing across the frequency so that those zeros get  like  flattened out. Mm-hmm. And that doesn't seems to be improving by trying it on the first time. So what I did was like I p- did this and then you - I plugged in the - one more - the same thing but with the smoothed filter the second time. And that seems to be working. Mm-hmm. Mm-hmm. So that's where I got like fifty-six point five percent improvement on SpeechDat-Car with that. And - So the other thing what I tried was I used still the ten frames of noise estimate but I used this channel zero VAD to drop the frames. So I'm not - still not estimating. And that has taken the performance to like sixty-seven percent in SpeechDat-Car  which is - which - which like sort of shows that by using a proper VAD you can just take it to further  better levels. And - So. So that's sort of like  you know  best-case performance? Yeah  so far I've seen sixty-seven - I mean  no  I haven't seen s- like sixty-seven percent. And  uh  using the channel zero VAD to estimate the noise also seems to be improving but I don't have the results for all the cases with that. So I used channel zero VAD to estimate noise as a lesser 2x frame  which is like  everywhere I use the channel zero V_A_D. And that seems to be the best combination  uh  rather than using a few frames to estimate and then drop a channel. So I'm - I'm still a little confused. Is that channel zero information going to be accessible during this test. Nnn  no. This is just to test whether we can really improve by using a better VAD. Mm-hmm. Mm-hmm. So  I mean - So this is like the noise compensation f- is fixed but you make a better decision on the endpoints. Mm-hmm. That's  like - seems to be - Mm-hmm. so we c- so I mean  which - which means  like  Yes. by using this technique what we improve just the VAD we can just take the performance by another ten percent or better. O_K. So  that - that was just the  uh  reason for doing that experiment. And  w- um - Yeah  but this - all these things  I have to still try it on the T_I-digits  which is like I'm just running. And there seems to be not improving a - a lot on the T_I-digits  so I'm like investigating that  why it's not. And  um  um - Well after that. So  uh - so the other - the other thing is - like I've been - I'm doing all this stuff on the power spectrum. So - Tried this stuff on the mel as well - mel and the magnitude  and mel magnitude  and all those things. But it seems to be the power spectrum seems to be getting the best result. So  one of - one of reasons I thought like doing the averaging  after the filtering using the mel filter bank  that seems to be maybe helping rather than trying it on the mel filter ba- filtered outputs. Mm-hmm. Mm-hmm. Ma- So just th- Yeah  th- that's - that's the only thing that I could think of why - why it's giving improvement on the mel. Makes sense. And  yep. So that's it. Uh  how about the subspace stuff? Subspace  I'm - I'm like - that's still in - a little bit in the back burner because I've been p- putting a lot effort on this to make it work  on tuning things and other stuff. So O_K. I was like going parallely but not much of improvement. I'm just - have some skeletons ready  need some more time for it. O_K. Mmm. Tha- that it? Yep. Yep. Cool. Do you wanna go  Stephane? Uh  yeah. So  I've been  uh  working still on the spectral subtraction. Um  So to r- to remind you a little bit of - of what I did before  is just to apply some spectral subtraction with an overestimation factor also to get  um  an estimate of the noise  uh  spectrum  and subtract this estimation of the noise spectrum from the  uh  signal spectrum  but subtracting more when the S_N_R is - is  uh  low  which is a technique that it's ""Subtracting more""  meaning - ? often used. So you overestimate the noise spectrum. You multiply the noise spectrum by a factor  Oh  O_K. uh  which depends on the S_N_R. So  above twenty D_B  I see. it's one  so you just subtract the noise. Mm-hmm. And then it's b- Generally - Well  I use  actually  a linear  uh  function of the S_N_R  Mm-hmm. which is bounded to  like  two or three  Mm-hmm. when the S_N_R is below zero D_B. Mm-hmm. Um  doing just this  uh  either on the F_F_T bins or on the mel bands  um  Oh! t- doesn't yield any improvement Um  uh  what are you doing with negative  uh  powers? o- Yeah. So there is also a threshold  of course  because after subtraction you can have negative energies  and - Mm-hmm. So what I - I just do is to put  uh - to - to add - to put the threshold first and then to add a small amount of noise  which right now is speech-shaped. Um - Speech-shaped? Yeah  so it's - a- it has the overall - overall energy  uh - pow- it has the overall power spectrum of speech. So with a bump around one kilohertz. So when y- when you talk about there being something less than zero after subtracting the noise  is that at a particular i- Uh-huh. frequency bin? Yeah. There can be frequency bins with negative values. O_K. And so when you say you're adding something that has the overall shape of speech  is that in a - in a particular frequency bin? Or you're adding something across all the frequencies when you get these negatives? For each frequencies I a- I'm adding some  uh  noise  but the a- the amount of - the amount of noise I add is not the same for all the frequency bins. Ah! O_K. I gotcha. Right. Uh. Right now I don't think if it makes sense to add something that's speech- shaped  because then you have silence portion that have some spectra similar to the sp- the overall speech spectra. But - Mm-hmm. Yeah. So this is something I can still work on  but - So what does that mean? I'm trying to understand what it means when you do the spectral subtraction and you get Hmm. a negative. It means that That means that - Mm-hmm. at that particular frequency range you subtracted more energy than there was actually - Yeah. So - so yeah  you have an - an estimation of the noise spectrum  but sometimes  of course  it's - as the noise is not perfectly stationary  sometimes this estimation can be  uh  too small  so you don't subtract enough. But sometimes it can be too large also. Mm-hmm. If - if the noise  uh  energy in this particular frequency band drops for some reason. Mm-hmm. Mmm. So in - in an ideal word i- world if the noise were always the same  then  when you subtracted it the worst that i- you would get would be a zero. I mean  the lowest you would get would be a zero  cuz i- Right. if there was no other energy there you're just subtracting exactly the noise. Mm-hmm  yeah. Yep  there's all - there's all sorts of  uh  deviations from the ideal here. I mean  for instance  you're - you're talking about the signal and noise  um  at a particular point. And even if something is sort of stationary in ster- terms of statistics  there's no guarantee that any particular instantiation or piece of it is exactly a particular number or bounded by a particular range. So  Mm-hmm. you're figuring out from some chunk of - of - of the signal what you think the noise is. Then you're subtracting that from another chunk  Mm-hmm. and there's absolutely no reason to think that you'd know that it wouldn't  uh  be negative in some places. Mm-hmm. Hmm. Uh  on the other hand that just means that in some sense you've made a mistake because you certainly have stra- subtracted a bigger number than is due to the noise. Mm-hmm. Um - Also  we speak - the whole - where all this stuff comes from is from an assumption that signal and noise are uncorrelated. And that certainly makes sense in s- in - in a statistical interpretation  that  you know  over  um  all possible realizations that they're uncorrelated or Mm-hmm. assuming  uh  ergodicity that i- that i- um  across time  uh  it's uncorrelated. But if you just look at - a quarter second  uh  and you cross-multiply the two things  uh  you could very well  uh  end up with something that sums to something that's not zero. So in fact  the two signals could have some relation to one another. And so there's all sorts of deviations from ideal in this. And - and given all that  you could definitely end up with something that's negative. But if down the road you're making use of something as if it is a power spectrum  um  then it can be bad to have something negative. Now  the other thing I wonder about actually is  what if you left it negative? What happens? I mean  because - Is that the log? Um  are you taking the log before you add them up to the mel? After that. No  after. Right. So the thing is  I wonder how - if you put your thresholds after that  I wonder how often you would end up with  uh - with negative values. But you will - But you end up reducing some neighboring frequency bins - @@ in the average  right? When you add the negative to the positive value which is the true estimate. Yeah. But nonetheless  uh  you know  these are - it's another f- kind of smoothing  right? that you're doing. Yeah. Right. So  you've done your best shot at figuring out what the noise should be  and now i- then you've subtracted it off. And then after that  instead of - instead of  uh  uh  leaving it as is and adding things - adding up some neighbors  you artificially push it up. Hmm. Which is  you know  it's - there's no particular reason that that's the right thing to do either  right? Yeah  yeah. So  um  uh  i- in fact  what you'd be doing is saying  ""well  we're d- we're - we're going to definitely diminish the effect of this frequency in this little frequency bin in the - in the overall mel summation"". It's just a thought. I d- I don't know if it would be - Yeah. Uh-huh. Sort of the opposite of that would be if - if you find out you're going to get a negative number  you don't do the subtraction for that bin. That is true. Nnn  yeah  although - Mm-hmm. That would be almost the opposite  right? Instead of leaving it negative  you don't do it. If your - if your subtraction's going to result in a negative number  you - you don't do subtraction in that. Yeah  but that means that in a situation where you thought that - that the bin was almost entirely noise  you left it. Yeah. Uh. Yeah. Well  yeah that's - that's the opposite  yeah. We just - Yeah. Yeah  I'm just saying that's like the opposite. Yeah. Mm-hmm. And  yeah  some people also - if it's a negative value they  uh  re-compute it using inter- interpolation from the edges and bins. Well  there are different things that you can do. For frames  frequency bins. Yeah. Oh. People can also  uh  reflect it back up and essentially do a full wave rectification instead of a - Oh. instead of half wave. But it was just a thought that - that it might be something to try. Mm-hmm. Mm-hmm. Yep. Well  actually I tried  something else based on this  um  is to - to put some smoothing  um  because it seems to - to help or it seems to help the Wiener filtering and  mmm - Mm-hmm. So what I did is  uh  some kind of nonlinear smoothing. Actually I have a recursion that computes - Yeah  let me go back a little bit. Actually  when you do spectral subtraction you can  uh  find this - this equivalent in the s- in the spectral domain. You can uh compute  y- you can say that d- your spectral subtraction is a filter  um  and the gain of this filter is the  um  signal energy minus what you subtract  divided by the signal energy. And this is a gain that varies over time  and  you know  of course  uh  depending on the s- on the noise spectrum and on the speech spectrum. And - what happen actually is that during low S_N_R values  the gain is close to zero but it varies a lot. Mmm  and this - this is the cause of musical noise and all these - the - the fact you - we go below zero one frame and then you can have an energy that's above zero. And - Mm-hmm. Mmm. So the smoothing is - I did a smoothing actually on this gain  uh  trajectory. But it's - the smoothing is nonlinear in the sense that I tried to not smooth if the gain is high  because in this case we know that  uh  the estimate of the gain is correct because we - we are not close to - to - to zero  um  and to do more smoothing if the gain is low. Mmm. Um. Yeah. So  well  basically that's this idea  and it seems to give pretty good results  uh  although I've just - just tested on Italian and Finnish. And on Italian it seems - my result seems to be a little bit better than the Wiener filtering  right? Mm-hmm. Yeah  the one you showed yesterday. Right? Yeah. Uh  I don't know if you have these improvement- the detailed improvements for Italian  Finnish  and Spanish there or you have - just have your own . Fff. No  I don't have  for each  I - I just - just have the final number here. Mm-hmm. So these numbers he was giving before with the four point three  and the ten point one  and so forth  those were Italian  right? Yeah  yeah  yeah. So - so  no  I actually didn't give you the number which is the final one  which is  after two stages of Wiener filtering. Uh - uh  no  we've - Yeah. I mean  that was I just - well  like the overall improvement is like fifty-six point five. Right. Mm-hmm. So  I mean  his number is still better than what I got in the two stages of Wiener filtering. Yeah. Right. On Italian. But on Finnish it's a little bit worse  apparently. Um - Mm-hmm. But do you have numbers in terms of word error rates on - on Italian? So just so you have some sense of reference? Yeah. Uh  so  it's  uh  three point  uh  eight. Uh-huh. Am I right? Oh  O_K. Yeah  right  O_K. And then  uh  d- uh  nine point  uh  one. Mm-hmm. And finally  uh  sixteen point five. And this is  um  spectral subtraction plus what? Plus - plus nonlinear smoothing. Well  it's - the system - it's exactly the sys- the same system as Sunil tried  but - On-line normalization and L_D_A? Yeah. Yeah. But instead of double stage Wiener filtering  it's - it's this smoothed spectral subtraction. Um  yeah. Yeah. Right. What is it the  um  France Telecom system uses for - Do they use spectral subtraction  or Wiener filtering  or - ? They use spectral subtraction  right. For what? French Telecom. It - it's Wiener filtering  am I right? Oh  it's - it's Wiener filtering. Sorry. Well  it's some kind of Wiener filtering - Oh. Yeah  filtering. Yeah  it's not exactly Wiener filtering but some variant of Wiener filtering. Yeah. I see. Yeah  plus  uh  I guess they have some sort of cepstral normalization  as well. Yeah. s- They have like - yeah  th- the - just noise compensation technique is a variant of Wiener filtering  plus they do some - Mm-hmm. some smoothing techniques on the final filter. The - th- they actually do the filtering in the time domain. Mmm. Yeah. Hmm. So they would take this H_F squared back  taking inverse Fourier transform. And they convolve the time domain signal with that. Oh  I see. And they do some smoothing on that final filter  impulse response. Hmm. But they also have two - two different smoothing @@ . One in the time domain and one in the frequency domain by just taking the first  um  coefficients of the impulse response. I mean  I'm - I'm @@ . But . So  basically it's similar. I mean  what you did  it's similar because you have also two - two kind of smoothing. One in the time domain  and one in the frequency domain  yeah. It's similar in the smoothing and - Yeah. Yeah. The frequency domain. Does the smoothing in the time domain help - Um - Well  do you get this musical noise stuff with Wiener filtering or is that only with  uh  spectral subtraction? No  you get it with Yeah. Wiener filtering also. Does the smoothing in the time domain help with that? Oh  no  you still end up with zeros in the s- spectrum. Sometimes. Or some other smoothing? Yeah. I mean  it's not clear that these musical noises hurt us in recognition. We don't know if they do. I mean  they - they sound bad. Hmm. Hmm. Yeah. Yeah  I know. But we're not listening to it  usually. Mm-hmm. Mm-hmm. Hmm. Uh  actually the - the smoothing that I did - do here reduced the musical noise. Well  it - Mm-hmm. Yeah  yeah  the - Mmm. Well  I cannot - you cannot hear beca- well  actually what I d- did not say is that this is not in the F_F_T bins. This is in the mel frequency bands. Um - So  it could be seen as a f- a - a smoothing in the frequency domain because I used  in ad- mel bands in addition and then the other phase of smoothing in the time domain. Mmm. But  when you look at the spectrogram  if you don't have an- any smoothing  you clearly see  like - in silence portions  and at the beginning and end of speech  you see spots of high energy randomly distributed over the - the spectrogram. Mm-hmm. Mm-hmm. Um - That's the musical noise? Which is musical noise  yeah  if - if it - Mm-hmm. If you listen to it - uh  if you do this in the F_F_T bins  then you have spots of energy randomly distributing. And if you f- if you re-synthesize these spot sounds as  like  sounds  uh - Mm-hmm. And - Well  none of these systems  by the way  have - I mean  y- you both are - are working with  um  our system that does not have the neural net  right? Yep. Yeah. Mm-hmm. O_K. Yeah. So one would hope  presumably  that the neural net part of it would - would improve things further as - as they did before. Yeah. Um - Yeah  although if - if we  um  look at the result from the proposals  one of the reason  uh  the n- system with the neural net was  um  more than - well  around five percent better  is that it was much better on highly mismatched condition. I'm thinking  for instance  on the T_I-digits trained on clean speech and tested on noisy speech. Mm-hmm. Uh  for this case  the system with the neural net was much better. But not much on the - in the other cases. And Mm-hmm. Yeah. if we have no  uh  spectral subtraction or Wiener filtering  um  i- the system is - Uh  we thought the neural - neural network is much better than before  even in these cases of high mismatch. So  maybe the neural net will help less but  um - Maybe. Could you train a neural net to do spectral subtraction? Yeah  it could do a nonlinear spectral subtraction but I don't know if it - I mean  you have to figure out what your targets are. Mm-hmm. Yeah  I was thinking if you had a clean version of the signal and - and a noisy version  Mm-hmm. Right. Mm-hmm. and your targets were the M_F_- uh  you know  whatever  frequency bins - Yeah  well  that's not so much spectral subtraction then  but - but - but it's - but at any rate  yeah  people  uh - Mm-hmm. People do that? y- yeah  in fact  we had visitors here who did that I think when you were here ba- way back when. Mm-hmm. Hmm. Uh  people - d- done lots of experimentation over the years with training neural nets. And it's not a bad thing to do. It's another approach. Mm-hmm. Hmm. M- I mean  it's - it  um - The objection everyone always raises  which has some truth to it is that  um  it's good for mapping from a particular noise to clean but then you get a different noise. Mm-hmm. And the experiments we saw that visitors did here showed that it - there was at least some  um  gentleness to the degradation when you switched to different noises. It did seem to help. So that - you're right  that's another - another way to go. How did it compare on - I mean  for - for good cases where it - it - uh  stuff that it was trained on? Did it do pretty well? Oh  yeah  it did very well. Mmm. Yeah. Mmm. Um  but to some extent that's Mm-hmm. kind of what we're doing. I mean  we're not doing exactly that  we're not trying to generate good examples but by trying to do the best classifier you possibly can  Mm-hmm. for these little phonetic categories  You could say it's sort of built in. It's - Yeah  it's kind of built into that. And - and that's why we have found that it - it does help. Hmm. Um - so  um  yeah  I mean  we'll just have to try it. But I - I would - I would - I would imagine that it will help some. Mm-hmm. I mean  it - we'll just have to see whether it helps more or less the same  but I would imagine it would help some. Mm-hmm. So in any event  all of this - I was just confirming that all of this was with a simpler system. O_K? Yeah  yeah. Um  Yeah  so this is th- the  um - Well  actually  this was kind of the first try with this spectral subtraction plus smoothing  and I was kind of excited by the result. Mm-hmm. Mm-hmm. Um  then I started to optimize the different parameters. And  uh  the first thing I tried to optimize is the  um  time constant of the smoothing. And it seems that the one that I chose for the first experiment was the optimal one  so uh  It's amazing how often that happens. Um  so this is the first thing. Um - Yeah  another thing that I - it's important to mention is  um  that this has a- this has some additional latency. Um. Because when I do the smoothing  uh  it's a recursion that estimated the means  so - of the g- of the gain curve. And this is a filter that has some latency. And I noticed that it's better if we take into account this latency. So  instead o- of using the current estimated mean to  uh  subtract the current frame  it's better to use an estimate that's some- somewhere in the future. Um - And that's what causes the latency? Yeah. Mm-hmm. O_K. You mean  the m- the mean is computed o- based on some frames in the future also? Or - or no? It's the recursion  so it's - it's the center recursion  right? Mm-hmm. Um - and the latency of this recursion is around fifty milliseconds. One five? @@ One five? Five zero? Five zero  yeah. Five zero. Yeah. Um  mmm. I'm sorry  why - why is that delay coming? Like  you estimate the mean? Yeah  the mean estimation has some delay  right? Oh  yeah. I mean  the - the filter that - that estimates the mean has a time constant. It isn't - O_K  so it's like it looks into the future also. Yeah. O_K. What if you just look into the past? It's  uh  not as good. It's not bad. Um  it helps a lot over the ba- the baseline but  mmm - it - How m- by how much? By how much? It's around three percent  um  relative. Worse. Yeah. Yeah. Um  mmm - Hmm. So  uh - It's depending on how all this stuff comes out we may or may not be able to add any latency. Yeah  but - Yeah. So  yeah  it depends. Uh  y- actually  it's - it's l- it's three percent. Right. Mmm. Yeah  b- but I don't think we have to worry too much on that right now while - you kno- . Mm-hmm. So - Um  s- Yeah  I mean  I think the only thing is that - I would worry about it a little. Mm-hmm. Because if we completely ignore latency  and then we discover that we really have to do something about it  we're going to be - find ourselves in a bind. Mm-hmm. So  um  you know  maybe you could make it twenty-five. You know what I mean? Yeah. Oh yes. Yeah  just  you know  just be - be a little conservative because we may end up with this crunch where all of a sudden we have to cut the latency in half or something. s- Mm-hmm. Yeah. O_K. Um. So  yeah  there are other things in the  um  algorithm that I didn't  uh  @@ a lot yet  which - Oh! Sorry. A quick question just about the latency thing. If - if there's another part of the system that causes a latency of a hundred milliseconds  is this an additive thing? Or c- or is yours hidden in that? Mm-hmm. No  it's - it's added. Uh - It's additive. O_K. Mm-hmm. We can - O_K. We can do something in parallel also  in some like - some cases like  if you wanted to do voice activity detection. Uh-huh. And we can do that in parallel with some other filtering you can do. So you can make a decision on that Mmm. voice activity detection and then you decide whether you want to filter or not. But by then you already have the sufficient samples to do the filtering. Yeah. Mm-hmm. So - So  sometimes you can do it anyway . I mean  couldn't  uh - I - Couldn't you just also - I mean  i- if you know that the l- the largest latency in the system is two hundred milliseconds  don't you - couldn't you just buffer up that number of frames and then Yeah. everything uses that buffer? And that way it's not additive? Well  in fact  everything is sent over in buffers cuz of - isn't it the T_C_P buffer some - ? You mean  the - the data  the super frame or something? Mm-hmm. Yeah  yeah. Yeah. Yeah  but that has a variable latency because the last frame doesn't have any latency and first frame has a twenty framed latency. So you can't r- rely on that latency all the time. Mm-hmm. Yeah. Because - I mean the transmission over - over the air interface is like a buffer. Twenty frame - twenty four frames. Yeah. Yeah. Yeah. So - But the only thing is that the first frame in that twenty-four frame buffer has a twenty-four frame latency. And the last frame doesn't have any latency. Mm-hmm. Because it just goes as - Yeah. Yeah  I wasn't thinking of that one in particular but more of  you know  if - if there is some part of your system that has to buffer twenty frames  Yeah. uh  can't the other parts of the system draw out of that buffer and therefore not add to the latency? Yeah. And - and that's sort of one of the - all of that sort of stuff is things that they're debating in their standards committee. Oh! Hmm. Mm-hmm. Yeah. So  um  there is uh  these parameters that I still have to - to look at. Like  I played a little bit with this overestimation factor  uh  but I still have to - to look more at this  um  at the level of noise I add after. Uh  I know that adding noise helped  um  the system just using spectral subtraction without smoothing  but I don't know right now if it's still important or not  and if the level I choose before is still the right one. Same thing for the shape of the - the noise. Maybe it would be better to add just white noise instead of speech shaped noise. That'd be more like the J_RASTA thing in a sense. Yeah. Mm-hmm. Um  yep. Uh  and another thing is to - Yeah  for this I just use as noise estimate the mean  uh  spectrum of the first twenty frames of each utterance. I don't remember for this experiment what did you use for these two stage - I used ten - just ten frames. Yeah  because - I mean  the reason was like in T_I-digits I don't have a lot. I had twenty frames most of the time. The ten frames? Mm-hmm. Um. But  so what's this result you told me about  the fact that if you use more than ten frames you can - improve by t- Well  that's - that's using the channel zero. If I use a channel zero VAD to estimate the noise. Oh  O_K. But this is ten frames plus - plus Which - Channel zero dropping. channel - Uh  no  these results with two stage Wiener filtering is ten frames but possibly more. I mean  if channel one V_A_D gives you - Yeah. Hmm. t- Oh  this - f- Yeah. Mm-hmm. Yeah. O_K. Yeah  but in this experiment I did - I didn't use any V_A_D. I just used the twenty first frame to estimate the noise. And - So I expected it to be a little bit better  if  uh  I use more - more frames. Um. O_K  that's it for spectral subtraction. The second thing I was working on is to  um  try to look at noise estimation  mmm  and using some technique that doesn't need voice activity detection. Um  and for this I u- simply used some code that  uh  I had from - from Belgium  which is technique that  um  takes a bunch of frame  um  and for each frequency bands of this frame  takes a look at the minima of the energy. And then average these minima and take this as an - an energy estimate of the noise for this particular frequency band. And there is something more to this actually. What is done is that  uh  these minima are computed  um  based on  um  high resolution spectra. So  I compute an F_F_T based on the long  uh  signal frame which is sixty-four millisecond - So you have one minimum for each frequency? What - what I - what I d- uh  I do actually  is to take a bunch of - to take a tile on the spectrogram and this tile is five hundred milliseconds long and two hundred hertz wide. Mmm. And this tile - Uh  in this tile appears  like  the harmonics if you have a voiced sound  because it's - it's the F_T_T bins. And when you take the m- the minima of - of these - this tile  when you don't have speech  these minima will give you some noise level estimate  If you have voiced speech  these minima will still give you some noise estimate because the minima are between the harmonics. And - If you have other - other kind of speech sounds then it's not the case  but if the time frame is long enough  uh  like s- five hundred milliseconds seems to be long enough  you still have portions which  uh  are very close - whi- which minima are very close to the noise energy. I'm confused. You said five hundred milliseconds but you said sixty-four milliseconds. Which is which? What? Mmm? Sixty-four milliseconds is to compute the F_F_T  uh  bins. The - the F_F_T. Yeah  yeah. Um  actually it's better to use sixty-four milliseconds because  um  if you use thirty milliseconds  then  uh  because of the - this short windowing and at low pitch  uh  sounds  the harmonics are not  wha- uh  correctly separated. Mm-hmm. So if you take these minima  it - b- they will overestimate the noise a lot. So you take sixty-four millisecond F_F_Ts and then you average them over five hundred? Or - ? Uh  what do you do over five hundred? So I take - to - I take a bunch of these sixty-four millisecond frame to cover five hundred milliseconds  Ah. O_K. and then I look for the minima  I see. Mmm. on the - on - on the bunch of uh fifty frames  right? I see. Mmm. So the interest of this is that  as y- with this technique you can estimate u- some reasonable noise spectra with only five hundred milliseconds of - of signal  so if the - the n- the noise varies a lot  uh  you can track - better track the noise  Mm-hmm. which is not the case if you rely on the voice activity detector. So even if there are no- no speech pauses  you can track the noise level. The only requirement is that you must have  in these five hundred milliseconds segment  you must have voiced sound at least. Cuz this - these will help you to - to track the - the noise level. Um. So what I did is just to simply replace the V_A_D-based  uh  noise estimate by this estimate  first on SpeechDat-Car - Well  only on SpeechDat-Car actually. And it's  uh  slightly worse  like one percent relative compared to the V_A_D-based estimates. Um  I think the reason why it's not better  is that the SpeechDat-Car noises are all stationary. Um. So  u- y- y- there really is no need to have something that's adaptive and - Uh  well  they are mainly stationary. Um. Mm-hmm. But  I expect s- maybe some improvement on T_I-digits because  nnn  in this case the noises are all sometimes very variable. Uh  so I have to test it. Mmm. But are you comparing with something - e- I'm - I'm - p- s- a little confused again  i- it - Uh  when you compare it with the V_A_ D-based  Mm-hmm. It's - which V_A_D- Is this - is this the - ? It's the France-Telecom-based spectra  s- uh  Wiener filtering and V_A_D. So it's their system but just I replace their noise estimate by this one. Oh  you're not doing this with our system? In i- I'm not - No  no. Yeah  it's our system but with just the Wiener filtering from their system. Right? Mmm. O_K. Yeah. Actually  th- the best system that we still have is  uh  our system but with their noise compensation scheme  right? Right. But - So I'm trying to improve on this  and - by - by replacing their noise estimate by  uh  something that might be better. O_K. But the spectral subtraction scheme that you reported on also re- requires a - a noise estimate. Yeah. Yeah. But I di- Couldn't you try this for that? Do you think it might help? Not yet  because I did this in parallel  and I was working on one and the other. Um  I see  I see. Yeah. Yeah  for - for sure I will. I can try also  mmm  the spectral subtraction. Yeah. So I'm also using that O_K. n- new noise estimate technique on this Wiener filtering what I'm trying. Mm-hmm. So I - I have  like  some experiments running  I don't have the results. Yeah. Yeah. So. I don't estimate the f- noise on the ten frames but use his estimate. Mm-hmm. Yeah. Um. Yeah. I  um  also implemented a sp- um - spectral whitening idea which is in the  um  Ericsson proposal. Uh  the idea is just to um  flatten the log  uh  spectrum  um  and to flatten it more if the - the probability of silence is higher. So in this way  you can also reduce - somewhat reduce the musical noise and you reduce the variability if you have different noise shapes  because the - the spectrum becomes more flat in the silence portions. Um. Yeah. With this  no improvement  uh  but there are a lot of parameters that we can play with and  um - Actually  this - this could be seen as a soft version of the frame dropping because  um  you could just put the threshold and say that ""below the threshold  I will flatten - comp- completely flatten the - the spectrum"". And above this threshold  uh  keep the same spectrum. So it would be like frame dropping  because during the silence portions which are below the threshold of voice activity probability  uh  w- you would have some kind of dummy frame which is a perfectly flat spectrum. And this  uh  whitening is something that's more soft because  um  you whiten - you just  uh  have a function - the whitening is a function of the speech probability  so it's not a hard decision. Mm-hmm. Um  so I think maybe it can be used together with frame dropping and when we are not sure about if it's speech or silence  well  It's interesting. I mean  um  you know  in - maybe it has something do with this. in J_RASTA we were essentially adding in  uh  white - uh  white noise dependent on our estimate of the noise. Mm-hmm. On the overall estimate of the noise. Uh  I think it never occurred to us to use a probability in there. Mm-hmm. You could imagine one that - that - that made use of where - where the amount that you added in was  uh  a function of the probability of it being s- speech or noise. Mm-hmm. Mm-hmm. Yeah  w- Yeah  right now it's a constant that just depending on the - There's - the noise spectrum. Yeah. Mm-hmm. Cuz that - that brings in Mm-hmm. sort of powers of classifiers that we don't really have in  uh  this other estimate. So it could be - it could be interesting. Mm-hmm. Mm-hmm. What - what - what point does the  uh  system stop recording? How much - It'll keep going till - It went a little long? I mean  disk - I guess when they run out of disk space  but - So. I think we're O_K. O_K. Yeah. Uh - Yeah  so there are - with this technique there are some - I just did something exactly the same as - as the Ericsson proposal but  um  the probability of speech is not computed the same way. And I think  i- for - yeah  for a lot of things  actually a g- a good speech probability is important. Like for frame dropping you improve  like - Mm-hmm. you can improve from ten percent as Sunil showed  if you use the channel zero speech probabilities. For this it might help  um - Mm-hmm. Mm-hmm. S- so  yeah. Uh  so yeah  the next thing I started to do is to  uh  try to develop a better voice activity detector. And  um - I d- um - yeah  for this I think we can maybe try to train the neural network for voice activity detection on all the data that we have  including all the SpeechDat-Car data. Um - And so I'm starting to obtain alignments on these databases. Um  and the way I mi- I do that is that I just use the H_T_K system but I train it only on the close-talking microphone. And then I aligned - I obtained the Viterbi alignment of the training utterances. Um - It seems to be  uh i- Actually what I observed is that for Italian it doesn't seem - No. Th- there seems to be a problem. Well. Because - So  it doesn't seems to help by their use of channel zero or channel one. What? Uh  you mean their d- the frame dropping  right? Yeah. Yeah  it doesn't - Yeah. So  u- but actually the V_A_D was trained on Italian also  so - Italian. Um  the c- the current V_A_D that we have was trained on  uh  t- SPINE  right? Italian  and T_I-digits with noise and - T_I-digits. @@ Uh  yeah. And it seems to work on Italian but not on the Finnish and Spanish data. So  maybe one reason is that s- s- Finnish and Spanish noise are different. And actually we observed - we listened to some of the utterances and sometimes for Finnish there is music in the recordings and strange things  right? Yeah. Um - Yeah  so the idea was to train all the databases and obtain an alignment to train on these databases  and  um  also to  um  try different kind of features  uh  as input to the V_A_D network. And we came up with a bunch of features that we want to try like  um  the spectral slope  the  um  the degree o- degree of voicing with the features that  uh  we started to develop with Carmen  um  e- with  uh  the correlation between bands and different kind of features  and - Yeah. Yeah. Mm-hmm. The energy also. The energy. Yeah. Of course. Yeah. Yeah  right. Yeah. O_K. Well  Hans-Guenter will be here next week so I think he'll be interested in all - all of these things. And  so. Mm-hmm. Mmm. O_K  shall we  uh  do digits? Yeah. Want to go ahead  Morgan? Sure. Transcript L_ dash two zero nine. Four six  two eight  eight nine  three zero  two zero. Four two two  zero eight zero  nine five two. Five  zero seven five  one two  one zero five  six. Nine three seven  one zero five  two seven six eight. Three one six  seven two seven  five three one one. Seven  three two nine  seven two  three zero four  two. Seven six four six  seven  zero one one. Five eight one  five two  two six eight eight. Transcript L_ dash two nine four. Six zero two  four five nine  two two eight. Zero five eight  two seven  three four f- Scratch that. three seven four six. Seven nine nine one  six zero zero seven  one four one eight. Six six seven  one seven seven  four four nine. Zero nine  four zero  one nine  six two  one three. Six zero  five nine  seven eight  two six  zero six. Eight  six one three  two nine  six nine seven  three. Eight  five five six  seven one  five seven six  six. Transcript L_ dash two eight four. Three six four  eight two seven  three six one one. Five  three six four  five four  nine four zero  eight. One three six  nine five four  two zero eight. Four  two three two  three five  seven seven three  eight. One nine nine six  six four eight eight  two four zero two. One five three  three nine  three eight three nine. Nine three six  five seven  eight zero six zero. Six eight nine  one five zero  seven nine three five. Transcript L_ dash two eight five. Seven two six nine  four  four one six  zero seven six nine  zero  four five four  One eight eight four  three eight five three  eight seven zero nine  zero five  three five  three nine  three two  six six  four  nine zero nine  nine zero  nine one zero  nine  six seven one three  zero five two seven  one two three six  seven eight three  two nine six  three three eight five  four seven five seven  one two seven six  four nine seven five. Transcript L_ dash two eight six. Three seven five  four five  one four six nine. Three  five four three  five three  one five zero  nine. Six eight five two  five eight two one  four three four four. Eight  two six five  five eight  zero zero eight  one. Five three nine six  one zero five five  three three eight three. Five zero one  one nine five  nine one zero. Five  three four three  one one  seven eight five  nine. Six zero five zero  one one eight seven  two three nine one. O_K. ",The ICSI Meeting Recorder Group at Berkley have a temporary new member on loan from research partner OGI. He began the meeting by reporting his recent activities  which included looking at the new baseline system. The other members of the group also reported their recent progress in areas such as spectral subtraction and voicing detection. They also explained some of their projects to their guest. The group shall soon be taking delivery of more machines for a computation farm  and they discussed some software tools for running large processes. Speaker me018 will construct an FAQ about the new computing tools and setup  and email details. Fn002 agrees to try an alternative approach to her new feature for voicing detection. Speaker mn007 has taken the spectral subtraction from another groups system  and is trying it with their own  with mixed results. He is also looking into alternative methods of removing noise. Fn002 is still working on voicing detection  and has run experiments with her new feature with disappointing results. Though not directly related to the groups work  speaker me006 has been putting together his proposal or a PhD  and me026 has been helping another researcher with his work on formants. 
"O_K  we're recording. We can say the word ""zero"" all we want  but just - I'm doing some square brackets  coffee sipping  square brackets. That's not allowed  I think. Curly brackets. Oops. Cur- curly brackets. Mmm! Is that voiced or unvoiced? Curly brackets. Well   correction for transcribers. Yeah. Curly brackets. Right. Gar- darn! Channel two. Yeah. Do we use square brackets for anything? u- These poor transcribers. Uh - Not ri- not right now. I mean - No. u- There's gonna be some zeros from this morning's meeting because I noticed that Barry  I think maybe you turned your mike off before the digits were - Oh  was it during digits? Oh  so it doesn't matter. Yeah. So it's not - it's not that bad if it's at the end  but it's - in the beginning  it's bad. It's still not a good idea. Yeah. Yeah. Yeah  you wanna - you wanna keep them on so you get good noise - noise floors  That's interesting. through the whole meeting. Uh  I probably just should have left it on. Yeah I did have to run  but - Hmm. Is there any way to change that in the software? Change what in the software? Where like you just don't - like if you - if it starts catching zeros  like in the driver or something - in the card  or somewhere in the hardware - Where if you start seeing zeros on w- across one channel  you just add some random  @@ noise floor - like a small noise floor. I mean certainly we could do that  but I don't think that's a good idea. We can do that in post-processing if - if the application needs it. Yeah. Well  I - u- I actually don't know what the default is anymore as to how we're using the - the front-end stuff but - Manual post-processing. for - for - when we use the ICSI front-end  but um  As an argument. there is an - there is an o- an option in - in RASTA  which  um  in- when I first put it in  uh  back in the days when I actually wrote things  uh  I did actually put in a random bit or so that was in it   but O_K. then I realized that putting in a random bit was equivalent to adding uh - adding flat spectrum  Right. and it was a lot faster to just add a constant to the - to the spectrum. So then I just started doing that instead of calling ""rand"" or something  so. Mmm. O_K. Right. So it d- it does that. Gee! Here we all are! Uh  so the only agenda items were Jane - was Jane wanted to talk about some of the I_B_M There's an agenda? transcription process. I sort of condensed the three things you said into that. And then just - I only have like  this afternoon and maybe tomorrow morning to get anything done before I go to Japan for ten days. So if there's anything that n- absolutely  desperately needs to be done  you should let me know now. Uh  and you just sent off a Eurospeech paper  so. Right. I hope they accept it. I mean  I - Right. both actu- as - as a submission and - you know  as a paper. Um - but - Yeah  I guess you - first you have to do the first one  and then - Yeah. but - Well yeah  you sent it in late. Yeah. We actually exceeded the delayed deadline by o- another day  so. Oops. Oh they - they had some extension that they announced or something? Well yeah. Liz had sent them a note saying ""could we please have another"" I don't know  ""three days"" or something  and they said yes. And then she said ""Did I say three? I meant four."" Oh  that was the other thing uh  But u- uh  Dave Gelbart sent me email  I think he sent it to you too  that um  there's a special topic  section in si- in Eurospeech on new  corp- corpors- corpora. And it's not due until like May fifteenth. Oh this isn't the Aurora one? It's another one? No. It's a different one. No it's - Yeah. Huh! Yeah. And uh  Oh! I got this mail from - I s- forwarded it to Jane as I thought being the most relevant person. Um - So  I thought it was highly relevant - have you - did you look at the U_R_L? That's - Yeah I'm - Yeah. I think so too. Um  I haven't gotten over to there yet  but what - our discussion yesterday  I really - I - I wanna submit one. Mm-hmm. Was this SmartKom message? Yeah. I think Christoph Draxler sent this  yeah. And  you offered to - to join me  if you want me to. Yeah. I'll help  but obviously I can't  I think several people - sent this  yeah. Yeah  that's right. really do  most of it  so. Yeah. Uh-huh. Yeah. But any - any help you need I can certainly provide. Well  that's - that's a great idea. Well - there - there were some interesting results in this paper  though. For instance that Morgan - uh  accounted for fifty-six percent of the Robustness meetings in terms of number of words. Wow. In - in terms of what? In term- Number of words. One? Wow! O_K. That's just cuz he talks really fast. Do you mean  n- No. Oh. Short words . because - I know- is it partly  eh  c- correctly identified words? Or is it - No. Well  according to the transcripts. or just overall volume? Yeah. But re- well regardless. Oh. O_K. I think it's - he's - he's in all of them  I mean  we didn't mention Morgan by name we just - and he talks a lot. Well - we have now  but - One participant. We - we - we - something about - Did you identify him as a senior member? No  we as- identify him as the person dominating the conversation. Well. Yeah. I mean I get these A_A_R_P things  but I'm not se- really senior yet  but - O_K. Right Hmm. Um  but uh  other than that delightful result  what was the rest of the paper about? Um  well it was about - it had three sections uh - three kinds of uh You sent it to me but I haven't seen it yet. results  if you will. Uh  the one was that the - just the - the amount of overlap um  The good  the bad  and the ugly. s- in terms of - in terms of number of words and also we computed something called a ""spurt""  which is essentially a stretch of speech with uh  no pauses exceeding five hundred milliseconds. Um  and we computed how many overlapped i- uh spurts there were and how many overlapped words there were. Um  for four different corpora  the Meeting Recorder meetings  the Robustness meetings Switchboard and CallHome  and  found - and sort of compared the numbers. Um  and found that the  uh  you know  as you might expect the Meeting Recorder meetings had the most overlap uh  but next were Switchboard and CallHome  which both had roughly the same  almost identical in fact  and the Robustness meetings were - had the least  so - One sort of unexpected result there is that uh I'm surprised. two-party telephone conversations have about the same amount of overlap  sort of in gen- you know - order of magnitude-wise as  uh - as face-to-face meetings with multiple - I have - I had better start changing all my slides! Yeah. Also  I - in the Levinson  the pragmatics book  in you know  uh  textbook  there's - I found this great quote where he says Mm-hmm. Yeah. Yeah. you know - you know  how people - it talks about how uh - how - how people are so good at turn taking  and so - they're so good that generally  u- the overlapped speech does not - is less than five percent. So  Oh  that's interesting. Yeah. this is way more than five percent. Um. Did he mean face - like face-to-face? Or - ? Well  in real conversations  everyday conversations. Hmm. It's s- what these conversation analysts have been studying for years and years there . Mm-hmm. Mm-hmm. Well  of course  no  it doesn't necessarily go against what he said  cuz he said ""generally speaking "". In order to - But - Hmm. to go against that kind of a claim you'd have to big canvassing. And in f- Well  he - he made a claim - Well - Well - Yeah  we - we have pretty limited sample here. all So - But - Five percent of time or five percent of what? @@ Yeah. Yeah  I was gonna ask that too. Well it's time. So - but still - but still - u- Yeah. Exactly. It's - i- it's not against his conclusion  it just says that it's a bi- bell curve  and that  Yeah  so - you have something that has a nice range  in your sampling. Yeah. So there are slight - There are differences in how you measure it  but still it's - You know  the difference between um - between that number and what we have in meetings  which is more like  Mm-hmm. you know  close to - in meetings like these  uh - you know  close to twenty percent. But what was it like  say  in the Robustness meeting  for instance? Mm-hmm. That - But - @@ Robustness meeting? It was about half of the r- So  in terms of number of words  it's like seventeen or eigh- eighteen percent for the Meeting Recorder meetings and about half that for  Maybe ten percent? uh  the Robustness. But I don't know if that's really a fair way of comparing between  multi-party  conversations and two-party conversations. Then - then - then you have to - Yeah. I - I - I don't know. I mean that's just something - Yeah  I just wonder if you have to normalize by the numbers of speakers or something. Yeah. Then - Yeah  then normalize by - by something like that  yeah. Well  we didn't get to look at that  but this obvious thing to see if - if there's a dependence on the number of uh - participants. Yeah  that's a good point. Yeah. Good idea. I mean - I bet there's a weak dependence. I'm sure it's - it's not a real strong one. Yeah. Right. Right? Because you- Cuz not everybody talks. Yeah. Right. Right. You have a lot of - a lot of two-party  Right. subsets within the meeting. Well regardless - it's an interesting result regardless. Uh-huh. So - Right. And - and - and then - and we also d- computed this both with and without backchannels  so you might think that backchannels have a special status because they're essentially just - Yes  that's right. Mm-hmm. Uh-huh. So  did - we all said ""uh-huh"" and nodded at the same time  so. R- right. But  even if you take out all the backchannels - so basically you treat backchannels l- as nonspeech  as pauses  you still have significant overlap. You know  it goes down from maybe - Mm-hmm. Mm-hmm. For Switchboard it goes down from - I don't know - f- um - I don't know - f- fourteen percent of the words to maybe uh I don't know  eleven percent or something - it's - it's not a dramatic change  so it's - Mm-hmm. Anyway  so it's uh - That was - that was one set of results  and then the second one was just basically the - Hmm. the stuff we had in the - in the H_L_T paper on how overlaps effect the recognition performance. Nope . Right. Mm-hmm. And we rescored things um  a little bit more carefully. We also fixed the transcripts in - in numerous ways. Uh  but mostly we added one - one number  which was what if you uh  basically score ignoring all - So - so the - the conjecture from the H_L_T results was that most of the added recognition error is from insertions due to background speech. So  we scored all the recognition results  uh  in such a way that the uh - Oh by the way  who's on channel four? You're getting a lot of breath. Yeah. That's - I j- was just wondering. Yeah. That's me. uh  well Don's been working hard. @@ That's right. O_K  so - so if you have the foreground speaker speaking here  and then there's some background speech  may be overlapping it somehow  um  and this is the time bin that we used  then of course you're gonna get insertion errors here and here. Right? So we scored everything  and I must say the NIST scoring tools are pretty nice for this  Right. where you just basically ignore everything outside of the  uh  region that was deemed to be foreground speech. And where that was we had to use the t- forced alignment  uh  results from s- for - so - That's somewhat - that's somewhat subject to error  but still we - we - Uh  Don did some ha- hand-checking and - and we think that - based on that  we think that the results are you know  valid  although of course  some error is gonna be in there. But basically what we found is after we take out these regions - so we only score the regions that were certified as foreground speech  the recognition error went down to almost uh  the level of the non-overlapped speech. So that means that even if you do have background speech  if you can somehow separate out or find where it is  That's great. uh  the recognizer does a good job  Yeah. even though there is this back- Yeah  I guess that doesn't surprise me  because  with the close-talking mikes  the - the signal will be so much stronger. Right. Right. Mm-hmm. Mm-hmm. Um  so - What - what sort of normalization do you do? Uh  well  we just - @@ we do - u- you know  vit- I mean in you recognizer  in the S_R_I recognizer. Well  we do uh  V_T_L - vocal tract length normalization  w- and we uh - you know  we - we uh  make all the features have zero mean and unit variance. And - Over an entire utterance? Over - over the entire c- over the entire channel. Over the - but you know. Or windowed? Don't train - Hmm. Um  now we didn't re-align the recognizer for this. We just took the old - So this is actually a sub-optimal way of doing it  right? So we took the old recognition output and we just scored it differently. Right. Right. So the recognizer didn't have the benefit of knowing where the foreground speech - a- start- Were you including the - the lapel in this? Yes. And did the - did - did the la- did the - the problems with the lapel go away also? Or - Um  it - Yeah. It u- not per - I mean  not completely  but yes  dramatically. fray- for - for insertions? Less so. So we have to um - I mean  you still - Well I should bring the - should bring the table with results. Maybe we can look at it Monday . I would presume that you still would have somewhat higher error with the lapel for insertions than - Yeah. Yes. It's - It's - Yes. Yeah. Cuz again  looking forward to the non-close miked case  I think that we s- still - Mm-hmm. I'm not looking forward to it. i- it's the high signal-to-noise ratio Right. here that - that helps you. u- s- Right. So - so that was number - that was the second set of - uh  the second section. And then  the third thing was  we looked at  uh  what we call ""interrupts""  although that's - that may be a misnomer  but basically we looked at cases where - Uh  so we - we used the punctuation from the original transcripts and we inferred the beginnings and ends of sentences. So  you know - Di- did you use upper-lower case also  or not? Um - Hmm? U- upper lower case or no? O_K. No  we only used  you know  uh periods  uh  question marks and exclamation. And we know that there's th- that's not a very g- I mean  we miss a lot of them  but - but it's f- i- i- Yeah. That's O_K but - Comma also or not? No commas. No. O_K. And then we looked at locations where  uh  if you have overlapping speech and someone else starts a sentence  you know  where do these - where do other people start their turns - not turns really  but you know  sentences  um - Ah. So we only looked at cases where there was a foreground speaker and then at the to- at the - so the - the foreground speaker started into their sentence and then someone else started later. O_K? Somewhere in between the start and the end? O_K. And so what - Sorry? Somewhere in between the start and the end of the foreground? Yes. Uh  so that such that there was overlap between the two sentences. Yeah. So  the - the question was how can we - what can we say about the places where the second or - or actually  several second speakers  um start their ""interrupts""  as we call them. Three words from the end. w- At pause boundaries. And we looked at this in terms of um - On T_closures  only. So - so we had - we had um u- to - for - for the purposes of this analysis  we tagged the word sequences  and - and we time-aligned them. Um  and we considered it interrupt - if it occurred in the middle of a word  we basically - you know  considered that to be a interrupt as if it were at - at the beginning of the word. So that  if any part of the word was overlapped  it was considered an interrupted word. Mm-hmm. And then we looked at the - the locatio- the  um  you know  the features that - the tags because we had tagged these word strings  um  that - that occurred right before these - these uh  interrupt locations. Tag by uh- And the tags we looked at are the spurt tag  which basically says - or actually - Sorry. End of spurt. So - whether there was a pause essentially here  because spurts are a - defined as being you know  five hundred milliseconds or longer pauses  and then we had things like discourse markers  uh  backchannels  uh  disfluencies. um  uh  filled pauses - So disfluen- the D_'s are for  um  the interruption points of a disfluency  so  where you hesitate  or where you start the repair there. Uh  what else do we had. Uh  repeated - you know  repeated words is another of that kind of disfluencies and so forth. So we had both the beginnings and ends of these - uh so  the end of a filled pause and the end of a discourse marker. And we just eyeballed - I mean we didn't really hand-tag all of these things. We just looked at the distribution of words  and so every ""so yeah""  and ""O_K""  uh  and ""uh-huh"" were - were the - were deemed to be backchannels and ""wow"" and ""so"" and uh ""right""  uh were um - Not ""right"". ""Right"" is a backchannel. But so  we sort of - just based on the lexical - um  identity of the words  we - we tagged them as one of these things. And of course the d- the interruption points we got from the original transcripts. So  and then we looked at the disti- so we looked at the distribution of these different kinds of tags  overall uh  and - and - and particularly at the interruption points. And uh  we found that there is a marked difference so that for instance after - so at the end after a discourse marker or after backchannel or after filled pause  you're much more likely to be interrupted than before. O_K? And also of course after spurt ends  which means basically in p- inside pauses. So pauses are always an opportunity for - So we have this little histogram which shows these distributions and  um  I wonder - you know  it's - it's - it's not - No big surprises  but it is sort of interesting from - It's nice to actually measure it though. Yeah. I wonder about the cause and effect there. In other words uh if you weren't going to pause you - you will because you're g- being interrupted. Well we're ne- Uh - Right. There's no statement about cause and effect. This is just a statistical correlation  yeah. Yeah  right. No  no  no. Right  I - I see. Yeah. But he - yeah  he's - he's right  y- I mean maybe you weren't intending to pause at all  but - You were intending to stop for fifty-seven milliseconds  but then Chuck came in and so you paused for a second Right. @@ Right. Yeah. Right. Anyway. So  or more. uh  and that was basically it. And - and we - so we wrote this and then  we found we were at six pages  and then we started Oops. cutting furiously and threw out half of the material again  and uh played with the LaTeX stuff and - uh  and - until it fi- Made the font smaller and the narrows longer. Font smaller  yeah. No  no. W- well  d- you couldn't really make everything smaller but we s- we put - Oh  I - I - Put the abstract end . you know the - the gap between the two columns is like ten millimeters  so I d- Took out white space. Yeah. shrunk it to eight millimeters and that helped some. And stuff like that. Wasn't there - wasn't there some result  Andreas - I - I thought maybe Liz presented this at some conference a while ago about uh  backchannels uh  Yeah - Mm-hmm. Mm-hmm. and that they tend to happen when uh the pitch drops. You know you get a falling pitch. Yeah. Well - And so that's when people tend to backchannel. Uh- i- i- do you rem- y- We didn't talk about  uh  prosodic  uh  properties at all  although that's - I - I take it that's something that uh Don will - will look at now that we have the data and we have the alignment  so. Right. Right. But - Yeah  we're gonna be looking at that. This is purely based on you know the words and - Mm-hmm. I have a reference for that though. Oh you do. Yeah. Uh-huh. So am I recalling correctly? About - Anyway   so. Well  I didn't know about Liz's finding on that  Uh-huh. but I know of another paper that talks about something Hmm. that - I'd like to see that reference too. It made me think about a cool little device that could be built O_K. to uh - to handle those people that call you on the phone and just like to talk and talk and talk. And you just have this little detector that listens for these drops in pitch and gives them the backchannel. And so then you hook that to the phone and go off and do the - do whatever you r- wanna do  while that thing keeps them busy. Yeah. Uh-huh. Oh yeah. Well - There's actually - uh there's this a former student of here from Berkeley  Nigel - Nigel Ward. Do you know him? Uh-huh. Sure. Yeah. He did a system uh  in - he - he lives in Japan now  and he did this backchanneling  automatic backchanneling system. It's a very - Right. Oh! So  exactly what you describe  but for Japanese. And it's apparently - for Japa- in Japanese it's really important that you backchannel. Huh. It's really impolite if you don't  and - So. Huh. Actually for a lot of these people I think you could just sort of backchannel continuously and it would pretty much be fine. It wouldn't matter? Yeah. Random intervals. Yeah. That's w- That's what I do. There was - there was of course a Monty Python sketch with that. Where the barber who was afraid of scissors was playing a - a tape of clipping sounds  and saying ""uh-huh""  ""yeah""  ""how about them sports teams?"" Anyway. So the paper's on-line and y- I - I think I uh - I C_C'ed a message to Meeting Recorder with the U_R_L so you can Yep. Yeah. get it. Printed it out  haven't read it yet. Um  uh one more thing. So I - I'm actually - Yeah. about to send Brian Kingsbury an email saying where he can find the - the s- the m- the material he wanted for the s- for the speech recognition experiment  so - but I haven't sent it out yet because actually my desktop locked up  like I can't type anything. Uh b- so if there's any suggestions you have for that I was just gonna Is it the same directory that you had suggested? send him the - I made a directory. I called it um - Well this isn't - He still has his Unix account here  you know. He does? Yeah. Yeah but - but - but he has to - And he - and he's - I'd hafta add him to Meeting Recorder  I guess  but - he prefe- he said he would prefer F_T_P O_K. and also  um  the other person that wants it - There is one person at S_R_I who wants to look at the um  you know  the uh - the data we have so far  O_K. and so I figured that F_T_P is the best approach. So what I did is I um - @@ I made a n- new directory after Chuck said that would c- that was gonna be a good thing. Uh  so it's ""F_T_P pub real"" - Exactly. Pub real. M_T_G_C - What is it again? C_R - Ask Dan Ellis. u- R_D_ - R_D_R  yeah. Or - Yeah. Right? The same - the same as the mailing list  and - Yeah. Yeah  the - No vowels. Yeah Um  and then under there - Um actually - Oh and this directory  is not readable. It's only uh  accessible. So  in other words  to access anything under there  you have to be told what the name is. So that's sort of a g- quick and dirty way of doing access control. So - Right. Mm-hmm. uh  and the directory for this I call it I- ""A_S_R zero point one"" because it's sort of meant for recognition. So anyone who hears this meeting now knows the - Beta? And then - then in there I have a file that lists all the other files  so that someone can get that file and then know the file names and therefore download them. If you don't know the file names you can't - I mean you can - Is that a dash or a dot in there? Don't - don't - don't say. Dash. @@ Anyway. So all I - all I was gonna do there was stick the - the transcripts after we - the way that we munged them for scoring  because that's what he cares about  and - um  and also - and then the - the waveforms that Don segmented. I mean  just basically tar them all up f- I mean - w- for each meeting I tar them all into one tar file and G_zip them and stick them there. I uh  And so. put digits in my own home directory - home F_T_P directory  but I'll probably move them there as well. Oh  O_K. O_K. So we could point Mari to this also for her Yeah. March O_one request? March O_one. Oh! Or - You n- Remember she was - Oh she wanted that also? Well she was saying that it would be nice if we had - they had a - Or was she talking - Yeah. She was saying it would be nice if they had eh the same set  so that when they did experiments they could compare. Right  but they don't have a recognizer even. Yeah. Um - But yeah  we can send - I can C_C Mari on this so that she knows - I- Yeah. So  for the thing that - We need to give Brian the beeps file  so I was gonna probably put it - That's good. Right. We can put it in the same place. Just put in another directory. Yeah  it- Well  make ano- make another directory. You don't n- m- Yeah. I'll make another directory. Yeah. Exactly. Yeah. And  Andreas  um  Yeah. sampled? They are? I think so. Yeah. O_K. Um  so either we should regenerate the original versions  or um  we should just make a note of it. Oh. Beca- Well - O_K  because in one directory there's two versions. Yeah  that's the first meeting I cut both versions. Just to check which w- if there is a significant difference. O_K. And so I - but - O_K so - but for the other meetings it's the downsampled version that you have. They're all downsampled  yeah. Oh  O_K. Oh that's th- important to know  O_K so we should probably - uh give them the non-downsampled versions. Yeah. So - O_K. Alright  then I'll hold off on that and I'll wait for you um - Probably by tomorrow I can - gen- O_K. I'll send you an email. Alright. O_K. Yeah  definitely they should have the full bandwidth version  yeah. Right. O_K. Yeah  because I mean - I- I think Liz decided to go ahead with the downsampled versions cuz we can - There was no s- like  r- significant difference. Well  it takes - it takes up less disk space  for one thing. It does take up less disk space  and apparently it did even better than the original - than the original versions  which Yeah. Yeah. Right. you know  is just  probably random. Yeah  it was a small difference but yeah. But  um they probably w- want the originals. Yeah. O_K. O_K  good. Good that - Well  it's a good thing that - O_K   I think we're losing  Don and Andreas at three-thirty  right? O_K. Yeah. Hey mon hafta booga. So  that's why it was good to have Andreas  say these things but - So  we should probably talk about the I_B_M transcription process stuff that - O_K. Hmm. So  um you know that Adam created um  a b- a script to generate the beep file? To then create something to send to I_B_M. And  um  you - you should probably talk about that. But - but you were gonna to use the originally transcribed file because I tightened the time bins and that's also the one that they had already in trying to debug the first stage of this. And uh  my understanding was that  um - I haven't - I haven't listened to it yet  but it sounded very good and - and I understand that you guys were going to have a meeting today  before this meeting. Mm-hmm. It was just to talk about how to generate it. Um  just so that while I'm gone  you can regenerate it if you decide to do it a different way. Excellent. O_K. So uh  Chuck and Thilo should  now more or less know how to generate the file and  the other thing Chuck pointed out is that  um  since this one is hand-marked  there are discourse boundaries. Mm-hmm. O_K. Right? So - so when one person is speaking  there's breaks. Whereas Thilo's won't have that. Mm-hmm . Oh! O_K. Ah  interesting. Yeah. Yeah. So what - what we're probably gonna do is just write a script  that if two  chunks are very close to each other on the same channel we'll just merge them. Oh  sure. Yeah  sure. Makes sense. So  uh  and that will get around the problem of  the  you know ""one word beep  one word beep  one word beep  one word beep"". Yeah. Ah! Clever. Yes. Clever. And - And it will be more - more close - close to - to the version they will get afterwards - after @@ . Yeah. Excellent. Yeah  in fact after our meeting uh  this morning Thilo came in and said that And that's the purpose. um  there could be other differences between the uh already transcribed meeting with the beeps in it and one that has just r- been run through his process. So tomorrow  Yeah. Yeah. when we go to make the um uh  chunked file for I_B_M  we're going to actually compare the two. So he's gonna run his process on that same meeting  Great idea! and then we're gonna do the beep-ify on both  and listen to them and see if we notice any real differences. Beep-ify! Yeah. O_K  now one thing that prevented us from apply- you - you from applying - Exactly. The training - So that is the training meeting. Yeah  it's the training meeting  but - Yeah. Yeah  w- and we know that. Wel- uh we just wanna if - if there're any major differences between O_K. But  yeah - Uh-huh. I only used the first twenty minutes for training  so we can use the @@ doing it on the hand- Oh  interesting. Ah! O_K. Interesting idea. But it's the same speakers and the same channels @@ Hmm. So this training meeting  uh w- Great. un- is that uh some data where we have uh very um  you know  accurate time marks? for - I went back and hand-marked the ba- the bins  I ment- I mentioned that last week. O_K  yeah. Because - But the - but there's - yeah  but there is this one issue with them in that there're - there are time boundaries in there that occur in the middle of speech. So - Like when we went t- to um - When I was listening to the original file that Adam had  it's like you - you hear a word then you hear a beep and then you hear the continuation of what is the same sentence. That's on the other channel. That's because of channel overlap. Well  and - and so the - th- Hmm. So there are these chunks that look like uh - It's - i- that have uh - I mean that's not gonna be true of the foreground speaker. That'll only be if it's the background speaker. Right. So you'll - you'll have a chunk of  you know  channel A_ which starts at zero and ends at ten  and then the same channel starting at eleven  ending at fifteen  and then again  starting at sixteen  ending at twenty. Right  so that's three chunks where actually we w- can just make one chunk out of that Mm-hmm. which is A_  zero  twenty. Yeah. Sure. That's what I just said  yeah. Sure. Yeah. So I just wanted to make sure that it was clear. So if you were to use these  you have to be careful not to pull out these individual - Yeah  I thought that was - Yeah. Oh! I mean it - Right  I mean w- I mean what I would - I was interested in is having - a se- having time marks for the beginnings and ends of speech by each speaker. Well  that's definitely a problem. Uh  because we could use that to fine tune our alignment process to make it more accurate. Yeah . Battery. Battery? Mm-hmm. So - uh  it - I don't care that you know  there's actually abutting segments that we have to join together. That's fine. But what we do care about is that O_K. the beginnings and ends um are actually close to the speech inside of that uh - Yeah  I think Jane tightened these up by hand. O_K  so what is the - sort of how tight are they? O_K. Yeah. Uh  it looks much better. Yeah. Looks good. They were  um  reasonably tight  but not excruciatingly tight. That would've taken more time. Oh. No  no! I don- actually I - I - I - it's f- I just wanted to get it so tha- So that if you have like ""yeah"" Let me make a note on yours. Yeah. in a - swimming in a big bin  then it's - That's fine because we don't want to - th- that's perfectly fine. In fact it's good. You always want to have a little bit of pause or nonspeech around the speech  say for recognition purposes. Uh  but just - just u- w- you know get an id- I just wanted to have an idea of the - of how much extra you allowed um - so that I can interpret the numbers if I compared that with a forced alignment segmentation. I can't answer that  but - but my main goal was um  in these areas where you have a three-way overlap and one of the overlaps involves ""yeah""  So. Mm-hmm. and it's swimming in this huge bin  I wanted to get it so that it was clo- more closely localized. Mm-hmm. Right. But are we talking about  I don't know  a tenth of a second? a - ? You know? How - how much - how much extra would you allow at most - I - I wanted to - I wanted it to be able to - l- he- be heard normally  Mm-hmm. so that if you - if you play back that bin and have it in the mode where it stops at the boundary  O_K. it sounds like a normal word. It doesn't sound like the person - i- it sounds normal. It's as if the person could've stopped there. Mm-hmm. O_K. And it wouldn't have been an awkward place to stop. Now sometimes you know  it's - these are involved in places where there was no time. And so  Mm-hmm. there wouldn't be a gap afterwards because - I mean some cases  there're some people um  who - O_K. who have very long segments of discourse where  you know  they'll - they'll breath and then I put a break. Mm-hmm. But other than that  it's really pretty continuous and this includes things like going from one sentence into the - u- one utterance into the next  one sentence into the next  um  w- without really stopping. I mean - i- they  i- you know in writing you have this two spaces and a big gap you know. But - but uh i- some people are planning and  you know  I mean  a lot - we always are planning what we're going to say next. But uh  in which case  Mm-hmm. Right. O_K. the gap between these two complete syntactic units  um  which of course n- spoken things are not always complete syntactically  but - but it would be a shorter p- shorter break than maybe you might like. But the goal there was to not have the text be Mm-hmm. so - so crudely parsed in a time bin. I mean  because from a discourse m- purpose it's - it's more - it's more useful to be able to see - and also you know  from a speech recognition purpose my impression is that if you have too long a unit  it's - it doesn't help you very much either  cuz of the memory. Well  yeah. That's fine. So  that means that the amount of time after something is variable depending partly on context  but my general goal when there was sufficient space  room  pause after it to have it be kind of a natural feeling gap. O_K. Which I c- I don't know what it would be quantified as. You know  Wally Chafe says that um  in producing narratives  the spurts that people use tend to be  uh  that the - the - what would be a pause might be something like two - two seconds. Mmm. And um  that would be  you know one speaker. The discourse - the people who look at turn taking often do use - I was interested that you chose uh  Mm-hmm. you know um  the - you know that you use cuz I think that's a unit that would be more consistent with sociolinguistics. Well we chose um  Yeah. you know  half a second because if - if you go much larger  you have a - y- you know  your - your statement about how much overlap there is becomes less  um  precise  because you include more of actual pause time into what you consider overlap speech. Mm-hmm. Um  so  it's sort of a compromise  and - it's also based - I mean Liz suggested that value based on Yeah. the distribution of pause times that you see in Switchboard and - Mm-hmm. and other corpora. Um - So - Yeah  I also used I think something around zero point five seconds for the speech-nonspeech detector - for the minimum silence length. Mm-hmm. I see. Yeah. So. Mm-hmm. O_K. In any case  this - this uh  meeting that I hand - I - I hand-adjusted two of them I mentioned before  and I sent - I sent email  so - Mm-hmm. O_K  So - so at some point we will try to fine-tune our forced alignment maybe using those as references And I sent the path. because you know  what you would do is you would play with different parameters. And to get an object- You need an objective measure of how closely you can align the models to the actual speech. And that's where your your data would be very important to have. So  I will - Um - Yeah and hopefully the new meetings which will start from the channelized version will - will have better time boundaries and alignments. Mm-hmm. Right. But I like this idea of - uh  for our purposes for the - for the I_B_M preparation  uh  n- Yeah. having these joined together  and uh - Yeah. It makes a lot of sense. And in terms of transcription  it would be easy to do it that way. The way that they have Yeah. Yeah . with the longer units  not having to fuss with adding these units at this time. Yeah. Right. Whi- which could have one drawback. If there is uh a backchannel in between those three things  Mm-hmm. the - the n- the backchannel will - will occur at the end of - of those three. And - Yes. and in - in the - in the previous version where in the n- which is used now  there  the backchannel would - would be in- between there somewhere  so. I see. Yeah. Well  that's - that's right  but you know  thi- this brings me to the other f- stage of this which I discussed with you earlier today  which is the second stage is um  That would be more natural but - Yeah. w- what to do in terms of the transcribers adjustment of these data. I discussed this with you too. Um  the tr- so the idea initially was  we would get uh  for the new meetings  so the e- E_D_U meetings  that Thilo ha- has now presegmented all of them for us  on a channel by channel basis. And um  so  I've assigned - I've - I've assigned them to our transcribers and um  so far I've discussed it with one  with uh - And I had a about an hour discussion with her about this yesterday  we went through uh E_D_U- one  at some extent. And it occurred to me that um - that basically what we have in this kind of a format is - you could consider it as a staggered mixed file  we had some discussion over the weekend a- about - at - at this other meeting that we were all a- at - um  about whether the tran- the I_B_M transcribers should hear a single channel audio  or a mixed channel audio. And um  in - in a way  by - by having this - this chunk and then the backchannel after it  it's like a stagal- staggered mixed channel. And um  it occurred to me in my discussion with her yesterday that um  um  the - the - the maximal gain  it's - from the I_B_M people  may be in long stretches of connected speech. So it's basically a whole bunch of words which they can really do  because of the continuity within that person's turn. So  what I'm thinking  and it may be that not all meetings will be good for this  but - but what I'm thinking is that in the E_D_U meetings  they tend to be driven by a couple of dominant speakers. And  if the chunked files focused on the dominant speakers  then  when - when it got s- patched together when it comes back from I_B_M  we can add the backchannels. It seems to me that um  you know  the backchannels per- se wouldn't be so hard  but then there's this question of the time @@ uh  marking  and whether the beeps would be uh y- y- y- And I'm not exactly sure how that - how that would work with the - with the backchannels. And  so um - And certainly things that are intrusions of multiple words  taken out of context and displaced in time from where they occurred  that would be hard. So  m- my thought is i- I'm having this transcriber go through the E_D_U- one meeting  and indicate a start time f- for each dominant speaker  endpoi- end time for each dominant speaker  and the idea that these units would be generated for the dominant speakers  and maybe not for the other channels. Yeah the only  um  disadvantage of that is  then it's hard to use an automatic method to do that. The advantage is that it's probably faster to do that than it is to use the automated method and correct it. So. Well  it - O_K. I think - I - I think um  you know  the original plan was that the transcriber would adjust the t- the boundaries  and all that for all the channels but  We'll just have to see. you know  that is so time-consuming  and since we have a bottleneck here  we want to get I_B_M things that are usable s- as soon as possible  then this seemed to me it'd be a way of gett- to get them a flood of data  which would be useful when it comes back to us. And um - Yeah. Oh also  at the same time she - when she goes through this  she'll be uh - If there's anything that was encoded as a pause  but really has something transcribable in it  then she's going to uh  make a mark - w- uh  so you know  so that - that bin would be marked as it - as double dots and she'll just add an S_. And in the other - in the other case  if it's marked as speech  and really there's nothing transcribable in it  then she's going to put a s- dash  and I'll go through and it - and um  you know  with a - with a substitution command  get it so that it's clear that those are the other category. I'll just  you know  recode them. But um  um  the transcribable events that um  I'm considering in this  uh  continue to be laugh  as well as speech  and cough and things like that  so I'm not stripping out anything  just - just you know  being very lenient in what's considered speech. Jane? In terms of the - this new procedure you're suggesting  um  Yeah? u- what is the - It's not that different. So I'm a little confused  because how do we know where to put beeps? Is it - i- d- y- is it - Oh  O_K. So what it - what it - what it involves is - is really a s- uh  Transcriber will do it. uh  the original pr- procedure  but only applied to uh  a certain strategically chosen s- aspect of the data. So - We pick the easy parts of the data basically  and transcriber marks it by hand. You got it. And because - But after we've done Thilo's thing. No. Yes! Yes! Oh yeah! Oh  after. Oh  O_K  I didn't - I didn't understand that. O_K. So  I'm @@ - now I'm confused. O_K. We start with your presegmented version - O_K  and I'm leaving. So  um - Yeah  I have to go as well. O_K  leave the mikes on  and just put them on the table. O_K. Thanks. We start with the presegmented version - You start with the presegmentation  r- yeah? Let me mark you as no digits. Yeah. And then um  the transcriber  instead of going painstakingly through all the channels and moving the boundaries around  and deciding if it's speech or not  but not transcribing anything. O_K? Instead of doing that  which was our original plan  Mm-hmm. the tra- They focus on the dominant speaker - They just do that on the main channels. Yeah. So what they do is they identify who's the di- dominant speaker  and when the speaker starts. O_K. Yeah? O_K. So I mean  you're still gonna - So we're - It's based on your se- presegmentation  that's the basic thing. And you just - and you just use the s- the segments of the dominant speaker then? For - for sending to - to I_B_M or - ? Yeah. Exactly. So  now Jane  my question is when they're all done adjusting the w- time boundaries for the dominant speaker  Mm-hmm. have they then also erased the time boundaries for the other ones? Uh No. No  no. So how will we know who - Huh-uh. S- Yeah. That's - that's why she's notating the start and end points of the dominant speakers. So  on a - you know  so i- in E_D_U- one  i- as far as I listened to it  you start off with a - a s- section by Jerry. So Jerry starts at minute so-and-so  and goes until minute so-and-so. And then Mark Paskin comes in. And he starts at minute such-and-such  and goes on till minute so-and-so. O_K. And then meanwhile  she's listening to both of these guys' channels  determining if there're any cases of misclassification of speech as nothing  and nothing as speech  Mm-hmm. O_K. and a- and adding a tag if that happens. So she does the adjustments on those guys? But you know  I wanted to say  his segmentation is so good  that um  the part that I listened to with her yesterday On that meeting. Mm-hmm. didn't need any adjustments of the bins. So far we haven't. So this is not gonna be a major part of the process  at least - least not in - not on ones that - that really - So if you don't have to adjust the bins  Mm-hmm? why not just do what it - for all the channels? Why not just throw all the channels to I_B_M? Well there's the question o- of whether - Well  O_K. She i- It's a question of how much time we want our transcriber to invest here when she's gonna have to invest that when it comes back from I_B_M anyway. Mm-hmm. Mm-hmm. So if it's only inserting ""mm-hmm""s here and there  Right. then  wouldn't that be something that would be just as efficient to do at this end  instead of having it go through I_B_ M  then be patched together  then be double checked here. Yeah. But - But then we could just use the - the output of the detector  and do the beeping on it  and send it to I_B_ M. Without having her check anything. Well  I guess - Right. Yeah. I think we just - we just have to listen to it and see how good they are. For some meetings  I'm - I'm sure it - i- n- I'm - I'm open to that  it was - @@ Yeah  if it's working well  that sounds like a good idea since as you say you have to do stuff with the other end anyway. That's - And some - on some meetings it's good. Yeah. Well yea- O_K  good. I mean the detector  this - Now  you were saying that they - they differ in how well they work depending on channel s- sys- systems and stuff. Yeah  I mean we have to fix it when it comes back anyhow. Yeah. Yeah. So we should perhaps just select meetings on which the speech-nonspeech detection works well  and just use  But E_D_U is great. those meetings to - to - to send to I_B_M and  Release to begin with. How interesting. You know - What's the problem - the l- I forget. Is the problem the lapel  or - or - do the other ones. Uh  it really depends. Um  my - my - my impression is that it's better for meetings with fewer speakers  and it's better for - for meetings where nobody is breathing. Oh  the dead meetings. Yeah  get - That's it. So in fact this might suggest an alternative sort of a - a c- a hybrid between these two things. So the - the one suggestion is you know we - No  the undead meeting  yeah. Yeah. Yeah? we run Thilo's thing and then we have somebody go and adjust all the time boundaries and we send it to I_B_M. The other one is Yeah? Yeah. we just run his thing and send it to I_B_M. There's a - a- another possibility if we find that there are some problems  and that is Yeah. Yeah. Yeah. if we go ahead and we just run his  and we generate the beeps file  then we have somebody listen beeps file. Yeah. And they listen to each section and say ""yes  no"" whether that section is And erase - Yeah. Is intelligible. i- i- intelligible or not. And it just - You know  there's a little interface which will - for all the ""yes""-es it - then that will be the final Yeah. Blech. That's interesting! Cuz that's - that's directly related to the e- end task. beep file. Stress test. Mm-hmm. How interesting! Yeah. I mean it wouldn't be that much fun for a transcriber to sit there  hear it  beep  yes or no. But it would be quick. Nope . I - I - I don't know. It would be - kind of quick but they're still listening to everything. But there's no adjusting. And that's what's slow. There's no adjusting of time boundaries. Well  eh  listening does take time too. Yeah. I don't know  I - I think I'm - I'm really tending towards - I mean  what's the worst that happens? Do the transcribers - I mean as long as th- on the other end they can say there's - Yeah. One and a half times real time. there's something - conventions so that they say ""huh?"" Yeah. Right. and then we can flag those later. i- i- It - i- They - they - Yeah. That's true. We can just catch it at the - catch everything at this side. Well maybe that's the best way to go  just - Yeah. How interesting! Well E_D_U - I mean it just depends on how - Yeah  u- u- u- Sorry  go ahead. So I was gonna say  E_D_U- one is good enough  maybe we could include it in this - in this set of uh  this stuff we send. Yeah. Yeah there's - I - I think there are some meetings where it would - would - Yeah I - I think  It's possible like this. we won't know until we generate a bunch of beep files automatically  listen to them and see how bad they are. Yeah. Yeah. Yeah. We won't be able to s- include it with this first thing  Mm-hmm. Hmm. If - Oh  O_K. because there's a part of the process of the beep file which requires knowing the normalization coefficients. Oh  I see. And - So a- That's not hard to do. O_K- Just - it takes - you know  it just takes five minutes rather than  Yeah. taking a second. Right  except I don't think that - the c- the instructions for doing that was in that directory  right? I - I didn't see where you had gener- So. I just hand - hard-coded it. No  but it's easy enough to do. What - @@ But I - but I have a - Doing the gain? It's no problem. n- Doing th- Adjusting the gain? No  getting the coefficients  for each channel. Know what numbers. Yeah  that's no problem. O_K. So we just run that one - We can do that. There are lots of ways to do it. I have one program that'll do it. You can find other programs. Yeah. I - I used it  so. We just run that J_sound-stat? Yep. Yeah. Yeah. O_K. O_K. But - Minus D_   capital D_ . Yeah. but - but I - I - I have another suggestion on that  which is  since  really what this is  is - is - is trying to in the large  send the right thing to them and there is gonna be this - this post-processing step  um  why don't we check through a bunch of things by sampling it? Mm-hmm. Right? In other words  rather than  um  uh  saying we're gonna listen to everything - I didn't mean listen to everything  I meant  Yeah. So y- you do a bunch of meetings  you listen to - to a little bit here and there  if it sounds like it's almost always right and there's not any big problem you send it to them. just see if they're any good. Yeah. Send it to them. O_K. Yeah. And  you know  then they'll send us back what we - w- what - what they send back to us  and we'll - we'll fix things up and Oh  that'd be great. some meetings will cost more time to fix up than others. We should - Yeah. Yeah. And we should just double-check with Brian on a few simple conventions on how they should mark things. O_K. Sure. When they - when there's either no speech in there  or Yeah. Yeah. something they don't understand  things like that. Yeah. Mm-hmm. Yeah  cuz @@ uh- what I had originally said to Brian was well they'll have to mark  Yeah. when they can't distinguish between the foreground and background  because I thought that was gonna be the most prevalent. Mm-hmm. But if we send them without editing  then we're also gonna hafta have m- uh  notations for words that are cut off  Yeah. Mm-hmm. Yeah. and other sorts of  uh  acoustic problems. And they may just guess at what those cut-off words are  but w- I mean we're gonna adjust - everything when we come back - They do already. Yeah. But what - what we would like them to do is be conservative so that they should only write down the transcript if they're sure. And otherwise they should mark it so that we can check. Yeah. Mark it. Sure. Yeah. Yeah. Mm-hmm. Well  we have the unintelligibility convention. And actually they have one also  which - Mm-hmm. Right. i- Can I maybe have - have an order of - it's probably in your paper that I haven't looked at lately  but - Uh  an order of magnitude notion of - of how - Certainty. on a good meeting  how often uh  do you get segments that come in the middle of words and so forth  and uh - in a bad meeting how often? Uh. Was is it in a - in a - what - what is the t- Well he's saying  you know  that the - the E_D_U meeting was a good - good meeting  right? Uh  and so - so - so it was almost - it was almost always doing the right thing. In a good meeting  what? Yeah. Yeah. Oh I see  the characteristics. So I wanted to get some sense of what - what almost always meant. And then  uh in a bad meeting  or p- some meetings where he said oh he's had some problems  what does that mean? Uh-huh. So I mean does one of the- does it mean one percent and ten percent? Or does it mean five percent and fifty percent? O_K. O_K. Uh - So - Or - Maybe percentage isn't the right word  but you know how many - how many per minute  or - You know. Just @@ Yeah th- Yeah  the - the problem is that  nnn  the numbers Ian gave in the paper is just uh  some frame error rate. So that's - that's not really - What will be effective for - for the transcribers  is - They have to - yeah  in- in- they have to insure that that's a real s- spurt or something. And - but  the numbers - Oops. Um - Hmm! Let me think. So the speech - the amount of speech that is missed by the detector  for a good meeting  I th- is around or under one percent  I would say. But there can be - Yeah. For - yeah  but there can be more - There's - There's more amount speech - uh  more amount of - Yeah well  the detector says there is speech  but there is none. So that - that can be a lot when - when it's really a breathy channel. But I think that's less of a problem. They'll just listen. It's just wasted time. Yeah. Yeah. And th- and that's for a good meeting. Now what about in a meeting that you said we've - you've had some more trouble with? I can't really - hhh  Tsk. I don't have really representative numbers  I think. That's really - I - I did this on - on four meetings and only five minutes of - of every meet- of - of these meetings so  it's not - not that representative  but  it's perhaps  Fff. Um - Yeah  it's perhaps then - it's perhaps five percent of something  which s- uh the - the frames - speech frames which are - which are missed  but um  I can't - can't really tell. Right. So I - So i- Sometime  we might wanna go back and look at it more in terms of Yeah. how many times is there a spurt that's - that's uh  interrupted? Yeah. Yeah. Something like that? The other problem is  that when it - when it uh d- i- on the breathy ones  where you get And - So - breathing  uh  inti- indicated as speech. And I guess we could just indicate to the transcribers not to encode that if they - We could still do the beep file. Yeah again I - I think that that is probably less of a problem because if you're - if there's - O_K. If - if a - if a word is - is split  then they might have to listen to it a few times to really understand that they can't quite get it. O_K. O_K. Whereas if they listen to it and there's - don't hear any speech I think they'd probably just listen to it once. So there'd - you'd think there'd be a - But - Yeah. a factor of three or four in - in  uh  cost function  you know  between them or something. O_K. Yeah  so - but I think that's - n- that really doesn't happen very often that - that - that a word is cut in the middle or something. That's - that's really not - not normal. So - so what you're saying is that nearly always what happens when there's a problem is that - is that uh  there's some uh  uh nonspeech that uh - that is b- interpreted as speech. That is marked as speech. Yeah. Yeah. Well then  we really should just send the stuff. That would be great. Right? Because that doesn't do any harm. Yeah  it's - You know  if they - they hear you know  a dog bark and they say what was the word  they you know  they - Yeah  I als- I - Ruff ruff! Yeah I also thought of - there - there are really some channels where it is almost um  only bre- breathing in it. Yeah? And to - to re-run's Eh  um. Yeah. I've got a - a P_- a method with loops into the cross-correlation with the P_Z_M mike  Uh-huh. and then to reject everything which - which seems to be breath. So  I could run this on those breathy channels  and perhaps throw out - That's a good idea. Wow  that's a great idea. Yeah. But I think - I th- Again  I think that sort of - that that would be good  and what that'll do is just cut the time a little further. Yeah. Mm-hmm. Yeah. But I think none of this is stuff that really needs somebody doing these - these uh  uh  Yeah. Excellent. explicit markings. Oh  I'd be delighted with that  I - I was very impressed with the - with the result. Yeah. Yeah  cuz the other thing that was concerning me about it was that it seemed kind of specialized to the E_D_U meeting  and - and that then when you get a meeting like this or something  and - and you have a b- a bunch of different dominant speakers you know  how are you gonna handle it. Whereas this sounds like a more general solution is - Yeah. Oh yeah  interesting. Oh yeah. Oh yeah  I pr- I much prefer this  I was just trying to find a way - Cuz I - I don't think the staggered mixed channel is awfully good as a way of handling overlaps. Yeah. Uh-huh. But - but uh - Well good. That - that really simplifies thing then. And we can just  you know  get the meeting  process it  Yeah. put the beeps file  send it off to I_B_M. You know? Mm-hmm. With very little work on our side. Yeah. Process it  hear into it. I would - Do what? Um  listen to it  and then - Or at least sample it. Well  sample it. Sample it. Yeah. I - I would just use some samples  make sure you don't Yeah. Yeah. Yeah. Yeah. send them three hours of ""bzzz"" or something. Yeah. No. Yeah. Right. That won't be good. Yeah. Yeah that would be very good. Yeah. Yeah. And then we can you know - That'll oughta be a good way to get the pipeline Oh  I'd be delighted. Yeah. going. And there's - there's one point which I uh - Great. yeah  which - which I r- we covered when I - when I r- listened to one of the E_D_U meetings  and that's that somebody is playing sound from his laptop. Uh-huh And i- the speech-nonspeech detector just assigns randomly the speech to - to one of the channels  so. Uh- I haven't- I didn't think of - of s- of What can you do? this before  but what - what shall we do about s- things like this? Well you were suggesting - You suggested maybe just not sending that part of the meeting. But - Yep. Mmm. But  sometimes the - the - the laptop is in the background and some - somebody is - is talking  and  that's really a little bit confusing  but - That's life. It's a little bit confusing. I mean  what're we gonna do? Yeah. Yeah. O_K. Yeah. Even a hand-transcription would - Do you - a hand-transcriber would have trouble with that. So. Yeah  that's - that's a second question  ""what - what will different transcribers do with - with the laptop sound?"" Would you - would - Yeah  go ahead. What was the l- what was the laptop sound? I mean was it speech  or was it - Yeah. It's speech. Great. Well  so - I mean - So my standard approach has been if it's not someone close-miked  then  they don't end up on one of the close-miked channels. They end up on a different channel. And we have any number of channels available  I mean it's an infinite number of channels. So just put them on some other channel. Uh-huh. Yeah. But  when thi- when this is sent to - to the I_M_- eh  I_B_ M transcribers  I don't know if - if they can tell that's really - Yeah  that's right. Yeah cuz there will be no channel on which it is foreground. Yeah. Yeah. Well  they have a convention  in their own procedures  which is for a background sound. Uh - Right  but  uh  in general I don't think we want them transcribing the background  cuz that would be too much work. Yeah. Right? For it - because in the overlap sections  then they'll- Well I don't think Jane's saying they're gonna transcribe it  but they'll just mark it as being - there's some background stuff there  right? But that's gonna be all over the place. How w- how will they tell the difference between that sort of background and the dormal - normal background of two people talking at once? Yeah. Yeah. Oh  I think - I think it'd be easy to to say ""background laptop"". But wait a minute  why would they treat them differently? How would they know that? Yeah. Well because one of them - Because otherwise it's gonna be too much work for them to mark it. They'll be marking it all over the place. Yeah. Oh  I s- background laptop or  background L_T wouldn't take any time. Sure  but how are they gonna tell bet- the difference between that and two people just talking at the same time? And - Oh  you can tell. Acoustically  can't you tell? Yeah. It's really good sound  so - Oh is it? Oh! Well  I mean  isn't there a category something like uh  ""sounds for someone for whom there is no i- close mike""? Yeah that would be very important  yeah. Yeah. But how do we d- how do we do that for the I_B_ M folks? How can they tell that? Well we may just have to do it when it gets back here. Yes  that's my opinion as well. So we don't do anything for it - with it. Yeah. O_K. That sounds good. Yeah . That sounds good. Yeah. And they'll just mark it however they mark it  and we'll correct it when it comes back. So th- Yeah. @@ O_K. there was a category for @@ speech. O_K. Yeah  the default. Yeah  s- a- No  not default. Well  as it comes back  we have a uh - when we can use the channelized interface for encoding it  then it'll be easy for us to handle. But - Yeah. but if - if out of context  they can't tell if it's a channeled speak- uh  you know  a close-miked speaker or not  then that would be confusing to them. O_K. Right. O_K. I don't know  I - it doesn't - I don't - Either way would be fine with me  I don't really care. Yeah. So. Shall we uh  do digits and get out of here? I have o- I have one question. Do you think we should send the um - that whole meeting to them and not worry about pre-processing it? Or - Uh  what I mean is we - we should Yep. Yes ma'am. leave the part with the audio in the uh  beep file that we send to I_B_M for that one  or should we start after the - that part of the meeting is over Which part? in what we send. So  the part where they're using sounds from their - from their laptops. With - with the laptop sound  or - ? w- If we have speech from the laptop should we just uh  excise that from what we send to I_B_M  or should we i- give it to them and let them do with it what they can? just - I think we should just - it - it's gonna be too much work if we hafta O_K  that'd be nice to have a - a uniform procedure. worry about that I think. Yeah  I think if we just - m- send it all to them. you know. Worry about it when we get back. Good. And see how well they do. Let - Yeah  worry about it when we get back in. Yeah. And give them freedom to - to indicate if it's just not workable. Yeah  O_K  excellent. Yeah. Yeah. Cuz  I wouldn't - don't think we would mind having that transcribed  if they did it. I think - Yeah  yeah. As I say  we'll just have to listen to it and see how horrible it is. Yeah  e- Yeah. O_K. Alright. Sample it  rather. Yeah. I think that - that will be a little bit of a problem as it really switches around between two different channels  I think. What - what I would - That's great. Mm-hmm  and - and they're very - it's very audible? on the close-talking channels? Yeah. Oh well. Yeah. I mean  it's the same problem as the lapel mike. Yeah. But - Oh  interesting. Comparable  yeah. Yeah. O_K  alright. O_K. Let's do digits. Digits. O_K  so we read the transcript number first  right? So - Are we gonna do it altogether or separately? What time is it? Uh  why don't we do it together  that's - that's a nice fast way to do it. Uh  quarter to four. Oh  O_K. Mm-hmm. One  two  three  go! Transcript L_ six three. Transcript L_ five nine. Transcript L_ five seven. Transcript L_ fifty-eight. Transcript L_ six one. Five zero  three seven  eight zero  nine eight  four seven. zero nine  seven five  one three four six  nine two one two Eight nine two  five one six  four seven three seven. Four six  four three  three seven  three one  nine eight  one nine. Nine nine seven  O_ four three  nine eight two. Eight eight six eight  zero four four one  three five six four. Four six one  zero two seven  zero three eight. One two eight  nine two two  eight three five two. Four nine  zero four  one eight  nine one  nine six five eight. six two one  two four six  O_ seven six Six one  nine seven  seven four  eight zero  eight six. Six five one  six nine seven  three eight eight one. Seven three  five two  four three  zero six  six one. one zero  nine one  six six  nine five  four eight Five eight eight  eight four  one nine nine five. Nine six  two zero  zero seven  nine seven  nine O_. Three six  four zero  six six  zero seven  nine three. four seven O_  @@ O_ four eight  O_ five O_ Two two four five  three four seven five  six eight four four. Six one one zero  seven eight O_ eight  nine five O_ four. One six two  four O_ nine  nine seven nine eight. Three two one nine  eight seven four eight  nine zero four two. Four nine five  four nine  six O_  one four. Six four six  six six three  eight one zero. Two  three five two  seven zero  seven six one  six. Three eight zero zero  eight  five six three. Four nine six four  two four eight nine  seven one four six. One two  one three  zero seven  two zero  five five. One nine nine  two seven zero  one two six  five O_  nine two  nine one three nine  three five three eight Six two five five  four one O_ one  three one nine one  Zero nine four six  six one nine zero  three six nine nine. nine eight zero five  five one one six  five eight five six. Three nine two  nine three  four one eight three. one six one  seven five  five two two three eight nine six  six three one  five one two. Five six seven  eight three zero  two five four two. Six  three two four  one seven  five O_ seven  one. seven six  one two  nine two  eight five  four seven. One six three  one two O_  two O_ two four. It's kind of interesting if there're any more errors in these  than we had the first set. Nnn  yeah  I think there probably will be. Yeah. Do you guys plug your ears when you do it? I do. I usually do. I didn't this time. No. I do. I don't. No. I haven't been  no. You don't? How can you do that? I - I - Uh  concentration. Perhaps there are lots of errors in it but Gah! I - I closed  so. Total concentration. Are you guys ready? You hate to have your ears plugged? Really? Yeah. ",The Main purpose of the meeting of ICSI's Meeting Recorder Group at Berkeley was to discuss the recent progress of it's members. This includes reports on the progress of the groups main digit recogniser project  with interest on voice-activity detectors and voiced/unvoiced detection  work on acoustic feature detection  and research into dealing with reverberation. There was also talk of comparing different recognition systems and training datasets  and a discussion of the pronunciation of the digit zero for the recording at the end of the meeting. In his next status report  me026 will summarise the work he has been researching. The digit recognition system is still not working well enough  they must get better results if they want to publish and be noticed. They have not really made many improvements  which may be due to their comparatively small training set  or the conditions the data is recorded under. The new VAD is quite a large network  and adds a delay to the process. This caused OGI to drop it  though speaker mn007 is assuming that a smaller and equally effective system can be developed. The alternative is to get yet another VAD form somewhere else  though it's not clear if they will even be required in the final system. There are some problems with the voiced/unvoiced feature detection  because some pitches are slipping through the filtering. The group have been comparing their recognition system to a few others  and theirs has not come off favourably. There could be many reasons for this  including smaller training set  more realistic data  or older technology. Speaker mn007 has put the best voice activity detector into the system  to great improvements along with designing new filters that run at the correct latency. Speaker fn002 has started to find parameters for voiced/unvoiced feature detection  and has found some classic ones  although there are other things she wishes to look at. Me013 offers a few ideas of simple things she may want to try  as he is not confident with everything she is trying. Speaker me006 is continuing with the idea of extending work on acoustic feature detection. He is continuing to read  and has discussed the suitability of factorial HMMs with a colleague. Speaker me026 has been learning more about previous work on reverberation  and is ready to start with a re-implementation of the theory. From there he wants to extend the work to look at time-varying reverb. 
"O_K. We seem to be recording. @@ Alright! We're not crashing. So  sorry about not - Number four. not pre-doing everything. The lunch went a little later than I was expecting  Chuck. Hmm? O_K. Chuck was telling too many jokes  or something? Yeah. Yep. Pretty much. O_K. Does anybody have an agenda? No. Well  I'm - I sent a couple of items. They're - they're sort of practical. I don't know if you're - I thought somebody had. Yeah  that's right. if - if that's too practical for what we're focused on. Yeah  we only want th- useless things. Yeah. No  why don't we talk about practical things? Sure. I mean  we don't want anything too practical. Yeah  that would be - O_K. Well  um  I can give you an update on the transcription effort. Great. Uh  maybe raise the issue of microphone  uh  um procedures b- with reference to the cleanliness of the recordings. O_K  transcription  uh  microphone issues - And then maybe ask  th- uh  these guys. The - we have great - great  uh  p- steps forward in terms of the nonspeech-speech pre-segmenting of the signal. O_K. Well  we have steps forward. Well  it's a - it's a big improvement. Yeah. Yes. I would prefer this. Yeah  well. O_K. Uh - We talk about the - the results of You have some - Yeah. O_K. I have a little bit of I_RAM stuff but I'm not sure if that's of general interest or not. use - Uh  bigram? I_RAM . I_RAM . I_RAM. Well  m- maybe. I_RAM  bigram  you know. Bi- Bigram. Yeah  let's - let's see where we are at three-thirty. Um - Hmm. Since  uh - since I have to leave as usual at three-thirty  can we do the interesting stuff first? I beg your pardon? Well - Yeah. What's the interesting stuff? Which is - ? I beg your pardon? Yeah. Th- now you get to tell us what's the interesting part. But - Yeah. Please specify. Yeah. Well  uh  I guess the work that's been done on segmentation would be most - Yeah. I think that would be a good thing to start with. O_K. Um  and  um  the other thing  uh  which I'll just say very briefly that maybe relates to that a little bit  which is that  um  uh  one of the suggestions that came up in a brief meeting I had the other day when I was in Spain with  uh  Manolo Pardo and Javier  uh  Ferreiros  who was here before  Yeah. was  um  why not start with what they had before but add in the non-silence boundaries. So  in what Javier did before when they were doing  um - h- he was looking for  uh  speaker change points. Mm-hmm. Um. As a simplification  he originally did this only using silence as  uh  a putative  uh  speaker change point. Yeah. And  uh  he did not  say  look at points where you were changing broad sp- uh  phonetic class  for instance. And for Broadcast News  that was fine. Here obviously it's not. Yeah. And  um  so one of the things that they were pushing in d- in discussing with me is  um  w- why are you spending so much time  uh  on the  uh  feature issue  uh  when perhaps if you sort of deal with what you were using before Uh-huh. and then just broadened it a bit  instead of just ta- using silence as putative change point also - ? So then you've got - you already have the super-structure with Gaussians and H_- you know  simple H_M_Ms and so forth. Nnn  yeah. And you - you might - So there was a - there was a little bit of a - a - a - a difference of opinion because I - I thought that it was - it's interesting to look at what features are useful. Yeah. But  uh  on the other hand I saw that the - they had a good point that  uh  if we had something that worked for many cases before  maybe starting from there a little bit - Because ultimately we're gonna end up with some s- su- kind of structure like that  where you have some kind of simple H_M_M and you're testing the hypothesis that  Yeah. Yeah. uh  there is a change. So - Yeah. so anyway  I just - reporting that. But  uh  uh - O_K. So. Yeah  why don't we do the speech-nonspeech discussion? Yeah. Do - I - I hear - you - you didn't - Speech-nonspeech? O_K. Uh-huh. Yeah. Um  so  uh  what we basically did so far was using the mixed file to - to detect s- speech or nonspeech portions in that. Mm-hmm. And what I did so far is I just used our old Munich system  which is an H_M_M-ba- based system with Gaussian mixtures for s- speech and nonspeech. And it was a system which used only one Gaussian for silence and one Gaussian for speech. And now I added  uh  multi-mixture possibility for - Mm-hmm. Mm-hmm. for speech and nonspeech. And I did some training on - on one dialogue  which was transcribed by - Jose. Yeah. We - we did a nons- s- speech-nonspeech transcription. Adam  Dave  and I  we did  for that dialogue and I trained it on that. And I did some pre-segmentations for - for Jane. And I'm not sure how good they are or what - what the transcribers say. They - they can use it or - ? Uh  they - they think it's a terrific improvement. And  um  it real- it just makes a - a world of difference. Hmm. And  um  y- you also did some- something in addition which was  um  for those in which there was  uh  quiet speakers in the mix. Yeah. Uh  yeah. That - that was one - one - one thing  uh  why I added more mixtures for - for the speech. So I saw that there were Mm-hmm. loud - loudly speaking speakers and quietly speaking speakers. And so I did two mixtures  one for the loud speakers and one for the quiet speakers. And did you hand-label who was loud and who was quiet  or did you just - ? I did that for - for five minutes of one dialogue and that was enough to - to train the system. And so it - it adapts  uh  on - Right. Yeah. W- What - ? while running. So. Hopefully. What kind of  uh  front-end processing did you do? O_K. It's just our - our old Munich  uh  loudness-based spectrum on mel scale twenty - twenty critical bands and then loudness. Mm-hmm. And four additional features  which is energy  loudness  modified loudness  and zero crossing rate. So it's twenty-four - twenty-four features. Mm-hmm. Mmm. And you also provided me with several different versions  which I compared. Yeah. Yeah. And so you change parameters. What - do you wanna say something about the parameters that you change? Yeah. You can specify the minimum length of speech or - and silence portions which you want. And so I did some - some modifications in those parameters  basically changing the minimum - minimum length for s- for silence to have  er- to have  um - yeah - to have more or less  uh  silence portions in- inserted. So. Right. So this would work well for  Yeah. uh  pauses and utterance boundaries and things like that. But for overlap I imagine that doesn't work at all  that you'll have plenty of s- sections that are - Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. That's it. Yeah. Mm-hmm  mm-hmm. That's true. But it - it saves so much time - the - the transcribers just enormous  enormous savings. Fantastic. Yeah. Um - But - Yep. That's great. Um  just qu- one quickly  uh  still on the features. So you have these twenty-four features. Uh  a lot of them are spectral features. Is there a - a transformation  uh  like principal components transformation or something? Just - Yeah. No. No. W- w- we - originally we did that Yeah. It was I_S two. but we saw  uh  when we used it  uh  f- for our close-talking microphone  which - yeah  for our - for our recognizer in Munich - we saw that w- it's - it's not - it's not so necessary. It - it works as well f- O_K. with - with - without  uh  a L_D_A or something. O_K. No  I was j- curious. Yeah  I don't think it's a big deal for this application  but - but - Yeah  it's a - Yeah. Mm-hmm. Yeah. Right. Mm-hmm. O_K. But then there's another thing that also Thilo's involved with  which is  um - O_K  and - and also Da- Dave Gelbart. So there's this - this problem of - and w- and - so we had this meeting. Th- the - also Adam  before the - the - before you went away. Uh we  um - regarding the representation of overlaps  because at present  um  because of the limitations of th- the interface we're using  overlaps are  uh  not being encoded by the transcribers in as complete and  uh  detailed a way as it might be  and as might be desired - I think would be desired in the corpus ultimately. Mm-hmm. So we don't have start and end points at each point where there's an overlap. We just have the - the overlaps encoded in a simple bin. Well  O_K. So @@ the limits of the over- of - of the interface are such that we were - at this meeting we were entertaining how we might either expand the - the interface or find other tools which already do what would be useful. Because what would ultimately be  um  ideal in my - my view and I think - I mean  I had the sense that it was consensus  is that  um  a thorough-going musical score notation would be the best way to go. Because you can have multiple channels  there's a single time-line  it's very clear  flexible  and all those nice things. Mm-hmm. O_K. So  um  um  I spoke - I had a meeting with Dave Gelbart on - on - and he had  uh  excellent ideas on how the interface could be modified to - to do this kind of representation. But  um  he - in the meantime you were checking into the existence of already  um  existing interfaces which might already have these properties. So  do you wanna say something about that? Yes. Um  I talked with  uh  Munich guys from - from Ludwi- Ludwig Maximilians University  who do a lot of transcribing and transliterations. Mm-hmm. And they basically said they have - they have  uh  a tool they developed themselves and they can't give away  uh  f- it's too error-prone  and had - it's not supported  a- a- a- and - Yeah. But  um  Susanne Bur- Burger  who is at se- C_M_U  he wa- who was formally at - in Munich and w- and is now at - with C_M_U  she said she has something which she uses to do eight channels  uh  trans- transliterations  eight channels simultaneously  Excuse me. but it's running under Windows. Under Windows. Mm-hmm. So I'm not sure if - if - if we can use it. She said she would give it to us. It wouldn't be a problem. Mm-hmm. And I've got some - some kind of manual down in my office. Well  maybe we should get it and if it's good enough Yeah. we'll arrange Windows machines to be available. Mm-hmm. We could - uh  potentially so. I also wanted to be sure - So. I mean  I've - I've seen the - this - this is called Praat  P_R_A_A_T  which I guess means spee- speech in Dutch or something. Yep. Yeah  but then I'm not sure that's the right thing for us. But - In terms of it being Windows versus - But I'm just wondering  is - ? Yeah. No  no. Praat isn't - Praat's multi-platform. No. No  Praat - Yeah. Yeah. Oh! I see. Oh  I see. So Praat may not be - Yeah. That's not Praat. It's called ""trans- transedit"" I think. The - the  uh - the tool from - from Susanne. It's a different one. I see. Oh  I see. O_K. O_K. Alright. The other thing  uh  to keep in mind  uh - I mean  we've been very concerned to get all this rolling so that we would actually have data  Mmm  yeah. but  um  I think our outside sponsor is actually gonna kick in and ultimately that path will be smoothed out. So I don't know Mm-hmm. if we have a long-term need to do lots and lots of transcribing. I think we had a very quick need to get something out and we'd like to be able to do some later because just it's inter- it's interesting. But as far a- you know  uh  with - with any luck we'll be able to wind down the larger project. Oh. But you s- What our decision was is that we'll go ahead with what we have with a not very fine time scale on the overlaps. Yeah. Right. Yeah. And - and do what we can later to clean that up if we need to. Mm-hmm. Right. And - and I was just thinking that  um  if it were possible to bring that in  like  you know  this week  then when they're encoding the overlaps Uh-huh. it would be nice for them to be able to specify when - you know  the start points and end points of overlaps. uh Th- they're making really quick progress. Yeah. That's great. And  um  so my - my goal was - w- m- my charge was to get eleven hours by the end of the month. And it'll be - I'm - I'm - I'm clear that we'll be able to do that. That's great. Yeah. And did you  uh  forward Morgan Brian's thing? I sent it to  um - who did I send that to? I sent it to a list and I thought I sent it to the - e- to the local list. You saw that? Meeting Recorder. Oh  you did? O_K. So you probably did get that. So Brian did tell me that in fact what you said  that  uh - that our - that they are making progress and that he's going - that they're going - he's gonna check the f- the output of the first transcription and - I mean  basically it's - it's all the difference in the world. I mean  basically he's - he's on it now. and - Yeah. Oh  that's - this is a new development. O_K. So - so - so this is - so i- it'll happen. Super. Super. O_K. Great. Yeah. I mean  basically it's just saying that one of our - one of our best people is on it  you know  who just doesn't happen to be here anymore. Someone else pays him. So - So. Yeah. Isn't that great? But about the need for transcription  I mean  don't we - didn't we previously Yeah. decide that the I_B_M transcripts would have to be checked anyway and possibly augmented? Yes. That's true. Mm-hmm. So  I think having a good tool is worth something no matter what. Yeah. S- O_K. That's - that's a good point. Yeah  and Dave Gelbart did volunteer  and since he's not here  I'll repeat it - Good. to at least modify Transcriber  which  if we don't have something else that works  I think that's a pretty good way of going. Mmm. Mm-hmm. And we discussed on some methods to do it. My approach originally  and I've already hacked on it a little bit - it was too slow because I was trying to display all the waveforms. But he pointed out that you don't really have to. I think that's a good point. Mm-hmm. Mm-hmm. Hmm. That if you just display the mix waveform and then have a user interface for editing the different channels  that's perfectly sufficient. Yeah  exactly. And just keep those things separate. And - and  um  Dan Ellis's hack already allows them to be able to display different waveforms to clarify overlaps and things  so that's already - No. They can only display one  but they can listen to different ones. Oh  yes  but - Well  uh  yes  but what I mean is that  uh  from the transcriber's perspective  uh  those two functions are separate. And Dan Ellis's hack handles the  um  choice - the ability to choose different waveforms from moment to moment. But only to listen to  not to look at. Um - Yeah. The waveform you're looking at doesn't change. Yeah. That's true. Yeah  but that's - that's O_K  cuz they're - they're  Yeah. you know  they're focused on the ear anyway. And then - and then Right. Hmm. the hack to preserve the overlaps better would be one which creates different output files for each channel  which then would also serve Liz's request of having  Right. you know  a single channel  separable  uh  cleanly  Mm-hmm. easily separable  uh  transcript tied to a single channel  uh  audio. Mm-hmm. Have  uh  folks from NIST been in contact with you? Not directly. I'm trying to think if - if I could have gotten it over a list. I don't - I don't think so. O_K. O_K. Well  holidays may have interrupted things  cuz in - in - in - They seem to want to get absolutely clear on standards for - transcription standards and so forth with - with us. Oh! This was from before December. Yeah. Right. Because they're - they're presumably going to start recording next month. O_K. O_K. Oh  we should definitely get with them then  and So. agree upon a format. Though I don't remember email on that. So was I not in the loop on that? Um. Yeah  I don't think I mailed anybody. I just think I told them to contact Jane - that  uh  if they had a - Oh  O_K. That's right. if  uh - that - that  uh  as the point person on it. But - Yeah  I think that's right. Just  uh - So  yeah. Maybe I'll  uh  ping them a little bit about it to O_K. I'm keeping the conventions absolutely as simple as possible. get that straight. Yeah. So is it - cuz with any luck there'll actually be a - a - there'll be collections at Columbia  collections at - at U_W - I mean Dan - Dan is very interested in doing some other things  Right. Yeah. Yeah. Well  I think it's important both and collections at NIST. So - for the notation and the machine representation to be the same. Yeah. So. N- there was also this  uh  email from Dan regarding the speech-non- nonspeech segmentation thing. Yep. Yeah. Yeah. I don't know if  uh  uh  we wanna  uh - and Dan Gel- and Dave Gelbart is interested in pursuing the aspect of using amplitude as a - a - a - as a basis for the separation. Oh  yeah. He was talking - he was talking - I mean  uh  we - he had - @@ Cross-correlation. Cross- Yeah  cross-correlation. I had mentioned this a couple times before  the c- the commercial devices that do  uh  Cross- Uh-huh. uh  voice  uh - you know  active miking  basically look at the amp- at the energy at each of the mikes. And - and you basically compare the energy here to some function of all of the mikes. So  Yeah. O_K. Yeah. by doing that  you know  rather than setting any  uh  absolute threshold  you actually can do pretty good  uh  selection of who - who's talking. Uh - O_K. And those - those systems work very well  by the way  I mean  so people use them in panel discussions and so forth with sound reinforcement differing in - in sort of  uh - Uh-huh. and  uh  those - if - Boy  the guy I knew who built them  built them like twenty - twenty years ago  so they're - it's - the - the techniques work pretty well. Hmm. Fantastic. So. Cuz there is one thing that we don't have right now and that is the automatic  um  channel identifier. Mm-hmm. That - that  you know  that would g- help in terms of encoding of overlaps. The - the transcribers would have less  uh  disentangling to do if that were available. Yeah. So I think  you know  basically you can But. look at some - p- you have to play around a little bit  uh  to figure out what the right statistic is  but you compare each microphone to some statistic based on the - Mm-hmm. Mm-hmm. on the overall - Yeah. Mm-hmm. O_K. Uh  and we also have these - we have the advantage of having distant mikes too. So that  you cou- yo- Yeah  although the - the - using the close-talking I think would be much better. Wouldn't it? Um. Yeah. I - I don't know. I just - it'd be - Yeah. If I was actually working on it  I'd sit there and - and play around with it  and - and get a feeling for it. I mean  the - the - the  uh - But  uh  you certainly wanna use the close-talking  as a - at least. Right. I don't know if the other would - would add some other helpful dimension or not. Mm-hmm. Mm-hmm. O_K. What - what are the different  uh  classes to - to code  uh  the - the overlap  you will use? Um  to code d- so types of overlap? What you - you - Yeah. Um  so at a meeting that wasn't transcribed  we worked up a - a typology. And  um - Yeah. Look like  uh  you t- you explaining in the blackboard? The - ? Yeah? Yeah. Yes  exactly. That hasn't changed. So it i- the - it's basically a two-tiered structure where the first one is whether the person who's interrupted continues or not. And then below that there're subcategories  uh  that have more to do with  you know  is it  Mm-hmm. uh  simply backchannel or is it  um  someone completing someone else's thought  or is it someone in- introducing a new thought. Right. Huh. And I hope that if we do a forced alignment with the close-talking mike  that will be enough to recover Yeah. at least some of the time the time information of when the overlap occurred. Mm-hmm. Well  one would - Yeah. We hope. Yeah. Who knows? That'd be - that'd be nice. I mean  I - I - I - I've - So who's gonna do that? Who's gonna do forced alignment? Well  u- uh  I_B_M was going to. Um - Oh  O_K. Oh. and I imagine they still plan to but - but  you know  I haven't spoken with them about that recently. O_K. Uh-huh. Well  uh  my suggestion now is - is on all of these things to  uh  contact Brian. This is wonderful to have a direct contact like that. O_K. I'll do that. Yeah. Yeah. uh Well  th- lemme ask you this. It occurs to me - Yeah. one of my transcribers t- told me today that she'll be finished with one meeting  Mm-hmm. um  by - well  she said tomorrow but then she said - you know  but - the  you know - let's - let's just  uh  say Mm-hmm. maybe the day after just to be s- on the safe side. I could send Brian the  um - the transcript. I know these are - er  uh  I could send him that if it would be possible  or a good idea or not  to try to do a s- forced alignment on what we're - on the way we're encoding overlaps now. Well  just talk to him about it. Yep. Good. I mean  you know  basically he's - he just studies  he's a colleague  a friend  and  Yeah! Super. Super. uh  they - and - and  you know  the - the organization always did wanna help us. It was just a question of getting  you know  the right people connected in  who had the time. So  Yeah  yeah. Right. um  eh - Is he on the mailing list? The Meeting Recorder mailing li- ? We should add him. Oh! Yeah. Yeah. I - I - I don't know for sure. Did something happen  Morgan  that he got put on this  or was he already on it  or - ? Add him. No  I  eh  eh  p- It - it oc- I - h- it's - Yeah  something happened. I don't know what. But he's on it now. He asked for more work. Huh. That would be like - that'd be like him. He's great. Right. So  uh  where are we? Maybe  uh  uh  brief - Well  let's - why don't we talk about microphone issues? That was - that was a - Yeah. That'd be great. Um  so one thing is that I did look on Sony's for a replacement for the mikes - for the head m- head-worn ones cuz they're so uncomfortable. @@ But I think I need someone who knows more about mikes than I do  because I couldn't find a single other model that seemed like it would fit the connector  which seems really unlikely to me. Does anyone  like  know stores or know about mikes who - who would know the right questions to ask? Oh  I probably would. I mean  my knowledge is twenty years out of date but some of it's still the same. So - Mm-hmm. Uh  so maybe we c- we can take a look at that. You couldn't - you couldn't find the right connector to go into these things? Yep. Huh! When I looked  i- they listed one microphone and that's it as having that type of connector. But my guess is that Sony maybe uses a different number for their connector than everyone else does. And - and so - Mm-hmm. Well  let's look at it together and - it seems - it seems really unlikely to me that there's only one. And there's no adaptor for it? Seems like there'd be a - O_K. Yeah. As I said  who knows? Mm-hmm. Who - who are we buying these from? That'd be a - Um  I have it downstairs. I don't remember off the top of my head. Yeah. O_K. Yeah. We - we can try and look at that together. And then  uh - just in terms of how you wear them - I mean  I had thought about this before. I mean  when - when - when you use a product like DragonDictate  they have a very extensive description about how to wear the microphone and so on. Oh. But I felt that in a real situation we were very seldom gonna get people to really do it and maybe it wasn't worth concentrating on. But - Well  I think that that's - that's a good back-off position. That's what I was saying earlier  th- that  you know  we are gonna get some recordings that are imperfect and  hey  that's life. But I - I think that it - it doesn't hurt  uh  the naturalness of the situation to try to have people wear the microphones properly  if possible  because  Mm-hmm. um  the natural situation is really what we have with the microphones on the table. I mean  I think  Oh. That's true. you know  in the target applications that we're talking about  people aren't gonna be wearing head-mounted mikes anyway. So this is just for u- these head-mounted mikes are just for use with research. Yeah. Yeah. Mm-hmm. And  uh  it's gonna make - Yeah. @@ You know  if - if An- Andreas plays around with language modeling  he's not gonna be m- wanna be messed up by people breathing into the microphone. So it's - it's  uh  uh - Right. Well  I'll dig through the documentation to DragonDictate and ste- s- see if they still have the little form. But it does happen. Right? I mean  and any - Yeah. It's interesting  uh  I talked to some I_B_M guys  uh  last January  I think  I was there. And - so people who were working on the - on their ViaVoice Yeah. dictation product. And they said  uh  the breathing is really a - a terrible problem for them  to - to not recognize breathing as speech. Wow. So  anything to reduce breathing is - Yeah. Well  that's the - is - is a good thing. It seemed to me when I was using Dragon Mm-hmm. that it was really microphone placement helped an - in  uh - an enormous amount. So you want it Right. enough to the side so that when you exhale through your nose  it doesn't - the wind doesn't hit the mike. Mm-hmm. And then  uh - Everyone's adjusting their microphones  of course. And then just close enough so that you get good volume. So you know  wearing it right about here Yeah. Yeah. seems to be about the right way to do it. Is - Uh-huh. I remember when I was - when I - I - I - I used  uh  um  a prominent laboratory's  uh  uh  speech recognizer about  uh - This was  boy  this was a while ago  this was about twelve - twelve years ago or something. And  um  they were - they were perturbed with me because I was breathing in instead of breathing out. And they had models for - they - they had Markov models for br- breathing out but they didn't have them for breathing in. Uh - Yeah. That's interesting. Well  what I wondered is whether it's possible to have - to maybe use the display at the beginning to be able to - to judge how - how correctly - I mean  have someone do some routine whatever  Yeah. and - and then see if when they're breathing it's showing. I don't know if the - if it's - I - I mean  when - when it's on  you can see it. You can definitely see it. Absolutely. Absolutely. Can you see the breathing? Cuz I - Oh. Yeah. I- And so  you know  I've - I've sat here and watched sometimes the breathing  and the bar going up and down  and I'm thinking  I could say something  but I mean  I think - I don't want to make people self-conscious. Stop breathing! It - it's going to be imperfect. You're not gonna get it perfect. Yeah. Uh-huh. And you can do some  uh  you know  first-order thing about it  which is to have people move it  uh  uh  a- away from being just directly in front of the middle Yeah. Good. but not too far away. Yeah  i- And then  you know  I think there's not much - Because you can't al- you know  interfere w- you can't fine tune the meeting that much  I think. It's sort of - Right. Yeah. That's true. It just seems like i- if something l- simple like that can be tweaked and the quality goes  you know  uh  dramatically Yep. up  then it might be worth doing. And then also - the position of the mike also. If it's more directly  you'll get better volume. Yeah. So - so  like  yours is pretty far down below your mouth. Yeah. But - Mm-hmm. Yeah. My - my feedback from the transcribers is he is always close to crystal clear and - and just fan- fantastic to - Yeah. Mmm  yeah. Mm-hmm. I don't know why that is. Well  I mean  you - Yeah  of course. You're - you're also - uh  your volume is - is greater. But - but still  I mean  they - they say - I've been eating a lot. Uh. I- it makes their - their job extremely easy. Yeah. And then there's mass. Anyway. Mm-hmm. I could say something about - about the - Well  I don't know what you wanna do. Yeah. About what? About the transcribers or anything or - ? I don't know. Well  the other - why don't we do that? But  uh  just to - to  um - One more remark  uh  concerning the S_R_I recognizer. Um. It is useful to transcribe and then ultimately train models for things like breath  and also laughter is very  very frequent and important to - Mm-hmm. to model. So  So  if you can in your transcripts mark - mark them? mark very audible breaths and laughter especially  um - O_K. Mmm. They are. They're putting - Eh  so in curly brackets they put ""inhale"" or ""breath"". Mm-hmm. It - they - and then in curly brackets they say ""laughter"". Now they're - Oh  great. they're not being awfully precise  uh  m- So they're two types of laughter that are not being distinguished. One is Mm-hmm. when sometimes s- someone will start laughing when they're in the middle of a sentence. Mm-hmm. And - and then the other one is when they finish the sentence and then they laugh. So  um  I - I did s- I did some double checking to look through - I mean  you'd need to have extra hhh e- extra complications  like time tags indicating the beginning and ending of - of the laughing through the utterance. And that - and what they're doing is in both cases just saying ""curly brackets laughing"" a- after the unit. It's not so - I don't think it's  um - As - as long as there is an indication that there was laughter somewhere between two words Yeah. Good. Oh! O_K. I think that's sufficient  because Against - they could do forced alignment. actually the recognition of laughter once you kn- um - Yeah. you know  is pretty good. So as long as you can stick a - Oh  I didn't know that. you know  a t- a tag in there that - that indicates that there was laughter  O_K. that would probably be  uh  sufficient to train models. Then - That would be a really interesting prosodic feature  when - And let me ask y- and I gotta ask you one thing about that. So  um  Yeah. Hmm. if they laugh between two words  you - you'd get it Mm-hmm. in between the two words. But if they laugh across three or four words you - you get it after those four words. Does that matter? Right. Yeah. Well  the thing that you - is hard to deal with is whe- when they speak while laughing. Yeah. Um  and that's  uh - I don't think that we can do very well with that. So - Right. Yeah. But  um  that's not as frequent as just laughing between O_K. speaking  so - Uh is it? So are - do you treat breath and laughter as I think he's right. Yeah. Huh. I - I think it's frequent in - in the meeting. phonetically  or as word models  or what? We tried both. Uh  currently  um  we use special words. There was a - there's actually a word for - uh  it's not just breathing but all kinds of mouth - uh  mouth - mouth stuff. Mm-hmm. Mouth stuff? And then laughter is a - is a special word. How would we do that with the hybrid system? @@ Same thing. Same thing? So train a phone in the neural net? Yeah. Yeah. You ha- Oh. And each of these words has a dedicated phone. No - Oh  it does? So the - so the - the mouth noise  uh  word has just a single phone  um  that is for that. Right. So in the hybrid system we could train the net with Yeah. a laughter phone and a breath sound phone. Yeah. Yeah. I mean  it's - it's - it's always the same thing. Right? I mean  you could - you could say well  let - Mm-hmm. @@ we now think that laughter should have three sub- sub- sub-units in the - the three states  uh - different states. And then you would have three - I mean  you know  eh  eh  it's u- Yeah. And the - the pronun- the pronunciations - the pronunciations are l- are somewhat non-standard. They actually are - Do whatever you want. Yeah. Yeah. Yeah  yeah. No. uh  it's just a single  s- uh  you know  a single phone in the pronunciation  but it has a self-loop on it  so it can - To go on forever? r- can go on forever. And how do you handle it in the language model? It's just a - it's just a word. We train it like any other word. Yeah. It's just a word in the language model. Cool. We also tried  um  absorbing these - uh  both laughter and - and actually also noise  and  um - Sorry to interrupt. Yeah. Is - If you want  I got it. Andreas. Yes. The copies are ready . O_K. Anyway. We also tried absorbing that into the pause model - I mean  the - the - the model that - Mm-hmm. that matches the stuff between words. And  um  it didn't work as well. So. Huh. O_K. Mm-hmm. Can you hand me your digit form? Sorry. I just wanna mark that you did not read digits. O_K. Say hi for me. Good. You - you did get me to thinking about - I - I'm not really sure which is more frequent  whether f- f- laughing - I think it may be an individual thing. Some people are more prone to laughing when they're speaking. Yeah. Yeah. I think - I was noticing that with Dan in the one that we  uh - But I can't - Yeah. we hand tran- hand-segmented  that - th- he has these little chuckles as he talks. I'm sure it's very individual. And - and - Yeah. O_K. one thing that c- that we're not doing  of course  is we're not claiming to  uh  get - be getting a representation of mankind in these recordings. We have this very  very tiny sample of - of - Yeah. Yeah. Yeah. Speech researchers? Uh  yeah. And - Yeah  r- right. So  uh  who knows. Speech research. Uh - Yeah. Why don- why don't we just - since we're on this vein  why don't we just continue with  uh  what you were gonna say about the transcriptions and - ? O_K. Um  um  the - I - I'm really very for- I'm extremely fortunate with the people who  uh  applied and who are transcribing for us. They are  um  um  uh really perceptive and very  um - and I'm not just saying that cuz they might be hearing this. Cuz they're gonna be transcribing it in a few days. No  they're super. They're - the- they - very quick. O_K. Turn the mikes off and let's talk. Yeah  I know. I am - I'm serious. They're just super. So I  um  e- you know  I - I brought them in and  um  trained them in pairs because I think people can raise questions - you know  i- i- the- they think about different things and they think of different - and um  That's a good idea. I trained them to  uh  f- on about a minute or two of the one that was already transcribed. This also gives me a sense of - You know  I can - I can use that later  with reference to inter-coder reliability kind of issues. But the main thing was to get them used to the conventions and  you know  the idea of the - th- th- the size of the unit versus how long it takes to play it back so these - th- sort of calibration issues. And then  um  I just set them loose and they're - they all have e- a- already background in using computers. They're  um - they're trained in linguistics. They got - Good. Uh-huh. Oh  no. Is that good or bad? Well  they- they're very perce- they'll - So one of them said ""well  you know  he really said "" n ""  not really "" and ""  Yeah. Yeah. so what - what should I do with that?"" And I said  ""well for our purposes  Yeah. Yeah. I do have a convention. If it's an - a noncanonical p-"" That one  I think we - you know  with Eric's work  I sort of figure we - we can just treat that as a variant. O_K. But I told them if - if there's an obvious speech error  Yes. uh  like I said in one thing  and I gave my - my example  like I said  ""microfon"" in- instead of ""microphone"". Didn't bother - I knew it when I said it. I remember s- thinking ""oh  that's not correctly pronounced"". But it - but I thought it's not worth fixing cuz often when you're speaking everybody knows what - what you mean. You'll self-repair. Yeah. Yeah. But I have a convention that if it's obviously a noncanonical pronunciation - a speech error with - you know  wi- within the realm of resolution that you can tell in this native English - American English speaker  you know that I didn't mean to say ""microfon."" Then you'd put a little tick at the beginning of the word  and that just signals that  um  this is not standard  and then in curly brackets ""pron error"". Yeah. And  um  and other than that  it's w- word level. But  you know  the fact that they noticed  you know  the "" nnn "". ""He said "" nnn ""  not "" and "". What shall I do with that?"" I mean  they're very perceptive. And - and s- several of them are trained in I_P_A. C- they really could do phonetic transcription if - if we wanted them to. Mm-hmm. Right. Well - But - Hmm. Where were they when we needed them? Well  you know  it might be something we'd wanna do with some  uh  s- small subset of the whole thing. I think - We certainly wouldn't wanna do it with everything. And I'm also thinking these people are a terrific pool. I mean  if  uh - so I - I told them that  um  we don't know if this will continue past the end of the month and I also - Uh-huh. m- I think they know that the data p- source is limited and I may not be able to keep them employed till the end of the month even  although I hope to. The other thing we could do  actually  uh  is  And - uh  use them for a more detailed analysis of the overlaps. Oh  that'd be so super. They would be so - s- so terrific. Right? I mean  this was something that we were talking about. We could get a very detailed overlap if they were willing to transcribe each meeting four or five times. Right? One for each participant. So they could by hand - Well  that's one way to do it. But I've been saying the other thing is just go through it for the overlaps. Right? Yeah. Yeah. Mm-hmm  that's right. And with the right in- interface - Given that y- and - and do - so instead of doing phonetic  uh  uh  transcription for the whole thing  which Yeah. we know from the - Steve's experience with the Switchboard transcription is  you know  very  very time-consuming. And - and you know  it took them I don't know how many months to do - to get four hours. And so that hasn't been really our focus. Uh  we can consider it. But  I mean  the other thing is since we've been spending so much time thinking about overlaps is - is maybe get a much more detailed analysis of the overlaps. Yeah. Mm-hmm. But anyway  I'm - I'm open to c- our consideration. That'd be great. I - I don't wanna say that by fiat. I'm open to every consideration of Hmm. Yeah. what are some other kinds of detailed analysis that would be most useful. And  uh  uh  Mm-hmm. Hmm. I - I - I think this year we - we actually  uh  can do it. Oh  wonderful. It's a - we have - we have - due to @@ variations in funding we have - we seem to be doing  uh  very well on m- money for this - this year  and next year we may have - have much less. So I don't wanna hire a - Is - you mean two thousand one? Calendar year or - ? Uh  I mean  calendar year two thousand one. O_K. Yeah. So it's - uh  it's - we don't wanna hire a bunch of people  a long-term staff  because Full-time. Yeah. Mm-hmm. Yeah. the - the funding that we've gotten is sort of a big chunk for this year. But having temporary people doing some specific thing that we need is actually a perfect match to that kind of  uh  funding. So. Wonderful. And then school will start in - in the sixt- on the sixteenth. Some of them will have to cut back their hours at that point. But - Yeah. Yeah. Are they working full-time now  or - ? Some of them are. Yeah. Wow. Well  why do- I wouldn't say forty-hour weeks. No. But what I mean is - Oh  I shouldn't say it that way because that does sound like forty-hour weeks. No. I th- I - I would say they're probably - they don't have o- they don't have other things that are taking away their time. I don't see how someone could do forty hours a week on transcription. Hmm. But it's - you can't. Yeah. Yeah. No. You're right. It's - i- it would be too taxing. But  um  they're putting in a lot of - Yeah. And - and I checked them over. I - I - I haven't checked them all  but just spot-checking. They're fantastic. I - I remember when we were transcribing BeRP  uh  uh  I think it would be - uh  Ron Kay  uh  volunteered to - to do some of that. And  he was - the first - first stuff he did was transcribing Chuck. And he's saying ""You - you know  I always thought Chuck spoke really well. "" Yeah. Yeah. Well  you know  and I also thought  y- Liz has this  eh  you know  and I do also  this - this interest in the types of overlaps that are involved. These people would be great choices for doing coding of that type if we wanted  or We'd have to mark them. Mm-hmm. whatever. So  um. Mm-hmm. Yeah. I think it would also be interesting to have  uh  a couple of the meetings have more than one transcriber do  Mm-hmm. cuz I'm curious about inter-annotator agreement. Yeah. O_K. Yeah. Th- that'd be - Yeah. I think that's a - a good idea. You know  there's also  the e- In my mind  I think A- An- Andreas was leading to this topic  the idea that  um  we haven't yet seen the - the type of transcript that we get from I_B_M  and it may just be  you know  pristine. But on the other hand  given the lesser interface - Cuz this is  you know - we've got a good interface  we've got great headphones  m- um - It could be that they will uh - theirs will end up being a kind of fir- first pass or something. Something like that. Maybe an elaborate one  cuz again they probably are gonna do these alignments  which will also That's - that's true. Al- although you have to s- Don't you have to start with a close enough approximation of the - clear things up. of the verbal part to be able to - ? Well  tha- that's - that's debatable. Right? I mean  so the - so the argument is that if your statistical system is good O_K. it will in fact  uh  clean things up. Right? So it- it's got its own objective criterion. O_K. Yeah. And  uh  so in principle you could start up with something that was kind of rough - I mean  to give an example of  um  something we used to do  uh  at one point  uh  back - back when Chuck was here in early times  is we would take  um  da- take a word and  uh  have a canonical pronunciation and  uh  if there was five phones in a word  you'd break up the word  uh  into five equal-length pieces which is completely gross. Wrong. Yeah. Right? I mean  th- the timing is off all over the place in just about any word. Yeah. Mm-hmm. O_K. But it's O_ K. You start off with that and the statistical system then aligns things  and eventually you get something that doesn't really look too bad. Oh  excellent. O_K. So - so I think using a - a good aligner  um  actually can - can help a lot. Um. But  uh  you know  they both help each other. If you have a - if you have a better starting point  then it helps the aligner. If you have a good alignment  it helps the  uh  th- the human in - in taking less time to correct things. So - so - O_K. Excellent. I guess there's another aspect  too  and I don't know - uh  this - this is - very possibly a different  uh  topic. But  uh  just let me say with reference to this idea of  um  higher-order organization within meetings. So like in a - you know  the topics that are covered during a meeting with reference to the other  uh  Mm-hmm. uses of the data  so being able to find where so-and-so talked about such-and-such  then  um  um - e- I mean  I - I - I did sort of a - a rough pass on encoding  like  episode-like level Mm-hmm. things on the  uh  transcribed meeting - already transcribed meeting. Mm-hmm. And I don't know if  um - where that - i- if that's something that we wanna do with each meeting  sort of like a  um - it's like a manifest  when you get a box full of stuff  or - or if that's  um - Mm-hmm. I mean  i- I - I don't know what uh  level of detail would be most useful. I don't know i- if that's something that I should do when I look over it  or if we want someone else to do  or whatever. Mm-hmm. But this issue of the contents of the meeting in an outline form. O_K. Yeah. Meaning really isn't my thing. Um - I think it just - whoever is interested can do that. I mean  so if someone wants to use that data - O_K. We're running a little short here. We  uh  uh  cou- trying to - That's fine. I'm finished. eh  was - p- Well  you know  the thing I'm concerned about is we wanted to do these digits and - and I haven't heard  uh  from Jose yet. So - Oh  yeah. Oh  yes. Mm-hmm. O_K. What do you want? Uh - We could skip the digits. We don't have to read digits each time. Uh - I - I - I think it - you know  another - another bunch of digits. More data is good. O_K. Yeah. Sure. So - so I'd like to do that. But I think  do you  maybe  eh - ? Did you prepare some whole thing you wanted us just to see? Or what was that? Yeah. It's - it's prepared. Yeah. Uh  how long a - ? Oh  k- Sorry. I - I think it's - it's fast  because  uh  I have the results  eh  of the study of different energy without the law length . Eh  um  eh  @@ in the - in the measurement  uh  the average  uh  dividing by the - by the  um  variance. Yeah. Um  I - th- i- the other  uh - the - the last w- uh  meeting - eh  I don't know if you remain - we have problem to - with the - with - with the parameter - with the representations of parameter  Yes. because the - the valleys and the peaks in the signal  eh  look like  eh  it doesn't follow to the - to the energy in the signal. Right. And it was a problem  uh  with the scale. Eh  the scale. With what? Scale. Scale. Eh  and I - I change the scale and we can see the - the variance. O_K. But the bottom line is it's still not  uh  separating out very well. Right? O_K. Yeah. Yeah. The distribution - the distribution is - is similar. So that's - that's - that's enough then. O_K. Yeah. No  I mean  that there's no point in going through all of that if that's the bottom line  really. So  I - I think we have to start - Yeah. Yeah. Mm-hmm. Uh  I mean  there- there's two suggestions  really  which is  uh - what we said before is that  Mmm  yeah. um  it looks like  @@ at least that you haven't found an obvious way to normalize so that the energy is anything like a reliable  uh  indicator of the overlap. Yeah. Um  I - I'm - I'm still a little f- think that's a little funny. These things l- @@ seems like there should be  but - Yeah. Yeah. but you don't want to keep  uh - keep knocking at it if it's - if you're not getting any - any result with that. But  I mean  the other things that we talked about is  uh  pitch-related things and harmonicity-related things  so - which we thought also should be some kind of a reasonable indicator. Yeah. Um - But  uh  a completely different tack on it wou- is the one that was suggested  uh  by your colleagues in Spain  Yeah. which is to say  don't worry so much about the  uh  features. Yeah. That is to say  use  you know  as - as you're doing with the speech  uh  nonspeech  use some very general features. Yeah. And  uh  then  uh  look at it more from the aspect of modeling. Yeah. You know  have a - have a couple Markov models and - Yeah. Yeah. and  uh  try to indi- try to determine  you know  w- when is th- when are you in an overlap  when are you not in an overlap. Hmm. And let the  uh  uh  statistical system determine what's the right way to look at the data. I - I  um  Yeah. I think it would be interesting to find individual features and put them together. I think that you'd end up with a better system overall. But given the limitation in time Yeah. and given the fact that Javier's system already exists doing this sort of thing  Yeah. uh  but  uh  its main limitation is that  again  it's only looking at silences which would - Yeah. Yeah. maybe that's a better place to go. Yeah. So. Mm-hmm. I - I - I think that  eh  the possibility  eh  can be that  eh  Thilo  eh  working  eh  with a new class  Mm-hmm. not only  eh  nonspeech and speech  but  eh  in - in - in the speech class  Mm-hmm. dividing  eh  speech  eh  of - from a speaker and overlapping  to try - to - to do  eh  eh  a fast - a fast  eh  experiment to - to prove that  nnn  this fea- eh  general feature  Yeah. eh  can solve the - the - the problem  and wh- what - Maybe. Yeah. nnn  how far is - And  I - I have prepared the - the pitch tracker now. Mm-hmm. And I hope the - the next week I will have  eh  some results and we - we will show - we will see  eh  the - the parameter - the pitch  I see. eh  tracking in - with the program. Ha- h- have you ever looked at the  uh  uh - Javier's  uh  And  nnn  nnn - speech segmenter? No. Oh. Maybe m- you could  you kn- uh show Thilo that. No. No. Yeah. Yeah. Yeah. Sure. Cuz again the idea is there - the limitation there again was that he was - he was only using it to look at silence as a - as a - as a - as a p- putative I - Yeah. split point between speakers. But if you included  uh  O_K. broadened classes then in principle maybe you can cover the overlap cases. Yeah. Mmm  yeah. Yeah  but I'm not too sure if - if we can really represent Uh - overlap with - with the s- detector I - I - I used up to now  the - to speech-nonspeech as - Mm-hmm. I think with - Ah. it's only speech or it's - it's - it's nonspeech. So. That's right. But I think Javier's - Yeah. Mm-hmm. N- n- I think Javier's might be able to. It doesn't have the same Gaus- uh  H_M_ M modeling  which is I think a drawback. Yeah. O_K. But  uh - Well  it's - sort of has a Mmm  yeah. simple one. Right? It's - Does it? it's just - it's just a - isn't it just a Gaussian for each - ? Yeah. Yeah. Hmm. Mm-hmm. Yeah. And then he ch- you choose optimal splitting. Oh  it doesn't have - it doesn't have any temporal  uh - ? I thought it - @@ Maybe I'm misremembering  but I did not think it had a Markov - Yeah. I gues- I guess I don't remember either. Uh. It's been a while. Javier - Uh. Yeah. Uh  I could have a look at it. So. You mean Ja- eh  eh  Javier program? No  Javier di- doesn't worked with  uh  a Markov - Mm-hmm. Yeah  I didn't think so. Oh  O_K . So he's just - he just computes a Gaussian over potential - Oh  I see. I see. And - and - He on- only train - Yep. Yeah. It was only Gaussian. This is the idea. And so I - I think it would work fine for detecting overlap. It's just  uh  that i- it - he has the two-pass issue that - What he does is  as a first pass he - he - p- he does  um  a guess at where the divisions might be and he overestimates. And that's just a data reduction step  so that you're not trying at every time interval. O_K. And so those are the putative places where he tries. Yeah. Yeah. O_K. And right now he's doing that with silence and that doesn't work with the Meeting Recorder. Yeah. So if we used another method to get the first pass  I think it would probably work. It's a good method. Yeah. Sure. Yeah. Yeah  O_K. As long as the len- as long the segments are long enough. Yeah. O_k- O_K. So let me go back to what you had  though. Um. That's the other problem. So - Yeah. Mm-hmm. The other thing one could do is - Couldn't - I mean  it's - So you have two categories and you have Markov models for each. Yeah. Couldn't you have a third category? So you have  uh - you have  uh  nonspeech  single-person speech  and multiple-person speech? He has this on his board actually. Don't you have  like those - those several different categories on the board? Right? And then you have a Markov model for each? Um - I'm not sure. I - I thought about  uh  adding  uh  uh  another class too. But it's not too easy  I think  the - the transition between the different class  to model them in - in the system I have now. But it - it - it could be possible  I think  I see. I see. in principle. Yeah  I mean  I - @@ This is all pretty gross. I mean  the - th- the reason why  uh  I was suggesting originally that we look at features is because I thought  well  we're doing something we haven't done before  @@ Yeah. Yeah. we should at least look at the space and understand - Yeah. Yeah. It seems like if two people - two or more people talk at once  it should get louder  Yeah. uh  and  uh  uh  there should be some discontinuity in pitch contours  I had the impression. Yeah. and  uh  there should overall be a  um  smaller proportion of the total energy that is explained by any particular harmonic sequence in the spectrum. Yeah. Yeah. Right. So those are all things that should be there. So far  Mm-hmm. Yeah. um  uh  Jose has - has been - By the way  I was told I should be calling you Pepe  but - by your friends  but- Yeah. Oh. Anyway  um  Yeah . uh  the - has - has  uh  been exploring  uh  e- largely the energy issue and  um  as with a lot of things  it is not - uh  like this  it's not as simple as it sounds. And then there's  you know - Is it energy? Is it log energy? Is it L_P_C residual energy? Is it - is it - Yeah. is it  uh  delta of those things? Uh  what is it no- Obviously  just a simple number - absolute number isn't gonna work. So it should be with - compared to what? Should there be a long window for the normalizing factor and a short window for what you're looking at? Or  you know  how b- short should they be? So  Yeah. th- he's been playing around with a lot of these different things and - and so far at least has not come up with Hmm. any combination that really gave you an indicator. So Yeah. I - I still have a hunch that there's - it's in there some place  but it may be - given that you have a limited time here  it - it just may not be the best thing to - Yeah. to - to focus on for the remaining of it. So pitch-related and harmonic-related  To overrule   yeah. Yeah. I'm - I'm somewhat more hopeful for it. Yeah. But it seems like if we just wanna get something to work  Yeah. that  uh  their suggestion of - of - Yeah. Th- they were suggesting going to Markov models  uh  but in addition there's an expansion of what Javier did. And one of those things  looking at the statistical component  One. Yeah. even if the features that you give it are maybe not ideal for it  it's just sort of this general filter bank or - Yeah. or cepstrum or something  um - Eee it's in there somewhere probably. But  eh  what did you think about the possibility of using the Javier software? Eh  I mean  the  uh - the  uh - the BIC criterion  the - the - t- to train the - the Gaussian  eh  using the - the mark  eh  by hand  eh  eh  to distinguish be- mmm  to train overlapping zone and speech zone. I mean  eh  I - I - I think that an interesting  eh  experiment  eh  could be  th- eh  to prove that  mmm  if s- we suppose that  eh  the - the first step - I mean  the - the classifier what were the classifier from Javier or classifier from Thilo? W- What happen with the second step? I - I mean  what - what happen with the  eh - the  uh  clu- the  uh - the clu- the clustering process? Mm-hmm. Using the - the Gaussian. Yeah. You mean Javier's? What do you mean? I - I mean  that is - is enough - is enough  eh  to work well  eh  to  eh  separate or to distinguish  eh  between overlapping zone and  eh  speaker zone? Because th- if - if we - if we  eh  nnn  develop an classifier - I - and the second step doesn't work well  eh  we have another problem. N- Yeah. I had tried doing it by hand at one point with a very short sample  and it worked pretty well  but I haven't worked with it a lot. So what I d- I d- I took a hand-segmented sample and I added Nnn  yeah. ten times the amount of numbers at random  Yeah. Oh. and it did pick out Yeah. But is - is - if - pretty good boundaries. But this was just very anecdotal sort of thing. But it's possible with my segmentation by hand that we have information about the - the overlapping  uh - Yeah. Right. So if we - if we fed the hand-segmentation to Javier's The - N- n- Yeah. No. and it doesn't work  then we know something's wrong. The demonstration by hand. Segmentation by hand I - I - I think is the fast experiment. Yeah. I think that's probably worthwhile doing. Uh-huh. Uh  we can prove that the - Whether it'll work or not. Yeah. this kind o- Yeah. emph- emphasizes parameter and Gaussian - Yep. Y- do you know where his software is? Have you used it at all? I yeah have. I have. O_K. So. I - I have as well  so if you need - need help let me know. O_K. Let's read some digits. O_K. Mm-hmm. uuh Transcript two nine five one two nine seven O_. six O_ three O_ nine seven eight zero O_ one five O_ two zero five eight four uh  one six two eight five eight three two three three O_ three one five four five zero nine nine seven one one two eight four zero zero nine four O_ seven one zero one two four one five three one two six seven two O_ - Pa- Uh  correct that. six seven two one O_ eight six. Transcript two eight seven one dash two eight nine zero. three three eight four four six five two five eight zero six seven eight zero O_ one four zero one one eight one three one one six two five three four six eight one three four five zero six zero one seven one one two eight three three six O_ eight O_ nine six five O_ seven three eight zero eight six nine one two O_ two. Transcript two eight three one dash two eight five zero. one eight six seven zero six five two three four zero six one seven seven six three eight five one nine seven seven five six O_ zero zero one three two O_ eight four four five five O_ six seven eight O_ seven eight nine zero zero five one three three five O_ O_ five. O_K. Transcript two seven nine one dash two eight one zero. O_ zero two three seven three three two four seven five three O_ two six seven eight zero eight O_ one O_ seven zero three one five O_ three six two eight O_ O_ three four five zero two zero zero seven two O_ three three one seven eight nine five one two two zero nine eight five three O_ seven O_ eight. Transcript two eight one one dash two eight three zero. zero six one two zero five zero zero eight four four one zero one five six six seven four seven eight nine O_ one two nine four eight five nine two two zero three three six four nine five six seven nine seven nine eight three five zero O_ two seven nine five one five zero eight two five four. Transcript number two seven seven one two seven nine zero. nine O_ nine eight nine zero one five zero one two two eight eight four five two five O_ O_ three eight two four five six zero zero eight eight one nine five six six O_ five O_ two one eight seven zero seven zero two nine one two three zero zero five three six three five two four seven seven six four eight six two eight zero nine. And we are - That's hard to focus on that  you know  really  it's like - ""alright  now where am I?"" ","The main focus of the meeting was firstly on the structure of the belief-net  its decision nodes and the parameters that influence them  and secondly  on the design of the data collection tasks. For the latter  there are already 30 subjects lined up and more are expected to be recruited off campus. It was agreed that making subjects select from categories of tasks  such as ""big place""  ""service""  etc. could provide a better range of data. The duration of each dialogue will probably be no more than 10 minutes. On the other hand  the organisation of the intermediate nodes of the belief-net and their properties is almost complete  although no conditional probabilities have been inserted yet. These nodes represent decisions that will function as parameters to action calls in the system. Their values will either be inferred from the user-system interaction  or -as a last resort- requested directly from the user. Finally  as to the semantic and syntactic constructions  work will start with more general and brief descriptions  before moving to exhaustive analysis of at least a subset. Similarly  the construction parser that is to be built within a year is expected to be relatively basic  yet robust. As the data collection is ready to start  it was agreed that for the first ten subjects the interaction with the system/instructor will be along the lines of a basic response system. Tasks will be divided in categories (""tour""  ""attend"" etc) and subjects are going to be asked to choose no more than one task out of each category . This first run will probably take a couple of weeks  but the first results (audio files and selected highlights) will be discussed shortly  in order to decide whether more detail (complex spatial relationships  temporal planning etc) should be included in the design or particular constructions be elicited. Regarding the completion of the belief-net  the remaining details  mainly the properties of the ontology and discourse nodes  should be added. After building in the conditional probability tables  a working prototype of the net will be ready. Finally  the initial work on constructions should focus on a general overview of the dialogues with brief descriptions. Further analysis will follow from there in a top-down fashion. Although there is an effort to include some of the key features of the belief-net in the design of the data gathering  not all of them can be built in. The tasks that the subjects will have to carry out will be categorised in ways that will indicate EVA intentions  however  this approach may limit the variety of possible constructions used within a single category of entities. On the other hand  generating more diverse dialogues may have an adverse effect from a speech recognition perspective. A minor problem has arisen with the laboratory where recordings are supposed to take place  but this is currently being sorted out. As regards the completion of the belief-net  no work has been done on the CPT's yet. Finally  it was noted that although a general overview of the pertinent constructions is attainable  no more than ten of them can be analysed in detail with the summer months. A detailed diagram of the EVA belief-net was presented and some of the intermediate nodes and their properties were discussed in depth. Some of the key features and properties are: ""Go-there""  which is binary  and defined by the user  situation  ontology and discourse models; ""timing"" (current/next tour); ""reason"" (business  sight-seeing  socialising); ""transport""; ""length of tour""; ""costs""; ""entity"" (open  accessible) etc. The data collection that will provide relevant dialogues is moving along  with thirty subjects already lined up. They will be given a reading task  which will include some german proper names  and a series of tasks from the tourist domain to choose from. In order to get directions  they will then communicate with a computer system and a human operator  using a sketchy map as an aid. A different set of data are already available from the SmartKom system and similar sources. A preliminary study using this data has shown that a large number of syntactic and semantic constructions can be derived from a small sample. "
"- what things to talk about. I'm - What? Really? Oh  that's horrible! Disincentive! O_K  we're recording. Hello? Hello? Which am I? Check check check check. Uh  yeah. Oh right. Alright. Good. Channel fi- O_K. O_K. Are you doing something? O_K  then I guess I'm doing something. So  um  So basically the result of m- much thinking since the last time we met  um  but not as much writing  um  is a sheet that I have a lot of  like  thoughts and justification of comments on but I'll just pass out as is right now. So  um  here. If you could pass this around? And there's two things. And so one on one side is - on one side is a sort of the revised sort of updated semantic specification. And the other side is  um  sort of a revised construction formalism. It's just one sheet. Um - The - wait. This is just one sheet  right? Ah! Just one sheet. O_K. It's just a - Nothing else. Um  Front  back. Enough to go around? O_K. And in some ways it's - it's - it's very similar to - There are very few changes in some ways from what we've  um  uh  b- done before but I don't think everyone here has seen all of this. So  uh  I'm not sure where to begin. Um  as usual the disclaimers are there are - all these things are - it's only slightly more stable than it was before. Mm-hmm. And  um  after a little bit more discussion and especially like Keith and I - I have more linguistic things to settle in the next few days  um  it'll probably change again some more. Yeah. Um  maybe I will - let's start b- let's start on number two actually on the notation  um  because that's  I'm thinking  possibly a little more familiar to  um - to people. O_K  so the top block is just sort of a - sort of abstract nota- it's sort of like  um  listings of the kinds of things that we can have. And certain things that have  um  changed  have changed back to this. There - there's been a little bit of  um  going back and forth. But basically obviously all constructions have some kind of name. I forgot to include that you could have a type included in this line. So something like  um - Well  there's an example - the textual example at the end has clausal construction. So  What I was gonna - Right. um  just to show it doesn't have to be beautiful LaTeX. It could be  you know  simple old text as well. Um  there are a couple of - Uh  these three have various ways of doing certain things. So I'll just try to go through them. So they could all have a type at the beginning. Um  and then they say the key word construction and they have some name. Oh  I see. So - so the current syntax is if it s- if there's a type it's before construct- O_K  that's fine. Yeah  right. O_K  and then it has a block that is constituents. And as usual I guess all the constructions her- all the examples here have only  um  tsk one type of constituent  that is a constructional constituent. I think that's actually gonna turn out to m- be certainly the most common kind. But in general instead of the word "" construct ""  th- here you might have ""meaning"" or ""form"" as well. O_K? So if there's some element that doesn't - that isn't yet constructional in the sense that it maps form and meaning. O_K  um  the main change with the constructs which - each of which has  um  the key word ""construct"" and then some name  and then some type specification  is that it's - it's pro- it's often - sometimes the case in the first case here that you know what kind of construction it is. So for example whatever I have here is gonna be a form of the word ""throw""  or it's gonna be a form of the word  you know  I don't know  "" happy ""  or something like that. Or  you know  some- it'll be a specific word or maybe you'll have the type. You'll say ""I need a p- uh spatial relation phrase here"" or ""I need a directional specifier here"". So- uh you could have a j- a actual type here. Um  or you could just say in the second case that you only know the meaning type. So a very common example of this is that  you know  in directed motion  the first person to do something should be an agent of some kind  often a human. Right? So if I - you know  the- um  uh  run down the street then I - I - I run down the street  it's typed  uh  ""I""  meaning category is what's there. The - the new kind is this one that is sort of a pair and  um  sort of skipping fonts and whatever. The idea is that sometimes there are  um  general constructions that you know  that you're going to need. It's - it's the equivalent of a noun phrase or a prepositional phrase  or something like that there. And usually it has formal Mm-hmm. um  considerations that will go along with it. And then Mm-hmm. uh  you might know something much more specific depending on what construction you're talking about  about what meaning - what specific meaning you want. So the example again at the bottom  which is directed motion  you might need a nominal expression to take the place of  you know  um  ""the big th-""  you- you know  ""the big - the tall dark man""  you know  ""walked into the room"". But Mm-hmm. because of the nature of this particular construction you know not just that it's nominal of some kind but in particular  that it's some kind of animate nominal  and which will apply just as well to like  you know  a per- you know  a simple proper noun or to some complicated expression. Um  so I don't know if the syntax will hold but something that gives you a way to do both constructional and meaning types. So. O_K  then I don't think the  um - at least - Yeah. None of these examples have anything different for formal constraints? But you can refer to any of the  um  sort of available elements and scope  right? which here are the constructs  to say something about the relation. And I think i- if you not- if you compare like the top block and the textual block  um  we dropped like the little F_ subscript. The F_ subscripts refer to the ""form"" piece of the construct. Good. And I think that  um  in general it'll be unambiguous. Like if you were giving a formal constraint then you're referring to the formal pole of that. So - so by saying - if I just said ""Name one "" then that means name one formal and we're talking about formal struc- Which - which makes sense. Uh  there are certain times when we'll have an exception to that  in which case you could just indicate ""here I mean the meaningful for some reason"". Right? Or - Actually it's more often that  only to handle this one special case of  you know  ""George and Jerry walk into the room in that order"". So we have a few funny things where Mm-hmm. something in the meaning might refer to something in the form. But - but s- we're not gonna really worry about that for right now and there are way- We can be more specific if we have to later on. O_K  and so in terms of the - the relations  you know  as usual they're before and ends. I should have put an example in of something that isn't an interval relation but in form you might also have a value binding. You know  you could say that  um  you know  ""name-one dot""  t- you know  "" number equals""  you know  Mm-hmm. a plural or something like that. There are certain things that are attribute-value  similar to the bindings below but I mean they're just - us- usually they're going to be value - value fillers  right? O_K  and then again semantic constraints here are just - are just bindings. There was talk of changing the name of that. And Johno and I - I - you - you and I can like fight about that if you like? but about changing it to ""semantic n- effects""  which I thought was a little bit too Well - order-biased and ""semantic bindings""  which I thought might be too restrictive in case we don't have Th- only bindings. And so it was an issue whether constraints - um  there were some linguists who reacted against ""constraints""  saying  ""oh  if it's not used for matching  then it shouldn't be called a constraint"". But I think we want to be uncommitted about whether it's used for matching or not. Right? Cuz there are - I think we thought of some situations where it would be useful to use whatever the c- bindings are  for actual  you know  sort of like modified constraining purposes. Well  you definitely want to de-couple the formalism from the parsing strategy. Yeah. So that whether or not Yeah  yeah. it's used for matching or only for verification  I - It's used shouldn't matter  right? Mm-hmm. s- For sure. I mean  I don't know what  uh  term we want to use but we don't want to - Mm-hmm. Yeah  uh  there was one time when - when Hans explained why ""constraints"" was a misleading word for him. And I think the reason that he gave was similar to the reason why Johno thought it was a misleading term  which was just an interesting coincidence. Um  but  Yep. uh - And so I was like  ""O_K  well both of you don't like it? Fine  we can change it"". But I - I - I'm starting to like it again. So that- that's why - That's why I'll stick with it. So - It's g- it's gone. But - Well  you know what? If you have an ""if-then"" phrase  do you know what the ""then"" phrase is called? Th- What? Con- uh  a consequent? Yeah. Yeah  but it's not an ""if-then"". I know. Anyway  so the other - the other strategy you guys could consider is No  but - Mm-hmm. when you don't know what word to put  you could put no word  just meaning. O_K? And the- then let - Yeah. Yeah  that's true. So that's why you put semantic constraints up top and meaning bindings down - down here? Oh  oops! No. That was just a mistake of cut and paste from when I was going with it . So  I'm sorry. I didn't mean - that one's an in- unintentional. O_K. O_K. So this should be semantic and - Sometimes I'm intentionally inconsistent cuz I'm not sure yet. Here  I actually - it was just a mistake. @@ Th- so this definitely should be ""semantic constraints"" down at the bottom? Sure. Yeah. O_K. Well  unless I go with ""meaning"" but i- I mean  I kind of like ""meaning"" better than ""semantic"" but I think there's Or - Oh  whatever. vestiges of other people's biases. Like - Or - wh- That- b- Right. Minor - min- problem - O_K. Minor point. O_K  um  Extremely. so I think the middle block doesn't really give you any more information  ex- than the top block. And the bottom block similarly only just illus- you know  all it does is illustrate that you can drop the subscripts and - and that you can drop the  um - uh  that you can give dual types. Oh  one thing I should mention is about ""designates"". I think I'm actually inconsistent across these as well. So  um  strike out the M_ subscript on the middle block. So basically now  um  this is actually - Mm-hmm. this little change actually goes along with a big linguistic change  which is that ""designates"" isn't only something for the semantics to worry about now. Good. So we want s- ""designates"" to actually know one of the constituents which acts like a head in some respects but is sort of  um  really important for say composition later on. So for instance  if some other construction says  you know  ""are you of type - is this part of type whatever""  um  the ""designates"" tells you which sort of part is the meaning part. O_K  so if you have like ""the big red ball""  you know  you wanna know if there's an object or a noun. Well  ball is going to be the designated sort of element of that kind of phrase. Um  Mmm. there is a slight complication here which is that when we talk about form it's useful sometimes to talk about  um - to talk about there also being a designated object and we think that that'll be the same one  right? So the ball is the head of the phrase  ""the r- the -""  um  ""big red ball""  and the entity denoted by the word ""ball"" is sort of the semantic head in some ways of - of this sort of  um  in- interesting larger element. A- a- and the - Yeah. And there's - uh there's ca- some cases where the grammar depends on some form property of the head. And - and this enables you to get that  if I understand you right. Mm-hmm. Yeah. Yeah. That's the idea. Yeah. Right  right. Yeah yeah. And  uh  you might be able to say things like if the head has to go last in a head-final language  you can refer to the head as a p- the  you know - the formal head as opposed to the rest of the form having to be at the end of that decision . So that's a useful thing so that you can get some internal structural constraints in. Right. O_K  so that all looks good. Let me - Oh  w- Oh. I don't know. Were you finished? Um  there was a list of things that isn't included but you - you can - you can ask a question. That might O_K. So  i- if I understand this @@ it. the - aside from  uh  construed and all that sort of stuff  the - the differences are mainly that  Mm-hmm. we've gone to the possibility of having form-meaning pairs for a type or actually gone back to  if we go back far enough - Right. Well  except for their construction meaning  so it's not clear that  uh - Well  right now it's a c- uh contr- construction type and meaning type. So I don't know what a form Oh  I see. Yeah  yeah  yeah. I'm sorry  you're right. A construction type. type is. Yeah. Uh  that's fine. But it  um - Right. A- well  and a previous  um  you know  version of the notation certainly allowed you to single out the meaning bit by it. So you could say ""construct of type whatever designates something "". Yeah. But that was mostly for reference purposes  just to refer to the meaning pole . I don't think that it was often used to give an extra meaning const- type constraint on the meaning  which is really what we want most of the time I think. Mm-hmm. Um  I - I don't know if we'll ever have a case where we actually h- if there is a form category constraint  you could imagine having a triple there that says  you know - that's kind of weird. No  no  no  I don't think so. I think that you'll - you'll do fine. In fact  I - these are  um  as long as - as Mark isn't around  these are form constraints. So a nominal expression is - uh  the fact that it's animate  is semantic. The fact that it's n- uh  a nominal expression I would say on most people's notion of - of f- you know  higher form types  this i- this is one. And I think that's just fine. Mm-hmm. Yeah. Right  right. Yeah  yeah. Which is fine  yeah. It's - that now  um  I'm mentioned this  I - I don't know if I ever explained this but the point of  um  Yeah. I mentioned in the last meeting  the point of having something called ""nominal expression"" is  um  because it seems like having the verb subcategorize for  you know  like say taking as its object just some expression which  um  designates an object or designates a thing  or whatever  um  that leads to some syntactic problems basically? So you wanna  you know - you sort of have this problem like ""O_K  well  I'll put the word""  uh  let's say  the word "" dog ""  you know. And that has to come right after the verb cuz we know verb meets its object. Mm-hmm. And then we have a construction that says  oh  you can have ""the"" preceding a noun. And so you'd have this sort of problem that the verb has to meet Right. the designatum. And you could get  you know  ""the kicked dog"" or something like that  meaning ""kicked the dog"". Right. Um  so you kind of have to let this phrase idea in there but - That I - I have no problem with it at all. I think it's fine. It- it- Yeah. Yeah. Right  n- s- you may be - you may not be like everyone else in - in Berkeley  but that's O_K. Uh  we don't mind either  so - Yeah. Yeah. I mean  we - we - we sort of thought we were getting away with  uh - with  a p- I mean  this is not reverting to the X_bar theory of - of phrase structure. But  uh  Right. Right. I just know that this is - Like  we didn't originally have in mind that  uh - that verbs would subcategorize for a particular sort of form. Um  Mm-hmm. But they do. Well  there's an alternative to this which is  um - The question was did we want directed motion  but they does. At least in English. Yeah. which is an argument structure construction - Yeah. Mm-hmm. did we want it to worry about  um  anything more than the fact that it  you know  has semantic - You know  it's sort of frame-based construction. So one option that  you know  Keith had mentioned also was like  Mm-hmm. well if you have more abstract constructions such as subject  predicate  basically things like grammatical relations  those could intersect with these in such a way that subject  predicate  or subject  predicate  subject  verb  ob- you know  verb object would require that those things that f- fill a subject and object are NOM expressions. Right. And that would be a little bit cleaner in some way. But you know  for now  I mean  Yeah. But it - y- y- it's - yeah  just moving it - moving the c- the cons- the constraints around. O_K  so that's - uh  you know. M- moving it to another place  right. Yeah. But there does - basically  the point is there has to be that constraint somewhere  right? So  yeah. Right. And so that was the - Robert's not happy now? No! Oh  O_K. O_K  and sort of going with that is that the designatum also now is a pair. Yes. Instead of just the meaning. And that aside from some terminology  Mm-hmm. that's basically it. Right. I just want to b- I'm - I'm asking. Mm-hmm. Yep. Yeah. Yeah  um  the un- sort of the un-addressed questions in this  um  definitely would for instance be semantic constraints we talked about. Yeah. Here are just bindings but  right? we might want to introduce mental spaces - You know  there's all these things that we don't - The whole - the mental space thing is clearly not here. Right? So there's going to be some extra - you know  definitely other notation we'll need for that which we skip for now. Mm-hmm. By the way  I do want to get on that as soon as Robert gets back. So  uh  the - the mental space thing. Um  obviously  Uh Yeah. O_K. Mm-hmm. construal is a b- is a b- is a big component of that so this probably not worth trying to do anything till he gets back. But sort of as soon as he gets back I think Mm-hmm. Mm-hmm. um  we ought to - So what's the - what's the time frame? I forgot again when you're going away for how long? Just  uh  as a - sort of a mental bridge  I'm not - I'm skipping fourth of July. O_K. So  uh  right afterwards I'm back. O_K. What? You're missing like the premier American holiday? What's the point of spending a year here? So  anyway. Uh  I've had it often enough. Well he w- he went to college here. Oh  yeah  I forgot. Oops. Sorry. O_K. Yeah. And furthermore it's well worth missing. Not in California. Yeah  that's true. I like - I - Yes. I like spending fourth of July in other countries  whenever I can. Right. Um - O_K  so that's great. Construal  O_K  so - Oh  so there was one question that came out. I hate this thing. Sorry. Um  which is  so something like "" past "" which i- you know  we think is a very simple - uh  we've often just stuck it in as a feature  you know  ""oh  Right. Right. this event takes place before speech time""  O_K  is what this means. Um  it's often thought of as - it is also considered a mental space  you know  by  you know  lots of people around here. So Right. Right. there's this issue of well sometimes there are really exotic explicit space builders that say ""in France  blah-blah-blah""  and you have to build up - you ha- you would imagine that would require you  you know  to be very specific about the machinery  Mm-hmm. whereas past is a very conventionalized one and we sort of know what it means but it - we doesn't - don't necessarily want to  you know  unload all the notation every time we see that it's past tense. So  you know  we could think of our - uh  just like X_schema ""walk"" refers to this complicated structure  Right. I think that's exactly right. past refers to  you know  a certain configuration of this thing with respect to it. So - so we're kind of like having our cake and eating it - you know  having it both ways  right? So  i- i- Yeah. Yeah. No  I think - I think that i- we'll have to see how it works out when we do the details but Mm-hmm. Mm-hmm. my intuition would be that that's right. Yeah  O_K. Do you want to do the same for space? Wha- sorry? Space? Space? Here? Now? Oh  oh  oh  oh  instead of just time? Yeah  yeah  yeah. Same thing. So there are very conventionalized like Mm-hmm. deictic ones  right? And then I think for other spaces that you introduce  you could just attach y- whatever - You could build up an appropriately - uh  appropriate structure according to the l- the sentence. Hmm. Yeah. Hmm  well this - this basically would involve everything you can imagine to fit under your C_ dot something - N- Yeah. Right. Mm-hmm. you know  where - where it's contextually dependent  ""what is now  what was past  what is in the future  where is this  what is here  what is there  what is -"" Mm-hmm. Yeah. So time and space. Um  we'll - we'll get that on the other side a little  like very minimally. There's a sort of- there's a slot for setting time and setting place. And Good. you know  you could imagine for both of those are absolute things you could say about the time and place  and then there are many in- more interestingly  linguistically anyway  there are relative things that  you know  you relate the event in time and space to where you are now. If there's something a lot more complicated like  or so - hypothetical or whatever  then you have to do your job  like or somebody's job anyway. I'm gonna point to - at random. Mm-hmm. Yeah. Yeah. I mean  I'm - I'm s- curious about how much of the mental - I mean  I'm not sure that the formalism  sort of the grammatical side of things  is gonna have that much going on in terms of the mental space stuff. You know  um  basically all of these so-called space builders that are in the sentence are going to sort of - I think of it as  sort of giving you the coordinates of  you know - assuming that at any point in discourse there's the possibility that we could be sort of talking about a bunch of different world scenarios  whatever  and the speaker's supposed to be keeping track of those. The  um - the construction that you actually get is just gonna sort of give you a cue as to which one of those that you've already got going  Mm-hmm. um  you're supposed to add structure to. So ""in France  uh  Watergate wouldn't have hurt Nixon"" or something like that. Um  well  you say  ""alright  I'm supposed to add some structure to my model of this hypothetical past France universe"" or something like that. The information in the sentence tells you that much but it doesn't tell you like exactly what it - what the point of doing so is. So for example  depending on the linguistic con- uh  context it could be - like the question is for example  what does ""Watergate"" refer to there? Does it  you know - does it refer to  um - if you just hear that sentence cold  the assumption is that when you say ""Watergate"" you're referring to ""a Watergate-like scandal as we might imagine it happening in France"". But in a different context  ""oh  you know  if Nixon had apologized right away it wouldn't - you know  Watergate wouldn't have hurt him so badly in the U_S and in France it wouldn't have hurt him at all"". Now we're s- now that ""Watergate"" - we're now talking about the real one  and the ""would"" They're real  right. sort of - it's a sort of different dimension of hypothe-theticality  right? We're not saying - I see - right. What's hypothetical about this world. In the first case  hypothetically we're imagining that Watergate happened in France. In the second case we're imagining hypothetically that Nixon had apologized right away Hmm. Mm-hmm. Right. or something. Right? So a lot of this isn't happening at the grammatical level. Uh  um  and so - Correct. Mm-hmm. I don't know where that sits then  sort of the idea of sorting out what the person meant. Hmm. It seems like  um  the grammatical things such as the auxiliaries that you know introduce these conditionals  whatever  Mm-hmm. give you sort of the - the most basi- th- those we - I think we can figure out what the possibilities are  right? There are sort of a relatively limited number. And then Mm-hmm. how they interact with some extra thing like ""in France"" or ""if such-and-such""  Yeah. that's like there are certain ways that they c- they can - You know  one is a more specific version of the general pattern that the grammat- grammar gives you. I think . But  you know  whatever  we - we're - Yeah. Yeah  in the short run all we need is a enough mechanism on the form side to get things going. Uh  I - uh  you - you - Mm-hmm. Yeah. But the whole point of - the whole point of what Fauconnier and Turner have to say about  uh  mental spaces  and blending  and all that stuff is that you don't really get that much out of the sentence. You know  there's not that much information contained in the sentence. It just says  ""Here. Add this structure to this space."" and exactly what that means for the overall ongoing interpretation is quite open. An individual sentence could mean a hundred different things depending on  quote  ""what the space configuration is at the time of utterance"". Mm-hmm. Mm-hmm. And so somebody's gonna have to be doing a whole lot of work but not me  I think. Well - I think that's right. Oh  I - yeah  I  uh  uh - I think that's - Not k- I th- I don't think it's completely right. I mean  in fact a sentence- examples you gave in f- did constrain the meaning b- the form did constrain the meaning  Yeah. and so  um  it isn't  uh - Sure  but like what - what was the point of saying that sentence about Nixon and France? That is not - there is nothing about that in the - in the sentence really. That's O_K. We usually don't know the point of Yeah. the sentence at all. But we know what it's trying to say. We - we know that it's - what predication it's setting up. That's all. Y- yeah. Yeah. But - but - bottom line  I agree with you  that - that - that we're not expecting much out of the  uh f- Yeah. Yeah. Purely linguistic cues  right? So. uh  the purely form cues  yeah. Mmm. And  um - I mean  you're - you're the linguist but  uh  it seems to me that th- these - we - we - you know  we've talked about maybe a half a dozen linguistics theses in the last few minutes or something. Yeah  I mean - Yeah  yeah. Yeah. Oh  yeah. uh  I - I mean  that - that's my feeling that - that these are really hard Mm-hmm. Yeah. uh  problems that decide exactly what - what's going on. Yeah. O_K. O_K  so  um  one other thing I just want to point out is there's a lot of confusion about the terms like ""profile  designate  focus""  et cetera  et cetera. Mm-hmm. Uh  right  right  right. Um  for now I'm gonna say like ""profile""'s often used - like two uses that come to mind immediately. One is in the traditional like semantic highlight of one element with respect to everything else. So ""hypotenuse""  you profiled this guy against the background of the right t- right triangle. O_K. Mm-hmm. And the second use  um  is in FrameNet. It's slightly different. Oh  I was asking Hans about this. They use it to really mean  um  this - in a frame th- this is - the profiles on the - these are the ones that are required. So they have to be there or expressed in some way. Which - which - I'm not saying one and two are mutually exclusive but they're - they're different meanings. So the closest thing - so I was thinking about how it relates to this notation. Right. Mm-hmm. For us  um - O_K  so how is it - so "" designate "" - Does that - Is that really what they mean in - in - FrameNet? FrameNet. I didn't know that. Yeah  yeah. I - I mean  I - I was a little bit surprised about it too. I knew that - Yeah. I thought that that would be something like - there's another term that I've heard for that thing but they - I mean - uh  well  at least Hans says they use it that way. And - Right  O_K. Well  I'll check. and may- maybe he's wrong. Anyway  so I think the - the ""designate"" that we have in terms of meaning is really the ""highlight this thing with respect to everything else"". O_K? So this is what - what it means. Right. But the second one seems to be useful but we might not need a notation for it? We don't have a notation for it but we might want one. So for example we've talked about if you're talking about the lexical item ""walk""  you know it's an action. Well  it also has this idea - it carries along with it the idea of an actor or somebody's gonna do the walking. Or if you talk about an adjective ""red""  it carries along the idea of the thing that has the property of having color red. So we used to use the notation ""with"" for this and I think that's closest to their second one. So I d- don't yet know  I have no commitment  as to whether we need it. It might be - Right. it's the kind of thing that w- a parser might want to think about whether we require - you know  these things are like it's semantically part of it - N- no  no. Well  uh  th- critically they're not required syntactically. Often they're pres- presu- presupposed and all that sort of stuff. Right. Right  right. Yeah  um  definitely. So  um  ""in"" was a good example. If you walk ""in""  like well  in what? You know  like you have to have the - So - so it's only semantically is it - it is still required  say  by simulation time though to have Right  there's - Right. something. So it's that - I meant the idea of like that - the semantic value is filled in by sim- simulation. I don't know if that's something we need to spa- to - to like say ever as part of the requirement? - or the construction? or not. We'll - we'll again defer. Or - I mean  or - or  uh so the - Have it construed  is that the idea? Just point at Robert. Whenever I'm confused just point to him. You tell me. O_K. Yeah  yeah. Right. It's - it's his thesis  right? Anyway  right  yeah  w- this is gonna be a b- you're right  this is a bit of in a mess and we still have emphasis as well  or stress  or whatever. O_K  well we'll get  uh uh  I - we have thoughts about those as well. Um  Yeah. Great. the- I w- I would just- s- some of this is just like my - you know  by fiat. I'm going to say  this is how we use these terms. I don't- you know  there's lots of different ways in the world that people use it. Yeah. I - that's fine. I think that  um  the other terms that are related are like focus and stress. So  s- Mm-hmm. I think that the way I - we would like to think  uh  I think is focus is something that comes up in  I mean  lots of - basically this is the information structure. O_K  it's like - uh  it's not - it might be that there's a syntactic  Mm-hmm. uh  device that you use to indicate focus or that there are things like  you know  I think Keith was telling me  things toward the end of the sentence  post-verbal  tend to be the focused - focused element  the new information. You know  if I - ""I walked into the room""  you - Mmm. Mm-hmm. tend to think that  whatever  ""into the room"" is sort of like the more focused kind of thing. And when you  uh  Yeah. uh  you have stress on something that might be  you know  a cue that the stressed element  or for instance  the negated element is kind of related to information structure. So that's like the new - the sort of like import or whatever of - of this thing. Uh  so - so I think that's kind of nice to keep ""focus"" being an information structure term. "" Stress "" - I th- and then there are different kinds of focus that you can bring to it. So  um  like ""stress""  th- stress is kind of a pun on - you might have like - whatever  like  um  accent kind of stress. And that's just a - Mm-hmm. uh  w- we'll want to distinguish stress as a form device. You know  like  oh  high volume or whatever. Um  Yeah. t- uh  and distinguish that from it's effect which is  ""Oh  the kind of focus we have is we're emphasizing this value often as opposed to other values""  right? So focus carries along a scope. Like if you're gonna focus on this thing and you wanna know - it sort of evokes all the other possibilities that it wasn't. Um  so my classic - Mm-hmm. my now-classic example of saying  ""Oh  he did go to the meeting?""  that was my way of saying - as opposed to  you know  ""Oh  he didn't g-"" or ""There was a meeting?"" I think that was the example that was Yeah. Yeah. Yeah. caught on by the linguists immediately. And so  um  the - like if you said he - you know  there's all these different things that if you put stress on a different part of it then you're  c- focusing  whatever  on  uh - ""he walked to the meeting"" as opposed to ""he ran ""  or ""he did walk to the meeting"" as opposed to ""he didn't walk"". You know  so Mm-hmm. Mm-hmm. we need to have a notation for that which  um  I think that's still in progress. So  sort of I'm still working it out. But it did - one - one implication it does f- have for the other side  which we'll get to in a minute is that I couldn't think of a good way to say ""here are the possible things that you could focus on""  cuz it seems like any entity in any sentence  you know  or any meaning component of anyth- you know - all the possible meanings you could have  any of them could be the subject of focus. Mmm. But I think one - the one thing you can schematize is the kind of focus  right? So for instance  you could say it's the - the tense on this as opposed to  um  the - the action. O_K. Or it's - uh  it's an identity thing or a contrast with other things  or stress this value as opposed to other things. So  um  it's - it is kind of like a profile - profile-background thing but I - I can't think of like the limited set of possible meanings that you would - that you would focu- light - highlight Light up with focus  yeah. as opposed to other ones. So it has some certain complications for the  uh  uh - later on. Li- I mean  uh  the best thing I can come up with is that information has a list of focused elements. For instance  you - Oh  one other type that I forgot to mention is like query elements and that's probably relevant for the like ""where is""  you know  ""the castle"" kind of thing? Because you might want to say that  um  Mm-hmm. location or cert- certain W_H words bring - you know  sort of automatically focus in a  you know  ""I don't know the identity of this thing"" kind of way on certain elements. So. O_K. Anyway. So that's onl- there are - there are many more things that are uncl- that are sort of like a little bit unstable about the notation but it's most - I think it's - this is  you know  the current - current form. Other things we didn't totally deal with  um  well  we've had a lot of other stuff that Keith and I have them working on in terms of like how you deal with like an adjective. You know  a - a nominal expression. And  um  Oh  there's a bunch. Yeah. Yeah. I mean  we should have put an example of this and we could do that later. But I think the not- inherently like the general principles still work though  that  Yeah. um  we can have constructions that have sort of constituent structure in that there is like  you know  for instance  one - Uh  you know  they - they have constituents  right? So you can like nest things when you need to  but they can also overlap in a sort of flatter way. So if you don't have like a lot of grammar experience  then like this - this might  you know  be a little o- opaque. But  you know  we have the properties of dependency grammars and some properties of constituents - constituent-based grammar. So that's - I think that's sort of the main thing we wanted to aim for and so far it's worked out O_K. So. Mm-hmm. Good. O_K. I can say two things about the f- Yes. Maybe you want to forget stress. This - my f- As a word? No  as - as - Just don't - don't think about it. If - As a - What's that? Sorry. canonically speaking you can - if you look at a - a curve over sentence  you can find out where a certain stress is and say  ""hey  that's my focus exponent. "" Mm-hmm. Right. It doesn't tell you anything what the focus is. If it's just that thing  Mm-hmm. Or the constituent that it falls in. a little bit more or the whole phrase. Um - Mm-hmm. You mean t- forget about stress  the form The form bit because  uh  as a form cue  um  not even trained experts can always - well  they can tell you where the focus exponent is sometimes. Yeah. cue? O_K. And that's also mostly true for read speech. In - in real speech  um  people may put stress. It's so d- context dependent on what was there before  phrase ba- breaks  um  restarts. It's just  um - Yeah. Mm-hmm. it's absurd. O_K  I believe you  yeah. It's complicated. And all - Yeah  I mean  I - I'm sort of inclined to say let's worry about specifying the information structure focus of the sentence and then  Mm-hmm. Ways that you can get it come from th- right. hhh  the phonology component can handle actually assigning an intonation contour to that. You know  I mean  later on we'll worry about exactly how - Or - or map from the contour to - to what the focus exponent is. y- Yeah. Mm-hmm. Exactly. But figure out how the - Yeah. But  uh  if you don't know what you're - what you're focus is then you're - you're hopeless-uh-ly lost anyways  Right. That's fine  yeah. Mm-hmm. and the only way of figuring out what that is  Mm-hmm. is  um  by sort of generating all the possible alternatives to each focused element  decide which one in that context makes sense and which one doesn't. And then you're left with a couple three. Mm-hmm. So  you know  again  that's something that h- humans can do  um  but far outside the scope of - of any - anything. So. You know. It's - O_K. Well  uh  yeah  I wouldn't have assumed that it's an easy problem in - in absence of all the oth- you need all the other information I guess. u- u- But it's - it's - what it - uh  it's pretty easy to put it in the formalism  though. I mean  because Yeah. you can just say whatever stuff  ""i- is the container being focused or the - the entire whatever  both  and so forth."" Mm-hmm  mm-hmm. Mm-hmm. Yeah. Exactly. So the sort of effect of it is something we want to be able to capture. Yeah  so b- b- but I think the poi- I'm not sure I understand but here's what I th- think is going on. That if we do the constructions right when a particular construction matches  it - the fact that it matches  does in fact specify the focus. W- uh  I'm not sure about that. Or it might limit - it cert- certainly constrains O_K. Uh - k- uh  at- at the very least it constrai- the possibilities of focus. I think that's - that's  th- that's certainly true. And depending on the construction it may or may not f- specify the focus  right? Oh  uh  for sure  yes. There are constrai- yeah  it's not every - but there are constructions  uh  where you t- explicitly take into account those considerations Yeah. Mm-hmm. that you need to take into account in order to decide which - what is being focused. Mm-hmm. Mm-hmm. So we talked about that a little bit this morning. "" John is on the bus  not Nancy."" So that's - focuses on John. ""John is on the bus and not on the train."" ""John is on the bus "" versus ""John is on the train."" And ""John is on the bus"" versus ""was""  and e- Mm-hmm. Right. Hmm. Mm-hmm. Right. Right. Is on. ""John is on the bus"". Yeah. Yeah. Right. ""it's the bu-"" so e- Yeah  all - all of those. Yeah. Right. All of these and will we have - u- is it all the same constructions? Just with a different foc- focus constituent? Yeah  I would say that argument structure in terms of like the main like sort of  I don't know - Mm-hmm. the fact that you can get it without any stress and you have some - whatever is predicated anyway should be the same set of constructions. So that's why I was talking about overlapping constructions. So  then you have a separate thing that picks out  you know  stress on something relative to everything else. Yeah. Mm-hmm. So  the question is actually - oh  I'm sorry  go ahead  finish. And it would - yeah  and it w- and that would have to - uh it might be ambiguous as  uh  whether it picks up that element  or the phrase  or something like that. But it's still is limited possibility. Hmm. So that should  you know  interact with - it should overlap with whatever other construction is there. Yeah. S- s- the question is  do we have a way on the other page  uh  when we get to the s- semantic side  of saying what the stressed element was  or stressed phrase  or something. Mm-hmm. Well  so that's why I was saying how - since I couldn't think of an easy like limited way of doing it  um  all I can say is that information structure has a focused slot and I think that should be able to refer to - Right. So that's down at the bottom here when we get over there. O_K. Yeah  and  infer - and I don't have - I don't have a great way or great examples but I think that - something like that is probably gonna be  uh  more - more what we have to do. But  um  O_K  that was one comment. And you had another one? I'll- I'll wait. O_K. Hmm. O_K. So Yeah  well the - once you know what the focus is the - everything else is background. How about ""topic-comment"" that's the other side of information. Topic-comment. How about what? Yeah  so that was the other thing. And so I didn't realize it before. It's like  ""oh!"" It was an epiphany that it - you know  topic and focus are a contrast set. So topic is - Topic-focused seems to me like  um  background profile  O_K  or a landmark trajector  or some- something like that. There's - there's definitely  um  Mmm. that kind of thing going on. Now I don't know whether - I n- I don't have as many great examples of like topic-indicating constructions on like focus  right? Um  topic - it seems kind of - you know  I think that might be an ongoing kind of thing. Mm-hmm. Japanese has this though. You know. Yeah  that's what ""wa"" is  uh  just to mark which thing is the topic. It doesn't always have to be the subject. Topic marker? Yeah. Mm-hmm. Mm-hmm. Right. So again  information structure has a topic slot. And  you know  I stuck it in thinking that we might use it. Um  I think I stuck it in. Um  and Mm-hmm. Yep  it's there. one thing that I didn't do consistently  um  is - when we get there  is like indicate what kind of thing fits into every role. I think I have an idea of what it should be but th- you know  so far we've been getting away with like either a type constraint or  um  you know  whatever. I forg- it'll be a frame. You know  it'll be - it'll be another predication or it'll be  um  I don't know  some value from - from some- something  some variable and scope or something like that  or a slot chain based on a variable and scope. O_K  so well that's - should we flip over to the other side officially then? I keep  uh  like  pointing forward to it. Yeah. Now we'll go back to s- O_K  so this Mm-hmm  hmm. O_K  side one. doesn't include something which mi- mi- may have some effect on - on it  which is  um  the discourse situation context record  right? So I didn't - I - I meant just like draw a line and like  you know  you also have  uh  some tracking of what was going on. And sort of - this is a big scale comment before I  you know  look into the details of this. But for instance you could imagine instead of having - Right. I - I changed the name of - um it used to be ""entities"". So you see it's ""scenario""  ""referent"" and ""discourse segment"". And ""scenario"" is essentially what kind of - what's the basic predication  what event happened. And actually it's just a list of various slots from which you would draw - draw in order to paint your picture  a bunch of frames  bi- and bindings  right? Um  and obviously there are other ones that are not included here  general cultural frames and general like  uh  other action f- you know  specific X_schema frames. O_K  whatever. Mm-hmm. The middle thing used to be ""entities"" because you could imagine it should be like really a list where here was various information. And this is intended to be grammatically specifiable information about a referent - uh  you know  about some entity that you were going to talk about. So ""Harry walked into the room""  ""Harry"" and ""room""  you know  the room - th- but they would be represented in this list somehow. And it could also have for instance  it has this category slot. Um  it should be either category or in- or instance. Basically  it could be a pointer to ontology. So that everything you know about this could be - could be drawn in. But the important things for grammatical purposes are for - things like number  gender  um - ki- the ones I included here are slightly arbitrary but you could imagine that  um  you need to figure out wheth- if it's a group whether  um  some event is happening  linear time  linear spaces  like  you know  are - are they doing something serially or is it like  um  uh I'm - I'm not sure. Because this partly came from  uh  Talmy's schema and I'm not sure we'll need all of these actually. But - Um  and then the ""status"" I used was like  again  in some languages  you know  like for instance in child language you might distinguish between different status. So  th- the - the big com- and - and finally ""discourse segment"" is about sort of speech-act-y information structure-y  like utterance-specific kinds of things. So the comment I was going to make about  um  changing entity - the entity's block to reference is that you can imagine your discourse like situation context  you have a set of entities that you're sort of referring to. And you might - that might be sort of a general  I don't know  database of all the things in this discourse that you could refer to. And I changed to ""reference"" cuz I would say  for a particular utterance you have particular referring expressions in it. And those are the ones that you get information about that you stick in here. For instance  I know it's going to be plural. I know it's gonna be feminine or something like that. And - and these could actually just point to  you know  the - the I_D in my other list of enti- active entities  right? So  um  uh  th- there's - there's all this stuff about discourse status. We've talked about. I almost listed ""discourse status"" as a slot where you could say it's active. You know  there's this  um  hierarchy - uh there's a schematization of  you know  things can be active or they can be  um  accessible  inaccessible. It was the one that  you know  Keith  um  emailed to us once  to some of us  not all of us. And Yeah. the thing is that that - I noticed that that  um  list was sort of discourse dependent. It was like in this particular set  s- you know  instance  it has been referred to recently or it hasn't been  or this is something that's like in my world knowledge but not active. So. Yeah. This - Uh - yeah  well there - there seems to be context properties. Yeah. Yeah  they're contex- and for instance  I used to have a location thing there but actually that's a property of the situation. And it's again  time  you know - at cert- certain points things are located  you know  near or far from you and - Well  uh  uh  this is recursive cuz until we do the Yeah. uh  mental space story  we're not quite sure - Th- th- which is fine. We'll just - we'll j- Yeah  yeah. So some of these are  uh - we just don't know yet. Right. So I - so for now I thought  well maybe I'll just have in this list the things that are relevant to this particular utterance  right? Everything else here is utterance-specific. Um  and I left the slot  ""predications""  open because you can have  um  things like ""the guy I know from school"". Or  you know  like your referring expression might be constrained by certain like unbounded na- amounts of prep- you know  predications that you might make. And it's Mm-hmm. unclear whether - I mean  you could just have in your scenario  ""here are some extra few things that are true""  right? Mm-hmm. And then you could just sort of not have this slot here. Right? You're - but - but it's used for identification purposes. So it's - it's a little bit different from just saying ""all these things are true from my utterance"". Right. Yeah. Yeah. Um. Right  ""this guy I know from school came for dinner"" does not mean  um  ""there's a guy  I know him from school  and he came over for dinner"". That's not the same effect. Yeah  it's a little bit - it's a little bit different. Right? So - Or maybe that's like a restrictive  non-restrictive - you know  it's like it gets into that kind of thing for - Yeah. um  but maybe I'm mixing  you know - this is kind of like the final result after parsing the sentence. So you might imagine that Mm-hmm. the information you pass to  you know - in identifying a particular referent would be  ""oh  some -"" you know  ""it's a guy and it's someone I know from school"". So maybe that would  Yeah. you know  be some intermediate structure that you would pass into the disc- to the  whatever  construal engine or whatever  discourse context  to find - you know  either create this reference  in which case it'd be created here  and - Mm-hmm. you know  so - so you could imagine that this might not - So  uh  I'm uncommitted to a couple of these things. Um. But - to make it m- precise at least in my mind  uh  it's not precise. So ""house"" is gender neuter? Um  it could be in - In reality or in - Semantically. semantically  yeah. Yeah. So it uh  uh  a table. semantically. So - You know  a thing that c- doesn't have a gender. So. Uh  it could be that - I mean  maybe you'd - maybe not all these - I mean  I wou- I would say that I tried to keep slots here that were potentially relevant to most - most things. No  just to make sure that we - everybody that's - completely agreed that it - it has nothing to do with  uh  form. Yeah. O_K  that is semantic as opposed to - Yeah. Yeah. That's right. Um. S- so again - Then "" predications "" makes sense to - to have it open for something like  uh  accessibility or not. Yeah. Open to various things. Right. O_K  so. Let's see. So maybe having made that big sca- sort of like large scale comment  should I just go through each of these slots - uh  each of these blocks  um  a little bit? Sure. Um  mostly the top one is sort of image schematic. And just a note  which was that  um - s- so when we actually ha- so for instance  um  some of them seem more inherently static  O_K  like a container or sort of support-ish. And others are a little bit seemingly inherently dynamic like ""source  path  goal"" is often thought of that way or ""force""  or something like that. But in actual fact  I think that they're intended to be sort of neutral with respect to that. And different X_schemas use them in a way that's either static or dynamic. So ""path""  you could just be talking about the path between this and this. And you know  ""container"" that you can go in and out. All of these things. And so  um  Mmm. I think this came up when  uh  Ben and I were working with the Spaniards  um  the other day - the ""Spaniettes""  as we called them - um  to decide like how you want to split up  like  s- image schematic contributions versus  like  X_schematic contributions. How do you link them up. And I think again  um  it's gonna be something in the X_schema that tells you ""is this static or is this dynamic"". So we definitely need - that sort of aspectual type gives you some of that. Um  that  you know  is it  uh  a state or is it a change of state  or is it a  um  action of some kind? Uh. Yeah. Uh  i- i- i- is there any meaning to when you have sort of parameters behind it and when you don't? Just means - Oh  oh! You mean  in the slot? Um  no  it's like X_sc- it's - it's like I was thinking of type constraints but X_schema  well it obviously has to be an X_schema. Mm-hmm. ""Agent""  I mean  the - the performer of the X_schema  that s- depends on the X_schema . You know  and I - in general it would probably be  you know - So the difference is basically whether you thought it was obvious what the possible fillers were. Yeah  basically. O_K. Mm-hmm. Um  ""aspectual type"" probably isn't obvious but I should have - So  I just neglected to stick something in. ""Perspective""  ""actor""  ""undergoer""  ""observer""  um  Mmm. I think we've often used ""agent""  ""patient""  obser- Yeah  exactly. Exactly. Um  and so one nice thing that  uh  ""Whee!"" That's that one  right? we had talked about is this example of like  if you have a passive construction then one thing it does is ch- you know - definitely  it is one way to - for you to  you know  specifically take the perspective of the undergoing kind of object. And so then we talked about  you know  whether well  does that specify topic as well? Well  maybe there are other things. You know  now that it's - subject is more like a topic. And now that  you know - Anyway. So. Sorry. I'm gonna trail off on that one cuz it's not that f- important right now. Um  N- now  for the moment we just need the ability to l- l- write it down if - if somebody figured out what the rules were. Yeah. To know how - Yeah. Yeah. Exactly. Um  some of these other ones  let's see. So  uh  one thing I'm uncertain about is how polarity interacts. So polarity  uh  is using for like Mm-hmm. action did not take place for instance. So by default it'll be like ""true""  I guess  you know  if you're specifying events that did happen. You could imagine that you skip out this - you know  leave off this polarity  you know  not - don't have it here. And then have it part of the speech-act in some way. There's some negation. But the reason why I left it in is cuz Mm-hmm. you might have a change of state  let's say  where some state holds and then some state doesn't hold  and you're just talking  you know - if you're trying to have the nuts and bolts of simulation you need to know that  you know  whatever  the holder doesn't and - No  I th- I think at this lev- which is - it should be where you have it. O_K  it's - so it's - it's - it's fine where it is. So  O_K. I mean  how you get it may - may in- will often involve the discourse but - but - May come from a few places. by the time you're simulating you sh- y- you should know that. Right. Right. So  I'm still just really not clear on what I'm looking at. The ""scenario"" box  like  what does that look like for an example? Like  not all of these things are gonna be here. This is just basically says Yeah. Correct. Mm-hmm. It's a grab bag of - "" part of what I'm going to hand you is a whole bunch of s- uh  schemas  image  and X_schemas. Here are some examples of the sorts of things you might have in there"". So that's exactly what it is. And for a particular instance which I will  you know  make an example of something   is that you might have an instance of container and path  let's say  as part of your  you know  ""into"" O_K. Mm-hmm. Mm-hmm. you know  definition. So you would eventually have instances filled in with various - various values for all the different slots. And they're bound up in  you know  their bindings and - and - and values. Mm-hmm. O_K. W- it c- Do you have to say about the binding in your - is there a slot in here for - that tells you how the bindings are done? No  no  no. I - let's see  I think we're - we're not - I don't think we have it quite right yet. O_K. So  uh  what this is  let's suppose for the moment it's complete. O_K. O_K  uh  then this says that when an analysis is finished  the whole analysis is finished  you'll have as a result  uh  some s- resulting s- semspec for that utterance in context  which is Mm-hmm. made up entirely of these things and  uh  bindings among them. Mm-hmm. And bindings to ontology items. Mm-hmm. So that - that the who- that this is the tool kit under whi- out of which you can make a semantic specification. Mm-hmm. So that's A_ . But B_   which is more relevant to your life  is this is also O_K. the tool kit that is used in the semantic side of constructions. Mm-hmm. So this is an- that anything you have  O_K. in the party line  anything you have as the semantic side of constructions comes  from pieces of this - ignoring li- I mean  in general  you ignore lots of it. But it's got to be pieces of this along with constraints among them. Right. O_K. Uh  so that the  you know  goal of the  uh uh  ""source  path  goal"" has to be the landmark of the conta- you know  the interior of this container. Or whate- whatever. So those constraints Mm-hmm. Yeah. appear in constructions but Mm-hmm. pretty much this is the full range of semantic structures available to you. O_K. Except for ""cause""  that I forgot. But anyway  there's som- some kind of causal structure for composite events. Yeah. O_K  good. Let's - let's mark that. So we need a c- Uh  I mean  so it gets a little funny. These are all - so far these structures  especially from ""path"" and on down  these are sort of relatively familiar  um  image schematic kind of slots. Now with ""cause""  uh  the fillers will actually be themselves frames. Right? So you'll say  ""event one causes event B_ - uh  event two ""  and - Right. Mm-hmm. And - and - and - and this - this - this again may ge- our  um - and we - and - and  of course  worlds. Yeah. So that's  uh these are all implicitly one - within  uh within one world. Um  Mm-hmm. even though saying that place takes place  whatever. Uh  if y- if I said ""time"" is  you know  ""past""  that would say ""set that this world""  you know  "" somewhere  before the world that corresponds to our current speech time"". Mm-hmm. Mm-hmm. Yeah. So. But that - that - that's sort of O_K. The - the - within the event it's st- it's still one world. Um. Yeah  so ""cause"" and - Other frames that could come in - I mean  unfortunately you could bring in say for instance  um  uh  "" desire "" or something like that  like "" want "". Mm-hmm. And actually there is right now under "" discourse segments""  um  ""attitude""? ""Volition""? could fill that. So there are a couple things where I like  ""oh  I'm not sure if Mm-hmm. I wanted to have it there or -"" Basically there was a whole list of - of possible speaker attitudes that like say Talmy listed. And  like  well  Well that's - I don't - you know  it was like ""hope  wish. desire""  blah-blah-blah. And it's like  well  Right. Uh-huh. I feel like if I wanted to have an extra meaning - I don't know if those are grammatically marked in the first place. So - They're more lexically marked  right? At least in English. Mmm. So if I wanted to I would stick in an extra frame in my meaning  saying  e- so th- it'd be a hierarchical frame them  right? You know  like ""Naomi wants - wants su- a certain situation and that situation itself is a state of affairs"". S- right. So - so  ""want"" itself can be - i- i- i- i- i- u- Can be just another frame that's part of your - Well  and it- i- basically it's an action. In - in our s- in our - in our - Yeah. Situation. Right  right. in - in our - in our s- terminology  ""want"" can be an action and ""what you want"" is a world. Mm-hmm. Hmm. Mmm. So that's - I mean  it's certainly one way to do it. Yeah  there - there are Mm-hmm. other things. Causal stuff we absolutely need. Mental space we need. The context we need. Um  so anyway  Keith - Mm-hmm. So is this comfortable to you that  uh  once we have this defined  Mm-hmm. it is your tool kit for building the semantic part of constructions. And then when we combine constructions semantically  the goal is going to be to fill out more and more of the bindings needed in order to come up with the final one. Mm-hmm. Yeah. And that's the wh- and - and I mean  that - according to the party line  that's the whole story. Mm-hmm. Yeah. Um. y- Right. That makes sense. So I mean  there's this stuff in the - off in the scenario  which just tells you how various - what schemas you're using and they're - how they're bound together. And I guess that some of the discourse segment stuff - Mm-hmm. is that where you would sa- I mean  that's - O_K  that's where the information structure is which sort of is a kind of profiling on different parts of  um  Right. Exactly. of this. I mean  what's interesting is that the information structure stuff - Hmm. There's almost - I mean  we keep coming back to how focus is like this - this  uh  trajector-landmark thing. So if I say  um  Yeah. You know  ""In France it's like this "". You know  great  we've learned something about France but the fact is that utterances of that sort are generally used to help you draw a conclusion also about some implicit contrast  like ""In France it's like this "". And therefore you're supposed to say  ""Boy  life sure -"" You know  ""in France kids are allowed to drink at age three "". And w- you're - that's not just a fact about France. You also conclude something about how boring it is here in the U_ S. Right? And so - Right. Right  right. Right. S- so I would prefer not to worry about that for right now and to think that there are  um  O_K. That comes in and  uh - discourse level constructions in a sense  topic - topic-focus constructions that would say  ""oh  when you focus something"" Mm-hmm. Yeah. then - just done the same way - just actually in the same way as the lower level. If you stressed  you know  "" John went to the -""  you know  ""the bar"" whatever  you're focusing that and Mm-hmm. a- in- a possible inference is ""in contrast to other things"". So similarly for a whole sentence  you know  ""in France such-and-such happens"". Yeah. Yeah. Yeah  yeah. So the whole thing is sort of like again implicitly as opposed to other things that are possible. Yeah. Uh  just - just  uh  look - read uh even sem- semi formal Mats Rooth. I mean - Yeah. Uh-huh. If you haven't read it. It's nice. And just pick any paper on alternative semantics. Uh-huh. O_K. So that's his - that's the best way of talking about focus  is I think his way. O_K  what was the name? Mats. M_A_T_S. Rooth. O_K. I think two O_'s  yes  T_H. I never know how to pronounce his name because he's sort of  O_K. S- Swede? uh  he is Dutch and  um - but very confused background I think . Dutch? Oh  Dutch. Yeah. Uh-huh. So and  um  Mats Gould . And sadly enough he also just left the I_M_S in Stuttgart. So he's not there anymore. But  um - Hmm. I don't know where he is right now but alternative semantics is - if you type that into an  uh  uh  browser or search engine you'll get tons of stuff. O_K. O_K. O_K  thanks. And what I'm kind of confused about is - is what the speaker and the hearer is - is sort of doing there. So for a particular segment it's really just a reference to some other entity again in the situation  right? So for a particular segment Yeah. the speaker might be you or might be me. Um  hearer is a little bit harder. It could be like multiple people. I guess that - that - that - that's not very clear from here - I mean  that's not allowed here. Yeah  but you - Don't we ultimately want to handle that analogously to the way we handle time and place  because ""you""  ""me""  ""he""  ""they""  you know  ""these guys""  all these expressions  nuh  are in - in much the same way contextually dependent as ""here "" and ""now "" and ""there"" - Mm-hmm. Now  this is - this is assuming you've already solved that. So it's - it's Fred and Mary  so the speaker would be Fred and the - Ye- yeah. So th- Ah! Right  so the constructions might - of course will refer  using pronouns or whatever. In which case they have to check to see  Mm-hmm. uh  who the  uh  speaker in here wa- in order to resolve those. But when you actually say that ""he walked into -""  whatever  um  the "" he "" will refer to a particular - You - you will already have figured who ""he"" or ""you""  mmm  or "" I ""  maybe is a bett- better example  who ""I"" refers to. Um  and then you'd just be able to refer to Harry  you know  in wherever that person - whatever role that person was playing in the event. Mmm. That's up at the reference part. Yeah  yeah. And down there in the speaker-hearer part? S- so  that's - I think that's just - n- for instance  Speaker is known from the situation  right? You're - when you hear something you're told who the speaker is - I mean  you know who the speaker is. In fact  that's kind of constraining how - in some ways you know this before you get to the - you fill in all the rest of it. I think. Mmm. I mean  how else would you um - You know  uh  uh  it's - the speaker may - in English is allowed to say ""I."" Uh  among the twenty-five percent most used words. But wouldn't the "" I "" then set up the - the s- s- referent - Yeah. Well  here - Yeah. Right. Mm-hmm. that happens to be the speaker this time and not ""they "" whoever they are. Or ""you"" - much like the ""you"" could n- Right  right. So - S- so - O_K  so I would say ref- under referent should be something that corresponds to ""I"". And maybe each referent should probably have a list of way- whatever  the way it was referred to. So that's ""I"" but  uh  uh  should we say it - it refers to  what? Uh  if it were "" Harry "" it would refer to like some ontology thing. If it were - if it's "" I "" it would refer to the current speaker  O_K  which is given to be like  you know  whoever it is. Well  not - not always. I mean  so there's ""and then he said  I w-"" Uh-huh. "" I "" within the current world. Uh - Yeah. That's right. So - so again  this - uh  this - this is gonna to get us into the mental space stuff and t- because Yeah. Yeah  yeah  yeah  yeah. Mm-hmm. Mmm. you know  ""Fred said that Mary said -""  and whatever. And - and so we're  uh Mm-hmm. Twhhh-whhh. gonna have to  um  chain those as well. But - Mm-hmm. So this entire thing Right. is inside a world  not just like the top part. Right. I - I think  uh - That's - Mm-hmm. Except s- it's - it's trickier than that because um  the reference for example - So he- where it gets really tricky is there's some things  and this is where blends and all terribl- So  some things which really are meant to be Yeah. Yeah. identified and some things which aren't. And again  all we need for the moment is some way to Right. Right. say that. So I thought of having like - for each referent  having the list of - of the things t- with which it is identified. You know  which - which  uh you know  you - you - you - for instance  um - So  I guess  You could do that. it sort of depends on if it is a referring exp- if it's identifiable already or it's a new thing. If it's a new thing you'd have to like create a structure or whatever. Mm-hmm. If it's an old thing it could be referring to  um  usually w- something in a situation  right? Or something in ontology. So  uh-huh. there's a- you know  whatever  it c- it could point at one of these. I just had a - I just had an - an idea that would be very nice if it works. For what? If it works. Uh  uh  uh  I haven't told you what it is yet. This was my Mm-hmm. Mmm. build-up. An i- an idea that would be nice i- Yeah. O_K  we're crossing our fingers. Right. If it worked. So we're building a mental space  good. Yeah. O_K. Right  it was a space builder. Um  we might be able to handle context in the same way that we handle mental spaces because  uh  you have somewhat the same things going on of  uh  things being accessible or not. And so  i- Mm-hmm. Yep. it c- it - it  uh I think if we did it right we might be able to get at least a lot of the same structure. Use the same - Yep. So that pulling something out of a discourse context is I think similar to other kinds of  uh  mental space phenomena. Uh  I've - I've - I've never seen anybody I see. Mm-hmm. And - And - write that up but maybe they did. I don't know. That may be all over the literature. Yeah. There's things like ther- you know  there's all kinds of stuff like  um  in - I think I mentioned last time in Czech if you have a - a verb of saying then So - so by default - um  you know  you say something like - or - or I was thinking you can say something like  ""oh  I thought  uh  you are a republican"" or something like that. Where as in English you would say  ""I thought you were "". Um  you know  sort of the past tense being copied onto the lower verb Right. doesn't happen there  so you have to say something about  you know  tense is determined relative to current blah-blah-blah. Same things happens with pronouns. There's languages where  um  Mm-hmm. Mm-hmm. if you have a verb of saying then  ehhh  where - O_K  so a situation like ""Bob said he was going to the movies""  where that lower subject is the same as the person who was saying or thinking  you're actually required to have ""I"" there. Mm-hmm. Um  and it's sort of in an extended function - Mm-hmm. So we would have it be in quotes in English. Yeah. Right. Right. But it's not perceived as a quotative construction. I mean  it's been analyzed by the formalists as being a logophoric pronoun  Yeah. um which means a pronoun which refers back to the person who is speaking or that sort of thing  right? Um  Oh  right. Yeah  that makes sense. O_K. but - uh  that happens to sound like the word for ""I"" but is actually semantically unrelated to it. Oh  no! Oh  good  I love the formali- Really? Um  Yeah. Yeah. You're kidding. There's a whole book which basically operates on this assumption. Uh  Mary Dalrymple  uh  this book  a ninety-three book on  uh No  that's horrible. O_K. on pronoun stuff. That's horrible. O_K. Well  yeah. And then the same thing for A_S_L where  you know  you're signing and someone says something. And then  you know  so ""he say""  and then you sort of do a role shift. And then you sign ""I  this  that  and the other"". And you know  ""I did this"". Uh-huh. That's also been analyzed as logophoric and having nothing to do with ""I"". And the role shift thing is completely left out and so on. So  I mean  the point is that pronoun references  uh  you know  sort of ties in with all this mental space stuff and so on  and so forth. And so  yeah  I mean - Uh-huh. Yeah. So that - that d- that does sound like it's co- consistent with what we're saying  yeah. Right. Yeah. O_K  so it's kind of like the unspecified mental spaces just are occurring in context. And then when you embed them sometimes you have to pop up to the h- you know  depending on the construction or the whatever  Mm-hmm. Mm-hmm. um  you - you - you're scope is - m- might extend out to the - the base one. Yeah. It would be nice to actually use the same  um  mechanism since there are so many cases where you actually need it'll be one or the other. It's like  oh  actually  it's the same - same operation. So. Yeah. Oh  O_K  so this - this is worth some thought. It's like - it's like what's happening - that  yeah  what- what's happening  uh  there is that you're moving the base space or something like that  right? So that's - that's how Fauconnier would talk about it. And it happens diff- under different circumstances in different languages. And so  Yeah  yeah. Mm-hmm. Mm-hmm. um  things like pronoun reference and tense which we're thinking of as being these discourse-y things actually are relative to a Bayes space which can change. And we need all the same machinery. Mm-hmm  right. Mm-hmm. Robert. Schade. Well  but  uh  this is very good actually cuz it - it - it - to the extent that it works  it y- Ties it all into it. Yeah. it - it ties together several of - of these things. Yep. Mm-hmm. Mm-hmm. And I'm sure gonna read the transcript of this one. So. But the  uh  - But it's too bad that we don't have a camera. You know  all the pointing is gonna be lost. Uh. Oh  yeah. Yeah. Yeah  that's why I said ""point to Robert""  when I did it. Well every time Nancy giggles it means - it means that it's your job. Yeah. Mmm  isn't - I mean  I'm - I was sort of dubious why - why he even introduces this sort of reality  you know  as your basic mental space and then builds up - Mm-hmm. d- doesn't start with some - because it's so obvi- it should be so obvious  at least it is to me  that whenever I say something I could preface that with ""I think."" Yeah. Nuh? So there should be no categorical difference between your base and all the others that ensue. Yeah. No  but there's - there's a Gricean thing going on there  that when you say ""I think"" you're actually hedging. Mmm. Yeah  I mean - It's like I don't totally think - Yeah. Y- Right. I mostly think  uh - Yeah  it's - Absolutely. Yeah  it's an - it's an evidential. It's sort of semi-grammaticalized. People have talked about it this way. And you know  you can do sort of special things. You can  th- put just the phrase ""I think"" as a parenthetical in the middle of a sentence and so on  and so forth. So - Actually one of the child language researchers who works with T- Tomasello studied a bunch of these constructions and it was like it's not using any kind of interesting embedded ways just to mark  you know  uncertainty or something like that. So. Yeah. Yeah. Yeah  but about linguistic hedges  I mean  those - those tend to be  um  funky anyways because they blur - So we don't have that in here either do we? Yeah. Hedges? Yeah  yeah. Hhh  I - there used to be a slot for speaker  um  it was something like factivity. I couldn't really remember what it meant so I took it out. But it's something - we - we were talking about sarcasm too  right? Yeah. Um. Well we were just talking about this sort of evidentiality and stuff like that  right? I mean  Oh  oh. Oh  yeah  yeah  right. that's what I think is  um  sort of telling you what percent reality you should give this or the  you know - So we probably should. Yeah. Mm-hmm. Confidence or something like that. Yeah  and the fact that I'm  you know - the fact maybe if I think it versus he thinks that might  you know  depending on how much you trust the two of us or whatever  you know - Yeah. Uh great word in the English language is called ""about"". If you study how people use that it's also - ""about."" What's the word? About. It's about - Oh  that - in that use of ""about""  yeah. clever. Oh  oh  oh  as a hedge. Yeah. And I think - And I think y- if you want us to spend a pleasant six or seven hours you could get George started on that. He wrote a paper about thirty-five years ago on that one. I r- I read that paper  the hedges paper? I read some of that paper actually. Yeah. Yeah. Would you believe that that paper lead directly to the development of anti-lock brakes? Yeah. What? No. Ask me about it later I'll tell you how. When we're not on tape. I'd love to know. Oh  man. So  and - and I think  uh  someone had raised like sarcasm as a complication at some point. And There's all that stuff. Yeah  let's - I - I don't - I think - we just won't deal with sarcastic people. Yeah  I mean - I don't really know what like - We - we don't have to care too much about the speaker attitude  right? Like there's not so many different - hhh  I don't know  m- Certainly not as some - Well  they're intonational markers I think for the most part. I don't Yeah. I just mean - know too much about the like grammatical - There's lots of different attitudes that - that the speaker could have and that we can clearly identify  and so on  and so forth. But like Yeah. what are the distinctions among those that we actually care about for our current purposes? Right. Right  so  uh  this - this raises the question of what are our current purposes. Mm-hmm. Oh  yeah  do we have any? Right? Oh  shoot. Here it is three-fifteen already. Mmm. Yeah. Uh  so  um  I - I don't know the answer but - but  um  it does seem that  you know  this is - this is coming along. I think it's - it's converging. It's - as far as I can tell there's this one major thing we have to do which is the mental - the whole s- mental space thing. And then there's some other minor things. Mm-hmm. Um  and we're going to have to s- sort of bound the complexity. I mean  if we get everything that anybody ever thought about you know  w- we'll go nuts. Yeah. So we had started with the idea that the actual  uh  constraint was related to this tourist domain and the kinds of interactions that might occur in the tourist domain  assuming that people were being helpful and weren't trying to d- you know  there's all sorts of - God knows  irony  and stuff like - which you - isn't probably of much use in dealing with a tourist guide. Yeah. Yeah ? Yeah. Uh. M- mockery. Right. Whatever. So y- uh  no end of things th- that - that  you know  we don't deal with. And - But it - i- isn't that part easy though because in terms of the s- simspec  it would just mean you put one more set of brack- brackets around it  and then just tell it to sort of negate whatever the content of that is in terms of irony or - Go ahead. Yeah. N- no. No. Mmm. Right. Maybe. Yeah  in model theory cuz the semantics is always like ""speaker believes not-P ""  you know? Like ""the speaker says P_ and believes not-P_"". But - No. Right. We have a theoretical model of sarcasm now. Yeah  right  I mean. Right  right  but  Right. No  no. Anyway  so - so  um  I guess uh  let me make a proposal on how to proceed on that  which is that  um  it was Keith's  uh  sort of job over the summer to come up with this set of constructions. Uh  and my suggestion to Keith is that you  over the next couple weeks  n- don't try to do them in detail or formally but just try to describe which ones you think we ought to have. Uh  and then when Robert gets back we'll look at the set of them. Mmm. O_K. O_K. Just - just sort of  you know  define your space. Yeah  O_K. And  um  so th- these are - this is a set of things that I think we ought to deal with. Yeah. And then we'll - we'll - we'll go back over it and w- people will - will give feedback on it. And then - then we'll have a - at least initial O_K. spec of - of what we're actually trying to do. And that'll also be useful for anybody who's trying to write a parser. Yeah. Mm-hmm. In case there's any around. Knowing uh - If we knew anybody like that. Right  ""who might want-"" et cetera. So  O_K. uh - So a- and we get this - this  uh  portals fixed and then we have an idea of the sort of initial range. And then of course Nancy you're gonna have to  uh  do your set of - but you have to do that anyway. For the same  yeah  data. Yeah  mm-hmm. So - so we're gonna get the- w- we're basically dealing with two domains  the tourist domain and the - and the child language learning. And we'll see what we need for Mmm. those two. And then my proposal would be to  um  not totally cut off more general discussion but to focus really detailed work on the subset of things that we've - we really want to get done. Mm-hmm. Mm-hmm. And then as a kind of separate thread  think about the more general things and - and all that. Mm-hmm. Well  I also think the detailed discussion will hit - you know  bring us to problems that are of a general nature and maybe even - Uh  without doubt. Yeah. Yeah. But what I want to do is - is - is to - to constrain the things that we really feel responsible for. even suggest some solutions. Yeah. Mmm. Mm-hmm. So that - that we say these are the things we're really gonna try do by the end of the summer and other things we'll put on a list of - of research problems or something  because you can easily get to the point where nothing gets done because every time you start to do something you say  ""oh  yeah  but what about this case?"" Mm-hmm. This is - this is called being a linguist. Mmm. Yeah. And  uh  Basically. Or me. Or me. Anyways - Huh? Right. There's that quote in Jurafsky and Martin where - where it goes - where some guy goes  ""every time I fire a linguist the performance of the recognizer goes up."" Yeah. Exactly. Right. But anyway. So  is - is that - does that make sense as a  uh - Sure  yeah. a general way to proceed? Yeah  yeah  we'll start with that  just figuring out what needs to be done then actually the next step is to start trying to do it. Got it. Exactly right. Mmm. Mmm. O_K. We have a little bit of news  uh  just minor stuff. The one big - Ooo  can I ask a - You ran out of power. Huh? Can I ask a quick question about this side? Yes. Yeah. Is this  uh - was it intentional to leave off things like ""inherits"" and - Oops. Um  No. not really - just on the constructions  right? Yeah  like constructions can inherit from other things  am I right? Yeah. Um  yeah. I didn't want to think too much about that for - for now. O_K. So  uh  maybe it was subconsciously intentional. Yeah. Yeah  uh - yeah. Um  yeah  there should be - I - I wanted to s- find out someday if there was gonna be some way of dealing with  uh  if this is the right term  multiple inheritance  where one construction is inheriting from  uh Mm-hmm. Uh-huh. from both parents  uh  or different ones  or three or four different ones. Cuz the problem is that then you have to - Yep. Yeah. So let me - Yeah. Refer to them. which of - you know  which are - how they're getting bound together. Yeah  right  right  right. Yeah  and - and there are certainly cases like that. Even with just semantic schemas we have some examples. So  Yeah  yeah  yeah. Right. and we've been talking a little bit about that anyway. Yeah. So what I would like to do is separate that problem out. So Inherits. O_K. um  my argument is there's nothing you can do with that that you can't do by just having more Yeah  yes. constructions. It's uglier and it d- doesn't have the deep linguistic insights and stuff. Uh  That's right. But whatever. Yeah  no  no  no no. No  by all means  right. Uh  sure. Uh  those are over rated. Right. And so I - what I'd like to do is - is in the short run focus on getting it right. And when we think we have it right then saying  ""aha!  can we make it more elegant?"" Yeah. Yeah  that's - Yeah. Can - can we  uh - What are the generalizations  and stuff? But rather than try to guess Connect the dots. Yeah. a inheritance structure and all that sort of stuff before we know what we're doing. So I would say in the short run Yep. Yeah. Yeah. we're not gonna b- First of all  we're not doing them yet at all. And - and it could be that half way through we say  ""aha!  we - we now see how we want to clean it up."" Mm-hmm. Uh  and inheritance is only one - I mean  that's one way to organize it but there are others. And it may or may not be the best way. Yeah. I'm sorry  you had news. Mmm. Oh  just small stuff. Um  thanks to Eva on our web site we can now  if you want to run JavaBayes  uh  you could see - get - download these classes. And then it will enable you - she modified the GUI so it has now a m- a m- a button menu item for saving it into the embedded JavaBayes format. So that's wonderful. Mm-hmm. Mmm. Great. And  um and she  a- You tested it out. Do you want to say something about that  that it works  right? With the - I was just checking like  when we wanna  um  get the posterior probability of  like  variables. You know how you asked whether we can  like  just observe all the variables like in the same list ? You can't. You have to make separate queries every time. Uh-huh. O_K  that's - that's a bit unfortunate but So - Yeah. for the time being it's - it's - it's fine to do it - You just have to have a long list of  you know  all the variables. Basically. Yeah. But uh - Uh  all the things you want to query  you just have to like ask for separately. Yeah  yeah. Well that's - probably maybe in the long term that's good news because it forces us to think a little bit more carefully how - how we want to get an out- output. Um  but that's a different discussion for a different time. And  um  I don't know. We're really running late  so I had  uh  an idea yesterday but  uh  I don't know whether we should even start discussing. W- what - Yeah  sure  tell us what it is. Um  the construal bit that  um  has been pointed to but hasn't been  um  made precise by any means  um  may w- may work as follows. I thought that we would  uh - that the following thing would be in- incredibly nice and I have no clue whether it will work at all or nothing. So that's just a tangent  a couple of mental disclaimers here. Um  imagine you - you write a Bayes-net  um - Bayes? Bayes-net  um  completely from scratch every time you do construal. So you have nothing. O_K. Just a white piece of paper. Mmm  right. You consult - consult your ontology which will tell you a bunch of stuff  and parts  and properties  uh-uh-uh Grout out the things that - that you need. Right. then y- you'd simply write  uh  these into - onto your - your white piece of paper. And you will get a lot of notes and stuff out of there. You won't get - you won't really get any C_P_ T's  therefore we need everything that - that configures to what the situation is  I_E  the context dependent stuff. So you get whatever comes from discourse but also filtered. Uh  so only the ontology relevant stuff from the discourse plus the situation and the user model. Mm-hmm. And that fills in your C_P_T's with which you can then query  um  the - the net that you just wrote and find out how thing X_ is construed as an utterance U_. And the embedded JavaBayes works exactly like that  that once you - we have  you know  precise format in which to write it  so we write it down. You query it. You get the result  and you throw it away. And the - the nice thing about this idea is that you don't ever have to sit down and think about it or write about it. You may have some general rules as to how things can be - can be construed as what  so that will allow you to craft the - the - the initial notes. But it's - in that respect it's completely scalable. Because it doesn't have any prior  um  configuration. It's just you need an ontology of the domain and you need the context dependent modules. And if this can be made to work at all  that'd be kind of funky. Um  it sounds to me like you want P_R_Ms. P_R_Ms- uh  P_R_M- I mean  since you can unfold a P_R_M into a straightforward Bayes-net - Beca- because it - b- because - No  no  you can't. See the - the critical thing about the P_R_M is it gives these relations in general form. So once you have instantiated the P_R_M with the instances and ther- then you can - then you can unfold it. Then you can. Mm-hmm  yeah. No  I was m- using it generic. So  uh  probabilistic  whatever  relational models. Whatever you write it. In - Well  no  but it matters a lot because you - what you want are these generalized rules about the way things relate  th- that you then instantiate in each case. And then - then instantiate them. That's ma- maybe the - the way - the only way it works. @@ Yeah  and that's - Yeah  that's the only way it could work. I - we have a - our local expert on P_R_Ms  uh  but my guess is that they're not currently good enough to do that. But we'll - we'll have to see. Uh - But  uh  Yes. This is - that's - that would be a good thing to try. It's related to the Hobbs abduction story in that you th- you throw everything into a pot and you try to come up with the  uh - Except there's no - no theorem prover involved. Best explanation. No  there isn't a theorem prover but there - but - but the  um  The cove- the - the P_R_Ms are like rules of inference and you're - you're coupling a bunch of them together. And then ins- instead of proving you're trying to  you know  compute the most likely. Mm-hmm  yeah. Uh - Tricky. But you - yeah  it's a good - it's a - it's a good thing to put in your thesis proposal. What's it? So are you gonna write something for us before you go? Yes. Um. Oh  you have something. In the process thereof  or whatever. O_K. So  what's - what - when are we gonna meet again? When are you leaving? Thursday  Friday? Fri- uh  Thursday's my last day here. So - Fri- O_K. Yeah. I would suggest as soon as possible. Do you mean by we  the whole ben- gang? N- no  I didn't mean y- just the two of us. We - obviously we can - we can do this. But the question is do you want to  for example  send the little group  uh  a draft of your thesis proposal and get  uh  another session on feedback on that? Or - We can do it Th- Thursday again. Yeah. Fine with me. Should we do the one P_M time for Thursday since we were on that before or - ? Sure. O_K. Alright. Hmm. Thursday at one? I can also maybe then sort of run through the  uh - the talk I have to give at E_M_L which highlights all of our work. O_K. And we can make some last minute changes on that. O_K. You can just give him the abstract that we wrote for the paper. That- that'll tell him exactly what's going on. Yeah  that - Alright. Can we do - can we do one-thirty? No. Oh  you already told me no. One  O_K  it's fine. I can do one. It's fine. But we can do four. It's fine. One or four. I don't care. To me this is equal. I don't care. If it's equal for all? What should we do? Yeah  it's fine. Fine. Yeah - no  no  no  uh  I don't care. It's fine. Four? It's equal to all of us  so you can decide one or four. The pressure's on you Nancy. Liz actually said she likes four because it forces the Meeting Recorder people to cut  you know - the discussions short. O_K. O_K  four. O_K? O_K. I am. Well  if you insist  then. ","This is a relatively short meeting of the Meeting Recorder group  with only a few agenda items. Transcription was discussed briefly because Jane was not present  however this appears to be progressing well in parallel with IBM. Web pages have been set up to show transcription status and to allow participants to approve transcripts. DARPA demos are progressing well with the back-end indexed to allow front-end filtering  and a potential demo ideas investigated which would use X Waves. Transcriber is now working for Windows  however live pitch contours may not work in the time available. Backed-up disk space is now fine  however temporary space is running out fast. Interim measures are discussed while sysadmin are away. Improvement has been made in the final version of the PLP  which shows better female performance  and combined with Mel Ceptra offers 1.5% improvement. Digit performance also improved thanks to training using scripted speech data. Progress has also been made in SRI alignment for tandem system. The group note that the annual report needs to be worked on for next week  and it is also suggested to hold recognition meetings separately  however these issues will be discussed in more detail at the next meeting. Jane will be contacted to see whether transcribers could work on a small amount of SmartKom data  in order to get the project moving. Details of the transcription status web page will be sent to the group. Participants will be contacted to approve transcripts or suggest edits  however this depends upon decisions regarding disk space and (un)compression of audio files. There are disk space problems regarding scratch space  although the group decide that a solution about adding extra drives cannot be resolved without talking to sysadmin. Until they can be contacted  the group will use the hard drives on each other's machines. Displaying pitch contours live in Transcriber would be desirable for the demo  however  if the group run out of time  the group note that they could generate this statically using Powerpoint. The group decide that it may be more beneficial to have separate meetings to discuss recognition  since Jane and Liz are less likely to be interested in this. They will discuss this with them before going ahead. Scratch disk space used to store the uncompressed meetings when they are being processed is getting short - the group are using 98%. Various options are suggested about adding extra drives  externally to their machines  or to the server  but these cannot be resolved until after the relevant sysadmin person returns from a break in 10 days time. Time is getting tight for some of the demo development  especially a lack of Tcl-TK skills slowing up back-end progress. Additionally  time constraints may impinge upon the ""live-ness"" of the Transcriber pitch contour demo. Annual report is due next week  so this will need to be discussed at the next meeting. The group are still waiting for IBM to return transcripts next week. This is not holding the group up  since parallel transcription is being used  and new transcribers have been recruited. The web page detailing transcription status is working and being updated when possible. Backed-up disk space has been seen as a priority by sysadmin  and the group are now only using 30% of this. DARPA demos are progressing  with indexing added to the back-end  which will allow the front end to do filtering. Transcriber has been developed to run on a Windows machine  with further development being undertaken to add a pitch contour display. There is also the potential for a demo to be developed from X Waves. The final version of PLP has been tested which shows similar performance as the system with the Mel Ceptra front-end (although performance for males is slightly  but non-significantly better). Combining the two systems gives a 1.5% improvement in performance. There has also been some improvement with digits  by training the system on read speech data  rather than training solely on TI digits or TIMIT data. Progress is being made with SRI system alignment  with label files converted to train the net. "
"Time. Thanks. Are you Fey? Oh. Hi. I think we've met before  like  I remember talking to you about Aspect or something like that at some point or other. So. I am Fey  yeah. Hi. What day is today? A couple times yeah. That's right  yeah. It's the uh twenty - Nineteenth? nineteenth. And you were my G_S_I briefly  @@ Right  right. until I dropped the class. Oh that's right. But. Well. No offense. Like. O_K  wh- wh- Yeah. O_K. Some in- some introductions are in order. Oh  O_K sorry. Getting ahead of myself. O_K. So. Um. For those who don't know - Everyone knows me  this is great. Um  apart from that  sort of the old gang  Johno and Bhaskara have been with us from - from day one and um Yay! Hi. they're engaged in - in various activities  some of which you will hear about today. Ami is um our counselor and spiritual guidance and um also interested in problems concerning reference of the more complex type  Well. Oh wow. and um he sits in as a interested participant and helper. Is that a good characterization? I don't know. u- That's pretty good  I think. Yeah. Thanks. O_K. Keith is not technically one of us yet  ha-ha. Not yet. ""One of us."" but um it's too late for him now. So. Yeah right. I've got the headset on after all. Um. Officially I guess he will be joining us in the summer. yes. And um hopefully it is by - by means of Keith that we will be able to get a b- a better formal and a better semantic um idea of what a construction is and um how we can make it work for us. Additionally his interest um surpasses um English because it also entails German  an extra capability of speaking and writing and understanding and reading that language. And um  is there anyone who doesn't know Nancy? I made that joke already  Nancy  sadly. The ""I don't know myself"" joke. Do you - do you know Nancy? O_K. Me? Mm-hmm. I know Nancy. What? You did? When? Uh before you came in. Oh. About me or you? Man! About me. O_K. O_K. You could do it about you. Well I didn't know. I didn't mean to be humor copying  but O_K  sorry. Yeah. Yes  I know myself. It's O_K. It's a - O_K. And um Fey is with us as of six days ago officially? Officially  but in reality already um much much longer and um Officially  yeah. um next to some - some more or less bureaucratic uh stuff with the - the data collection she's also the wizard in the data collection Of Oz. Um  we're sticking with the term ""wizard""  O_K. It's very exciting. Yes. Yes. and um Not witch-like. Wizardette. Wizard. Wizardess. Sorceress  I think. O_K. Wizard. wizard uh by by popular vote um O_K. Didn't take a vote? O_K. O_K  um  why don't we get started on that subject anyways. Um  so we're about to collect data and um the uh s- the following things have happened since we last met. When will we three meet again? And um More than three of us. what happened is that um  ""A_""  there was some confusion between you and Jerry with the - that leading to your talking to Catherine Snow  and he was uh he - he agreed completely that some- something confusing happened. Um his idea was to get sort of the l- the lists of mayors of the department  the students. It - it's exactly how you interpreted it  The list of majors in the department? sort of s- M- m- Majors? Majors? O_K  mayor - Something I don't know about these O_K. O_K. Ma- majors  majors. ""Mayors"". Majors. The department has many mayors. Majors and um just sending the - the little write-up that we did on to those email lists uh - Yeah  yeah  yeah. But - Yeah. So it was really Carol Snow who was confused  not Yep  yep  yep. O_K. me and not Jerry. So. So  that is uh - That's good. So I should still do that. Yep. O_K. And And - using the thing that you wrote up. Yep. O_K. Wonderful. And um we have a little description of asking peop- subjects to contact Fey for you know recruiting them for our thing and um there was some confusion as to the consent form  which is basically that - that what what you just signed and since we have one already um - Right. Did Jerry talk to you about maybe using our class? the students in the undergrad class that he's teaching? Um well he said um we - definitely ""yes""  however there is always more people in a - in a facul- uh in a department than are just taking his class or anybody else's class at the moment and one should sort of reach out and try and get them all. e- Yeah. O_K  but th- I guess it's that um people in his class cover a different set so - than the c- is the CogSci department that you were talking about? I guess. See that's what I suggested to him  uh reaching out to? Cuz we have you know people from other Yeah. areas that people like - like Jerry and George and et cetera just - advertise in their classes as well. Yeah or even I could - you know I could do the actual - Cuz I mean I - I know how to contact our students  so if there's something that you're sending out you can also s- um send Mm-hmm. That's generally the way it's done. Yeah. me a copy  me or Bhaskara could - either of us could post it to- uh is it - if it's a general solicitation that you know is just contact you then we can totally pro- post it to the news group so. A mailing list. Mm-hmm. Yeah. Yeah. Do it. Yeah. That's - O_K  so you'll send it or something so . As a matter of fact  if you - if - I can send it. I'll send it  yeah. You can send it to me. O_K. Now  i- Don't worry  we - this doesn't concern you anymore  Robert. It's fine. How - however I suggest that if you - if you look at your email carefully you may think - you may find that you already have it. Oops. Already? Really? Oops. Probab- Maybe. O_K. I don't remember getting anything. W- we'll see. Anyhow  um the uh Yeah  not only CogSci. Also we will talk about Linguistics and of course Computer Science. Mm-hmm. Um and then  secondly  we had  you may remember  um the problem with the re-phrasing  that subject always re-phrase sort of the task that uh we gave them  and so we had a meeting on Friday talking about how to avoid that  and it Right. proved finally fruitful in the sense that we came up with a new scenario for how to get the - the subject m- to really have intentions and sort of to act upon those  and um there the idea is now that next actually we - we need to hire one more person to actually do that job because it - it's getting more complicated. So if you know anyone interested in - in what i'm about to describe  tell that person to - to write a mail to me or Jerry soon  fast. Um the idea now is to sort of come up with a high level of sort of abstract tasks ""go shopping"" um ""take in uh a batch of art"" um ""visit - do some sightseeing"" blah-blah-blah-blah-blah  sort of analogous to what Fey has started in - in - in compiling - compiling here and already - she has already gone to the trouble of - of anchoring it with specific um o- um entities and real world places you will find in Heidelberg. And um. So out of these f- s- these high level categories the subject can pick a couple  such as if - if there is a cop- uh a category in emptying your roll of film  the person can then decide ""O_K  I wanna do that at this place""  sort of make up their own itinerary a- and - and tasks and the person is not allowed to take sort of this h- high level category list with them  but uh the person is able to take notes on a map that we will give him and the map will be a tourist's sort of schematic representation with - with symbols for the objects. And so  the person can maybe make a mental note that ""ah yeah I wanted to go shopping here"" and ""I wanted to maybe take a picture of that"" and ""maybe um eat here"" and then goes in and solves the task with the system  I_E Fey  and um and we're gonna try out that - Any questions? so um y- you'll have those say somewhere what their intention was - so you still have the - the nice thing about having data where you know what the actual intention was? Mm-hmm. Yeah. But they will um - There's nothing that says you know ""these are the things you want to do"" so they'll say ""well these are the things I want to do"" and - Right  so they'll have a little bit more natural interaction? O_K. Hopefully. Mm-hmm. So they'll be given this map  which means that they won't have to like ask the system for in- for like high level information about where things are? Yeah it's a schematic tourist map. So it'll be uh i- it'll still require the - that information and An- It w- it doesn't have like streets on it that would allow them N- not - not - not really the street network. to figure out their way - O_K. Nuh. So you're just saying like what part of town Yeah a- and um the map is more a means for them to have the things are in or whatever? the buildings and their names and maybe some ma- ma- major streets and their names Mm-hmm. and we want to maybe ask them  if you have - get it sort of isolated street the - the  whatever  ""River Street""  and they know that - they have decided that  yes  that's where they want to do this kind of action um that they have it with them and they can actually read them or sort of have the label for the object because it's too hard to memorize all these st- strange German names. And then we're going to have another - we're gonna have w- another trial run I_E the first with that new setup tomorrow at two and we have a real interesting subject which is Ron Kay for who - those who know him  he's the founder of I_C_I. So he'll - he's around seven- seventy years old  or something. I didn't know he was the founder. That's - O_K. And he also approached me and he offered to help um our project and he was more thinking about some high level thinking tasks and I said ""sure we need help you can come in as a subject"" and he said ""O_K"". So that's what's gonna happen  tomorrow  Using this new - new data. New - new set up. um plan  O_K. Yeah. Which I'll hopefully sort of scrape together t- But  thanks to Fey  we already have sort of a nice blueprint and I can work with that. Questions? Comments on that? If not  we can move on. No? No more questions? So what's the s- this is what you I'm not sure I totally understand this but - Hmm? made  Fey? Like so - I'm not sure I totally understand everything that's being talked about but I - I imagine I'll c- just catch on. Um are you familiar with - with the - with the very rough setup of the data? So it's just based on like the materials you had about Heidelberg. Based on the web site  yeah  at the - Oh O_K there's a web site and then you could like um figure out what the cate- O_K. experiment? Right. Uh  this is where they're supposed to - It's a tourist information web site  so. O_K. Talk to a machine and it breaks down and then the human comes on. Yeah. Yeah. The question is just sort of how do we get the tasks in their head that they have an intention of doing something and have a need to ask the system for something without giving them sort of a clear wording or phrasing of the task. O_K. O_K. O_K. O_K. Because what will happen then is that people repeat - repeat  Hmm. or as much as they can  of that phrasing. Um  are you worried about being able to identify - O_K. Um. The - The goals that we've d- you guys have been talking about are this - these you know identifying which of three modes um their question Mm-hmm. uh concerns. So it's like the Enter versus View - Yeah  we - we - we will sort of get a protocol of the prior interaction  right? That's where the instructor  the person we are going to hire  Uh-huh. Uh-huh. Mm-hmm. um and the subjects sit down together with these high level things and so th- the q- first question for the subject is  ""so these are things  you know  we thought a tourist can do. Is there anything that interests you?"" Mm-hmm. And the person can say ""yeah  sure sh- this is something I would do. I would go shopping"". Yeah? and then we can sort of - this s- instructor can say ""well  Mm-hmm. uh then you - you may want to find out how to get over here because this is where the shopping district is"". So the interaction beforehand will give them hints about how specific or how whatever though the kinds of questions that are going to ask during the actual session? No. Just sort of - O_K  what - what - what would you like to buy and then Yeah. um O_K there you wanna buy a whatever cuckoos clocks O_K and the- there is a store there. So the task then for that person is t- finding out how to get there  right? That's sort of what's left. Mm-hmm. Mm-hmm. Mm-hmm. And we know that the intention is to enter because we know that the person wants to buy a cuckoos clock. O_K  that's what I mean so like those tasks are all gonna be um unambiguous about which of the three Hopefully. modes. Right. O_K. So. Well  Hopefully. t- so the idea is to try to get the actual phrasing that they might use and try to interfere as little as possible with their choice of words. Yes. In a sense that's exactly the - the - the idea  which is never possible in a - in a s- in a lab situation  nuh? uh uh Well  u- u- the one experiment th- that - that - that I've read somewhere  it was - they u- used pictures. Yep. Mm-hmm. So to - to uh actually um uh specify the - the tasks. Yeah. Uh  but you know i- We had exactly that on our list of possible way- things so we - uh I even made a sort of a silly i- thing how that could work  how you control you are here you - you want to know how to get someplace  and this is the place and it's a museum and you want to do some- and - and - and there's a person looking at pictures. So  you know  this is exactly getting someplace with the intention of entering and looking at pictures. Right. However  not only was - the common census were - among all participants of Friday's meeting was it's gonna be very laborious to - to make these drawings for each different things  all the different actions  if at all possible  and also people will get caught up in the pictures. So all of a sudden we'll get descriptions of pictures in there. Right. Right. And people talking about pictures and pictorial representations and - Hmm. Right. um I would s- I would still be willing to try it. I mean  I- I'm - I'm not saying it's necessary but - but uh i- uh uh i- you might be able to combine you know text uh and - and some sort of picture and also uh I think it - it will be a good idea to show them the text and kind of chew the task and then take the test away - the - the - the - the - the text away so that they are not uh guided by - by Mm-hmm. Yeah. We will - by what you wrote  but can come up with their - with their own - Yeah  they will have no more linguistic matter in front of them when they enter this room. Right. O_K. Then I suggest we move on to the - to we have um uh the E_D_U Project  let me make one more general remark  has sort of two - two side uh um actions  its um action items that we're do- dealing with  one is modifying the SmartKom parser and the other one is modifying the SmartKom natural language generation module. And um this is not too complicated but I'm just mentioning it - put it in the framework because this is something we will talk about now. Um  I have some news from the generation  do you have news from the parser? Um  not - By that look I - Yes  uh  I would really p- It would be better if I talked about it on Friday. O_K. Yeah  wonderful. Um  did you run into problems or did you run into not h- having time? If that's O_K. Yeah. But not - not any time part . O_K  so that's good. That's better than running into problems. O_K. And um I - I do have some good news for the natural language generation however. And the good news is I guess it's done. Uh  meaning that Tilman Becker  who does the German one  actually took out some time and already did it in English for us. And so the version he's sending us is already producing the English that's needed to get by in version one point one. So I take it that was similar to the - what - what we did for the parsing? Yeah. I - I - it - even though the generator is a little bit more complex and it would have been  not changing one hundred words but maybe four hundred words  but it would have been O_K. O_K. @@ but this - this is I guess good news  and the uh - the time and especially Bhaskara and uh - and um - Oh do I have it here? No. The time is now pretty much fixed. It's the last week of April until the fourth of May so it's twenty-sixth through fourth. That they'll be here? That they'll be here. So it's - it's extremely important that the two of you are also present in this town during that time. Wait  what - what are the days? April twenty-sixth to the - May fourth? Yeah  something like that. I'll probably be here. Yeah. It's - You will be here. There is a d- Isn't finals coming up then pretty much after that? Finals was that. Yeah w- it doesn't really have much meaning to grad students but final projects might. Yeah actually  that's true. O_K. That - Anyway  so this is - Well I'll be here working on something. Guaranteed  it's just uh will I be here  you know  in uh - I'll be here too actually but - Hmm. No it's just um you know they're coming for us so that we can bug them and ask them more questions and sit down together and write sensible code and they can give some nice talks and stuff. But uh Ye- just make a - But it's not like we need to be with them twenty-four hours a day s- for the seven days that they're here. Not - not unless you really really want to. They're very dependent Not unless you really want to. And they're both nice guys so you may - may want to. O_K  that much from the parser and generator side  unless there are more questions on that. So  no sample generator output yet? No. O_K. It - Just a mail that  you know  he's sending me the - the - the stuff soon and I was completely flabbergasted here and I - and that's also it's - it's going to produce the concept-to-speech uh blah-blah-blah information for - necessary for one point one in English - based on the English  you know  in English. So. This is being sent  mm-hmm. O_K. Mm-hmm. I was like ""O_K  We're done. we're done!"" So that was like one of the first l- You know  the first task was getting it working for English. So that's basically over now. Is that right? Yeah. So the basic requirement fulfilled. Um  the basic requirement is fulfilled almost. When Andreas Stolcke and - and his gang  when they have um changed the language model of the recognizer and the dictionary  then we can actually a- put it all together Mm-hmm. Mm-hmm. So the speech recognizer also works. Uh-huh. Mm-hmm. and you can speak into it and ask for T_V and movie information and then when if - if something actually happens and some answers come out  then we're done. Toll. Mm-hmm. If - and they're kind of correct. And they kind of are - are correct. It's not just like anything. So it's not done basically. Hmm? Right. Perhaps if the answers have something to do with the questions for example. And they're mostly in English. So. Then um - Are they - is it using the database? the German T_V movie. O_K. So all the actual data might be German names? Yeah. Um well actually th- um Or are they all like American T_V programs? um well - I want to see ""Die Dukes Von Hazard"" @@ The - O_K  so you don't know how the German dialogue - uh the German - the demo dialogue actually works. It works - the first thing is what's  you know  showing on T_V  and then the person is presented with what's running on T_V in Germany on that day  on that evening Mm-hmm  mm-hmm. and so you take one look at it and then you say ""well that's really nothing - there's nothing for me there"" ""what's running in the cinemas?"" So maybe there's something better happening there. And then you get - you're shown what movies play which films  and it's gonna be of course all the Heidelberg movies and Mm-hmm. what films they are actually showing. And most of them are going to be Hollywood movies. Mm-hmm. Hmm. So  ""American Beauty"" is ""American Beauty""  right? Yeah. Mm-hmm. Right. And um. But they're shown like on a screen. It's a - I mean so would the generator  like the English language sentence of it is - ""these are the follow- you know the following films are being shown"" or something like that? N- Yeah  but it in that sense it doesn't make - In that case uh it doesn't really make sense to read them out loud. S- Right. So it'll just display - O_K. So we don't have to worry about um - if you're displaying them. But uh it'll tell you that this is what's showing in Heidelberg Yeah. and there you go. And the presentation agent will go ""Hhh!"" Nuh? O_K. Like that - the avatar. And um. O_K. And then you pick - pick a movie and - and - and it show- shows you the times and you pick a time and you pick seats and all of this. So. O_K. Pretty straightforward. O_K. But it's - so this time we - we are at an advantage because it was a problem for the German system to incorporate all these English movie titles. Yeah. Nuh? Right. But in English  that's not really a problem  Mm-hmm. unless we get some - some topical German movies that have just come out and that are in their database. Right. So the person may select ""Huehner Rennen"" or whatever. ""Chicken Run"". O_K. Then uh on to the modeling. Right? Yeah  yeah  I guess. Um then modeling  there it is. O_K. What's the next thing? Yep. e- This is very rough but this is sort of what um Johno and I managed to come up with. The idea here is that - This is the uh s- the schema of the X_M_L here  not an example or something like that. Yeah this is not an X_M_L this is sort of towards an - a schema   nuh? definition. O_K. Right. The idea is  so  imagine we have a library of schema such as the Source-Path-Goal and then we have forced uh motion  we have cost action  Mm-hmm. Mm-hmm. we have a whole library of schemas. And they're gonna be  you know  fleshed out in - in their real ugly detail  Source-Path-Goal  and there's gonna be s- a lot of stuff on the Goal and blah-blah-blah  that a goal can be and so forth. What we think is - And all the names could - should be taken ""cum grano salis"". So. This is a - the fact that we're calling this ""action schema"" right now should not entail that we are going to continue calling this ""action schema"". But what that means is we have here first of all on the - in the - in the first iteration a stupid list of Source-Path-Goal actions Actions that can be categorized with - or that are related to Source-Path-Goal. wi- O_K. to that schema Mm-hmm. and we will have you know forced motion and cost action actions. And then those actions can be in multiple categories at the same time if necessary. So a push may be in - in - in both you know push uh in this or this uh - Forced motion and caused action for instance  O_K. Exactly. Yeah. Also  these things may or may not get their own structure in the future. So this is something that  you know  may also be a res- As a result of your work in the future  we may find out that  you know  there're really s- these subtle differences between um even within the domain of entering in the light of a Source-Path-Goal schema  that we need to put in - fill in additional structure up there. But it gives us a nice handle. So with this we can basically um you know s- slaughter the cow any- anyway we want. Uh. It - it is - It was sort of a - it gave us some headache  how do we avoid writing down that we have sort of the Enter Source-Path-Goal that this - But this sort of gets the job done in that respect and maybe it is even conceptually somewhat adequate in a sense that um we're talking about two different things. We're talking more on the sort of intention level  up there  and more on the - this is the - your basic bone um schema  down there. Uh one question  Robert. When you point at the screen is it your shadow that I'm supposed to look at? O_K. Whereas I keep looking where your hand is  Yeah. It's the shadow. and it doesn't - Well  that wouldn't have helped you at all. Right. Yeah. Basically  what this is - is that there's an interface between what we are doing and the action planner Spit right here. and right now the way the interface is ""action go"" and then they have the - what the person claimed was the source and the person claimed as the goal passed on. Mm-hmm. And the problem is  is that the current system does not distinguish between goes of type ""going into""  goes of type ""want to go to a place where I can take a picture of""  et cetera. So this is sort of what it looks like now  some simple ""Go"" action from it - from an object named ""Peter's Kirche"" of the type ""Church"" to an object named ""Powder-Tower"" of the type ""Tower"". This is the uh - what the action planner uses? This is - O_K. Right? Right. Currently. And is that - and tha- that's changeable? or not? Currently. Yeah  well - Like are we adapting to it? Or - No. We - This is the output  sort of  of the natural language understanding  right? Oh  yeah. Uh-huh. the input into the action planning  as it is now. Mm-hmm. Mm-hmm. And what we are going to do  we going to - and you can see here  and again for Johno please - please focus the shadow  O_K. um we're gon- uh uh here you have the action and the domain object and w- and on - on - What did you think he was doing? O_K  sorry. I just - A laser pointer would be most appropriate here I think. Eee. Yeah I - I um have - I have no - Robert likes to be abstract and that's what I just thought he was doing. You look up here. O_K. Sort of between here and here  so as you can see this is on one level and we are going to add another Mm-hmm. um ""Struct""  if you want  I_E a rich action description on that level. So it's just an additional information - So in the future - Exactly. In the future though  the content of a hypothesis will not only be an object and an - an action and a domain object but an action  a domain object  and a rich action description  Right? that doesn't hurt the current way. Mm-hmm. Mm-hmm. which is - Which - which we're abbreviating as "" RAD "". Good. Hmm. Rad! So um you had like an action schema and a Source-Path-Goal schema  right? So how does this Source-Path-Goal schema fit into the uh action schema? Like is it one of the tags there? Hmm. Hmm. Mm-hmm. Yeah can you go back to that one? So the Source-Path-Goal schema in this case  I've - if I understand how we described - we set this up  um cuz we've been arguing about it all week  but uh we'll hold the - the - Well in this case it will hold the - I mean the - the features I guess. I'm not - it's hard for me to exactly s- So basically that will store the - the object that is w- the Source will store the object that we're going from  the Goal will store the - Mm-hmm. So the fillers of the role source. the f- we'll fill those in fill those roles in  right? O_K. Yeah. The S- Action-schemas basically have extra - See we - so those are - schemas exist because in case we need extra information instead of just making it an attribute and which - which is just one thing we - we decided to make it's own entity so that we could explode it out later on in case there is some structure that - that we need to exploit. O_K  so th- sorry I just don't kn- um um um - This is just uh X_M_L mo- notational but um the fact that it's action schema and then sort of slash action schema that's a whole entit- That's a block  That's a block  yeah. whereas source is just an attribute? Is that - No  no  no. Source is just not spelled out here. Source meaning - Source will be uh will have a name  a type  Oh  O_K  O_K. maybe a dimensionality  maybe canonical Uh-huh  uh-huh. uh orientation - O_K could it - it could also be blocked out then as - O_K. Yeah  the - So - Yeah. s- Source it will be  you know we'll f- we know a lot about sources so we'll put all of that in Source. But it's independent whether we are using the S_P_G schema in an Enter  View  or Approach mode  right? This is just properties of the S_P_G schema. O_K. Mm-hmm. We can talk about Paths being the fastest  the quickest  the nicest and so forth  uh or - or - and the Trajector should be coming in there as well. O_K. And then G- the same about Goals. O_K. So I guess the question is when you actually fill one of these out  it'll be under action schema? Those are - It's gonna be one - y- you'll pick one of those for - O_K these are - this is just a layout of the possible that could go - play that role. O_K  go it. Uh-huh. Right. Right  so the - the - the roles will be filled in with the schema and then what actual a- Hmm? O_K. action is O_K. chosen is - will be in the - in the action schema section. S- S- O_K  so one question. This was - in this case it's all um clear  sort of obvious  but you can think of the Enter  View and Approach as each having their roles  right? the - I mean it's - it's implicit that the person that's moving is doing entering viewing and approaching  but you know the usual thing is we have bindings between sort of - they're sort of like action specific roles and the more general Source-Path-Goal specific roles. So are we worrying about that or not for now? O_K. Yes  yes. Since you bring it up now  we will worry about it. Tell us more about it. O_K. What's that? Oh I guess it - What do you - what do you - I - I may be just um reading this and interpreting it into my head in the way that I've always viewed things Hmm. Hmm. and that - that may or may not be what you guys intended. But if it is  then the top block is sort of like um  you know  you have to list exactly what X_schema or in this action schema  there'll be a certain one  that has its own s- structure and maybe it has stuff about that specific to entering or viewing or approaching  but those could include roles like the thing that you're viewing  the thing that you're entering  the thing that you're whatever  you know  So very specific role names are ""viewed thing""  ""entered thing"" - that - which are - think - think of enter  view and approach as frames and they have frame-specific parameters and - and roles Yeah. Mm-hmm. Yeah. and you can also describe them in a general way as Source-Path-Goal schema and maybe there's other image schemas that you could you know add after this that you know  Mm-hmm. how do they work in terms of you know a force dynamics or how do they work in f- terms of other things. So all of those have um Mm-hmm  Mm-hmm  Mm-hmm. basically f- either specific - frame specific roles or more general frame specific roles that might have binding. So the question is are um - how to represent when things are linked in a certain way. So we know for Enter that there's Container potentially involved and it's not - Mm-hmm. uh I don't know if you wanna have in the same level as the action schema S_P_G schema it - it's somewhere in there that you need to represent that there is some container and the interior of it corresponds to some part of the Source-Path-Goal um you know goal - uh goal I guess in this case. So uh is there an easy way in this notation to show when there's identity basically between things and I di- don't know if that's something we need to invent or you know just - Mm-hmm. Yeah. The - wa- wasn't there supposed to be a link in the Right. I don't know if this answers your question  I was just staring at this while you were talking  sorry. It's O_K. Uh a link between the action schema  a field in the s- in the schema for the image schemas that would link us to which action schema we were supposed to use so we could - Yeah. Um  well that's - that's one - one thing is that we can link up  think also that um we can have one or m- as many as we want links from - from the schema up to the s- action um description of it. Hmm. But the notion I got from Nancy's idea was that we may f- find sort of concepts floating around i- in the a- action description of the action f- ""Enter"" frame up there that are  e- when you talk about the real world  actually identical to the goal of the - Exactly. the S- Source-Path-Goal schema  and Right  right. do we have means of - of telling it within that a- and the answer is absolutely. Right. Yeah. The way - we absolutely have those means that are even part of the M_-three-L_ A_ A_P_I  Oh great. s- Uh-huh. meaning we can reference. So meaning - Great. That's exactly what is necessary. And um. This referencing thing however is of temporary nature because sooner or later the W_-three-C_ will be Yeah. St- finished with their X_path  uh  um  specification and then it's going to be even much nicer. Then we have real means of pointing at an individual instantiation of one of our elements here and link it to another one  and this not only within a document but also via documents  Mm-hmm. Mm-hmm. Mm-hmm. O_K. and - and all in a v- very easy e- homogenous framework. So you know - happen to know how - what - what ""sooner or later"" means like in practice? That's @@ but it's soon. So it's g- it's - the spec is there and it's gonna part of the M_-three-L_ A_P - A_P_I filed by the end of this year so that this means we can start using it basically now. But this is a technical detail. Or estimated. O_K  O_K. Mm-hmm. So a pointer - a way to really say pointers. Basically references from the roles in the schema - the bottom schemas to the action schemas is wha- uh I'm assuming. Yeah. O_K  yeah. Yeah. Yeah  I mean personally  I'm looking even more forward O_K. to the day when we're going to have X_ forms  which l- is a form of notation where it allows you to say that if the S_P_G action up there is Enter  Uh-huh. then the goal type can never be a statue. Right. Mm-hmm. So you have constraints that are dependent on the c- actual s- specific filler  uh  Mm-hmm  yeah. W- Yeah e- exactly. Um  of some attribute. Uh-huh. you know this  of course  does not make sense in light of the Statue of Liberty  Right. however it is uh you know sort of - these sort of things are imaginable. Tsk. Hhh. Yeah. Or the Gateway Arch in St. Louis. So. Yeah? S- Yeah. So um  like are you gonna have similar schemas for F_M like forced motion and caused action and stuff like you have for S_P_G? Yeah. And if so like can - are you able to enforce that you know if - if it's - if it's S_P_G action then you have that schema  if it's a forced motion then you have the other schema present in the - Um we have absolute - No. We have absolutely no means of enforcing that  so it would be considered valid if we have an S_P_G action ""Enter"" and no S_P_G schema  but a forced action schema. Could happen. Whi- which is not bad  because I mean  that there's multiple sens- I mean that particular case  there's mult- there - there's a forced side of - of that verb as well. Hmm. It - maybe it means we had nothing to say about the Source-Path-Goal. O_K. What's also nice  and for a- i- for me in my mind it's - it's crucially necessary  is that we can have multiple schemas and multiple action schemas in parallel. Right. And um we started thinking about going through our bakery questions  so when I say ""is there a bakery here?"" you know I do ultimately want our module to be able to first of all f- tell the rest of the system ""hey this person actually wants to go there"" and ""B_""  that person actually wants to buy something to eat there. Nuh? And if these are two different schemas  I_E the Source-Path-Goal schema of getting there and then the buying snacks schema  nuh? - Would they both be listed here in - O_K. Under so o- under action schema there's a list that can include both - Yes. ye- Yeah  they they would - both schemas would appear  so what is the uh is - is there a ""buying s- snacks"" schema? What is the uh - Right. both things. Snack action. That's interesting. What? have the buying snack schema? See. Buying - I'm sure there's a commercial event schema in there somewhere. Oop. buying his food - I- Yeah  a ""commercial event"" or something. Yeah? So uh - d- f- Yeah I - I - so we would - we would instantiate the S_P_G schema with a Source-Path-Goal blah-blah-blah and the buying event I see. Uh-huh. Uh-huh. you know at which - however that looks like  the place f- Interesting. Would you say that the - like - thing to buy. I mean you could have a flat structure and just say these are two independent things  but there's also this sort of like causal  well  so one is really facilitating the other and it's part of a compound action of some kind  which has structure. Yeah. Now it's technically possible that you can fit schema within schema  uh I - I think that's nicer for a lot of reasons but might be a pain so uh - and schema within schemata - um Well  for me it seems that uh - r- I mean there are truly times when you have two totally independent goals that they might express at once  but in this case it's really like Yes. there's a purpo- means that you know f- for achieving some other purpose. Well  if I'm - if I'm recipient of such a message and I get a Source-Path-Goal where the goal is a bakery and then I get a commercial action which takes place in a bakery  right? Uh-huh. and - and - and they - they are obviously  via identifiers  identified to be the same thing here. Yeah. See that - that bothers me that they're the same thing. No  no  just the - Yeah? Yeah because they're two different things one of which is l- you could think of one a sub you know pru- whatever pre-condition for the second. Yeah  yeah! Right. Yeah  yeah. So. So. O_K. So there's like levels of granularity. So uh there's - there's um a single event of which they are both a part. And they're - independently they - they are events which have very different characters as far as Source-Path-Goal whatever. So when you identify Source-Path-Goal and Mm-hmm  yeah. whatever  there's gonna to be a desire  whatever  eating  hunger  whatever other frames you have involved  they have to match up in - in nice ways. So it seems like each of them has its own internal structure and mapping to these Mm-hmm. schemas you know from the other - But you know that's just - That's just me. Like - I - I - Well  I think we're gonna hit a lot of interesting problems and as I prefaced it this is the result of one week of arguing Mm-hmm. about it Between you guys uh O_K. Yeah. and um - and so - Yeah I mean I - I still am not entirely sure that I really fully grasp the syntax of this. You know  like what - Well it's not - it's not actually a very - actually  it doesn't actually - Um it occur - it occurs to me that I mean ne- Right. Or the intended interpretation of this. Yeah. um well I should have - we should have added an ano- an X_M_L example  or some X_M_L examples Yeah. yeah that would be - that would be nice. and - and this is on - on a - on - on my list of things until next - next week. O_K. It's also a question of the recursiveness and - and a hier- hierarchy Yeah. Yeah. um in there. Do we want the schemas just blump blump blump blump? I mean it's - if we can actually you know get it so that we can  out of one utterance  activate more than one schema  I mean  then we're already pretty good  right? Mm-hmm. Well - well you have to be careful with that uh uh thing because uh I mean many actions presuppose some - um almost infinitely many other actions. So if you go to a bakery you have a general intention of uh Yeah. Mayb- yeah. not being hungry. You have a specific intentions to cross the traffic light to get there. You have a further specific intentions to left - to lift your right foot and so uh uh I mean y- you really have to focus on Mm-hmm. Yeah. Hmm? Right. on - on and decide the level of - of abstraction that - that you aim at it kind of zero in on that  and more or less ignore the rest  unless there is some implications Right. Yeah. that - that you want to constant draw from - from sub-tasks um that are relevant M- Th- uh I mean but very difficult. The other thing that I just thought of is that you could want to go to the bakery because you're supposed to meet your friend there or som- you know so Yeah. Mm-hmm. you - like being able to infer the second thing is very useful and probably often right. But having them separate - Well the - the - the utterance was ""is there a bakery around here?""  not Well maybe their friend said they were going to meet them in a bakery around the area. ""I want to go to a bakery."" Right. And I'm  yeah - I'm - I'm inventing contexts which are maybe unlikely  but yeah I mean like - but it's still the case that um you could - you could override that default by giving extra information which is to me a reason why you would keep Right. Sure it - O_K. Yeah. Mm-hmm  yeah. the inference of that separate from the knowledge of ""O_K they really want to know if there's a bakery around here""  which is direct. Yeah. Well there - there - there should never be a hard coded uh shortcut from the bakery question to the uh double schema Right. thing  how uh - And  as a matter of fact  when I have traveled with my friends we make these - exactly these kinds of appointments. Mm-hmm. Yeah. Exactly. It's - I met someone at the bakery you know in the Victoria Station t- you know train station London before  yeah. Mm-hmm. We o- o- Mm-hmm. Right. Yep. Well. I have a question about the slot of the S_P_G action. So the Enter-View-Approach the - the - the EVA um  those are fixed slots in this particular action. Every action of this kind It's like - will have a choice. Or - or - or - or will it just um uh - is it change - Every S_P_G - every S_P_G action either is an Enter or a View or an Approach  right? O_K. Right  right. So - so I - I mean for - Mm-hmm. for each particular action that you may want to characterize you would have some number of slots that define uh uh uh you know in some way what this action is all about. It can be either A_  B_ or C_. Um. So is it a fixed number or - or do you leave it open - it could be between one and fifteen uh - it's - it's - it's flexible. Um  the uh - Well  it sort of depends on - on if you actually write down the - the schema then you have to say it's either one of them or it can be none  or it can be any of them. However the uh - it seems to be sensible to me to r- to view them as mutually exclusive um maybe even not. J- Do you mean within the Source-Path-Goal actions? Those three? uh Yeah. ye- uh uh b- I- uh I - u- I understand uh but - And um how - how where is the end? So that's - No  no. There - a- a- actually by I think my question is simpler than that  um is - O_K  so you have an S_P_G action and - and it has three different um uh aspects um because you can either enter a building or view it or - or approach it and touch it or something. Um now you define uh another action  it's - it's called um Forced action or forced motion. uh s- S_P_G_one action that has to do with writing a letter  let's say. I mean not even within this context  but Yeah. a different action. Um and this - uh action-two would have various variable possibilities of interpreting what you would like to do. And - i- in - in a way similar to either Enter-View-Approach you may want to send a letter  read a letter  or dictate a letter  let's say. Oh the - O_K uh maybe I'd - So  h- The uh - These actions - I don't know if I'm gonna answer your question or not with this  but the categories inside of action schemas  so  S_P_G action is a category. Real- although I think what we're specifying here is this is a category where the actions ""enter  view and approach"" would fall into because they have a related Source-Path-Goal schema in our tourist domain. Cuz viewing in a tourist domain is going up to it and - or actually going from one place to another to take a picture  in this - in a - Right. Oh  s- so it's sort of automatic derived fr- from the structure that - that is built elsewhere. derived I don't know if I u- This is a cate- this a category structure here  right? Action schema. Right. What are some types of action schemas? Well one of the types of action schemas is Source-Path-Goal action. And what are some types of that? And an Enter  Right. Hmm. a View  an Approach. Those are all Source-Path-Goal actions. Inside of Enter there will be roles that can be filled basically. So if I want to go from outside to inside then you'd have the roles that need to filled  where you'd have a Source-Path-Goal set of roles. So you'd the Source would be outside and Path is to the door or whatever  right? Right. So if you wanted to have a new type of action you'd create a new type of category. Then this category would - we would put it - or not necessarily - We would put a new action in the m- uh in the categories that - in which it has the um - Well  every action has a set of related schemas like Source-Path-Goal or force  whatever  right? Mm-hmm. Right. So we would put ""write a letter"" in the categories uh that - in which it had - it w- had uh There could be a communication event action or something like that and you could write it. schemas u- Mm-hmm. Exactly. Schemas uh that of that type. And then later  you know  there - the - we have a communication event action where we'd define it down there as - Hmm. So there's a bit a redundancy  right? in - in which the things that go into a particular - You have categories at the top under action schema and the things that go under a particular category are um supposed to have a corresponding schema definition for that type. So I guess what's the function of having it up there too? I mean I guess I'm wondering whether - You could just have under action schema you could just sort of say whatever you know it's gonna be Enter  View or Approach or whatever number of things and pos- partly because Mm-hmm. you need to know somewhere that those things fall into some categories. And it may be multiple categories as you say which is um the reason why it gets a little messy um but if it has - Yeah. if it's supposed to be categorized in category X_ then the corresponding schema X_ will be among the structures that - that follow. That's like - Right. Yeah. Well  this is one of things we were arguing about. th- this is - this r- this is - this is more - this is probably the way that O_K  sorry. You didn't tell me to - th- that's the way that seemed more intuitive to Johno I guess also for a while - for No  no  no. Uh we have not - we have not seen the light. Uh-huh. But now you guys have seen the light. So. No. I- it's easy to go back and forth isn't it? Uh-huh. The - the reason - One reason we're doing it this way is in case there's extra structure that's in the Enter action that's not captured by the schemas  right? I agree. Right. Right. Which is why I would think you would say Enter and then just say all the things that are relevant specifically to Enter. And then the things that are abstract will be in the abstract things as well. And that's why the bindings become useful. Right  but - Ri- You'd like - so you're saying you could practically turn this structure inside out? or something  or - ? Um No basically w- Ye- I see what you mean by that  but I - I don't if I would - I would need to have t- have that. Get - get rid of the sort of S_P_G slash something Right. uh or the sub-actions category  because what does that tell us? Uh-huh. Yeah. Um and I agree that you know this is something we need to discuss  yeah. I- in fact what you could say is for Enter  you could say ""here  list all the kinds of schemas that - on the category that - List all the parent categories. you know i- list all the parent categories"". It's just like a frame hierarchy  right? like you have these blended frames. So you would say enter and you'd say my parent Yeah. Mm-hmm. frames are such-and-such  h- and then those are the ones that actually you then actually define and say how the roles bind to your specific roles which will probably be f- richer and fuller and have other stuff in there. Yeah. This sounds like a paper I've read around here recently in terms of - Yeah it could be not a coincidence. Like I said  I'm sure I'm just hitting everything with a hammer that I developed  but I mean you know uh it's - Yeah. I'm just telling you what I think  you just hit the button and it's like - And   I guess fr- uh Yeah I mean but there's a good question here. Like  I mean uh do you - When do you need - Hhh. Damn this headset! When you this uh  eh - Metacomment. Yeah. That's all recorded. Um. ""Damn this project."" No just kidding. Why do you - Hhh. I don't know. Like - How do I - how do I come at this question? Um. I just don't see why you would - I mean does th- Who uses this uh - this data structure? You know? Like  do you say ""alright I'm going to uh - do an S_P_G action"". And then you know somebody ne- either the computer or the user says ""alright  well  I know I want to do a Source-Path-Goal action so what are my choices among that? "" And ""oh  O_K  so I can do an Enter-View-Approach"". It's not like that  right? It's more like you say ""I want to  uh - I want to do an Enter."" And then you're more interested in knowing what the parent categories are of that. Well only one of - Right? So that the um - the uh sort of representation that you were just talking about seems more relevant to the kinds of things you would have to do? Hmm. I'd - I- I think I'd - I'm not sure if I understand your question. Only one of those things are gonna be lit up when we pass this on. O_K. So only Enter will be - if we - if our - if our module decided that Enter is the case  O_K. View and Approach will not be there. O_K. Well uh it's - it sort of came into my mind that sometimes even two could be on  and would be interesting. Yeah. um Mayb- nevertheless um l- let's - let's not - Well in that case  we can't - O_K. Well maybe I'm not understanding where this comes from and where this goes to. What are we doing with this? we can't w- @@ if - if - well the thing is if that's the case we - our - I don't think our system can handle that currently. No  not at all. But - U- s- t- In principle. ""Approach and then enter."" So - the - I think the - in some sense we - we ex- Run like this uh - get the task done extremely well because this is exactly the discussion we need - Mm-hmm. need. Period. No more qualifiers than that. So. No  this is the useful  you know  don- don't worry. and um and - and I th- I hope um uh let's make a - a - a - a sharper claim. We will not end this discussion anytime soon. Yeah  I can guarantee that. Sigh. And it's gonna get more and more complex the - the l- complexer and larger our domains get. And I think um we will have all of our points in writing pretty soon. So this is nice about being being recorded also. The um - Right. That's true. The r- uh the - in terms of why is - it's laid out like this versus some other - the people - Yeah. Yeah. um that's kind of a contentious point between the two of us but In my view. this is one wa- so this is a way to link uh the way these roles are filled out to the action. Mm-hmm. Because if we know that Enter is a t- is an S_P_G action  right? Mm-hmm. we know to look for an S_P_G schema and put the appropriate - fill in the appropriate roles later on. Mm-hmm. Yeah. And you could have also indicated that by saying ""Enter  what are the kinds of action I am?"" Mm-hmm  yeah. Right. Yeah. Right? So there's just like sort of reverse organization  right? So like unless @@ - Are there reasons why one is better than the other I mean that come from other sources? Again - Yes because nobod- no- the modules don't - This is - Yeah. uh this is a schema that defines X_M_L messages that are passed from one module to another  Mm-hmm. mainly meaning from the natural language understanding  or from the deep language understanding to the action planner. Mm-hmm. Now the - the reason for - for not using this approach is because you always will have to go back  each module will try - have to go back to look up which uh you know entity can have which uh  you know  entity can have which parents  and then - So you always need the whole body of - of y- your model um to figure out what belongs to what. Or you always send it along with it  nuh? So you always send up ""here I am - I am this person  and I can have these parents"" in every message. Mm-hmm. Mm-hmm. Mm-hmm. O_K  so it's just like a pain to have to send it. which e- It may or may not be a just a pain it's - it's - I'm completely willing to - to - to throw all of this away and completely redo it  O_K  I understand. @@ Well - you know and - and - and it after some iterations we may just do that. Mm-hmm. I - I would just like to ask um like  if it could happen for next time  I mean  just beca- cuz I'm new and I don't really just - I just don't know what to make of this and what this is for  and stuff like that  you know  so if someone could make an example Mm-hmm. Yeah. of what would actually be in it  like first of all what modules are talking to each other using this  right? And - Yeah  we - I will promise for the next time to have fleshed out N_ X_M_L examples for a - a run through and - and see how this - this then translates  O_K. Be great. and how this can come about  nuh? including the sort of ""miracle occurs here"" Right. um part. And um is there more to be said? I think um - In principle what I - I think that this approach does  and e- e- whether or not we take the Enter-View and we all throw up - up the ladder um wha- how do how does Professor Peter call that? The uh Yeah. hhh  silence su- sublimination? Throwing somebody up the stairs? Have you never read the Peter's Principle anyone here? Nope. Oh  uh People reach their level of uh Yeah. max- their level of - at which they're incompetent or whatever. Maximum incompetence and then you can throw them up the stairs Alright. Yeah. Right  right. Oh! um. Promote them  yeah. Yeah. O_K  so we can promote Enter-View all - all up a bit and and get rid of the uh blah-blah-X_blah uh asterisk sub-action item altogether. O_K. No - no problem with that and we - w- we - we will play around with all of them but the principal distinction between having the - the pure schema and their instantiations on the one hand  and adding some whatever  more intention oriented specification um on parallel to that - that - this approach seems to be uh workable to me. I don't know. @@ If you all share that opinion then that made my day much happier. Uh yeah wait - This is a simple way to basically link uh roles to actions. R- Yeah  yeah. That's fine. Sure. That's the - that was the intent of - of it  basically. Uh that's true. Although um roles - Sure. Yeah. So I - I do- I'm - I'm not - Yeah I - I - I'm - I'm never happy when he uses the word ""roles""  I'm - Yeah. I was going to - I b- I mean R_O_L_L_S so Oh you meant pastries  then? Bread rolls? Yeah  pastries is what I'm talking about. Pastry oh ba- oh the bak- bakery example. I see. Bakery. Bakery. This is the bakery example. Got it. Alright. Right. O_K. Help! I guess I'll agree to that  then. O_K. That's all I have for today. Oh no  there's one more issue. Bhaskara brought that one up. Meeting time rescheduling. I n- Didn't you say something about Friday  or - ? Yeah. Hmm. So it looks like you have not been partaking  the Monday at three o'clock time has turned out to be not good anymore. So people have been thinking about an alternative time and the one we came up with is Friday two-thirty? three? What was it? You have class until two  right? so if we don't want him - if we don't want him to run over here Mm-hmm. Two-th- So do I. Yeah. Two-thirty-ish or three or Friday at three or something around that time. Mm-hmm. two thirty-ish or three is - Yeah. Yeah. e- Um how - how are your - That would be good. uh Friday uh Yeah  that's fine. Uh - And I know that you have until three - You're busy? Yeah. So three is - sounds good? Yeah. Yeah. I'll be free by then. I could do that. Yeah I mean earlier on Friday is better but three - you know I mean - if it were a three or a three thirty time then I would take the three or whatever  but yeah sure three is fine. Mm-hmm. Yeah  and you can always make it shortly after three probably. I mean. Yeah  and I don't need to be here particularly deeply. Often  no  but Yeah. uh  whenever. You are more than welcome if you think that this kind of discussion But yeah. It's fascinating. gets you anywhere in - in your life then uh ""That's the right answer."" you're free I'm just glad that I don't have to work it out because. to c- @@ Yeah. Hmm? I'm just glad that don't have to work it out myself  that I'm not involved at all in the working out of it because. Yeah. Uh but you're a linguist. You should - Oh yeah. That's why I'm glad that I'm not involved in working it out. O_K. So it's at Friday at three? there that's @@ And um So already again this week  huh? How diligent do we feel? Yeah. Do feel that we have done our chores for this week or - Yeah. So I mean clearly there's - I can talk about the um the parser changes on Friday O_K  Bhaskara will do the big show on Friday. at least  so. And you guys will argue some more? Yeah. Between now and then. And between now and then yeah. and have some? probably . Promise? We will - r- Yeah. We will. Don't worry. Yeah. Yeah. And we'll get the summary like  this - the c- you know  short version  like - An- and I would like to second Keith's request. An example wo- would be nice t- to have kind of a detailed example. S- Yes. Yes. I've - I've - I've - I guess I'm on record for promising that now. So um - Yeah. O_K. Like have it - we'll have it in writing. So. or  better  speech. So. This is it and um The other good thing about it is Jerry can be on here on Friday and he can weigh in as well. Yeah. and um if you can get that binding point also maybe with a nice example that would be helpful for Johno and me. Oh yeah uh O_K. let's uh yeah they're - Give us - No problem  yeah. I think you've got one on hand  huh? I have several in my head  yeah. Always thinking about binding. Well the - the - the binding is technically no problem but it's - it - for me it seems to be conceptually important that we find out if we can s- if - if there - if there are things @@ in there that are sort of a general nature  we should distill them out and put them where the schemas are. Mm-hmm. If there are things that you know are intention-specific  then we should put them up somewhere  So  in general they'll be bindings across both intentions and the actions. So - a- Yep. That's wonderful. Yeah. So it's gen- it's general across all of these things it's like - Yeah. I mean Shastri would say you know binding is like an essential cognitive uh process. So. Um. O_K. So I don't think it will be isolated to one or the two  but you can definitely figure out where - Yeah  sometimes things belong and - So actually I'm not sure - I would be curious to see how separate the intention part and the action part are in the system. Like I know the whole thing is like intention lattice  or something like that  right? So Mm-hmm. is the ri- right now are the ideas the rich - rich the R_A_D or whatever is one you know potential block inside intention. It's still - Yeah. it's still mainly intention hypothesis and then that's just one way to describe the - the action part of it. O_K. Yeah. Yeah. It's - It's an a- attempt to refine it basically. And yeah  it's an - an - it's - it's sort of - O_K  great uh-huh. Not just that you want to go from here to here  it's that the action is what you intend and this action consists of all com- complicated modules and image schemas and whatever. So. Yeah. Yeah. And - and there will be a - a - a relatively high level of redundancy in the sense that Mm-hmm. which is  yeah  It's fine um ultimately one - so th- so that if we want to get really cocky we we will say ""well if you really look at it  you just need our RAD ."" You can throw the rest away  Mm-hmm. right? Right. Because you're not gonna get anymore information out of Right. the action a- as you find it there in the domain object. Mm-hmm. But then again um in this case  the domain object may contain information that we don't really care about either. Mm-hmm. So. H- But w- we'll see that then  and how - how it sort of evolves. Mm-hmm. I mean if - if people really like our - our RAD  I mean w- what might happen is that they will get rid of that action thing completely  you know  and leave it up for us to get the parser input Mmm. We know the things that make use of this thing so that we can just change them so that they make use of RAD. um Yeah. Yeah. I can't believe we're using this term. So I'm like You don't have to use the acronym. RAD! Like every time I say it  it's horrible. O_K. Mm-hmm. I see what you mean. Is the - RAD's a great term. But what is the ""why""? It's rad  even! Why? Why? It happened to c- be what it stands for. Well - It just happened to be the acronym. Yeah. That's - doesn't make it a great term. It's just like those jokes where you have to work on both levels. Do you see what I mean? Like ye- no but i- but if you - if you - if you work in th- in that X_M_L community it is a great acronym because it e- evokes Just think of it as - as ""wheel"" in German. Oh. whatever R_D_F - R_D_F is the biggest thing right? That's the rich - sort of ""Resource Description Framework"" Oh ""rich de-"" Oh. and um - and also - So  description  having the word d- term ""description"" in there is wonderful  Mm-hmm. uh ""rich"" is also great  rwww. Hmm. Who doesn't like to be a- Oh. Everybody likes action. Yeah. O_K. Yeah. Plus it's hip. The kids'll like it. But what if it's not an action? Hmm. Yeah all the kids'll love it. It's - it's rad  yeah. And intentions will be "" RID ""? Like  ""O_K"". Um are the - are the sample data that you guys showed sometime ago - like the things - maybe - maybe you're gonna run a trial tomorrow. I mean  I'm just wondering whether the ac- some the actual sentences from this domain will be available. Cuz it'd be nice for me to like look if I'm thinking about examples I'm mostly looking at child language which you know will have some overlap but not total with the kinds of things that you guys are getting. So you showed some in this - Mm-hmm. here before and maybe you've posted it before but where would I look if I want to see? Oh I - You want audio? or do you want transcript? You know. No just - just transcript. Yeah  well just transcript is just not available because nobody has transcribed it yet. Sorry. Oh  O_K. Um I can e- I can uh I'll transcribe it though. It's no problem. I take that back then. O_K  well don't - don't make it a high priority - I - In fact if you just tell me like Yeah. you know like two examples I mean  y- Mm-hmm. The - the - the representational problems are - I'm sure  will be there  like enough for me to think about. So. O_K. O_K  so Friday  whoever wants and comes  and can. O_K. O_K. Here. O_K. This Friday. The big parser show. Now you can all turn off your - ",The Berkeley Even Deeper Understanding group discussed plans and concerns regarding the architecture of SmartKom  its proposed modules  and the types of interactions expected to take place between modules. The meeting was largely focused on SmartKom's decision making capacity and how to adapt this functionality to the tourist information domain. The group set a date for assessing SmartKom plans. It was decided that SmartKom's action plans should be represented in XML as a state transition network. It was proposed that the term 'dialogue planner' should replace 'dialogue manager'. Prolog will be phased out completely and replaced by Java code. The dialogue manager must be capable of changing states  i.e. go from being event driven to answering a question from a planning module. SmartKom should feature a well defined core interface  with domain-specific information kept external. A syntactic analysis component that performs chunk parsing will be added to the system. As a functional module  the action planner is too restrictive for the tourist domain and requires complex slots from the dialogue manager. What form will the language input have  and what will the action planner do with it? Links must be in place between the input end  action planner  parser  and language feedback components for communicating the current state of plan. Interactions in a deep map system between the spatial planner and the route planner are too convoluted. SmartKom requires a fast and robust parser that includes language-specific extensions. Which form of semantic construction grammar should be used  and how would such information be derived from the parsed input? Efforts are in progress to complete and test the code  generate an English grammar like that used in the German system  and get the parser interface working. A 'wizard of Oz' style data collection experiment is in progress to model users' underlying intentions when communicating with the dialogue component of a tourist domain GPS. Corresponding interfaces and a belief net will be incorporated into the knowledge modelling module. While the current focus is on decision making  a fuller implementation of SmartKom will enable it to query the user for desired information. Stemming information is connected to the lexicon through the use of knowledge bases from Verbmobil. 
"We're going? O_K. O_K. Sh- Close your door on - door on the way out? Thanks. Thanks. Oh. Yeah. Probably wanna get this other door  too. O_K. So. Um. What are we talking about today? Uh  well  first there are perhaps these uh Meeting Recorder digits Oh  yeah. That was kind of uh interesting. The - both the uh - that we tested. So. Um. the S_R_I System and the oth- And for one thing that - that sure shows the difference between having a lot of uh Of data? training data Yeah. or not  uh  the uh - The best kind of number we have on the English uh - on near microphone only is - is uh three or four percent. Mm-hmm. And uh it's significantly better than that  using fairly simple front-ends on - on the uh - Mm-hmm. uh  with the S_R_I system. So I th- I think that the uh - But that's - that's using uh a - a pretty huge amount of data  mostly not digits  of course  but - but then again - Well  yeah. In fact   mostly not digits for the actual training the H_M_ Ms whereas uh in this case we're just using digits for training the H_M_Ms. Yeah. Right. Did anybody mention about whether the - the S_R_I system is a - is - is doing the digits um the wor- as a word model or as uh a sub- s- sub-phone states? I guess it's - it's uh allophone models  so  well - Yeah. Probably. Huh? Yeah. I think so  because it's their very d- huge  their huge system. Yeah. And. But. So. There is one difference - Well  the S_R_I system - the result for the S_R_I system that are represented here are with adaptation. So there is - It's their complete system and - including on-line uh unsupervised adaptation. That's true. And if you don't use adaptation  the error rate is around fifty percent worse  I think  if I remember. Yeah. O_K. It's tha- it's that much  huh? Nnn. It's - Yeah. It's quite significant. Yeah. Oh. O_K. Still. Mm-hmm. But - but uh what - what I think I'd be interested to do given that  is that we - we should uh take - I guess that somebody's gonna do this  right? - is to take some of these tandem things and feed it into the S_R_I system  right? Yeah. Yeah. We can do something like that . Yeah. Yeah. Because - But - But I guess the main point is the data because uh I am not sure. Our back-end is - is fairly simple but until now  well  the attempts to improve it or - have fail- Ah  well  I mean uh what Chuck tried to - to - to do Yeah  but he's doing it with the same data  right? I mean so to - Yeah. So it's - Yeah. So there's - there's - there's two things being affected. I mean. One is that - that  you know  there's something simple that's wrong with the back-end. We've been playing a number of states uh I - I don't know if he got to the point of playing with the uh number of Gaussians yet but - but uh  Mm-hmm. Mm-hmm. uh  you know. But  yeah  so far he hadn't gotten any big improvement  but that's all with the same amount of data which is pretty small. Mm-hmm. Yeah. Mmm. And um. So  yeah  we could retrain some of these tandem Well  you could do that  but I'm saying even with it not - with that part not retrained  on - on huge - Ah  yeah. Just - just - just using - having the H_M_Ms - f- for the H_M_M models. Yeah. Mm-hmm. much better H_M_ Ms. Yeah. Mm-hmm. Um. But just train those H_M_Ms using different features  the features coming from our Aurora stuff. So. Yeah. Yeah. But what would be interesting to see also is what - what - perhaps it's not related  the amount of data but the um recording conditions. I don't know. Because it's probably not a problem of noise  because our features are supposed to be robust to noise. Well  yeah. It's not a problem of channel  because there is um normalization with respect to the channel. So - I - I - I'm sorry. What - what is the problem that you're trying to explain? The - the fact that - the result with the tandem and Aurora system are That the - Oh. So much worse? uh so much worse. Yeah. Oh. I uh but I'm - I'm almost certain that it - it - It - I mean  that it has to do with the um amount of training data. It - it's - it's orders of magnitude off. Yeah but - Yeah. Yeah but we train only on digits and it's - it's a digit task  so. Well. But - but having a huge - If - It - if you look at what commercial places do  they use a huge amount of data. This is a modest amount of data. Mm-hmm. Alright. Yeah. Mm-hmm. So. I mean  ordinarily you would say ""well  given that you have enough occurrences of the digits  you can just train with digits rather than with  you know"" - Mm-hmm. But the thing is  if you have a huge - in other words  do word models - But if you have a huge amount of data Right. Mmm. then you're going to have many occurrences of similar uh allophones. Yeah. And that's just a huge amount of training for it. So it's um - Mm-hmm. I - I think it has to be that  because  as you say  this is  you know  this is near-microphone  it's really pretty clean data. Mm-hmm. Um. Now  some of it could be the fact that uh - let's see  in the - in these multi-train things did we include noisy data in the Yeah. training? I mean  that could be hurting us actually  for the clean case. Yeah. Well  actually we see that the clean train for the Aurora proposals are - It is if - are better than the multi-train  yeah. Yeah. Yeah. Cuz this is clean data  and so that's not too surprising. Mm-hmm. But um. Uh. So. Well  o- I guess what I meant is that well  let's say if we - if we add enough data to train on the um Uh-huh. on the Meeting Recorder digits  Mm-hmm. I guess we could have better results than this. And. What I meant is that perhaps we can learn something uh from this  what's - what's wrong uh what - what is different between T_I-digits and these digits and - What kind of numbers are we getting on T_I-digits? It's point eight percent  so. Oh. I see. Four- Fourier. @@ So in the actual T_I-digits database we're getting point eight percent  Yeah. Yeah. and here we're getting three or four - three  let's see  three for this? Mm-hmm. Yeah. Sure  but I mean  um point eight percent is something like double uh or triple what people have gotten who've worked very hard at doing that. And - and also  as you point out  there's adaptation in these numbers also. Mm-hmm. Mmm. So if you  you know  put the ad- adap- take the adaptation off  then it - for the English-Near you get something like two percent. And here you had  you know  something like three point four. Mm-hmm. And I could easily see that difference coming from this huge amount of data that it was trained on. So it's - Mm-hmm. You know  I don't think there's anything magical here. It's  you know  we used a simple H_T_K system with a modest amount of data. And this is a - a  you know  modern uh system uh has - has a lot of nice points to it. Yeah. Yeah. Mm-hmm. Um. So. I mean  the H_T_K is an older H_T_K  even. So. Mm-hmm. Yeah it - it's not that surprising. But to me it just - it just meant a practical point that um if we want to publish results on digits that - that people pay attention to we probably should uh - Cuz we've had the problem before that you get - show some nice improvement on something that's - that's uh  uh - it seems like too large a number  and uh uh people don't necessarily take it so seriously. Mm-hmm. Um. Yeah. Yeah. So the three point four percent for this uh is - is uh - So why is it - It's an interesting question though  still. Why is - why is it three point four percent for the d- the digits recorded in this environment as opposed to the uh point eight percent for - for - for the original T_I-digits database? Um. Yeah. th- that's - th- that's my point I - I - I don't Given - given the same - Yeah. So ignore - ignoring the - the - the S_R_I system for a moment  just looking at I - Mm-hmm. the T_I-di- the uh tandem system  if we're getting point eight percent  which  yes  it's high. It's  you know  it - it's not awfully high  but it's  you know - it's - it's high. Mm-hmm. Um. Why is it uh four times as high  or more? Yeah  I guess. Right? I mean  there's - even though it's close-miked there's still - there really is background noise. Mm-hmm. Um. And uh I suspect when the T_I-digits were recorded if somebody fumbled or said something wrong or something that they probably made them take it over. Mm-hmm. It was not - I mean there was no attempt to have it be realistic in any - in any sense at all. Well. Yeah. And acoustically  it's q- it's - I listened. It's quite different. T_I-digit is - it's very  very clean and it's like studio recording Mm-hmm. whereas these Meeting Recorder digits sometimes you have breath noise and Mmm. Right. Yeah. So I think they were - It's not controlled at all  I mean. Bless you. Thanks. I - Yeah. I think it's - it's - Mm-hmm. But- So. Yes. It's - I think it's - it's the indication it's harder. Yeah. Uh. Yeah and again  you know  i- that's true either way. I mean so take a look at the uh - um  the S_R_I results. I mean  they're much much better  but still you're getting something like one point three percent Mm-hmm. for uh things that are same data as in T_ - T_I-digits the same - same text. Uh. And uh  I'm sure the same - same system would - would get  you know  point - point three or point four or something on the actual T_I-digits. So this - I think  on both systems the these digits are showing up as harder. Mmm. Um. Mm-hmm. Which I find sort of interesting cause I think this is closer to - uh I mean it's still read. But I still think it's much closer to - to what - what people actually face  um when they're - they're dealing with people saying digits over the telephone. I mean. I don't think uh - I mean  I'm sure they wouldn't release the numbers  but I don't think that uh the uh - the - the companies that - that do telephone speech get anything like point four percent on their digits. I'm - I'm - I'm sure they get - Uh  I mean  for one thing people do phone up who don't have uh uh Middle America accents and Mm-hmm. it's a we- we it's - it's - it's U_S. it has - has many people who sound in many different ways. So. Um. I mean. O_K. That was that topic. What else we got? Um. But - Did we end up giving up on - on  any Eurospeech submissions  or - ? I know Thilo and Dan Ellis are - are submitting something  but uh. Yeah. I - I guess e- the only thing with these - the Meeting Recorder and  well  - So  I think  yeah - I think we basically gave up. Um. But - Now  actually for the - for the Aur- uh we do have stuff for Aurora  right? Because - because we have ano- an extra month or something. Yeah. Yeah. Yeah. So. Yeah  for sure we will Yeah. do something for Well  that's fine. So th- so - so we have a couple - a couple little things on Meeting Recorder and we have - the special session. Yeah. Mm-hmm. We don't - we don't have to flood it with papers. We're not trying to prove anything to anybody. so. That's fine. Um. Anything else? Yeah. Well. So. Perhaps the point is that we've been working on is  yeah  we have put the um the good V_A_D in the system and it really makes a huge difference. Um. So  yeah. I think  yeah  this is perhaps one of the reason why our system was not - not the best  because with the new V_A_D  it's very - the results are similar to the France Telecom results and Hmm. perhaps even better sometimes. Huh. Um. So there is this point. Uh. The problem is that it's very big and we still have to think how to - where to put it and - um  Mm-hmm. because it - it - well  this V_A_D uh either some delay and we - if we put it on the server side  it doesn't work  because on the server side features you already have L_D_A applied from the f- from the terminal side and so you accumulate the delay so the V_A_D should be before the L_D_A which means perhaps on the terminal side and then smaller and So wha- where did this good V_A_D come from? So. It's um from O_G_I. So it's the network trained - it's the network with the huge amounts on hidden - of hidden units  and um nine input frames compared to the V_A_D that was in the proposal which has a very small amount of hidden units and fewer inputs. This is the one they had originally? Oh. Yeah. Yeah  but they had to get rid of it because of the space  didn't they? Yeah. So. Yeah. But the abso- assumption is that we will be able to make a V_A_D that's small and that works fine. And. Well. So that's a problem. Yeah. So we can - Yeah but - nnn. But the other thing is uh to use a different V_A_D entirely. I mean  uh i- if - if there's a if - if - I - I don't know what the thinking was amongst the - the - the the ETSI folk but um Mm-hmm. if everybody agreed sure let's use this V_A_D and take that out of there - Mm-hmm. They just want  apparently - they don't want to fix the V_A_D because they think there is some interaction between feature extraction and - and V_A_D or frame dropping But they still want to - just to give some um requirement for this V_A_D because it's - it will not be part of - they don't want it to be part of the standard. O_K. So. So it must be at least uh somewhat fixed but not completely. So there just will be some requirements that are still not - uh not yet uh ready I think. Determined. I see. Nnn. But I was thinking that - that uh s ""Sure  there may be some interaction  but I don't think we need to be stuck on using our or O_G_I's V_A_D. We could use somebody else's if it's smaller or - Yeah. You know  as long as it did the job. Mm-hmm. So that's good. Uh. So there is this thing. There is um - Yeah. Uh I designed a new - a new filter because when I designed other filters with shorter delay from the L_D_A filters  there was one filter with fif- sixty millisecond delay and the other with ten milliseconds and Right. uh Hynek suggested that both could have sixty-five sixty-s- I think it's sixty-five. Yeah. Yeah. Both should have sixty-five because - Yeah. You didn't gain anything  right? And. So I did that and uh it's running. So  let's see what will happen. Uh but the filter is of course closer to the reference filter. Mm-hmm. Mmm. Um. Yeah. I think - So that means logically  in principle  it should be better. So probably it'll be worse. Yeah Or in the basic perverse nature uh of reality. Yeah. O_K. Yeah. Sure. Yeah. O_K. Yeah  and then we've started to work with this of um voiced-unvoiced Mm-hmm. stuff. And next week I think we will perhaps try to have um a new system with uh uh M_S_G stream also see what - what happens. So  something that's similar to the proposal too  but with M_S_G stream. Mm-hmm. Mm-hmm. Mmm. O_K. No  I w- I begin to play with Matlab and to found some parameter robust for voiced-unvoiced decision. But only to play. And we - they - we found that maybe w- is a classical parameter  the sq- the variance between the um F_F_T of the signal and the small spectrum of time we - after the um mel filter bank. Uh-huh. And  well  is more or less robust. Is good for clean speech. Is quite good Huh? for noisy speech. Mm-hmm. but um we must to have bigger statistic with TIMIT  and is not ready yet Mm-hmm. Yeah. to use on  well  I don't know. Yeah. Yeah. So  basically we wa- want to look at something like the ex- the ex- excitation signal and - Right. Mm-hmm. which are the variance of it and - I have here. I have here Mmm. for one signal  for one frame. Yeah. The - the mix of the two  noise and unnoise  and the signal is this. Uh-huh. Clean  and this noise. Uh. These are the two - the mixed  the big signal is for clean. Well  I'm s- uh - There's - None of these axes are labeled  so I don't know what this - What's this axis? Uh this is uh - this axis is nnn  ""frame"". Frame. Mm-hmm. And what's th- what this? Uh  this is uh energy  log-energy of the spectrum. Of the - No  this is the variance  the difference between the spectrum of the signal and F_F_T of each frame of the signal and this mouth spectrum of time after the f- For this one. may fit For the noi- for the two  this big  to here  they are to signal. This is for clean and this is for noise. Oh. There's two things on the same graph. Yeah. I don't know. I - I think that I have d- another graph  but I'm not sure. Yeah. So w- which is clean and which is noise? I think the lower one is noise. The lower is noise and the height is clean. O_K. So it's harder to distinguish It's height. but it - but it g- with noise of course but - but - Yeah. Oh. I must to have. Pity  but I don't have Uh. two different And presumably when there's a - a - So this should the - the - the t- voiced Uh-huh. Yeah  it is the height portions. is voiced portion. The p- the peaks should be voiced portion. And this is the noise portion. Uh-huh. And this is more or less like this. But I meant to have see @@ two - two the picture. Yeah. Yeah. This is  for example  for one frame. Yeah the - the spectrum of the signal. And this is the small version of the spectrum after M_L mel filter bank. Yeah. And this is the difference? And this is I don't know. This is not the different. This is trying to obtain with L_P_C model the spectrum but using Matlab without going factor and s- No pre-emphasis? Yeah. Not pre-emphasis. Nothing. Yeah so it's - doesn't do too well there. And the - I think that this is good. This is quite similar. this is - this is another frame. ho- how I obtained the envelope  this envelope  with the mel filter bank. Right. So now I wonder - I mean  do you want to - I know you want to get at something orthogonal from what you get with the smooth spectrum Um. But if you were to really try and get a voiced-unvoiced  do you - do you want to totally ignore that? I mean  do you - do you - I mean  clearly a - a very big - very big cues for voiced-unvoiced come from uh spectral slope and so on  right? Mm-hmm. Um. Yeah. Well  this would be - this would be perhaps an additional parameter  simply isn't - Yeah. I see. Yeah. Yeah because when did noise Uh. clear in these section is clear Mm-hmm. if s- @@ val- value is indicative that is a voice frame and it's low values @@ Yeah. Yeah. Well  you probably want - I mean  certainly if you want to do good voiced-unvoiced detection  you need a few features. Each - each feature is by itself not enough. But  you know  people look at - at slope and uh Mmm. first auto-correlation coefficient  divided by power. Or - or uh um there's uh - I guess we prob- probably don't have enough computation to do a simple pitch detector or something? I mean with a pitch detector you could have a - Mmm. have a - an estimate of - of what the - Uh. Or maybe you could you just do it going through the P_ F_F_T's figuring out some um probable um harmonic structure. Right. And - and uh. Mmm. you have read up and - you have a paper  the paper that you s- give me yesterday. Oh  yeah. But - they say that yesterday they are some problem Yeah  but it's not - it's  yeah  it's - it's another problem. Yeah and the - Is another problem. Um. Yeah  there is th- this fact actually. If you look at this um spectrum  Yeah. What's this again? Is it the mel-filters? Yeah like this. Of kind like this. Yeah. O_K. So the envelope here is the output of the mel-filters Mm-hmm. and what we clearly see is that in some cases  and it clearly appears here  and the - the harmonics are resolved by the f- Well  there are still appear after mel-filtering  Mm-hmm. and it happens for high pitched voice because the width of the lower frequency mel-filters is sometimes even smaller than the pitch. Yeah. It's around one hundred  one hundred and fifty hertz Right. Nnn. And so what happens is that this uh  add additional variability to this envelope and Yeah. um so we were thinking to modify the mel-spectrum to have something that - that's smoother on low frequencies. That's as - as a separate thing. Yeah. i- Yeah. This is a separate thing. Yeah. Separate thing? Yeah. And. Yeah. Maybe so. Um. Yeah. So  what - Yeah. What I was talking about was just  starting with the F_F_T you could - you could uh do a very rough thing to estimate - estimate uh pitch. Yeah. Mm-hmm. And uh uh  given - you know  given that  uh you could uh uh come up with some kind of estimate of how much of the low frequency energy was - was explained by - by uh Mm-hmm. uh those harmonics. Uh. It's uh a variant on what you're s- what you're doing . The - I mean  the - the the mel does give a smooth thing. But as you say it's not that smooth here. And - and so if you - if you just you know subtracted off uh your guess of the harmonics then something like this would end up with quite a bit lower energy in the first fifteen hundred hertz or so and - Mm-hmm. and our first kilohertz  even. And um if was uh noisy  the proportion that it would go down would be if it was - if it was unvoiced or something. Mm-hmm. So you oughta be able to pick out voiced segments. At least it should be another - another cue. Mm-hmm. So. Anyway. O_K? That's what's going on. Uh. What's up with you? Um our t- I went to talk with uh Mike Jordan this - this week Mm-hmm. um and uh shared with him the ideas about um extending the Larry Saul work and um I asked him some questions about factorial H_M_Ms so like later down the line when we've come up with these - these feature detectors  how do we - how do we uh you know  uh model the time series that - that happens um and and we talked a little bit about factorial H_M_Ms and how um when you're doing inference - or w- when you're doing recognition  there's like simple Viterbi stuff that you can do for - for these H_M_Ms and the uh - the great advantages that um a lot of times the factorial H_M_Ms don't um don't over- alert the problem there they have a limited number of parameters and they focus directly on - on uh the sub-problems at hand so you can imagine um five or so parallel um features um transitioning independently and then at the end you - you uh couple these factorial H_M_Ms with uh - with uh undirected links um based on - Hmm. based on some more data. So he - he seemed - he seemed like really interested in - in um - in this and said - said this is - this is something very do-able and can learn a lot and um yeah  I've just been continue reading um about certain things. Mm-hmm. um thinking of maybe using um um m- modulation spectrum stuff to um - as features um also in the - in the sub-bands because Mm-hmm. it seems like the modulation um spectrum tells you a lot about the intelligibility of - of certain um words and stuff So  um. Yeah. Just that's about it. O_K. O_K. And um so I've been looking at Avendano's work and um uh I'll try to write up in my next stat- status report a nice description of what he's doing  but it's - it's an approach to deal with reverberation or that - the aspect of his work that I'm interested in the idea is that um normally an- analysis frames are um too short to encompass reverberation effects um in full. You miss most of the reverberation tail in a ten millisecond window and so you - you'd like it to be that um the reverberation responses um simply convolved um in  but it's not really with these ten millisecond frames cuz you j- But if you take  say  a two millisecond um window - I'm sorry a two second window then in a room like this  most of the reverberation response is included in the window and the - then it um then things are l- more linear. It is - it is more like the reverberation response is simply c- convolved and um - and you can use channel normalization techniques like uh in his thesis he's assuming that the reverberation response is fixed. He just does um mean subtraction  which is like removing the D_C component of the modulation spectrum and that's supposed to d- um deal - uh deal pretty well with the um reverberation and um the neat thing is you can't take these two second frames and feed them to a speech recognizer um so he does this um method training trading the um the spectral resolution for time resolution and um come ca- uh synthesizes a new representation which is with say ten second frames but a lower s- um frequency resolution. So I don't really know the theory. I guess it's - these are called ""time frequency representations"" and h- he's making the - the time sh- um finer grained and the frequency resolution um less fine grained. Mm-hmm. s- so I'm - I guess my first stab actually in continuing his work is to um re-implement this - this thing which um changes the time and frequency resolutions cuz he doesn't have code for me. So that that'll take some reading about the theory. I don't really know the theory. Mm-hmm. Oh  and um  another f- first step is um  so the - the way I want to extend his work is make it able to deal with a time varying reverberation response um and um we don't really know how fast the um - the reverberation response is varying the Meeting Recorder data um so um we - we have this um block least squares um imp- echo canceller implementation and um I want to try finding the - the response  say  between a near mike and the table mike for someone using the echo canceller and looking at the echo canceller taps and then see how fast that varies from block to block. That should give an idea of how fast the reverberation response is changing. Mm-hmm. Mm-hmm. O_K. Um. I think we're sort of done. Yeah. So let's read our digits and go home. O_K. I'm reading transcript L_ dash forty three four nine one one O_ five one eight three six two O_ five seven O_ four one nine six O_ one four four nine five seven three seven six one four two O_ five two seven eight nine nine three six six three eight nine seven one seven eight three one nine three O_ one nine five seven five five one eight zero eight two nine eight four six one nine four eight one two Um. S- so um y- you do - I think you read some of the - the zeros as O_'s and some as zeros. Yeah. Is there a particular way we're supposed to read them? There are only zeros here. Well. No. ""O_"" - ""O_"" - ""O_"" and ""zero"" are two ways that we say that digit. Eee. Yeah. So it's - But - Ha! so it's - i- Perhaps in the sheets there should be another sign for the - if we want to - the - the guy to say ""O_"" or No. I mean. I think people will do what they say. It's O_K . It's - Yeah. O_K. Alright. I mean in digit recognition we've done before  you have - you have two pronunciations for that value  ""O_"" and ""zero"". O_K. But it's perhaps more difficult for the people to prepare the database then  if - because here you only have zeros and - and people pronounce ""O_"" or zero - No  they just write - they - they write down O_H. or they write down Z_E_R_O a- and they - and they each have their own pronunciation. Yeah but if the sh- the sheet was prepared with a different sign for the ""O_"". But people wouldn't know what that wa- I mean there is no convention for it. O_K. Yeah. O_K. See. I mean  you'd have to tell them ""O_K when we write this  say it tha-""  you know  and you just - They just want people to read the digits as you ordinarily would and - and people Mm-hmm. Yeah. Yep. say it different ways. O_K. Is this a change from the last batch of - of um forms? Because in the last batch it was spelled out which one you should read. Yeah  it was orthographic  so. Yes. That's right. It was - it was spelled out  and they decided they wanted to get at more the way people would really say things. That's also why they're - they're bunched together in these different groups. So - so it's - Yeah. So it's - it's - Everything's fine. Oh. O_K. O_K. O_K. O_K. Uh. Transcript L_ dash three nine. one three two six one zero one four two four seven five nine three eight seven two six two six two seven six seven three four two two two four two nine six four O_ four O_ eight eight two eight seven nine nine four O_ O_ eight two seven eight zero three nine five one two three eight four three five five nine eight one four two zero two zero nine two nine two six Actually  let me just s- since - since you brought it up  I was just - it was hard not to be self-conscious about that when it after we - since we just discussed it. But I realized that - that um when I'm talking on the phone  certainly  and - and saying these numbers  I almost always say zero. And uh - cuz - because uh i- it's two syllables. It's - it's more likely they'll understand what I said. So that - that - that's the habit I'm in  but some people say ""O_"" and - Yeah I normally say ""O_"" cuz it's easier to say. Yeah it's shorter. Yeah. So it's - So. So uh. Now  don't think about it. ""O_"" Oh  no! O_K. I'm reading transcript L_ thirty eight five four five O_ three two eight five eight three three eight nine O_ four one O_ nine eight five O_ seven one one one four O_ two one six one eight two five six seven eight five seven six eight two O_ O_ O_ four seven O_ O_ five eight seven seven seven five six five six one three seven one nine one three four three six O_ O_ nine nine two two O_ I'm reading transcript L_ dash thirty seven. O_ five one nine O_ three two seven one six six nine six two seven O_ two six four five one O_ five four two nine five O_ O_ one seven one one two seven one eight one two three four O_ eight four five seven six two two nine eight two three six seven two six seven six four nine two seven three one two five three nine three six eight six one nine one seven seven Transcript L_ dash thirty six O_ four four one six two nine O_ four seven three eight four five six five four six eight eight three one two five two five four five nine nine seven zero six nine eight five eight five one zero eight three two nine seven three one four five six four nine three eight four four two two three four five three O_ two four five four two two six eight two one eight nine eight one nine O_K. We're done. ",The Berkeley Meeting Recorder group discussed the preparation of a data sample for IBM  the manual adjustment of time bins by transcribers  recognition results for a test set of digits data  and forced alignments. Participants also talked about Eurospeech 2001 submissions  and exchanged comments on the proceedings of the recently attended Human Language Technologies conference (HLT'01). Preliminary recognition results were presented for a subset of digits data. Efforts to deal with cross-talk and improve forced alignments for non-digits data were also discussed. Subsequent manual adjustment of speech and non-speech boundaries will be delegated to the transcriber pool. A subset of Meeting Recorder data will be prepared (i.e. pre-segmented and manually adjusted) for delivery to IBM. The Transcriber interface may require modifications if it becomes necessary for transcribers to quickly switch among waveform displays. Transcribers risk overlooking speech that is deeply embedded in the mixed signal. Should transcriptions be derived from each of the close-talking channels or from the mixed signal alone? The pre-segmentation tool does not perform well on short utterances  e.g. backchannels. The Transcriber interface does not allow the user to quickly switch among visual displays  i.e. multi-channel waveforms. Forced alignments were problematic for non-digits data due to cross-talk. This problem was reported to be particularly bad for cross-talk featuring more than one word. Echo cancellation was considered as a means of improving forced alignments  but was ultimately deemed to be too time-consuming given the dynamic aspect of adapting distances between speakers. Comparing error rates in terms of the recording device used  i.e. lapel versus wireless microphones  is tedious. Deleting segments of the recordings is expected to be very time-consuming for transcribers. More results are needed for generating adequate submissions for Eurospeech'01. Participants have complained that the head-mounted microphone is uncomfortable. One meeting recording has been channelized and pre-segmented for delivery to IBM. A sample of digits data is being prepared for IBM. Preliminary recognition results were obtained for a subset of digits data. The error rate distribution was multimodal  reflecting differences in performance for native versus non-native speakers  and also possible pre-processing errors. Future efforts will involve an attempt to get good forced alignments on digits data and generate a report for Eurospeech'01. A program has been developed for replacing sections of recorded speech with editing bleeps. The tightening of time bins for one NSA meeting was checked and judged to be highly accurate. Efforts are ongoing to improve forced alignments for a subset of non-digits data  including acoustic adaptation manipulations. 
"I  uh  had some communications with him Ah  so comfortable. Smooth. yesterday or the day before. So  he doesn't plan on coming real soon  but he is taking sabbatical this year and so he'll probably come for six weeks at some point. Mm-hmm. Good. I know that he's going to like  Taiwan and other places to eat. So. @@ Right. Everyone - So are your microphones on? Make sure to turn the batteries - the switch on. Everybody? On? Am I on? I think I'm on? Good. Good. Yep. Yep. Bye. Yeah. O_K. Great. Thanks very much. Actually - I just had one of the most frustrating meetings of my career. It's definitely not the most frustrating meeting I've ever had. You a- You're - you remember you're being recorded at this point. Yeah. Oh  yeah  so  w- we didn't yet specify with whom. But um. Yeah. Right. Uh  right. Uh. So that's why Keith and I are going to be a little dazed for the first half m- the meeting. Huh. Yeah  I'm just gonna sit here and Right. Yeah  I - growl. I - I avoided that as long as I could for you guys  but  uh - Yeah. Mm-hmm. For which we thank you  by the way. I know you were - you were doing that  but  anyway. Are very appreciative  yeah. Right. Oh yeah  how di- how d- exactly did  uh  that paper lead to anti-lock brakes? Oh  I could tell you had a rough day  man! Nah. What? I love that story. Yeah  it's a great story. O_K. Oh my goodness. Oh yeah  um  Liz suggested we could start off by uh  doing the digits all at the same time. What? All at the same time. I don't know if - I would get distracted and confused  probably. e- Really? Do we have to like  synchronize? Well  I think you're supposed to - O_K. We can do this. Everybody's got different digits  right? Are you being silly? Oh wait do we have t- Uh . Yep. Yeah  do we have to time them at the same time or just overlapping - You're kidding. No. No  no  just - just start whenever you want. And any rate? e- yeah   the- Alright. Well  they - they have s- they have the close talking microphones for each of us  so - Yeah  that's true. Alright. yeah  there's separate channels. Yeah. O_K. Just plug one ear. So when I say Yeah. ""two two four  one nine three  six eight four one"" This is transcript L_ two nine five. nine four six  six five one  nine three five five two three three zero  six seven eight seven  six two two four nine five five  nine five seven  seven six two three seven four one  two six O_  six O_ seven seven. five five six two  zero  nine seven four. nine three five  eight four two  O_ six six nine  seven seven zero three  two  six two five four six four  five three  two three one five zero four zero eight  nine  zero nine one two one seven five  three nine six five  three eight seven two. three zero  three nine  four O_  eight eight  five eight six two eight five  four  four three eight six four six  nine O_  two O_ one eight eight four two  O_ six three  six nine five six six nine nine eight  six  six eight - zero eight four. one  five six eight  eight nine  nine four nine  nine. one five eight  zero seven two  three seven five five one O_ seven  four  s- five six one nine six  six five  nine seven  four O_ two six eight two nine  nine zero  six five five nine two five seven seven  two  five seven two. six six two  three five six  five eight nine seven five four one six  six  one two five five four two six  five  seven O_ two. eight five  eight three  two six  seven five  two seven five two nine four  eight  eight four nine. six seven four  four six zero  five one three two seven eight five  nine  eight four two O_ eight  eight three  seven five  five five  two O_. four one one eight  eight five one three  seven one seven five seven four  six three  eight zero  one one  eight zero. six three O_  two five  three four seven eight five nine eight  two six  nine five one six three five nine nine  O_ seven two nine  six three two zero two six seven  four O_ four  three three five seven  zero  one six seven  one five  four four seven zero six O_ six  six nine one  seven four six nine. five six three zero  three four zero four  five nine nine one seven  three eight five  six six  five nine one  nine. one  one one eight  five five  O_ one eight  one O_ six seven  nine two  one three nine six. seven four eight  nine one two  one five four. six two nine  three nine six  six two zero five. eight  nine seven one  three seven  two four nine  one four seven one  six one six  five two one two. one six eight three  six  eight nine six six two zero s- three. two one six  seven three  nine four seven three. You lose. O_K. O_K  bye! That was a great meeting! Right. So- Alright. Now  uh  why? Just to save time. O_K. Does matter for them. Are we gonna start all our meetings out that way from now on? No. Oh. Too bad. I kinda like it. Well  could we? It's strangely satisfying. Yeah. It's a ritual. Are we to r- Just to make sure I know what's going on  we're talking about Robert's thesis proposal today? Is that We could. true? We are? We might. O_K. Well  you - you had s- you said there were two things that you might wanna do. One was rehearse your i- i- talk - Is - Oh yes  and that too. Not - not rehearse  I mean  I have just not spent any time on it  so I can show you what I've got  get your input on it  and maybe some suggestions  that would be great. And the same is true for the proposal. I will have time to do some revision and some additional stuff on various airplanes and trains. So  um. I don't know how much of a chance you had to actually read it because - I haven't looked at it yet  but you could always send me comments per electronic mail but I will. and they will be incorporated. O_K. Um  the - It basically says  well ""this is construal""  and then it continues to say that one could potentially build a probabilistic relational model that has some general  domain-general rules how things are construed  and then the idea is to use ontology  situation  user  and discourse model to instantiate elements in the classes of the probabilistic relational model to do some inferences in terms of what is being construed as what Hmm. in our beloved tourism domain. But  with a focus on @@ Can I s- I think I need a copy of this  yes. I- is there an extra copy around? Sorry. Hmm? O_K  we can - we can - we can pass - pass my  uh - we can pass my extra copy around. Er  actually  my only copy  now that I think about it  but. I already read half of it  so it's O_K. Uh. He sent it. O_K. You can keep it. Alrigh- Um  I don't - I  uh - I don't need it. Um  actually this is the - the newest version after your comments  and - O_K. O_K. Yeah  no I s- I s- I see this has got the castle in it  and stuff like that. Yep. Yeah. Oh  maybe the version I didn't have that I - mine - the w- did the one you sent on the email have the - Yeah. That was the most recent one? Uh  yeah  I think so. O_K. Yep. Cuz I read halfway but I didn't see a castle thing. I'm changing this. Just so you know. But  anyway. Yeah  um  if you would have checked your email you may have received a note from Yees asking you to send me the  uh  up-to-d- Oh. Oh  sorry. O_K. Sorry. current formalism thing that you presented. O_K. I will. O_K. O_K. O_K. But for this it doesn't matter. But  uh - Hmm! We can talk about it later. That's not even ready  so. Um  O_K! Go on t- to  uh  whatever. And - I'm making changes. ""Don't worry about that."" O_K. Mmm-mmm. Oh! O_K  sorry  go on. And any type of comment whether it's a spelling or a syntax or Mm-hmm. There's only one ""S_"" in ""interesting"". readability - Hmm? There's only one ""S_"" in ""interesting"". On page five. Interesting. Anyway. And y- uh  email any time  but most usefully before - The twenty-first I'm assuming. The twenty-first? No  this is the twenty-first. Twenty-ninth. That's - What   today's the twenty-first? Oh  man! Well  better hurry up then! The twenty-ninth. Before the twenty-ninth  O_K. O_K. That's when I'm meeting with Wolfgang Wahlster to Mm-hmm. sell him this idea. O_K. O_K? Then I'm also going to present a little talk at E_M_L  about what we have done here and so of course  I'm - I'm gonna start out with this slide  so the most relevant aspects of our stay here  and um  then I'm asking them to imagine that they're standing somewhere in Heidelberg and someone asks them in the morning - The Cave Forty-Five is a - is a well-known discotheque which is certainly not open at that - that time. O_K. And so they're supposed to imagine that  you know  do they think the person wants to go there  or just know where it is? Uh  which is probably not  uh  the case in that discotheque example  or in the Bavaria example  you just want to know where it is. And so forth. So basically we can make a point that here is ontological knowledge but if it's nine - nine P_M in the evening then the discotheque question would be  for example  one that might ask for directions instead of just location. Um  and so forth and so forth. That's sort of motivating it. Then what have we done so far? We had our little bit of  um  um  SmartKom stuff  that we did  um  everth- Oh  you've got the parser done. Sorry. That's the - not the construction parser. That's the  uh  tablet-based parser  and O_K. Easy parser. O_K. the generation outputter. Halfway done? Yeah. That's done. You have to change those strategies  right? That's  ten words? Mmm. O_K. Yeah. Well  i- it  you know. Maybe twelve. Twelve? O_K. And  um  and Fey is doing the synthesis stuff as we speak. That's all about that. Then I'm going to talk about the data  you know these things about - uh  actually I have an example  probably. Two s- Where is a nightclub? Can you hear that? Or should I turn the l- volume on. Mm-hmm. I- I can hear it. I could hear it. I heard it. They might not hear it in the - well maybe they will. I don't know. Where is a nightclub? This was an actual  um  subject? Ah. Mm-hmm. Sounds like Fey. Yeah. But they're - they're mimicking the synthesis when they speak to the computer  the - you can observe that all the time  they're trying to match their prosody onto the Oh  O_K. Oh really. Interesting. machine. A popular disco is the Young Ganz . Oh  it's pretty slow. Where is it? Yeah  you have to - In order to get there take bus number forty-two towards the @@ and exit at the birdband . O_K. Wh- The system breaking. What is the s- ? Oh! Hello? Hello? I'm sorry. Something seems to have gone wrong with our system. So  I'm going to have to take over from here. What can I help you with? Uh. I'm looking for the University. O_K. @@ I can get the address  or if you know where you are  I can tell you how to get there. The address would be fine. It's at the Hauptstrasse and @@ gasse. Is that anywhere near the castle? @@ O_K. And so forth and so forth. Um  I will talk about our problems with the rephrasing  and how we solved it  and some preliminary observations  also  um  I'm not gonna put in the figures from Liz  but I thought it would interesting to  uh  um  point out that it's basically the same. Um  as in every human-human telephone conversation  and the human-computer telephone conversation is of course quite d- quite different from  uh  some first  uh  observations. Then sort of feed you back to our original problem cuz  uh - how to get there  what actually is happening there today  and then maybe talk about the big picture here  e- tell a little bit - as much as I can about the N_T_L story. I - I wa- I do wanna  um - I'm not quite sure about this  whether I should put this in  um  that  you know  you have these two sort of different ideas that are - or two different camps of people envisioning how language understanding works  and then  talk a bit about the embodied and simulation approach favored here and as a prelude  I'll talk about monkeys in Italy. And  um  Srini was gonna send me some slides but he didn't do it  so from - but I have the paper  I can make a resume of that  and then I stole an X_schema from one of your talks I think. Oh. I was like  ""where'd you get that?"" O_K. ""Looks familiar."" Yeah  that looks familiar. I think that's Bergen  Chang  something  or the other. Whatever. Uh. Um  and that's - now I'm not going to bring that. So that's basically what I have  so far  and the rest is for airplanes. O_K. So X_schemas  then  I would like to do - talk about the construction aspect and then at the end about our Bayes-net. Mm-hmm. End of story. Anything I forgot that we should mention? Oh  maybe the F_M_R_I stuff. Should I mention the fact that  um  we're also actually started - going to start to look at people's brains in a more direct way? You certainly can. I mean I- y- I- you know  I don't know - You might just wanna like  tack that on  as a comment  Right  um. to something. ""Future activities"" something. Well  the time to mention it  if you mention it  is when you talk about mirror neurons  then you should talk about the more recent stuff  about the kicking and  you know  the - yeah  yeah - and - that the plan is to see to what extent Yeah. the - you'll get the same phenomena with stories about this  so that - Mm-hmm. and that we're planning to do this  um  which  we are. So that's one thing. Um. Depends. I mean  there is a  um  whole language learning story  O_K? which  Yeah. uh  actually  i- i- even on your five-layer slide  you - you've got an old one that - that leaves that off. Yeah  I - I - I do have it here. Hmm. Yeah. Um. And  of course  you know  the - the big picture is this bit. But  you know  it would - Right. But I don't think I - I am capable of - of do- pulling this off and doing justice to the matter. I mean  there is interesting stuff in her- terms of how language works  so the emergentism story would be nice to be - you know  it would be nice to tell people how - what's happening there  plus how the  uh  language learning stuff works  but - O_K  so  so anyway  I - I agree that's not central. What Mm-hmm. you might wanna do is  um  and may not  but you might wanna - this is - rip off a bunch of the slides on the anal- there - the - there - we've got various i- generations of slides that show language analysis  and matching to the underlying image schemas  and  um  how the construction and simulation - that ho- that whole th- Yeah  th- that - that's c- that comes up to the X_schema slide  so basically I'm gonna steal that from Nancy  one of Nancy's st- O_K  right. O_K  I can give you a more recent - if you want - well  that might have enough. Uh  I - yeah  but I also have stuff you - trash you left over  your quals and your triple-A_I. O_K. The quals w- the - the - the quals slides would be fine. Yeah. You could get it out of there  or some- Which I can even email you then  you know  like there probably was a little - few changes  not a big deal. Yeah  you could steal anything you want  I don't care. Which you've already done  obviously. So. Well  I - I don't feel bad about it at all because - because you are on the  uh  title. I mean on the - the  you're - that's - see  that's you. Sorry No  you shouldn't. Oh  that's great  that's great. I'm glad to see propagation. Yeah. Yeah. Mmm. Hmm? Propagated? Yes. That's great. I mean I might even mention that this work you're doing is sort of also with the M_P_I in Leipzig  so. It's - it's certainly related  um  Because  um  E_M_L is building up a huge thing in Leipzig. might wanna say. Is it? So it - It's on biocomputation. Would - Yeah  it's different  this is the  uh  D_N_A building  or someth- the double helix building. Yeah. Yeah. Kind of a different level of analysis. The - yeah it was - it turns out that if - if you have multiple billions of dollars  y- you can do all sorts of weird things  and - Wait  they're building a- What? building in the shape of D_N_A  is that what you said? Roughly  yeah. Oh! Oh boy! Including cr- cross-bridges  and O- What? That's brilliant! Oh my god! Hhh. Oy. You d- you really - now I- I spent - the last time I was there I spent maybe two hours hearing this story which is  um - Of Y- You definitely wanna w- don't wanna waste that money on research  you know? what the building? Right. That's horrible. Right. Well  no  no  y- i- there's infinite money. See you th- you th- you then fill it with researchers. And give them more money. They just want a fun place for them to - to work. Right. Right. And everybody gets a trampoline in their office. Well  the - the offices are actually a little - the  think of um  ramps  coming out of the double helix and then you have these half-domes  glass half-domes  and the offices are in - in the glass half-dome. Really? Alright  let's stop talking about this. Yeah. Does it exist yet? Yeah. Uh  as a model. They are w- now building it? Hmm. But I th- So  yeah  I think that's - that's a good point  th- th- that the date  the  uh  a lot of the - this is interacting with  uh  people in Italy but also definitely the people in Leipzig and the - the b- the combination of the biology and the Leipzig connection might be interesting to these guys  yeah. O_K. O_K. Anyway! Enough of that  let's talk about your thesis proposal. Yeah  if somebody has something to say. Yep. You might want to  uh  double-check the spellings of the authors' names on your references  you had a few  uh  misspells in your slides  there. Like I believe you had ""Jackendorf"". Um. Uh  unless there's a person called ""Jackendorf""  yeah. No  no  no. On that one? But that's the only thing I noticed in there. In the presentation? In the presentation. I'll probably - I c- might have - I'll probably have comments for you separately  not important . Anyway. Oh  in the presentation here. I was ac- actually worried about bibtex. Uh. No  that's quite possible. Yeah  that's what he was talking about. Yeah. That's copy and paste from something. So I did note- i- i- it looks like the  uh  metaphor didn't get in yet. Uh  it did  there is a reference to Srini - Well  s- reference is one thing  the question is is there any place - Oh  did you put in something about  Metonymy and metaphor here  right? uh  the individual  we'd talked about putting in something about people had  uh - Oh yeah  O_K. Good. I see where you have it. So the top of the second - of pa- page two you have a sentence. Mm-hmm. But  what I meant is  I think even before you give this  to Wahlster  uh  you should  unless you put it in the text  and I don't think it's there yet  about - we talked about is the  um  scalability that you get by  um  combining the constructions with the general construal mechanism. Is that in there? Yeah  mmm. Um. Uh  O_K  so where - where is it  cuz I'll have to take a look. Um  but I - I did not focus on that aspect but  um - Ehhh  um  it's just underneath  uh  um  that reference to metaphor. So it's the last paragraph before two. So on page two  um  the main focus - Uh  O_K. Yeah. But that's really - That's not about that  @@ is it? Yeah. No  it - it - it s- says it but it doesn't say - it doesn't - it d- it d- yeah  it doesn't give the punch line. Why. Mm-hmm. Cuz let me tell the gang what I think the punch line is  because it's actually important  which is  that  the constructions  that  uh  Nancy and Keith and friends are doing  uh  are  in a way  quite general but cover only base cases. And to make them apply to metaphorical cases and metonymic cases and all those things  requires this additional mechanism  of construal. And the punch line is  he claimed  that if you do this right  you can get essentially orthogonality  that if you introduce a new construction at - at the base level  it should com- uh  interact with all the metonymies and metaphors so that all of the projections of it also should work. Mm-hmm. And  similarly  if you introduce a new metaphor  Mm-hmm. it should then uh  compose with all of the constructions. Yeah. And it - to the extent that that's true then - then it's a big win over anything that exists. So does that mean instead of having tons and tons of rules in your context-free grammar you just have these base constructs and then a general mechanism for coercing them. Yeah. Mm-hmm. So that  you know  for example  uh  in the metaphor case  that you have a kind of direct idea of a source  path  and goal and any metaphorical one - and abstract goals and all that sort of stuff - you can do the same grammar. And it is the same grammar. Mmm. But  um  the trick is that the - the way the construction's written it requires that the object of the preposition for example be a container. Well  ""trouble"" isn't a container  but it gets constr- construed as a c- container. Right. Et cetera. So that's - that's where this  um  So with construal you don't have to have a construction for every possible thing that can fill the rule. Right. So's it's - it - it's a very big deal  i- i- in this framework  and the thesis proposal as it stands doesn't  um  I don't think  say that as clearly as it could. No  it doesn't say it at all. No. Even though - One could argue what - if there are basic cases  even. I mean  it seems like nothing is context- free. Oh  nothing is context- free  but there are basic cases. That is  um  there are physical containers  there are physical paths  there - you know  et cetera. But ""walked into the cafe and ordered a drink "" and ""walked into the cafe and broke his nose "" that's sort of - Oh  it doesn't mean that they're unambiguous. I mean  a cafe can be construed as a container  or it can be construed Mmm. Yeah. Uh-huh. you know as - as a obstacle  or as some physical object. So there are multiple construals. And in fact that's part of what has to be done. This is why there's this interaction between the analysis Mm-hmm. and the construal. Yep . The b- the - the double arrow. Yep. So  uh  yeah  I mean  it doesn't magically make ambiguity go away. No . But it does say that  uh  if you walked into the cafe and broke your nose  then you are construing the cafe Mm-hmm. as an obstacle. And if that's not consistent with other things  then you've gotta reject that reading. Yep. You con - you conditioned me with your first sentence  and so I thought  ""Why would he walk into the cafe and then somehow break his nose?"" uh  He slipped on the wet floor. oh  uh - Right. You don't find that usage  uh - uh  I checked for it in the Brown national corpus. The ""walk into it"" never really means  w- as in walked smack - Yeah. But ""run into"" does. Yeah  but  y- y- if you find ""walked smacked into the cafe"" or "" slammed into the wall"" - Yeah  no  but "" run into"" does. Mm-hmm. Because you will find ""run into "" uh  Cars run into telephone poles all the time. well  or ""into the cafe "" for that m- you know - ""His car ran into the cafe."" Right. Yeah. Or you can run into an old friend  or run. Well  you can ""run into"" in that sense too. No! But  uh  Yeah  ""run into"" might even be more impact sense than  you know  container sense. Right. Depends. But - Like  ""run into an old friend""  it probably needs its own construction. I mean  uh  you know  George would have I'm sure some exa- complicated ex- reason why it really was an instance of Mm-hmm. Mm-hmm. something else and maybe it is  but  um  there are idioms and my guess is that's one of them  but  um - I don't know. All contact. I mean  there- there's contact that doesn't - social contact  whatever. I mean. Uh. Sudden surprising contact  right? Yeah  but it's - it's - it's - it's - Right. i- Yeah  it's more - Forceful. But of course  no  i- i- I mean it has a life of its own. It's sort of partially inspired by the spatial - Well  this is this motivated - but yeah - oh yeah  mo- for sure  motivated  but then Yeah. Yeah. you can't parse on motivated. Yeah. Right. Too bad. Uh  You should get a T_shirt that says that. O_K. There's - there's lots of things you could make T_shirts out of  but  uh  this has gotten - @@ I mean wh- We don't need the words to that. Pro- probably not your marks in the kitchen  today. Not - not your marks. What? Oh  no no no no no no no no no  we're not going there. O_K. In other news. O_K  so  um  anything else you want to ask us about Well  the thesis proposal  you got - We could look at a particular thing and give you feedback on it. Well there - actually - the - i- what would have been really nice is to find an example for all of this  uh  from our domain. So maybe if we w- if we can make one up now  that would be c- incredibly helpful. So  w- where it should illustrate O_K. How - uh - wh- when you say all this  do you mean  like  I don't know  the related work stuff  as well as  Right - right - r- mappings? w- Well we have  for example  a canonical use of something and y- it's  you know  we have some constructions and then it's construed as something  and then we - we may get the same constructions with a metaphorical use that's also relevant to the - to the domain. O_K  f- let's - let's suppose you use ""in"" and ""on"". I mean  that's what you started with. Mm-hmm. So ""in the bus"" and ""on the bus "" um  that's actually a little tricky in English because to some extent they're synonyms. O_K. I had two hours w- with George on this  so it  um - O_K  what did he say. Did you? Join the club. Right. Oh  h- that's - Um. ""On the bus"" is a m- is a metaphorical metonymy that relates some meta- path metaphorically and you're on - on that path and th- w- I mean it's - he - there's a platform notion  right? ""he's on the - standing on the bus waving to me."" But th- the regular as we speak ""J- Johno was on the bus to New York "" um  uh  he's - Yeah  I - I believe all that  it's just - Yeah. Yeah. Yeah. that's  uh  what did I call it here  the transportation schema  something  where you can be on the first flight  on the second flight  and you can be  you know  Yeah. Yeah. Right. So - so that - that may or may not be what you - what you want to do. I mean you could do something much simpler on the wagon. Yeah. like "" under the bus "" or something  where - But it's - it's - unfortunately  this is not really something a tourist would ever say. So. Well  unless he was repairing it or something  but yeah. Yeah. But um. Uh  but O_K. So in terms of the - this - We had - we had - initially we'd - started discussing the ""out of film."" I see. Right. And there's a lot of ""out of"" analysis  so  um  Right. could we capture that with a different construal of - Yeah  it's a little - it's  uh - we've thought about it before  uh t- uh - to use the examples in other papers  and it's - it's a little complicated. Out of - out of film  in particular. Cuz you're like  it's a state of - there's resource  right  and like  Yeah. what is film  the state - you know. You're out of the state of having film  right? and somehow film is standing for the re- the resour- the state of having some resource is just labeled as that resource. It's - yeah  I mean  but - and plus the fact that there's also s- I mean  can you say  like  I mean. It's a little bit - ""The film ran out"" you know  or  maybe you could say something like ""The film is out"" so like the - the film went away from where it should be  namely with you  or something  right? You know. Yeah  is film the trajector? The - the film - the film is gone  right? Um  I never really knew what was going on  I mean I - I find it sort of a little bit farfetched to say that - that ""I'm out of film"" means that I have left the state of having film or something like that  but. It's weird. That - Uh. Or  ""having"" is also  um  associated with location  right? so if the film left   you know - state is being near film. Yeah. Yeah. So running - running out of something is different from being out of somewhere. Or being out of something as  uh - as well. So ""running out of it"" definitely has a process aspect to it. Mm-hmm. But that's from run  yeah. Mm-hmm. So  b- that's O_K  I mean - b- but the difference Yeah. Is the d- the final state of running out of something is being out of it. is - Yeah. Right. Yeah. So th- You got there. You got to out of it. Yeah. That part is fine. Yeah. But  uh - Hmm! Yeah  so - so nob- so no one has in - in - of the  uh  professional linguists  they haven't - there was this whole thesis on ""out of"". Uh. There was? Who? Well  there - I thought - or there was a paper on it. Huh? Out. There was one on - on ""out"" or ""out of""? There was a- Well  it may be just ""out"". Yeah. O_K. I think there was ""over"" but there was also a paper on ""out"". Yeah  Lind- Susan Lindner  right? The - the - ""the syrup spread out""? That kind of thing? Or something. Oh  yeah  you're right. Yeah. Yeah  and all that sort of stuff. Yeah. And undoubtably there's been reams of work about it in cognitive linguistics  but. O_K. But anyway. We're not gonna do that between now and next week. Yeah. Yeah. O_K. So  um - It's not one of the y- it's more straightforward ones - forward ones to defend  so you probably don't want to use it for the purposes - th- these are - you're addressing like  computational linguists  right. Or - are you? Mm-hmm. Right. O_K. There's gonna be four computational linguists  O_K. But more computer- emphasis on the computational? Or emphasis on the linguist? it's - More - there's going to be the - just four computational linguists  by coincidence  but the rest is  whatever  biocomputing people and physicists. No no no  but not for your talk. I'm- we're worrying about the th- the thes- it's just for one guy. Oh  O_K. Oh  the thesis! That's - that's computa- should be very computational  and  uh  someth- Oh  I meant this  you know  like - O_K. So I would try to - I would stay away from one that involves weird construal stuff. Yeah. Yeah. Right. You know  it's an obvious one - but  uh - Totally weird stuff. I mean the - the old bakery example might be nice  ""Is there a bakery around here"". So if you c- we really just construe it as a - Yeah. Around? No  it's the bakery itself - is it a building? uh  that you want to go to? or is it something to eat that you want to buy? Oh. Oh  oh yeah. Yeah  we've thought about that. Right. Right. And then - Nnn. No. What? "" Bakery "" can't be something you're gonna eat. No  no. The question is d- do you wanna - do you wanna construe - do you wanna constr-strue Sh- It's a speech-act. r- Exactly. It's because do you wanna c- do you want to view Yeah. the bakery as a p- a place that - that - i- for example  if - y- Where you can get baked goods. Well th- well  that's one. You want to buy something. But the other is  uh  yo- you might have smelled a smell and are just curious about whether there'd be a bakery in the neighborhood  or  Mm-hmm. um  pfff you know  you wonder how people here make their living  and - there're all sorts of reasons why you might Yeah. be asking about the existence of a bakery that doesn't mean  ""I want to buy some baked goods."" O_K. But um  those are interesting examples but it's not clear that they're mainly construal examples. Yeah. So it's a lot of pragmatics  there  that Mmm. There's all sorts of stuff going on. So might be beyond what you want to do. let's - so let's think about this from the point of view of construal. So let's first do a - So the metonymy thing is probably the easiest and a- and actually the - Though  the one you have isn't quite - You mean the s- You mean ""the steak wants to pay""? N- no not that one  that's - that's a - the sort of background. This is the t- uh  page five. About Plato and the book? No. Oh. Um. No. Onward. Just beyond that. How much does it cost? Where is the castle? How old is it? How much does it cost? Yeah. A castle. Oh. Mm-hmm. To go in  that's like - Two hundred million dollars. Right. It's not for sale. Uh. So Yeah  I think that's a good example  actually. S- Yeah  that's good. u- But as Nancy just su- suggested it's probably ellipticus. Huh. Ellipsis. Like  "" it "" doesn't refer to ""thing "" it refers to acti- you know  j- thing standing for activ- most relevant activity for a tourist - you could think of it that way  but. Yeah. Well  shoot  isn't that - I mean  that's what - Well  I mean  my argument here is - it's - it's - it's the same thing as ""Plato's on the top shelf "" I'm con- you know  th- that you can refer to figuring that out is what this is about. Yeah  yeah  no  I- I agree. a book of Plato by using ""Plato "" and you can Yeah. No no  I - I'm agreeing that this is a good  um - refer back to it  and so you can - Castles have - as tourist sites  have admission fees  so you can say ""Where is the castle  how much does it cost?"" Um. ""How far is it Mm-hmm. from here?"" So  You're also not referring to the Hmm. width of the object  or so  www. Mm-hmm. Mmm. O_K. Can we think of a nice metaphorical use of ""where"" in the tourist's domain? Um. Hmm. So you know it's - you - you can sometimes use "" where "" f- for "" when "" O- in the sense of  you know  um  where - wh- where - where was  um  ""where was Heidelberg  um  in the Thirty Years' War?"" Or something. Uh  yeah. Mm-hmm. You know  or some such thing. Like what side were they on  or - ? Um. What? Yeah. Essentially  yeah. O_K. I was like  ""Huh? It was here. "" Like - Um. But anyway th- so there are - there are cases like that. Um  Ah! Or like its developmental state or something like that  you could - I guess you could get that. Yeah. Um. Um. I mean  there's also things like - I mean  s- um  I guess I could ask something like ""Where can I find out about blah-blah-blah"" in a sort of - doesn't nece- I don't necessarily have to care about the spatial location  just give me a phone number and I'll call them or something like that? Yeah. There certainly is that  yeah. You know  ""Where could I learn its opening hours "" or something. Yeah. But that's Hmm. not metaphorical. It's another - Yeah. So we're thinking about  um  or we could also think about  uh - Well  I - I - I - How about ""I'm in a hurry""? State. It i- But it's a state - and the - the issue is  Hmm? is that - it may be just a usage  you know  that it's not particularly metaphorical  I don't know. Mmm. Right. Oh. So you want a more exotic one - version of that. Yeah. Yeah  right. I'm really into - Ah! How about I - I - I - you know  ""I'm in - I'm in a state of exhaustion""? or something like that  which a tourist w- Huh? Do you really say that? Would you really say that? A st- uh  well  you can certainly say  um  you know  ""I'm in overload."" Yeah. I- I'm really into art. Uh - Tu- stur- tourists will often say that. Yeah  I was gonna say  like - Oh  you can do that? Really? Of course that's - that - that's definitely a  uh - Fixed. A fixed expression  yeah. that's a  uh - Right. But. - There're too - there're all sorts of fixed expressions I don't - like uh ""I'm out of sorts now!"" Right. Like ""I'm in trouble!"" Well I - when  uh - just f- u- the data that I've looked at so far that rec- I mean  there's tons of cases for polysemy. Yeah. Right. So  you know  mak- re- making reference to buildings as institutions  as containers  as build- you know  whatever. Uh-huh. Right. Um  so ib- in mus- for example  in museums  you know  as a building or as something where pictures hang versus  you know  ev- something that puts on exhibits  so forth. But - Right. As an institution  yeah. Um. Why don't you want to use any of those? Hmm? So y- you don't wanna use one that's - Yeah  well - No  but this - that's what I have  you know  started doing. The castle - the - that old castle one is sort of - Metonymy  polysemy. I love Van Gogh. Yeah. Ah! ""I wanna go see the Van Gogh."" Oh geez. Anyway  I'm sorry. But I think the argument should be - uh  can be made that  you know  despite the fact that this is not the most met- metaphorical domain  because people interacting with H_T_I systems try to be straightforward and less lyrical  Yeah. construal still is  uh  you know  completely  um  key in terms of finding out any of these things  so  Right. um. So that's - that's - that's a - that's a reasonable point  that it - in this domain you're gonna get less metaphor and more metonymy. We  uh - I - with a - I looked - with a student I looked at the entire database that we have on Heidelberg for cases of metonymy. And polysemy  and stuff like that. Yeah. Hardly anything. So not even in descriptions w- did we find anything  um  relevant. I have to go. Alright. Yeah. But O_K this is just something we'll - we'll see  um  Right. s- See you. and deal with. O_K  well. I guess if anybody has additional suggestions  w- Yeah. I mean maybe the ""where is something"" question as a whole  you know  can be construed as  u- i- locational versus instructional request. So  if we're not talk about the lexic- Location versus what? instruction. Instruction. Oh  directions? Yeah. Oh  I thought that was - Sure. Yeah. definitely treated as an example of construal. Right? @@ Yeah but then you're not on the lexical level  that's sort of one level higher. Oh  you want a lexical example. But I don't need it. Well  you might want both. Mm-hmm. Yeah. Also it would be nice to get - ultimately to get a nice mental space example  so  even temporal references are - We - just in the spatial domain are rare. But it's - it's easy to make up plausible ones. You know. When - when you're getting information on objects. So  I mean - Right  you know - you know  where r- Yeah. What color was this in - in in the nain- nineteenth century. What was this p- instead of - wh- what - you know - how was this painted  what color was this painted  um  Yeah. was this alleyway open. Yeah  maybe we can include that also in our second  uh  data run. We c- we can show people pictures of objects and then Uh. have then ask the system about the objects and engage in conversation on the history and the art and the architecture and so forth. Mm-hmm. O_K. So why don't we plan to give you feedback electronically. Wish you a good trip. All success. For some reason when you said ""feedback electronically"" I thought of that - you ever see the Simpsons where they're - like the family's got the buzzers and they buzz each other when they don't like what the other one is saying? Yeah. That's the - first one  I think. The very very first one. It was a very early one. I don't know if it's the first one. Mmm. Mmm. ",The Meeting Recorder Group of ICSI at Berkeley met without their most senior member  but attending instead was a visitor from research partner OGI. He reported on a recent project meeting from his group's perspective. There was much politics involved  and disagreement between groups. He also brought the ICSI members up to date with his group's latest work. The ICSI group reported their most recent progress and detailed their recent findings. Having discussed this with the ICSI project leader  the OGI member told of some future investigation they had devised  which would look at the adjusting the importance of some features. This led to a great deal of discussion. There were also further calls for greater communication between the groups. ICSI currently has no mailing list which includes OGI personnel  so they will set one up. There was disagreement at the project meeting over two group development of Voice Activity Detectors  particularly since one group makes their code available to all and the others do not. There were also issues relating to the amount of improvement required if the baseline is improved. OGI are looking at methods of making the initial estimations more robust to noise  though with little success. Most of their effort is now on TRAP recognition from temporal patterns. ICSI members have almost finished their report  and have been trying some things out which has led to the conclusion that C-1 channel is not at all useful. 
"And we're on. O_K. Might wanna close the door so that - Uh  Stephane will - I'll get it. Yeah Hey Dave? Could you go ahead and turn on  uh  Stephane's - Mm-hmm. So that's the virtual Stephane over there. O_K. Do you use a P_C for recording? Or - Uh  yeah  a Linux box. Yeah. It's got  uh  like sixteen channels going into it. Uh-huh. Uh-huh. The quality is quite good? Or - ? Mm-hmm. Yeah  so far  it's been pretty good. Mm-hmm. Yeah. So  uh  yeah - the suggestion was to have these guys start to - O_K. Why don't you go ahead  Dave? O_K. Um  so  yeah  the - this past week I've been main- mainly occupied with  um  getting some results  u- from the S_R_I system trained on this short Hub-five training set for the mean subtraction method. And  um  I ran some tests last night. But  um  c- the results are suspicious. Um  it's  um  cuz they're - the baseline results are worse than  um  Andreas - than results Andreas got previously. And it could have something to do with  um - That's on digits? That's on digits. It c- it - it could h- it could have something to do with  um  Hmm. downsampling. That's - that's worth looking into. Um  d- and  um  ap- ap- apart from that  I guess the - the main thing I have t- ta- I have to talk is  um  where I'm planning to go over the next week. Um. So I've been working on integrating this mean subtraction approach into the SmartKom system. And there's this question of  well  so  um  in my tests before with H_T_K I found it worked - it worked the best with about twelve seconds of data used to estimate the mean  but  we'll often have less in the SmartKom system. Um. So I think we'll use as much data as we have at a particular time  and we'll - we'll concatenate utterances together  um  to get as much data as we possibly can from the user. But  um  there's a question of how to set up the models. So um  we could train the models. If we think twelve seconds is ideal we could train the models using twelve seconds to calculate the mean  to mean subtract the training data. Or we could  um  use some other amount. So - like I did an experiment where I  um  was using six seconds in test  um  but  for - I tried twelve seconds in train. And I tried  um  um  the same in train - I'm a- I tried six seconds in train. And six seconds in train was about point three percent better. Um  and - um  it's not clear to me yet whether that's something significant. So I wanna do some tests and  um  actually make some plots of  um - for a particular amount of data and test what happens if you vary the amount of data in train. Mm-hmm. Uh  Guenter  I don't know if you t- followed this stuff but this is  uh  a uh  uh  long-term - long-term window F_F_Ts. Yeah  we - we spoke about it already  yeah. Yeah. Yeah  he - you talked about it. Oh  O_K. So you know what he's doing. Alright. y- s- so I was - I actually ran the experiments mostly and I - I was - I was hoping to have the plots with me today. I just didn't get to it. But  um - yeah  I wou- I would be curious about people's feedback on this cuz I'm - @@ I p- I think there are some I think it's - it's kind of like a - a bit of a tricky engineering problem. I'm trying to figure out what's the optimal way to set this up. So  um  I'll try to make the plots and then put some postscript up on my - on my web page. And I'll mention it in my status report if people wanna take a look. You could clarify something for me. You're saying point three percent  you take a point three percent hit  Hello. when the training and testing links are - don't match or something? Is that what it is? Or - ? w- Well  it c- I - I don't think it - it's just for any mismatch you take a hit. i- In some cases it might be Yeah. u- better to have a mismatch. Like I think I saw something like - like if you only have two seconds in test  Yeah. or  um  maybe it was something like four seconds  you actually do a little better if you  um  train on six seconds than if you train on four seconds. Um  Right. but the case  uh - with the point three percent hit was using six seconds in test  um  comparing train on twelve seconds versus train on six seconds. And which was worse? The train on twelve seconds. O_K. But point three percent  uh  w- from what to what? That's point three percent - On - The - the - the accuracies w- went from - it was something vaguely like ninety-five point six accuracy  um  improved to ninety-five point nine wh- when I - So four point four to four point one. O_K. So - yeah. So about a - about an eight percent  uh  seven or eight percent relative? O_K. Uh  Yeah. Well  I think in a p- You know  if - if you were going for an evaluation system you'd care. But if you were doing a live system that people were actually using nobody would notice. It's - uh  I think the thing is to get something that's practical  that - that you could really use. Huh. That's - that's interesting. Alright  the e- uh  I see your point. I guess I was thinking of it as  um  an interesting research problem. Yeah. The - how to g- I was thinking that for the A_S_R_U paper we could have a section saying  ""For SmartKom  we - we d- in - we tried this approach in  uh  interactive system""  which I don't think has been done before. Mm-hmm. Mm-hmm. And - and then there was two research questions from that. And one is the k- does it still work if you just use the past history? Mm-hmm. Alright  and the other was this question of  um what I was just talking about now. So I guess that's why I thought it was interesting. I mean  a short-time F_F_T - short-time cepstrum calculation  uh  mean - u- mean calculation work that people have in commercial systems  they do this all the time. They - the - they calculate it from previous utterances and then use it  you know. But - but  uh  Yeah  um. as you say  there hasn't been that much with this long - long-time  uh  spectra work. Uh  Oh  o- Oh  O_K . So that's - that's - that's standard. Um - Yeah. Pretty common. Yeah. O_K. Um  but  u- uh  yes. No  it is interesting. And the other thing is  I mean  there's two sides to these really small  uh  gradations in performance. Um  I mean  on the one hand in a practical system if something is  uh  four point four percent error  four point one percent error  people won't really tell - be able to tell the difference. On the other hand  when you're doing  uh  research  you may  eh - you might find that the way that you build up a change from a ninety-five percent accurate system to a ninety-eight percent accurate system is through ten or twelve little things that you do that each are point three percent. So - so the - they - they - it's - I don't mean to say that they're - they're irrelevant. Uh  they are relevant. But  um  i- for a demo  you won't see it. Mm-hmm. Right. O_K. Yeah. And  um  Let's - l- let's see. Um  O_K. And then there's um  another thing I wanna start looking at  um  wi- is  um  the choice of the analysis window length. So I've just been using two seconds just because that's what Carlos did before. Uh  I wrote to him asking about he chose the two seconds. And it seemed like he chose it a bit informally. So  um  with the - with the H_T_K set-up I should be able to do some experiments  on just varying that length  say between one and three seconds  in a few different reverberation conditions  um  say this room and also a few of the artificial impulse responses we have for reverberation  just  um  making some plots and seeing how they look. And  um  so  with the - the sampling rate I was using  one second or two seconds or four seconds is at a power of two um  number of samples and  um  I'll - I'll jus- f- for the ones in between I guess I'll just zero-pad. Mm-hmm. I guess one thing that might also be an issue  uh  cuz part of what you're doing is you're getting a - a spectrum over a bunch of different kinds of speech sounds. Um  and so it might matter how fast someone was talking for instance. Oh. You know  if you - if - if - if there's a lot of phones in one second maybe you'll get a - a really good sampling of all these different things  and - and  uh  on the other hand if someone's talking slowly maybe you'd need more. So - Huh. I don't know if you have some samples of faster or slower speech but it might make a difference. I don't know. Uh  yeah  I don't - I don't think the T_I-digits data that I have  um  i- is - would be appropriate for that. Yeah  probably not. Yeah. But what do you - What about if I w- I fed it through some kind of  um  speech processing algorithm that changed the speech rate? Yeah  but then you'll have the degradation of - of  uh  whatever you do uh  added onto that. But maybe. Yeah  maybe if you get something that sounds - that - that's - does a pretty job at that. Yeah. Well  uh  just if you think it's worth looking into. I mean  You could imagine that. it - it is getting a little away from reverberation. Um  yeah. It's just that you're making a choice - uh  I was thinking more from the system aspect  if you're making a choice for SmartKom  that - that - that it might be Yeah. that it's - it c- the optimal number could be different  Right. depending on - Could be. I don't know. And - and th- the third thing  um  uh  is  um  Barry explained L_D_A filtering to me yesterday. And so  um  Mike Shire in his thesis um  did a - a series of experiments  um  training L_D_A filters in d- on different conditions. And you were interested in having me repeat this for - for this mean subtraction approach? Is - is that right? Or for these long analysis windows  I guess  is the right way to put it. I guess  the - the - the issue I was - the general issue I was bringing up was that if you're - have a moving - moving window  uh  a wa- a - a set of weights times things that  uh  move along  shift along in time  that you have in fact a linear time invariant filter. And you just happened to have picked a particular one by setting all the weights to be equal. And so the issue is what are some other filters that you could use  Mm-hmm. uh  in that sense of ""filter""? And  um  as I was saying  I think the simplest thing to do is not to train anything  but just to do some sort of  uh  uh  hamming or Hanning  uh  kind of window  Right. Mm-hmm. kind of thing  just sort of to de-emphasize the jarring. So I think that would sort of be the first thing to do. But then  yeah  the L_D_A i- uh  is interesting because it would sort of say well  suppose you actually trained this up to do the best you could by some criterion  what would the filter look like then? Uh-huh. Uh  and  um  that's sort of what we're doing in this Aur- Aurora stuff. And  uh  it's still not clear to me in the long run whether the best thing to do would be to do that or to have some stylized version of the filter that looks like these things you've trained up  because you always have the problem that it's trained up for one condition and it isn't quite right for another. So. uh - that's - that's why - that's why RASTA filter has actually ended up lasting a long time  people still using it quite a bit  because y- you don't change it. So doesn't get any worse. Uh  Huh. Anyway. o- O_K. So  um  a- actually I was just thinking about what I was asking about earlier  wi- which is about having less than say twelve seconds in the SmartKom system to do the mean subtraction. You said in systems where you use cepstral mean subtraction  they concatenate utterances and  do you know how they address this issue of  um  testing versus training? Can - I think what they do is they do it always on-line  I mean  that you just take what you have from the past  Go ahead. that you calculate the mean of this and subtract the mean. O_K. Um - And then you can - yeah  you - you can increase your window whi- while you get - while you are getting more samples. O_K  um  and  um  so - so in tha- in that case  wh- what do they do when they're t- um  performing the cepstral mean subtraction on the training data? So - because you'd have hours and hours of training data. So do they cut it off and start over? At intervals? Or - ? So do you have - uh  you - you mean you have files which are hours of hours long? Or - ? Oh  well  no. I guess not. But - Yeah. I mean  usually you have in the training set you have similar conditions  I mean  file lengths are  I guess the same order or in the same size as for test data  or aren't they? O_K. But it's - O_K. So if someone's interacting with the system  though  uh  Morgan - uh  Morgan said that you would tend to  um  chain utterances together um  r- Well  I think what I was s- I thought what I was saying was that  um  Oh. at any given point you are gonna start off with what you had from before. From - and so if you're splitting things up into utterances - So  for instance  in a dialogue system  where you're gonna be asking  uh  you know  th- for some information  there's some initial th- something. And  you know  the first time out you - you might have some general average. But you - you d- you don't have very much information yet. But at - after they've given one utterance you've got something. You can compute your mean cepstra from that  Mm-hmm. and then can use it for the next thing that they say  uh  so that  you know  the performance should be better that second time. Um  @@ and I think the heuristics of exactly how people handle that and how they handle their training I'm sure vary from place to place. But I think the - ideally  it seems to me anyway  that you - you would wanna do the same thing in training as you do in test. But that's - that's just  uh  a prejudice. And I think anybody Right. working on this with some particular task would experiment. I g- I guess the question I had was  um  amount of data e- u- was the amount of data that you'd give it to  um update this estimate. Because say you - if you have say five thousand utterances in your training set  um  and you - you keep the mean from the last utterance  by the time it gets to the five thousandth utterance - No  but those are all different people with different - I mean  i- in y- So for instance  in - in the - in a telephone task  these are different phone calls. So you don't wanna @@ chain it together from a - from a different phone call. O_K  so - so - so they would - So it's within speaker  within phone call  if it's a dialogue system  it's within g- s- Yeah. Hmm. whatever this characteristic you're trying to get rid of is expected to be consistent over  right? r- and it - right. O_K  so you'd - you - and so in training you would start over at - at every new phone call or at every new speaker. Yeah  O_K. Yeah. Yeah. Now  you know  maybe you'd use something from the others just because at the beginning of a call you don't know anything  and so you might have some kind of general thing that's your best guess to start with. But - So  s- I - I - you know  a lot of these things are proprietary so we're doing a little bit of guesswork here. I mean  what do comp- what do people do who really face these problems in the field? Well  they have companies and they don't tell other people exactly what they do. But - R- right. but I mean  when you - the - the hints that you get from what they - when they talk about it are that they do - they all do something like this. Right  O_K. I see. Bec- because I - so this SmartKom task first off  it's this T_V and movie information system. And - Yeah  but you might have somebody who's using it and then later you might have somebody else who's using it. And so you'd wanna set some - Yeah. Yeah. Yeah. Right. Right. I - I see. I was - I was about to say. So if - if you ask it ""What - what movies are on T_V tonight?""  if I look at my wristwatch when I say that it's about Yeah. two seconds. The way I currently have the mean subtraction  Yeah. um  set up  the - the analysis window is two seconds. So what you just said  about what do you start with  raises a question of what do I start with then? I guess it - because - Mm-hmm. Well  w- O_K  so in that situation  though  th- maybe what's a little different there  is I think you're talking about - there's only one - it - it - it also depends - we're getting a little off track here. r- But - but - but - Oh  right. Uh  there's been some discussion about whether the work we're doing in that project is gonna be for the kiosk or for the mobile or for both. And I think for this kind of discussion it matters. If it's in the kiosk  then the physical situation is the same. It's gonna - you know  the exact interaction of the microphone's gonna differ depending on the person and so forth. But at least the basic acoustics are gonna be the same. So f- if it's really in one kiosk  then I think that you could just chain together and - and you know  as much - as much speech as possible to - because what you're really trying to get at is the - is the reverberation characteristic. Yeah. But in - in the case of the mobile  uh  presumably the acoustic's changing all over the place. Right. And in that case you probably don't wanna have it be endless because you wanna have some sort of - it's - it's not a question of how long do you think it's - you can get an approximation to a stationary something  given that it's not really stationary. So. @@ Right. Hmm. Right. And I - I g- I guess I s- just started thinking of another question  which is  for - for the very first frame  w- what - what do I do if I'm - if I take - if I use that frame to calculate the mean  Mm-hmm. then I'm just gonna get n- nothing. Right. Um  so I should probably have some kind of default mean for the first f- couple of frames? O_K. Yeah. Yeah. Yeah. Or subtract nothing. I mean  it's - Or subtract nothing. And - and that's - that's - I guess that's something that's p- people have figured out how to deal with in cepstral mean subtraction as well? Yeah  yeah. Yeah  people do something. They - they  uh  they have some  um  uh  in - in cepstral mean subtraction  for short-term window - analysis windows  as is usually done  you're trying to get rid of some very general characteristic. And so  uh  if you have any other information about what a general kind of characteristic would be  then you - you can do it there. You can also - you can also reflect the data. So you take  uh - you know  I'm not sure how many frames you need. But you take that many from the front and flip it around to - Uh-huh. Yeah  that's - a- as the negative value. So you can always - Yeah. The other thing is that - and - and - I - I remember B_B_ N doing this  is that if you have a multi-pass system  um  if the first pass ta- it takes most of the computation  the second and the third pass could be very  very quick  just looking at a relatively small n- small  uh  space of hypotheses. Mmm . Uh-huh. Then you can do your first pass without any subtraction at all. Oh. And then your second pass  uh  uh  eliminates those - most of those hypotheses by  uh - by having an improved - improved version o- of the analysis. So. O_K . O_K. O_K. So that was all I had  for now. Yeah. Do you wanna go  Barry? Yeah  O_K. Um  so for the past  uh  week an- or two  I've been just writing my  uh  formal thesis proposal. Um  so I'm taking this qualifier exam that's coming up in two weeks. And I - I finish writing a proposal and submit it to the committee. Um. And uh  should I - should I explain  uh  more about what - what I'm proposing to do  and s- and stuff? Yes  briefly. Yeah briefly. O_K. Um  so briefly  I'm proposing to do a n- a new p- approach to speech recognition using um  a combination of  uh  multi-band ideas and ideas  um  about the uh  acoustic phonec- phonetic approach to speech recognition. Um  so I will be using these graphical models that - um  that implement the multi-band approach to recognize a set of intermediate categories that might involve  uh  things like phonetic features or other - other f- feature things that are more closely related to the acoustic signal itself. Um  and the hope in all of this is that by going multi-band and by going into these  um intermediate classifications  that we can get a system that's more robust to - to unseen noises  and situations like that. Um  and so  some of the research issues involved in this are  um  one  what kind of intermediate categories do we need to classify? Um  another one is um  what - what other types of structures in these multi-band graphical models should we consider in order to um  combine evidence from the sub-bands? And  uh  the third one is how do we - how do we merge all the  uh  information from the individual uh  multi-band classifiers to come up with word - word recognition or - or phone recognition things. Um  so basically that's - that's what I've been doing. And  So you've got two weeks  huh? I got two weeks to brush up on d- um  presentation stuff and  um  Oh  I thought you were finishing your thesis in two weeks. But. Oh  that too. Yeah. Yeah. Are you gonna do any dry runs for your thing  or are you just gonna - Yes. Yes. I  um - I'm - I'm gonna do some. Would you be interested? Sure. To help out? Sure. O_K. Thanks. Yeah. Is that it? Hhh. That's it. O_K. Uh. Hhh. Let's see. So we've got forty minutes left  and it seems like there's a lot of material. An- any suggestions about where we - where we should go next? Mmm  @@ . Uh. Do you wanna go  Sunil? Maybe we'll just start with you. Yeah. But I actually stuck most of this in our m- last meeting with Guenter. Um  but I'll just - Um  so the last week  uh  I showed some results with only SpeechDat-Car which was like some fifty-six percent. And  uh  I didn't h- I mean  I - I found that the results - I mean  I wasn't getting that r- results on the T_I-digit. So I was like looking into ""why  what is wrong with the T_I-digits?"". Why - why I was not getting it. And I found that  the noise estimation is a reason for the T_I-digits to perform worse than the baseline. So  uh  I actually  picked th- I mean  the first thing I did was I just scaled the noise estimate by a factor which is less than one to see if that - because I found there are a lot of zeros in the spectrogram for the T_I-digits when I used this approach. So the first thing I did was I just scaled the noise estimate. And I found - So the - the results that I've shown here are the complete results using the new - Well  the n- the new technique is nothing but the noise estimate scaled by a factor of point five. So it's just an ad-hoc - I mean  some intermediate result  because it's not optimized for anything. So the results - The trend - the only trend I could see from those results was like the - the p- the current noise estimation or the  uh  noise composition scheme is working good for like the car noise type of thing. Because I've - the only - only - p- very good result in the T_I-digits is the noise - car noise condition for their test-A_   which is like the best I could see that uh  for any non-stationary noise like ""Babble"" or ""Subway"" or any - ""Street""  some ""Restaurant"" noise  it's like - it's not performing w- very well. So  the - So that - that's the first thing I c- uh  I could make out from this stuff. And - Yeah  I think what is important to see is that there is a big difference between the training modes. Uh-huh. If you have clean training  Yeah. Yeah. you get also a fifty percent improvement. But if you have muddy condition training you get only Yeah. twenty percent. Yeah. Mm-hmm. Mm-hmm. Uh  and in that twenty percent @@ it's very inconsistent across different noise conditions. So I have like Mmm. a forty-five percent for ""Car noise"" and then there's a minus five percent for the ""Babble""  Mmm. and there's this thirty-three for the ""Station"". And so it's - it's not - it's not actually very consistent across. So. The only correlation between the SpeechDat-Car and this performance is the c- stationarity of the noise that is there in these conditions and the SpeechDat-Car. Mm-hmm. And  uh - so - so the overall result is like in the last page  which is like forty-seven  which is still very imbalanced because there are like fifty-six percent on the SpeechDat-Car and thirty-five percent on the T_I-digits. And - uh  ps- the fifty-six percent is like comparable to what the French Telecom gets  but the thirty-five percent is way off. I'm sort of confused but - this - I'm looking on the second page  Oh  yep. and it says ""fifty percent"" - looking in the lower right-hand corner  ""fifty percent relative performance"". For the clean training. Is that - u- And if you - if you look - is that fifty percent improvement? Yeah. Yeah. For - that's for the clean training and the noisy testing for the T_I-digits. So it's improvement over the baseline mel cepstrum? Yeah. Yeah. But the baseline mel cepstrum under those training doesn't do as well I - I'm - I'm trying to understand why it's - it's eighty percent - That's an accuracy number  I guess  right? Yeah  yeah  yeah. So that's not as good as the one up above. No. But the fifty is better than the one up above  so I'm confused. Yeah. Uh  actually the noise compensation whatever  uh  we are put in it works very well for the high mismatch condition. I mean  it's consistent in the SpeechDat-Car @@ and in the clean training also it gives it - But this fifty percent is - is that the - the high mismatch performance - equivalent to the high mismatch performance in the speech. So n- s- So since the high mismatch performance is much worse to begin with  Yeah. it's easier to get a better relative improvement. Yeah. I do. Yeah  yeah. So by putting this noise - Yeah. Yeah  if we look at the figures on the right  we see that Oh. Yeah. the reference system is very bad. The reference drops like a very fast - Oh  oh  oh  oh  oh  oh. I see. Like for clean - clean training condition. Yeah. I see. Nnn. This is - this is T_I digits we're looking at? This whole page is T_I-digits or this is - ? Yeah. Yeah. Oh - Oh. Yeah. It's not written anywhere. Yeah  it's T_I-digits. The first r- spreadsheet is T_I-digits. Hmm. Mmm. How does clean training do for the  uh  ""Car"" stuff ? The ""Car""? Oh. Still - it still  uh - that - that's still consistent. I mean  I get the best performance in the case of ""Car""  which is the third column in the A_ condition. No. I mean  this is added noise. I mean  this is T_I-digits. I'm sorry. I meant - in - in the - in the  uh  multi-language  uh  Uh - uh  Finnish and - This is next - next page. Hmm. That's the next - next spreadsheet  is - So that is the performance for Italian  Finnish and Spanish. ""Training condition"" - Oh  right. So ""clean"" corresponds to ""high mismatch"". Yeah. And ""increase""  That's increase e- Improvement. Improvement. That's - ""Percentage increase"" is the percentage improvement over the baseline. So that's - Yeah. It's - it's a - Which means decrease in word error rate? Yeah. O_K  so ""percentage increase"" means decrease? O_K. Yeah  yeah. Yeah. The - the w- there was a very long discussion about this on - on the - on the  uh  Amsterdam meeting. Yeah. How to - how to calculate it then. Yeah. There's - there's a - I - I - I guess you are using finally this - the scheme which they - Which is there in the spreadsheet. I'm not changing anything in there. O_K. Mmm. Alright. So. Uh  yeah. So all the hi- H_M_ numbers are w- very good  in the sense  they are better than what the French Telecom gets. So. But the - the only number that's still - I mean  which Stephane also got in his result was that medium mismatch of the Finnish  which is very - which is a very strange situation where we used the - we changed the proto for initializing the H_M_M - I mean  this - this is basically because it gets stuck in some local minimum in the training. That seventy-five point seven nine in the Finnish mismatch Uh-huh. which is that - the eleven point nine six what we see. Mmm. Yeah. So we have to jiggle it somehow? Yeah - so we start with that different proto and it becomes eighty-eight  which is like some fifty percent improvement. S- Wait a minute. Start with a different what? Different prototype  which is like a different initialization for the  uh  s- transition probabilities. It's just that right now  the initialization is to stay more in the current state  which is point four point six  right? Yeah. Yeah. And if it changes to point five point five  which is equal @@ for transition and self loop where it becomes eighty-eight percent. Well  but that involves mucking with the back-end  which is not allowed. Yeah. We can't do it. Yeah. Yeah. Mmm. So. I mean  it uh  like  i- i- i- It is well known  this - this medium match condition of the Finnish data has Yeah. Very s- some strange effects. I mean  that is - It has a very few at - uh  actually  c- uh  tran- I mean  words also. It's a very  very small set  actually. Yeah  that too. Yeah. Uh-huh. There is a l- a - There is a lot of - Uh  there are a lot of utterances with music in - with music in the background. So there is - Yeah. Yeah  yeah  yeah. Yeah. Mmm. Uh-huh. Yeah. It has some music also. I mean  very horrible music like like 4x . I know. So maybe for that one you need a much smarter V_A_D? Mmm  @@ if it's music. Uh - So  that - that's the - that's about the results. And  uh  the summary is like - O_K. So there are - the other thing what I tried was  which I explained in the last meeting  is using the channel zero for  uh  for both dropping and estimating the noise. And that's like just to f- n- get a feel of how good it is. I guess the fifty-six percent improvement in the SpeechDat-Car becomes like sixty-seven percent. Like ten percent better. But that's - that's not a - that's a cheating experiment. So. That's just - So  m- w- But the - but the  uh  forty-seven point nine percent which you have now  that's already a remarkable improvement in comparison to the first proposal. Yeah. So we had forty- four percent in the first proposal. Yeah. O_K. Mm-hmm. We have f- a big im- So the major improvement that we got was in all the high mismatch cases  because all those numbers were in sixties and seventies because we never had any noise compensations. Mmm. So that's where the biggest improvement came up. Not much in the well match and the medium match and T_I-digits also right now. So this is still at three or four percent improvement over the first proposal. Mmm. Mmm. Yeah  so that's good. Yeah. So. Then if we can improve the noise estimation  then it should get better. Yeah  I - I started thinking about also - I mean yeah  uh  I discovered the same problem when I started working on - uh  on this Aurora task almost two years ago  that you have the problem with this mulit- a- at the beginning we had only this multi- condition training of the T_I-digits. Yeah. And  uh  I - I found the same problem. Just taking um  what we were used to u- use  I mean  uh  some type of spectral subtraction  y- you get even worse results than the basis and uh - Yeah. Yeah  yeah. I - I tried to find an explanation for it  so - Mmm. So . Yes. Stephane also has the same experience of using the spectral subtraction right? Mmm. Mm-hmm. Yeah. Yeah. So here - here I mean  I found that it's - if I changed the noise estimate I could get an improvement. So that's - so it's something which I can actually pursue  is the noise estimate. Mm-hmm. And - Yeah  I think what you do is in - when - when you have the - the - this multi-condition training mode  um then you have - then you can train models for the speech  for the words  as well as for the pauses where you really have all information about the noise available. Yeah. And it was surprising - At the beginning it was not surprising to me that you get really the best results on doing it this way  I mean  in comparison to any type of training on clean data and any type of processing. But it was - So  u- u- it - it seems to be the best what - wh- wh- what - what we can do in this moment is multi-condition training. And every- when we now start introducing some - some noise reduction technique we - we introduce also somehow artificial distortions. Yeah. And these artificial distortions - uh  I have the feeling that they are the reason why - why we have the problems in this multi-condition training. That means the H_M_Ms we trained  they are - they are based on Gaussians  Yeah. and on modeling Gaussians. And if you - Can I move a little bit with this? Yeah. And if we introduce now this - this u- spectral subtraction  or Wiener filtering stuff - So  usually what you have is maybe  um - I'm - I'm showing now an envelope um maybe you'll - f- for this time. So usually you have - maybe in clean condition you have something which looks like this. And if it is noisy it is somewhere here. And then you try to subtract it or Wiener filter or whatever. And what you get is you have always these problems  that you have this - these - these - these zeros Yeah. in there. And you have to do something if you get these negative values. I mean  this is your noise estimate and you somehow subtract it or do whatever. Uh  and then you have - And then I think what you do is you introduce some - some artificial distribution in this uh in - in the models. I mean  i- you - you train it also this way but  i- somehow there is - u- u- there is no longer a - a Gaussian distribution. It is somehow a strange distribution which we introduce with these artificial distortions. And - and I was thinking that - that might be the reason why you get these problems in the - especially in the multi-condition Mm-hmm. Yeah  yeah. Th- That's true. training mode. Yeah - the c- the models are not complex enough to s- absorb that additional variability that you're introducing. Thanks Adam. Yeah. Yes. Well  that's - I also have the feeling that Yeah. So - um  the reason ye- why it doesn't work is - yeah  that the models are much - are t- um  not complex enough. Because I - actually I als- always had a good experience with spectral subtraction  just a straight spectral subtraction algorithm when I was using neural networks  big neural networks  which maybe are more able to model strange distributions and - Mm-hmm. But - Yeah. Then I tried the same - exactly the same spectral subtraction algorithm on these Aurora tasks and it simply doesn't work. It's even - Hmm. it  uh  hurts even. So. We probably should at some point here try the tandem - the - the - the system- two Hmm. kind of stuff with this  with the spectral subtraction for that reason. Cuz again  it should do a transformation to a domain where it maybe - Mm-hmm. looks more Gaussian. Hmm. Mm-hmm. Yeah  y- I - I was - whe- w- w- just yesterday when I was thinking about it um w- what - what we could try to do  or do about it - I mean  if you - if you get at this - in this situation that you get this - this negative values and you simply set it to zero or to a constant or whatever @@ It's - if we - if we would use there a somehow  um - a random generator which - which has a certain distribution  u- not a certain - yeah  a special distribution we should see - we - we have to think about it. And that we  so  introduce again some natural behavior Mm-hmm. in this trajectory. Mm-hmm. Very different from speech. Still  I mean  it shouldn't confuse the - Yeah  I mean  similar to what - what you see really u- in - in the real O_K. um noisy situation. Mm-hmm. Or i- in the clean situation. But - but somehow a - a natural distribution. But isn't that s- again sort of the idea of the additive thing  if it - as - as we had in the J_ stuff? I mean  basically if - if you have random data  um  in - in the time domain  then when you look at the s- spectrum it's gonna be pretty flat. Mm-hmm. And - and  uh  so just add something everywhere rather than just in those places. It's just a constant  right? Mm-hmm. Yeah. I think - e- yeah. It's - it's just especially in these segments  I mean  you introduce  um  very artificial Yeah. behavior. And - Yeah. Well  see if you add something everywhere  it has almost no effect up - up - up on - on top. Mm-hmm. And it - and it - and it has significant effect down there. That was  sort of the idea. Mm-hmm. Hmm. Yeah the - that's true. That - those - those regions are the cause for this I- Mm-hmm. @@ - those negative values or whatever you get. Yeah. Mm-hmm. So. I mean  we - we could trit- uh  we - we could think how Yeah. w- what - what we could try. I mean  it - it was just an idea. I mean  we - Yeah  yeah. Mm-hmm. I think when it's noisy people should just speak up. to - Mmm. So - If we look at the France Telecom proposal  they use some kind of noise addition. They have a random number generator  right? And they add noise Oh  they do! on the trajectory of  uh  the log energy only  right? Yep. Oh. C_z- C_zero and log energy also  yeah. Yeah. Um  But I don't know how much effect it - this have  but Now? they do that. Yeah. Uh-huh. Oh. Hmm. So it - it - it - it - it is l- somehow similar to what - I think because they have th- log energy  yeah  and then just generate random number. They have some kind of mean and variance  and they add this number to - to the log energy simply. To the l- Um - Yeah - the - the log energy  the - after the clean - cleaning up. Mm-hmm. So they add a random - random noise to it. To the - just the energy  or to the mel - uh  to the mel filter? No. On- only to the log energy. Only - Yeah. Oh. Uh-huh. So it - Cuz I mean  I think this is most interesting for the mel filters. Uh-huh. Right? Or - or F_F_Ts  one or the other. But - but they do not apply filtering of the log energy or what - Like  uh - I mean - like - like a spectral subtraction or - No - their filter is not M_ domain . S- so they did filter their time signal and then Yeah. I kn- And then they calculate from this  the log energy or - ? what @@ - u- Yeah - then after that it is s- almost the same as the baseline prop- system. Mm-hmm. And then the final log energy that they - that they get  that - to the - to that they add some random noise. Yeah  but again  that's just log energy as opposed to Yeah. So it's not the mel. You know  it's not the mel filter bank output. filter bank energy. Yeah. Mmm. Mm-hmm. These are log energy computed from the time s- domain signal  not from the mel filter banks. Mm-hmm. So - Hmm. Maybe it's just a way to decrease the importance of did - this particular parameter in the - in the world feature vector cu- if you add noise to one of the parameters  you widen the distributions and - Hmm. Becomes flat. The variance  yeah  reduces  so. Hmm  yeah. Eee-sss-uh. So it could reduce the dependence on the amplitude and so on. Yeah. Yeah. Yeah. Although - Maybe. So is  uh - Is that about it? Mm-hmm. Uh  so the - O_K. So the other thing is the - Or - ? I'm just looking at a little bit on the delay issue where the delay of the system is like a hundred and eighty millisecond. So I just - just tried another sk- system - I mean  another filter which I've like shown at the end. Which is very similar to the existing uh  filter. Only - Uh  only thing is that the phase is - is like a totally nonlinear phase because it's a - it's not a symmetric filter anymore. This is for the L_D_A? Yeah - so - so this - this is like - So this makes the delay like zero for L_D_A because it's completely causal . So - Oh. So I got actually just the results for the Italian for that and that's like - So the fifty-one point O_ nine has become forty-eight point O_ six  which is like three percent relative degradation. So I have like the fifty-one point O_ nine and - So. Mm-hmm. I don't know it f- fares for the other conditions. So it's just like - it's like a three percent relative degradation  But - but is there - is there a problem with the one hundred eighty milliseconds? Or - ? with the - Th- Well  this is - u- Uh  may- Yeah  I mean  I talked to - to - uh  I ta- Uh  I talked  uh  about it with - with Hynek. I mean  there is - This is - So - So  basically our - our position is that  um  we shouldn't be unduly constraining the latency at this point because we're all still experimenting with trying to make the performance better in the presence of noise. Uh  there is a minority in that group who is a- arguing - who are arguing for um  uh  having a further constraining of the latency. So we're s- just continuing to keep aware of what the trade-offs are and  you know  what - what do we gain Mmm. from having longer or shorter latencies? But since we always seem to at least get something out of longer latencies not being so constrained  we're tending to go with that if we're not told we can't do it. Mm-hmm. What - where was the  um - the smallest latency of all the systems last time? The French Telecom. Well  France Telecom was - was - was very short latency and they had a very good result. It's - What - what was it? It was thirty-five. It was in the order of thirty milliseconds or - Yeah. Thirteen? Thirty. th- th- Thirty. Thirty-four. Yeah. Yeah. @@ Yeah  so it's possible to get very short latency. But  again  we're - the - the approaches that we're using are ones that Yeah. I was just curious about where we are compared to  you know  the shortest that people have done. take advantage of - But - but I think this thirty milliseconds - they - they did - it did not include the - the delta Yeah. calculation. And this is included now  Yeah. Yeah. Yeah. you know ? Yeah. So if they include the delta  it will be an additional forty millisecond. Mm-hmm. Yeah. Yeah. I - I don't remember the - i- th- They were not using the H_T_K delta? No  they're using a nine-point window  which is like a four on either side  which is like - Nine-point. O_K. f- so - Mmm. they didn't include that. Yeah. Mm-hmm. So - O_K. Where does the comprish- compression in decoding delay comes from? @@ That's the way the - the - the frames are packed  like you have to wait for one more frame to pack. Because it's - the C_R_C is computed for two frames always. Mm-hmm. Well  that - the- they would need that forty milliseconds also. Right? No. They actually changed the compression scheme altogether. Mm-hmm. So they have their own compression and decoding scheme and they - I don't know what they have. But they have coded zero delay for that. Oh. Because they ch- I know they changed it  their compression. They have their own C_R_C   their - their own error correction mechanism. So they don't have to wait more than one more frame to know whether the current frame is in error. Oh. Oh  O_K. So they changed the whole thing so that there's no delay for that compression and - part also. Hmm. Mm-hmm. Even you have reported actually zero delay for the compression. I thought maybe you also have some different - Mmm. Mmm. No  I think I - I used this scheme as it was before. O_K. Ah. Mm-hmm. O_K  we've got twenty minutes so we should probably try to move along. Uh  did you wanna go next  Stephane? I can go next. Yeah. Mmm. Oh. It's - Wait a minute. It's - Yeah  we have to take - Wait a minute. I think I'm confused. Well - O_K. Alright. So you have w- w- one sheet? This one is - you don't need it  alright. Uh - So you have to take the whole - the five. There should be five sheets. @@ O_K  I have four now because I left one with Dave because I thought I was dropping one off and passing the others on. So  no  we're not. O_K. Thanks. Please give me one. Ah  we need one more over here. O_K  maybe there's not enough for everybody. But - I can share with Barry. Oh  O_K. Yeah. O_K. Can we look Yeah. at this? So  yeah  there are two figures showing actually the  mmm  um  performance of the current V_A_D. So it's a n- neural network based on P_L_P parameters  uh  which estimate silence probabilities  and then I just put a median filtering on this to smooth the probabilities  right? Um - I didn't use the - the scheme that's currently in the proposal because I don't want to - In the proposal - Well  in - in the system we want to add like speech frame before every word and a little bit of - of  uh  s- a couple of frames after also. Uh  but to estimate the performance of the V_A_D  we don't want to do that  because it would artificially increase the um - the false alarm rate of speech detection. Right? Um  so  there is u- normally a figure for the Finnish and one for Italian. And maybe someone has two for the Italian because I'm missing one figure here. No. Well - Well  whatever. Uh - Yeah  so one surprising thing that we can notice first is that apparently the speech miss rate is uh  higher than the false alarm rate. So. So - so what is the lower curve and the upper curve? It means - Mm-hmm. Yeah  there are two curves. Yeah. One curve's for the close-talking microphone  which is the lower curve. And the other one is for the distant microphone Ah  O_K. which has more noise so  it's logical that it performs worse. So as I was saying  the miss rate is quite important uh  which means that we tend to label speech as - as a silence. And  uh  I didn't analyze further yet  but I think it's - it may be due to the fricative sounds which may be - in noisy condition maybe label - labeled as silence. And it may also be due to the alignment because - well  the reference alignment. Because right now I just use an alignment obtained from - from a system trained on channel zero. And I checked it a little bit but there might be alignment errors. Um  yeah  e- like the fact that the - the models tend to align their first state on silence and their last state o- on silence also. So the reference - reference alignment would label as speech some silence frame before speech and after speech. This is something that we already noticed before when - mmm  So this cus- this could also explain  uh  the high miss rate maybe. And - and this - this curves are the average over the whole database  so. Uh - Yeah. Right. Mmm. Um - Yeah  and the different points of the curves are for five uh  thresholds on the probability uh from point three to point seven. So that threshold - O_K. Mm-hmm. Yeah. S- O_K - so d- the detection threshold is very - So the v- The V_A_D? Yeah. There first  a threshold on the probability Yeah  yeah. @@ That puts all the values to zero or one. And then the median filtering. Mmm. Yeah  so the median filtering is fixed. You just change the threshold? Yeah. Yeah. It's fixed  yeah. Mm-hmm. So  going from channel zero to channel one  uh  almost double the error rate. Um  Yeah. Well  so it's a reference performance that we can - you know  if we want to - to work on the V_A_D  we can @@ . work on this basis and - Mm-hmm. O_K. Is this - is this V_A_D a M_L_P? Yeah. O_K. How - how big is it? It's a very big one. I don't remember. m- So three - three hundred and fifty inputs  uh  six thousand hidden nodes and two outputs. t- t- O_K. Yeah. Mm-hmm. Middle-sized one. Yeah. Mm-hmm. @@ . Yeah. Uh  ppp. I don't know  you have questions about that  or suggestions? Mmm. S- so - It seems - the performance seems worse in Finnish  which - Well  it's not trained on Finnish. uh - It's worse. It's not trained on Finnish  yeah. What's it trained on? I mean  the M_L_P's not trained on Finnish. Right  what's it trained on? Oh - oh. Sorry. Uh  it's Italian T_I-digits. Yeah . Oh  it's trained on Italian? Yeah  O_K. Yeah. That's right . Mm-hmm. And - O_K. And also there are like funny noises on Finnish more than on Italian. I mean  like music and um - Mm-hmm. Yeah. Yeah  the - Yeah  it's true. So  yeah  we were looking at this. But for most of the noises  noises are - um  I don't know if we want to talk about that. But  well  the - the ""Car"" noises are below like five hundred hertz. And we were looking at the ""Music"" utterances and in this case the noise is more about two thousand hertz. Well  the music energy's very low apparently. Yeah. Uh  uh  from zero to two - two thousand hertz. So maybe just looking at this frequency range for - from five hundred to two thousand would improve somewhat the V_A_D and - Mmm. Yeah. Mmm - So there are like some - Yeah  but - some s- some parameters you wanted to use or something? Or - Yeah. Yes. Mm-hmm. Uh  the next  um - Oh  it's there. So is the - is the - is the training - is the training based on these labels files which you take as reference here? Wh- when you train the neural net y- y- you - Yeah. No. It's not. It's - it was trained on some alignment obtained um  uh - For the Italian data  I think we trained the neural network on - with embedded training. So re-estimation of the alignment using the neural network  I guess. That's right? Yeah. We actually trained  uh  the - on the Italian training part. We - we had another Yeah. system with u- So it was a f- f- a phonetic classification system for the Italian Aurora data. Yeah. It must be somewhere. Yeah. For the Aurora data that it was trained on  it was different. Like  for T_I-digits you used What - a - a previous system that you had  I guess. @@ No it - Yeah  yeah. That's true. So the alignments from the different database that are used for training came from different Syste- system. Yeah. Then we put them tog- together. Well  you put them together and trained the V_A_D on them. Mmm. Yeah. Hmm. Yeah. Uh  But did you use channel - did you align channel one also? Or - I just took their entire Italian training part. So it was both channel zero plus channel one. Yeah. So di- Yeah. So the alignments might be wrong then on channel one  right? On one. Possible. So we might  yeah  We can do a realignment. That's true. at least want to retrain on these alignments  which should be better because they come from close-talking microphone. Yeah  the - that was my idea. I mean  Yeah. if - if it ha- if it is not the same labeling which is taking the spaces. O_K. Yeah  possible. Yeah. Mmm. Yeah. I mean  it - so the system - so the V_A_D was trained on maybe different set of labels for channel zero and channel one and - Mm-hmm. Mm-hmm. Mm-hmm. was the alignments were w- were different for - s- certainly different because they were independently trained. We didn't copy the channel zero alignments to channel one. Mm-hmm. Mm-hmm. Yeah. Yeah. But for the new alignments what you generated  you just copied the channel zero to channel one  right? Right. Yeah. Yeah. Um. And eh  hhh actually when we look at - at the V_A_D  for some utterances it's almost perfect  I mean  it just dropped one frame  the first frame of speech or - So there are some utterances where it's almost one hundred percent Hmm. V_A_D performance. Uh  but - Yeah. Mmm - Yep. So the next thing is um  I have the spreadsheet for three different system. But for this you only have to look right now on the SpeechDat-Car performance uh  because I didn't test - so - I didn't test the spectral subtraction on T_I-digits yet. Uh  so you have three she- sheets. One is the um proposal-one system. Actually  it's not exe- exactly proposal-one. It's the system that Sunil just described. Um  but with uh  Wiener filtering from um  France Telecom included. Um  so this gives like fifty-seven point seven percent  uh  s- uh  error rate reduction on the SpeechDat-Car data. Mmm  and then I have two sheets where it's for a system where - uh  so it's again the same system. But in this case we have spectral subtraction with a maximum overestimation factor of two point five. Uh  there is smoothing of the gain trajectory with some kind of uh  low-pass filter  which has forty milliseconds latency. And then  after subtraction um  I add a constant to the energies and I have two cases d- where - The first case is where the constant is twenty-five D_B below the mean speech energy and the other is thirty D_B below. Um  and for these s- two system we have like fifty-five point  uh  five-percent improvement  and fifty-eight point one. So again  it's around fifty-six  fifty-seven. Uh - Cuz I notice the T_I-digits number is exactly the same for these last two? Yeah  because I didn't - For the France Telecom uh  spectral subtraction included in the - our system  the T_I-digits number are the right one  but not for the other system because I didn't test it yet - this system  including - with spectral subtraction on the T_I-digits data. I just tested it on SpeechDat-Car. Ah! So - so that means the only thing - Mm-hmm. So - so - so these numbers are simply - Yeah. O_K. This  we have to - Yeah. Yes. Yeah. So you - so you just should look at that fifty-eight perc- point O_ nine percent and so on. O_K. Good. But this number. Right. Right. Mm-hmm. Um  Yeah. So this - s- So by - uh  by - by reducing the noise a - a decent threshold like minus thirty D_B  it's like - Yeah. Uh  you are like r- r- reducing the floor of the noisy regions  right? Yeah. The floor is lower. Um  Uh-huh. mm-hmm. I'm sorry. So when you say minus twenty-five or minus thirty D_B  with respect to what? To the average um  speech energy which is estimated on O_K  so basically you're creating a signal-to-noise ratio of twenty-five or thirty D_B? the world database. Yeah. But it's not - it - it's - I - I - I think what you do is this. i- When - when you have this  after you subtracted it  I mean  then you get something w- w- uh r- with this  uh  where you set the values to zero and then you simply add an additive constant again. Yeah. So you shift it somehow. This - this whole curve is shifted again. Right. But did you do that before the thresholding to zero  or - ? It's - But  it's after the thresholding. So  @@ Oh  so you'd really want to do it before  right? maybe - maybe we might do it before  yeah. Yeah  because then the - then you would have less of that phenomenon. Yeah. E- Hhh. I think. Uh - Yeah. c- But still  when you do this and you take the log after that  it - it reduce the - the variance. But - Mmm  Yeah  it - it - Right. Yeah  that will reduce the variance. That'll help. But maybe if you does - do it before you get less of these funny-looking things he's drawing . Mm-hmm. Um  But - but - So before it's like adding this  col- to the - to the - o- exi- original - We would - Right at the point where you've done the subtraction. O_K. Um  essentially you're adding a constant into everything. Mm-hmm. But the way Stephane did it  it is exactly the way I have implemented in the phone  so. Oh  yeah  better do it different  then. Yeah. Um. Yeah. Just you - you just ta- you just set it for a particular signal-to-noise ratio that you want? Yeah I - I made s- similar investigations like Stephane did here  just Yeah. uh  adding this constant and - and looking how dependent is it on the value of the constant Yeah. Mm-hmm. and then  must choose them somehow to give on average the best results for Yeah. Uh-huh. a certain range of the signal-to-noise ratios. Mm-hmm. So - Oh  it's clear. I should have gi- given other results. Also it's clear when you don't add noise  it's much worse. Like  around five percent worse I guess. Uh-huh. And if you add too much noise it get worse also. And it seems that right now this - this is c- a constant that does not depend on - on anything that you can learn from the utterance. It's just a constant noise addition. Um. And I - I think w- w- I - I'm sorry. Then - then I'm confused. I thought - I think - you're saying it doesn't depend on the utterance but I thought you were adding an amount that was twenty-five D_B down from the signal energy. Yeah  so the way I did that  i- I just measured the average speech energy of the - all the Italian data. Oh! And then - I - I have - I used this as mean speech energy. Mm-hmm. Oh  it's just a constant amount Yeah. And - over all. wha- what I observed is that for Italian and Spanish  when you go O_K. Oh. to thirty and twenty-five D_B  uh it - it's good. It stays - In this range  it's  uh  the p- u- well  the performance of the - this algorithm is quite good. But for Finnish  you have a degradation already when you go from thirty-five to thirty and then from thirty to twenty-five. And - I have the feeling that maybe it's because just Finnish has a mean energy that's lower than - than the other databases. And due to this the thresholds should be - Yeah. the - the a- the noise addition should be lower But in - I mean  in the real thing you're not gonna be able to measure what people are doing over half an hour or an hour  or anything  right? So you have to come up with this number and - from something else. Yeah. So - Uh  but you are not doing it now language dependent? Or - ? It's not. It's just something that's fixed. Yeah. Mm-hmm. Um - No. It's overall. O_K. But what he is doing language dependent is measuring what that number i- reference is that he comes down twenty- five down from. Yeah  so I g- No. It - No. Because I did it - I started working on Italian. I obtained this average energy and then I used this one. No? Yeah. For all the languages. O_K. Yeah. So it's sort of arbitrary. I mean  so if y- if - Yeah. Yeah. Yeah. Yep. Um  yeah  so the next thing is to use this as - as maybe initialization and then use something on-line. But - And I expect improvement at least in Finnish because Uh-huh. Something more adaptive  yeah. O_K. eh - the way - Well  um  for Italian and Spanish it's - th- this value works good but not necessarily for Finnish. Mmm. But unfortunately there is  like  this forty millisecond latency and  um - Yeah  so I would try to somewhat reduce this @@ . I already know that if I completely remove this latency  so. um  it - um there is a three percent hit on Italian. Mm-hmm. i- d- Does latency - Sorry. Go ahead. Yeah. Your - your smoothing was @@ uh  over this s- so to say  the - the factor of the Wiener. And then it's  uh - What was it? This - Mm-hmm. this smoothing  it was over the subtraction factor  so to say. It's a smoothing over Was this done - the - the gain of the subtraction algorithm. Mm-hmm. And - and you are looking into the future  into the past. Right. And smoothing. Mm-hmm. So  to smooth this thing. Yeah. Um - And did - did you try simply to smooth um to smooth the - the - t- to - to smooth stronger the - the envelope? Um  no  I did not. Mmm. Mmm. Because I mean  it should have a similar effect if you - Yeah. I mean  you - you have now several stages of smoothing  so to say. You start up. As far as I remember you - you smooth somehow the envelope  you smooth somehow the noise estimate  and - Mm-hmm. Mmm - and later on you smooth also this subtraction factor. Uh  no  it's - it's just the gain that's smoothed actually but it's smoothed - Uh  actually I d- I do all the smoothing. Yeah  yeah. Ah. Oh  it w- it was you. Yeah. Uh - Yeah. Yeah. Yeah. No  in this case it's just the gain. And - Uh-huh. But the way it's done is that um  for low gain  there is this non- nonlinear smoothing actually. For low gains um  I use the smoothed sm- uh  smoothed version but - for high gain Uh. @@ it's - I don't smooth. Mm-hmm. I just  uh - it - Experience shows you  if - if you do the - @@ The best is to do the smoo- smoothing as early as possible. Uh-huh. So w- when you start up. I mean  you start up with the - with the - somehow with the noisy envelope. Mm-hmm. And  best is to smooth this somehow. Mm-hmm. Uh  yeah  I could try this. Um. And - So  before estimating the S_N_R  @@ smooth the envelope. Yeah. Yeah. Uh-huh. Mm-hmm. But - Yeah. Then I - I would need to find a way to like smooth less also when there is high energy. Cuz I noticed that it - it helps a little bit to s- like smooth more during low energy portions and less during speech  because if you smooth then y- you kind of distort the speech. Yes  y- Yeah. Yeah. Right. Um. Mm-hmm. Yeah  I think when w- you - you could do it in this way that you say  if you - if I'm - you have somehow a noise estimate  Mm-hmm. and  if you say I'm - I'm - with my envelope I'm close to this noise estimate  then you have a bad signal-to-noise ratio and then you - you would like to have a stronger smoothing. Yeah. Mm-hmm. So you could - you could Yeah. base it on your estimation of the signal-to-noise ratio on your actual - Mm-hmm. Mm-hmm. Mmm. Yeah  or some silence probability from the VAD if you have - Um  yeah  but I don't trust the current VAD. So. Yeah  uh  so not - not right now maybe. Well  maybe. The VAD later will be much better. Yeah. So. Maybe. I see. So is that it? Uh  fff I think that's it. Yeah. Uh. s- So to summarize the performance of these  SpeechDat-Car results is similar than - than yours so to say. Yeah  so the fifty-eight is like the be- some fifty-six point - Yeah  that's true. Yeah. Y- you have - you have fifty-six point four and - and - and dependent on this additive constant  it is Yeah. @@ Slightly better. Mm-hmm. s- better or - or worse. Yeah. @@ Yeah. Mm-hmm. And  yeah  i- i- i- the condition where it's better than your approach  it's - it - just because maybe it's better on well matched and that the weight on well matched is - Yeah. Yeah  you - you caught up . Yep  that's true. is bigger  because - if you don't weigh differently the different condition  you can see that your - well  the win- the two-stage Wiener filtering is maybe better or - Yeah. It's better for high mismatch  right? Yeah  it's better for high mismatch. Mm-hmm. But So over all it gets  yeah  worse for the well matched condition  so y- a little bit worse for well matched. Uh-huh. So we need to combine these two. Uh  that's - that's the best thing  is like the French Telecom system is optimized for the well matched condition. They c- Yeah. Mm-hmm. So they know that the weighting is good for the well matched  and so there's - everywhere the well matched's s- s- performance is very good for the French Telecom. Yeah. Mm-hmm. Mm-hmm. T- we are - we may also have to do something similar @@ . Mm-hmm. Well   our tradition here has always been to focus on the mismatched. Cuz it's more interesting. Um the - Mu- my - mine was it too  I mean. Yeah. Before I started working on this Aurora. so. Yeah. Yeah. Yeah. O_K. Carmen? Do you  uh - Well  I only say that the - this is  a summary of the - of all the V_T_S experiments and say that the result in the last um  for Italian - the last experiment for Italian  are bad. I make a mistake when I write. Up at D_ I copy one of the bad result. So you - And - There. You know  this. Um  well. If we put everything  we improve a lot u- the spectral use of the V_T_S but the final result are not still mmm  good like the Wiener filter for example. I don't know. Maybe it's - @@ That's somewhere - it's possible to - to have the same result. I don't know exactly. Mmm. Because I have  mmm  You s- you have a better r- worse result in medium mismatch Yeah. You have some results that are good for the high mismatch. and high mismatch. And - Yeah. I someti- are more or less similar but - but are worse. And still I don't have the result for T_I-digits. The program is training. Maybe for this weekend I will have result T_I-digits and I can complete that s- like this. Well. Uh. Right. One thing that I note are not here in this result but are speak - are spoken before with Sunil I - I improve my result using clean L_D_A filter. Mm-hmm. Mm-hmm. If I use  eh  the L_D_A filter that are training with the noisy speech  that hurts the res- my results. So what are these numbers here? Are these with the clean or with the noisy? This is with the clean. O_K. With the noise I have worse result  that if I doesn't use Uh-huh. it. But m- that may be because with this technique we are using really - really clean speech. The speech - the representation that go to the H_T_K is really clean speech because it's from the dictionary  the code book and maybe from that. I don't know. Because I think that you - did some experiments using the two - Mm-hmm. It's - the two L_D_A filter  clean and noi- and noise  and it doesn't matter too much. Um  yeah  I did that but it doesn't matter on SpeechDat-Car  but  it matters  uh  a lot on T_I-digits. It's better to use clean. Using the clean filter. Yeah  d- uh  it's much better when you - we used the clean derived Mm-hmm. Maybe you can do d- also this. L_D_A filter. Yeah. To use clean speech. Uh  but  yeah  Sunil in - in your result it's - Yeah  I'll try. I - I'll try the cle- No  I - I - my result is with the noisy - It's with the noisy one. Yeah. noisy L_D_A. Yeah. Oh! It's with the noisy. Yeah. It's - it's not the clean L_D_A. So - Um - It's - In - in the front sheet  I have like - like the summary. Yeah. And - and your result is with the - It's with the clean L_D_A. Oh. This is - Your results are all with the clean L_D_A result ? Yeah  with the clean L_D_A. Yeah. O_K. Is that the reason ? And in your case it's all - @@ . All noisy  yeah. all noisy  yeah. But - And - Uh - Yeah. @@ Uh - But I observe my case it's in  uh  uh  at least on SpeechDat-Car it doesn't matter but T_I-digits it's On T_I-digits this matters. like two or three percent absolute  uh  Absolute. better. Uh - So you really might wanna So if - Yeah  I - I - I will have to look at it. Yeah  that's true. try the clean I think. Yeah. Yeah  that could be sizeable right there. And this is everything. Yeah. Maybe you - you are leaving in - in about two weeks Carmen. No? O_K. Yeah. Yeah. So I mean  if - if - if I would put it - put on the head of a project mana- manager - I - I - I- I would say  uh  um - I mean there is not so much time left now. I mean  if - um  Be my guest. what - what I would do is I - I - I would pick @@ the best consolation  which you think  and And prepare at the s- c- create - create all the results for the whole database that you get to the final number as - as Sunil did it and um and maybe also to - to write somehow a document where you describe your approach  and what you have done. Yeah  I was thinking to do that next week. Yeah. Yeah. Yeah  I'll - I'll borrow the head back and - and agree. Yeah  I wi- I - I will do that next week. Yeah  that's - that's - Right. In fact  actually I g- I guess the  uh - the Spanish government  uh  requires that anyway. They want some kind of report from everybody who's in the program. So. Mm-hmm. And of course I'd - we'd - we'd like to see it too. So  yeah. O_K. So  um  what's - Do you think we  uh  should do the digits or skip it? Or what are - what do you think? Uh  we have them now? Yeah  got them. Uh  why don- why don't we do it? O_K. Just - just take a minute. I can send yet. Would you pass those down? Oh! Sorry. O_K  um  so I guess I'll go ahead. Um  This is transcript L_ dash two eight three. Six six  two eight  three six  five one  five zero. Eight  four six eight  seven five  two three nine  zero. Six two  three nine  zero six  four three  three nine. Five five  one six  nine four  three five  zero six. @@ Seat? Six seven one  three seven  nine four five one. Six five six nine  eight two eight four  nine four four six. Nine three one nine  eight  six five eight. Six six  five four  five eight  eight eight  nine five. Me? Transcript L_ dash three O_ three. One three six O_ eight four  O_ one O_. Dave? Is it the channel  or the mike? I don't remember. It's the mike? One O_ four  O_ one  two three eight O_. Mike? Mike five. Six seven three  seven four  seven four three O_. O_ six one  five three six  three nine one one. It's not four. Three two one  four seven  two four one seven. Eight  one eight zero  one eight  three eight seven  seven. Seven  three five eight  eight nine  four three eight  one. O_ eight three  seven three four  six one nine. Transcript L_ dash three zero four. This is date and time. One one  six four  eight zero  zero three  four nine. Eight two one  nine seven  four two zero eight. No. @@ Six two nine one  two six four five  six seven six one. On the channel  channel. Four eight two  seven four nine  five five eight. Six one zero  one one  nine zero  nine two. Six seven  zero three  four four  nine five  one two. Zero three  eight zero  two seven  four six  nine six. Eight one nine  one six three  eight zero four. This is transcript L_ dash three O_ five. Two four  two one  eight seven  two zero  eight three. Three  one O_ eight  nine six  three five one  zero. Six zero eight zero  O_ one eight three  four seven eight eight. Nine  nine one seven  zero six  seven O_ two  six. Five none - f- five nine one six  zero  four zero zero. One six six  one O_ four  eight nine eight five. Two nine zero  five three six  three six three seven. One four  two five  two two  seven zero  three one. Transcript L_ dash three O_ two. Five seven  eight five  eight seven  six two  zero nine. Seven two five  one six three  zero four four four. Eight nine  three zero  six two  eight two  seven six. Eight seven two nine  seven  zero one five. Three eight  zero four  five zero  four three  four seven. Eight nine two six  six one one nine  five seven three six. @@ Six two three  one zero five  nine zero seven. What is this? QUAL whispering Seven one four four  four two two two  seven two one eight. t- Transcript L_ dash three zero zero. five one  two four  eight nine  zero two  four three. Zero eight eight seven  six two two two  zero nine five six. Two two nine three  nine two four four  four nine three zero. Two three seven four  zero four two two  six five seven one. Five  four six zero  eight five  five zero six  five. Eight nine  one six  eight six  four zero  seven eight. Zero one two  nine four seven  seven eight one. Nine four two two  three two six four  eight zero five three. Transcript L_ dash three zero one. Seven six nine  eight eight seven  nine nine two five. Two  one four nine  one nine  eight nine eight  nine. Nine four one  one three four  nine five four three. Eight three nine  five six zero  four nine one one. One seven six  seven three four  three zero four. Six  four six nine  three eight  eight one eight  five. One two nine three  eight  two one two. One seven five eight  two zero zero nine  zero two five five. Transcript L_ dash two eighty-two. Six seven three  zero zero  zero five nine six. Seven three three  zero five one  eight five zero. Zero four  five seven  five five  five eight  two eight. Two nine one five  one  five eight one. Four three zero  six six nine  two zero seven three. Eight six three  eight one seven  seven three eight six. One five eight one  nine six three nine  five nine six seven. Five six  four two  five three  four seven  six two. O_K  if you could just leave  um  your mike on top of your  uh  digit form I can fill in any information that's missing. O_K. That's uh - I didn't get a chance to fill them out ahead of time. Yeah. The seat numbers have fallen off here. Yeah  we're gonna have to fix that. What - What are the seat numbers  I wonder? Uh  let's see  it starts with one here  and then goes around and ends with nine Seven. here. So I - I'm eight  So he's eight  you're seven  you're seven. I just put "" yes "". Would that be @@ Yeah. ","The Berkley Meeting Recorder project is well underway  and this meeting discusses the progress and ongoing issues. A pressing concern for the group is the DARPA meeting in July  which is only a short time away  and for which they would like to have some progress. Specifically  the group would like to have transcripts available  which would mean resolving legal issues for data use and on the basis of feedback from IBM get more transcription underway. Additionally they would also like to have the question answering mock-up and transcriber interface ready for then. PLP results for the front-end look good  with the group also reporting progress in segmentation: Thilo's segmenter will now be used and ways of improving performance investigated; The classifier segmentation is progressing well  especially in the use of prosody for identifying interruption. Work on the front end continues  with improvements of 3-5% being made. The group discussed how the digits should be recorded in the meeting. In the end they decided to record these in unison for all of the meeting participants as a whole. To improve the performance of Thilo's automatic segmenter  this is going to be retrained and adapted to run with Thilo's posteriors and speaker background models. Regarding transcription  no new transcribers will be employed until situation regarding IBM is clarified. Legal issues surrounding the approval and signing off of transcripts by participants has proved to be very complicated  and so will be sorted out off line by those involved by July. After finding discrepancies with the CMU researchers  the ICSI group have decided to tune the size of their Gaussian system. After raising the difficulty of checking for bugs in their generation of tandem features  they decide to check with Stephane who has more experience of these procedures. For the DARPA meeting in July  the group propose that they should have the question answering mock-up and transcriber interface ready for then  and also have data available. Unfortunately  there are legal issues regarding the approval of transcripts. Additionally  the group would like to have their data transcriptions in ""production mode"" by then. However the group do not want to hire more transcribers until IBM confirms in the next 2-3 weeks the acceptability of the data. Segmentation for the recogniser has been done by hand which the group consider ""cheating""  instead now they want to use Thilo's automatic segmenter. The classifier segmentation work is going well  but needs more data to improve results since non-native speaker data cannot be used. For the front-end  so far the group have been using a high number of Gaussians per cluster (64) rather than the ten per cluster used by researchers at CMU  therefore they need to tune their Gaussian system to the feature vector. The group  observed that it would be difficult to check for bugs in the generation of tandem features for the SRI system. Experimentation is taking place using different front-ends with the SRI recogniser. This is not yet complete  but PLP results are improving to match those of MFCC  with vocal tract length normalisation working ""beautifully"" on a training set of 24 hours  and giving overall improvement of between 3 and 5%. Thilo's automatic segmenter is now working  and although it has low precision  this is mediated by the high recall. The group will send IBM another sample file to check that the beep problems are fixed  and this should take 2-3 weeks. Progress on transcriptions has been made on 5 ""set one"" meetings  and two more transcribers set on. Pre-segmentation has proved useful. Meeting Recorder data of the 62 hours of meetings already analysed has been organised into a spreadsheet with the aim to make this available over the WWW. Classifier segmentation is expected to give better results from more data: currently ""cheating"" using word features for forced alignment  but looking to use other data such as ""spurts"". Prosodaic features looking promising for identifying interruptions. Generally the ICSI data offers better pitch features and vowel voicing than the Switchboard corpus due to the use of close talking mikes rather than telephone handsets. "
"Sorry. Mental - mental Palm Pilot. Right. Hence no problem. Let's see. So. What? I'm supposed to be on channel five? Her . Nope. Doesn't seem to be  yeah. Hello I'm channel one. What does your thing say on the back? @@ Nnn  five. Testing. Alright  I'm five. Sibilance. Sibilance. Three  three. I am three. Eh. See  that matches the seat up there. So. Yeah  well  I g- guess it's coming up then  or - Cuz it's - That starts counting from zero and these start counting from one. Ergo  the classic off-by-one error. But mine is correct. Is it? No. It's one. Channel one. Look at the back. Your mike number is what we're t- Oh  oh  oh! Oh. Ho! So - I've bested you again  Nancy. But your p- No  but the paper's correct. The paper is correct. Look at the paper. I didn't det- I was saying the microphone  not the paper. Nnn  it's n- Oh. It's always offset. Yeah. O_K. Yes  you've bested me again. That's how I think of our continuing interaction. Damn! Foiled again! So is Keith showing up? He's talking with George right now. Uh  is he gonna get a rip - uh - rip himself away from - What - He'll probably come later. He- he- he's from that? probably not  is my guess. Oh  then it's just gonna be the five of us? Yeah. Well  he - he was very affirmative in his way of saying he will be here at four. But you know  that was before he knew about that George lecture probably. Right. This - this is not - It's not bad for the project if Keith is talking to George. O_K. So my suggestion is we just Forge ahead. Forge ahead  yeah. Cool. Are you in charge? Sure. Um. Well  I sort of had informal talks with most of you. So  Eva just reported she's really happy about the C_B_T's being in the same order in the X_M_L as in the Yeah. um - be- Java declaration format so you don't have to The e- Uh  yeah. do too much in the style sheet Yeah  so. transversion . The - uh  Java - the embedded Bayes wants to take input - uh  uh  a Bayes-net - in - in some Java notation and Eva is using the Xalan style sheet processor to convert the X_M_L that's output by the Java Bayes for the - into the  uh  E_Bayes input. Mmm. Actually  maybe I could try  like  emailing the guy and see if he has any- something already. Sure. Hmm. That'd be weird  that he has both the Java Bayes and the embedded Bayes in - Yeah. But that's some sort of conversion program? Yeah. And put them into different formats. Oh - Yep  he could do that  too. I think you should demand things from him. He charges so much. Right. Yeah. No  I think it's a good idea that you may as well ask. Sure. Yeah. And  um  well pretty mu- pretty much on t- on the top of my list  I would have asked Keith how the ""where is X_?"" hand parse is standing. Um. But we'll skip that. Uh  there's good news from Johno. The generation templates are done. So the trees for - the X_M_L trees for the - for the gene- for the synthesizer are written. So I just need to do the  uh - write a new set of tree combining rules. But I think those'll be pretty similar to the old ones. So. Just gonna be - you know - Oh! You were gonna send me a note about hiring - Yes. I didn't finish the sentence but he understood it. I know what he's talking about. O_K. But Nancy doesn't. Hiring somebody. We - w- um - The guy. O_K  so No. natural language generation produces not a - just a surface string that is fed into a text-to-speech Yeah. but  a surface string with a syntax tree that's fed into a concept-to-speech. Mm-hmm. Now and this concept-to-speech module has certain rules on how if you get the following syntactic Better. structure  how to map this onto prosodic rules. Mm-hmm. Sure . Mm-hmm. And Fey has foolheartedly agreed to rewrite uh  the German concept- uh syntax-to-prosody rules - I didn't know she spoke German. No  she doesn't. But she speaks English. Oh  O_K. Oh. Rewrite the German ones into English. O_K  got it. Into English. And um therefore the  uh - if it's O_K that we give her a couple of more hours per week  then she'll do that. O_K  got it. What language is that written i- Is that that Scheme thing that you showed me? Yeah. That's the LISP-type scheme. She knows how to program in Scheme? I hope? No  I - @@ My guess is - I - I asked for a commented version of that file? If we get that  then it's doable  even without getting into it  even though the Scheme li- uh  stuff is really well documented in the Festival. Well  I guess if you're not used to functional programming  Scheme can be completely incomprehensible. Cuz  there's no - Like there's lots of unnamed functions and - You know? Syntax. Yeah. Mm-hmm. Anyway  it - We'll sort this out. Um. But anyway  send me the note and then I'll- I'll check with  uh  Morgan on the money. I - I don't anticipate any problem but we have to ask. Oh  so this was - You know  on the generation thing  um if sh- y- she's really going to do that  then we should be able to get prosody as well. So it'll say it's nonsense with perfect intonation. Are we gonna - Can we change the voice of the - of the thing  because right now the voice sounds like a murderer. Yep. We ha- we have to change the voice. Wh- Which one? The - the little Smarticus - Smarticus sounds like a murderer. Oh. That's good to know. ""I have your reservations."" But I will not give them to you unless you come into my lair. It is - Uh  we have the choice between the  uh  usual Festival voices  which I already told the SmartKom people we aren't gonna use because they're really bad. Festival? It's the name of some program  the - the synthesizer. Oh  oh. Got it. O_K. But  um You know  the usual party voices. Yeah  I know. That doesn't sound  exactly right either. O_G_I has  uh  crafted a couple of diphone type voices that are really nice and we're going to use that. We can still  um  d- agree on a gender  if we want. So we still have male or female. I think - Well  let's just pick whatever sounds best. Hmm? Whatever sounds best. Uh. Does O_G_I stand for - ? Unfortunately  probably male voices  a bit more research on. So. Orego- Original German Institute? Or- Oregon. Try Oregon. Oregon @@ Graduate Institute Oh. Oregon Graduate Insti- Ah. It turns out there's the long-standing links with these guys in the speech group. Hmm! Very long. Hmm! Hmm. In fact  there's this guy who's basically got a joint appointment  Hynek Hermansky. He's- spends a fair amount of time here. Anyway. Leave it. Won't be a problem. O_K. And it's probably also absolutely uninteresting for all of you to  um learn that as of twenty minutes ago  David and I  per accident  uh managed to get the whole SmartKom system running on the - uh  ICSI Linux machines with the ICSI N_T machines thereby increasing the number of running SmartKom systems in this house from one on my laptop to three. Mmm  that's good. How was this by accident? Yeah  I know. Tha- that's the part I didn't understand. Um  I suggested to try something that was really kind of - even though against better knowledge shouldn't have worked  but it worked. Hmm! Intuition. Maybe - maybe - maybe a bit for the A_I i- intuition thing. Will it work again  or - ? Yeah. Yeah. O_K. And  um  we'll never found out why. It- it's just like why - why the generation ma- the presentation manager is now working? Hmm! Which This is something you ha- you get used to as a programmer  right? You know  and it's cool  it works out that way . Hmm. So  the - the people at Saarbruecken and I decided not to touch it ever again. Yeah  that would work . O_K. Um - I was gonna ask you where something is and what we know about that. Where - O_K. Where the ""where is"" construction is. Where is X_? What - what thing is this? O_K. Oh  but by - Uh  we can ask  uh  did you get to read all four hundred words? Was it O_K? Was it? I did. I - I wa- I was looking at it. It doesn't follow logically. Yeah. It doesn't - The first paragraph doesn't seem to have any link to the second paragraph. And so on. Hmm. That - Yeah. Yeah. You know  i- Yeah  it - Each paragraph is good  though. I li- I- i- Yeah. Well  it- it's fine. It was written by committee. Anyway. Um. But c- the meeting looks like it's  it's gonna be good. So. Yeah. I think it's uh - Yeah  I didn't know about it until Robert told me  like  Yeah  I - I ra- I ran across it in - I don't even know where  you know - some just - some weird @@ place. And  uh  yeah  I- I'm surprised I didn't know about it Y- yeah. Well  yeah. I was like  why didn't Dan tell me? since we know all the invited speakers  an- Right  or some- Anyway. Right. So - But anyway  yeah. I- so I - I did see that. Oh wha- Yeah. Before we get started on this st- so I also had a nice email correspondence with Daphne Kohler  who said yes indeed she would love to work with us on the  um  - you know  using these structured belief-nets and stuff but starting in August  that she's also got a new student working on this and that we should get in touch with them again in August and then we'll figure out a way for you - uh - you to get seriously connected with  um their group. So that's  uh - looks pretty good. And um - Yeah  I'll say it now. So  um - And it looks to me like we're now at a good point to do something - start working on something really hard. We've been so far working on things that are easy. Oh! Uh  w- Which is mental spaces and uh - Hmm! and-or - It's hard. Huh? Yeah  it's hard. Yeah. It's a hard puzzle. But the other part of it is the way they connect to Yeah. these  uh  probabilistic relational models. So there's all the problems that the linguists know about  about mental spaces  and the cognitive linguists know about  but then there's this problem of the belief-net people have only done a moderately good job of dealing with temporal belief-nets. Uh  which they call dynamic - they incorrectly call dynamic belief-nets. So there's a term "" dynamic belief-net""  doesn't mean that. It means time slices. And Srini used those and people use them. Mmm. Uh. But one of the things I w- would like to do over the next  uh  month  it may take more  is to st- understand to what extent we can not only figure out the constructions for them for multiple worlds and uh sort of what the formalism will look like and where the slots and fillers will be  but also what that would translate into in terms of belief-net and the inferences. So the story is that if you have these probabilistic relational models  they're set up  in principle  so that you can make new instances and instances connect to each other  and all that sort of stuff  so it should be feasible to set them up in such a way that if you've got the past tense and the present tense and each of those is a separate uh  belief structure that they do their inferences with just the couplings that are appropriate. But that's g- that's  as far as I can tell  it's - it's putting together two real hard problems. One is the linguistic part of what are the couplings and - and when you have a certain  uh  construction  that implies certain couplings and other couplings  you know  between let's say between the past and the present  or any other one of these things and then we have this inference problem of exactly technically how does the belief-net work if it's got um  let's say one in - in  you know  different tenses or my beliefs and your beliefs  or any of these other ones of - of multiple models. So um you know  in the long run we need to solve both of those and my suggestion is that we start digging into them both  uh  in a way we- that  you know  th- hopefully turns out to be consistent  so that the - Um. And sometimes it's actually easier to solve two hard problems than one because they constrain each other. I mean if you've got huge ra- huge range of possible choices Yeah. um - We'll see. But anyway  so that's  um - Oh yeah  like uh  I solved the - the problem of um - we were talking about how do you - various issues of how come a plural noun gets to quote ""count as a noun phrase""  you know  occur as an argument of a higher construction  but a bare singular stem doesn't get to act that way. Um  Right. and it would take a really long time to explain it now  but I'm about to write it up this evening. I solved that at the same time as ""how do we keep adjectives from floating to the left of determiners and how do we keep all of that from floating outside the noun phrase"" to get something like ""I the kicked dog"". Um. That's great. Did it - did it at once. So maybe - maybe it'll be a similar thing. Yeah. Cool. No  I know  I th- I- I think that is gonna be sort of the key to this wh- to th- the big project of the summer of - of getting the constructions right is that people do manage to do this so there probably are some  uh  relatively clean rules  they're just not context-free trees. Right. And if we - if the formalism is - is good  then we should be able to have  you know  sort of moderate scale thing. And that by the way is - is  Keith  what I encouraged George to be talking with you about. Not the formalism yet Mm-hmm. but the phenomena. Yeah. The p- And - Oh  another thing  um there was this  uh thing that Nancy agreed to in a - in a weak moment this morning Hmm! that I was really strong. Hmm! Hmm. Uh  sorry. In a - in a friendly moment. Same thing. Anyway  uh  that we were - that we're gonna try to get a uh  first cut at the revised formalism by the end of next week. O_K? Probably skipping the mental spaces part. Alright. Seems - Right. I do. Uh  just trying to write up essentially what - what you guys have worked out so that everybody has something to look at. We've talked about it  but only the innermost inner group Mm-hmm. currently  uh  knows  uh Knows. O_K. Yeah  and - and not even all of them really do. But like - Yeah. Right. There's - The group as a whole knows but no individual member kno- Well that that - yeah th- there's one of the advantages of a document  right?  is - is that Yeah. Right. it actually transfers from head to head. So anyway. So um - O_K. Ah  communication! Huh? Communication. Communication  documentation and stuff. Anyway  so  uh  with a little luck - Hunh! Uh - l- let's  let's have that as a goal anyway. And - So  uh  what was the date there? Monday or - ? No  no  no. It's a Friday. No  w- uh - we're talking about a week fr- e- end of next week. But  uh  but - but the two of us will probably talk to you at well before th- I mean. End of next week. I thought you said beginning of n- Yeah. Anyway  w- let's talk separately about how t- Yeah  I have a busy weekend but after that - Yeah  gung-ho. O_K. Yeah  so - so someti- sometime next week. Great  Now if it turns out that that effort leads us into some big hole Mm-hmm. that's fine. O_K. You know  if you say we're - we're dump - dump - dump . There's a really hard problem we haven't solved yet - that  that's just fine. O_K. Mm-hmm. But at - at least sort of try and work out what the state of the art is right now. Right  t- t- if - to the extent that we have it  let's write it O_K. and to the extent we don't  let's find out what we need to do. O_K. So  uh Can we - ? Is it worth thinking of an example out of our tourism thing domain  that involves a - a - a decent mental space shift or setting up - I think it is  but - uh - but I interrupted before Keith got to tell us what happened with ""where is the Powder-Tower?"" or whatever Right. Well. Uh  what was supposed to happen? I've sort of been actually caught up in some other ones  so  um  you know  I don't have a write-up of - or I haven't elaborated on the ideas that we were already talking about which were - Hmm  yeah. I think - I think we already came to the conclusion that we have two alternative paths that we - two alternative ways of representing it. One is sort of a - has a um It's gone. um The question of whether the polysemy is sort of like in the construction or pragmatic. One of them was th- Right. or comes - is resolved Right. later. Yeah. I think it has to be the - the second case. Yeah. Um  so d'you - Is it clear what we're talking about here? I agree. Uh - The question is whether the construction is semantic or like ambiguous between asking for location and asking for path. Um It's - So you might be - yeah  y- And asking for directions. Should we have a - a - a - or - or whether the construction semantically  uh  is clearly only asking for location but pragmatically that's construed as meaning ""tell me how to get there"". Uh - Mm-hmm. So assume these are two  uh  nodes we can observe in the Bayes-net. So these are either Yep. Yeah. Right. true or false and it's also just true or false. If we encounter a phrase such as ""where is X_?""  should that set this to true and this to true  and the Bayes-net figures out which under the c- situation in general is more likely? Um  or should it just activate this  have this be false  and the Bayes-net figures out whether this actually now means - ? Uh w- that's a s- O_K  so that's a - that's a separate issue. So I a- I I th- I agree with you that  um  it's a disaster to try to make separate constructions for every Slightly different. O_K. uh  pragmatic reading  although there are some that will need to be there. Mm-hmm. Good. Mm-hmm. Right. Right. I mean  there- there's some that - Or have every construction list all the possible pragmatic implications of the same one. Right. You can't do that either. Yeah. But  you know  c- um - almost certainly ""can you pass the salt"" is a construction worth Yeah. Yeah. noting that there is this th- this - this - this - uh Request. Mm-hmm. Yeah. Very yeah. So right  this one is maybe in the gray area. Is it - is it like that or is it just sort of Ri- Mmm. Yeah. One obvious from world knowledge that no one - you wouldn't want to know the location without wanting to know how to get there or whatever. Or in some cases  it's - it's quite definitely s- so that you just know - wanna know Yeah. Yeah. where it is. Well the question is basically  is this conventional or conversational implicature? Exactly. Yeah. Might be  yeah. And I guess  see  the more important thing at this stage is that we should be able to know how we would handle it in ei- f- in the short run it's more important to know how we would treat - technically what we would do if we decided A_ and what we would do if we decided B_  than it is t- to decide A_ or B_ r- Right. O_K  right. Right. Which one it is. Which of that is. Yeah  O_K Hmm. right now. Cuz there will be other k- examples that are one way or the other. Right. W- we know for sure that we have to be able to do both. So I guess Yeah. In the short run  let's - let's be real clear on h- what the two alternatives would be. O_K. And then the- we had another idea floating around um  which we wanted to  uh  get your input on  and that concerns the - But the nice thing is w- we would have a person that would like to work on it  and that's Ir- Irina Gurevich from E_M_L who is going to be visiting us  uh  the week before  uh  August and a little bit into August. And she would like to apply the ontology that is  um being crafted at E_M_L. That's not the one I sent you. The one I sent you was from G_M_D  out of a European CRUMPET. It was terrible. Agreed. Um  and one of the reas- one of the - those ideas was  so  back to the old Johno observation that if y- if you have a dialogue history and it said the word "" admission fee"" was uh  mentioned um  it's more likely that the person actually wants to enter than just take a picture of it from the outside. Now what could imagine - to  you know  have a list for each construction of things that one should look up in the discourse history  yeah? That's the really stupid way. Then there is the really clever way that was suggested by Keith and then there is the  uh  middle way that I'm suggesting and that is you - you get X_   which is whatever  the castle. The ontology will tell us that castles have opening hours  that they have admission fees  they have whatever. And then  this is - We go via a thesaurus and look up certain linguistic surface structures that are related to these concepts and feed those through the dialogue history and check dynamically for each e- entity. We look it up check whether any of these were mentioned and then activate the corresponding nodes on the discourse side. But Keith suggested that a - a much cleaner way would be - is  you know  to keep track of the discourse in such a way that you - if you know that something like that ha- has been mentioned before  this just a- continues to add up  you know  in th- in a - So if someone mentions admission f- fees  that activates an Enter schema which sticks around for a little while in your rep- in the representation of what's being talked about. And then when someone asks ""where is X_?"" you've already got the - the Enter schema activated and you're able to - to conclude on it. Yeah. Mm-hmm. Kind of a priming Yeah. priming a spreading activation Right. Yeah. So that's certainly more realistic. I m- I mean psychologically. Now technically Right. Yeah. Um Well  uh  is it - doesn't it seem like if you just managed the dialogue history with a - a thread  that you know  kept track of ho- of the activity of - I mean  cuz it would - the - the thread would know what nodes like  needed to be activated  so it could just keep track of how long it's been since something's been mentioned  and automatically load it in. Yeah. You could do that. Um. But here's - here's a way - in th- in the bl- Bayes-net you could - you could think about it this way  that if um at the time ""admissions fee"" was mentioned you could increase the probability that someone wanted to enter. Turn prior on . We- yeah - th- th- that's what I wa- I wasn't - I was - I wasn't thinking in terms of Enter schemas. I was just - Fair enough  O_K  but  but  in terms of the c- c- the current implementation - right? so that um It would already be higher in the context. th- that th- the - the - the conditional probability that someone - So at the time you mentioned it - This is - this is essentially the Bayes-net equivalent Mm-hmm. of the spreading activation. It's - Yeah. In some ways it's not as good but it's the implementation we got. We don't have a connectionist implementation. Yeah  sure. No  I mean Now - Now my guess is that it's not a question of time but it is a question of whether another intervening object has been mentioned. Yeah  relevance. Yeah. Yeah. I mean  we could look at dialo- this is - Of course the other thing we ha- we do is  is we have this data coming Yeah. which probably will blow all our theories  but - but skipping that - so - so - but my guess is what - what'll probably will happen  Yeah  right. Here's a - here's a proposed design. is that there're certain constructions which  uh  for our purposes do change the probabilities of EVA decisions and various other kinds and th- that the  uh  standard way that - that the- these contexts work is sort of stack-like or whatever  but that's sort of the most recent thing. And so it could be that when another uh  en- tourist entity gets mentioned  you Renew @@ Yeah. re- re- essentially re-initiali- you know  re-i- essentially re-initialize the state. Mmm. Mm-hmm. And of course i- if we had a fancier one with multiple worlds you could have - uh  you could keep track of what someone was uh saying about this and that. You know  ""I wanna go - in the morning I wanna -"" Yeah. ""Here's my plan for today. Here's my plan for tomorrow."" hypothetically. Yeah  or - Yeah  in the morning morning I- I'm planning t- to go shopping  in the afternoon to the Powder-Tower - Yeah. Uh  tal- so I'm talking about shopping and then you say  uh  you know  well  um ""What's it cost?"" or something. Or - Anyway. So one could well imagine  but not yet. Mm-hmm. Yeah. But I do th- think that the - It'll turn out that it's gonna be - depend pretty much on whether there's been an override. Yeah  I mean  if - if you ask ""how much does a train ride and - and cinema around the vineyards cost?"" and then somebody tells you it's sixty dollars and then you say ""O_K How much is  uh - I would like to visit the -"" whatever  something completely different  ""then I go to  you know  Point Reyes""  Yeah. it - it's not more likely that you want to enter anything  but it's  as a matter of fact  a complete rejection of entering Right. Right. Right. Yeah. by doing that. So when you admit- have admission fee and it changes something  it's only for that particular - It's relational  right? It's only for that particular object. Yeah  I th- th- Yeah. Well  and - and - and the simple idea is that it's on- it's only for m- for the current Yeah. uh  tourist e- entity of instre- interest. Yeah. Right. Yeah. But that's - I mean this - this function  so  has the current object been mentioned in - in - with a question about - No  no. It's - it - It goes the other d- it goes in the other direction. concerning its - Is - When th- When the - this is mentioned  the uh probability of - of  let's say  entering changes- Of that object. changes. For - But - Right. You could just hav- uh  just basically  ob- it - It observes an - er  it sets the - a node for Yeah. Yeah. ""entered"" or ""true"" or something  ""discourse enter"". Now  uh - But I think Ro- Robert's right  that to determine that  O_K? you may well want to go through a th- thesaurus and - and - So  if the issue is  if - so now th- this construction has been matched and you say ""O_K. Does this actually have any implications for our decisions? "" Then there's another piece of code that presumably does that computation. So  sort of forward chaining in a way  rather than backward. O_K. Yeah. Yeah. But - but what's Robert's saying is - is  and I think he's right  is you don't want to try to build into the construction itself all the synonyms and all - you know  all the wo- Uh maybe. I'll have to think about that. Hmm. I don't know. I mean it - th- I can thi- I can think of arguments in either direction on that. But somehow you want to do it. Mm-hmm. Well  it's just another  sort of  construction side is how to get at the possible inferences we can draw from the discourse history or changing of the probabilities  and-or - Guess it's like - I g- The other thing is  whether you have a m- m- user model that has  you know  whatever  a current plan  whatever  plans that had been discussed  and I don't know  I mean - What - uh  what's the argument for putting it in the construction? Is it just that the s- synonym selection is better  or - ? Oh  wel- Well  the ar- the - The argument is that you're gonna have the - If you've recognized the word  you've recognized the word  which means you have a lexical construction for it  so you could just as well tag the lexical construction with the fact that it's a uh  you know  thirty percent increase in probability of entering. You - So you could - you could - you could invert - invert the whole thing  so you s- you tag that information on to the lexicon since you Mmm. Oh  I see. had to recognize it anyway. That - that's the argument in the other direction. at - at - Yeah  and this is - Even though uh the lexical construction itself - out - out of context  uh  won't do it. I mean  y- you have to keep track whether the person says ""But I- but I'm not interested in the opening times"" Yeah. is sort of a- more a V_ type. Yeah there's  yeah ther- there's that as well. Yep. Hmm. So. But  we'll - @@ uh  we have time to - This is a s- just a sidetrack  but uh I think it's also something that people have not done before  is um  sort of abuse an ontology for these kinds of  uh  inferences  on whether anything relevant to the current something has been - uh  has crept up in the dialogue history already  or not. And  um I have the  uh - If we wanted to have that function in the dialogue hi- dialogue module of SmartKom  I have the written consent of Jan to put it in there. Good. O_K. Well  this - this is highly relevant to someone's thesis. Yes  um. That's - uh  I'm - I'm You've noticed that. keeping on good terms with Jan. Yeah. O_K. So the point is  it's very likely that Robert's thesis is going to be along these lines  and the local rules are Oh  s- if it's your thesis  you get to decide how it's done. O_K. So if  you know - if this is - seriously  if this becomes part of your thesis  you can say  hey we're gonna do it this way  that's the way it's done. Mmm. Yay  it's not me. It's always me when it's someone's thesis. No  no  no! No  no. We've got a lot - we've got a lot of theses going. Now it's not. Yay! I know it is. It's so nice! There's a few of us around now. Yeah. Right. Well  let's - let's talk after Friday the twenty-ninth. Then we'll see how f- f- Right. So h- he's got a th- he's got a meet- meeting in Germany with his thesis advisor. Yeah  he said he's gonna f- finish his thesis by then. Oh yeah. Yeah. I should try to finish it by then. Yeah. So. Oh  right. Um. Yeah. So I think in fact  That's the other thing. uh  this is - this is  speaking of hard problems  this is a very good time um  to start trying to make explicit where construal comes in and - you know  where c- where the construction per- se ends and where construal comes in  cuz this is clearly part of th- Huh? Mm-hmm. Mm-hmm. Yeah  we've - we've done quite a bit of that. We've been doing quite a bit of that. Yeah. Yeah. Well I said. But that's part of what the f- We have many jobs for you  Ro- Robert. The conclusion. Yeah  it seems to always land in your category. You're lucky. Yeah. Well  he's gonna need this. Yeah. Right. So. Right. So thing - That's part of why we want the formalism  is - is because th- it is gonna have implicit in it Yeah. Was I? In the room? No  you weren't there on purpose. Like - Yeah. Made it much easier to make these decisions. Uh. Obviously. Yeah. Right. Well I - That's tentative. They aren't decisions  they're ju- they're just proposals. Yeah. Right  right  right. No  they're decisions. O_K. Yes. Excuse me. Yeah  that - That's the point  is - is th- Yeah. Constraints. Let's call them constraints  around which one has to - Yeah. Actually  yeah. There's a problem with that word  too  though. Yeah. Anyway. But so that's that's w- Yeah  but it - he- the decisions I made wer- had to do with my thesis. So consequently don't I get to decide then that it's Robert's job? Yeah. No. Uh. Anyhow. Well  I'll just pick a piece of the problem and then just push the hard stuff into the center and say it's Robert's. Like. I've always been completely in favor of consensus decisions  so we'll - we'll find a way. I can - Right. Well  we - we - we will  but um I haven't. O_K. It - it might even be interesting then to say that I should be forced to not - um  sort of pull some of the ideas that have been floating in my head out of the  uh - out of the top hat Yes. and  um - Always good. Right. That metaphor is not going anywhere  you know. So- Yeah. Ri- No. Absolutely. So  uh  wh- you had - you know you ha- You had done one draft. Yes  and  um  it's - And a- another draft I didn't get Ha- None of that is basically still around  but it's - O_K. Oh  I guess it's good I didn't read it. D- i- @@ That's normal. I- i- I - this is - I'm shocked. This is the first time I've seen a thesis proposal change. Really? Right. Anyway  uh. So. But  yeah  a second - that would be great. So  uh  a sec- I mean you're gonna need it anyway. Hmm. Yeah  and I would like to d- discuss it and  you know  get you guys's input and make it sort of bomb-proof. and Right. Yep. Bomb proof! Good. Oh! Oh  O_K. Bullet-proof. That's the word I was looking for. Both proof . Either way. Both. Right. Good luck. Really. Uh So that  so th- thi- this - I mean  so this is the point  is we - we're going to have to cycle through this  but Yeah. th- the draft of the p- proposal on the constructions is - is going to tell us a lot about what we think needs to be done by construal. O_K. And  um  we oughta be doing it. Yeah  we need - we need some - Then we need to make some dates. Um. @@ Meeting - regular meeting time for the summer  we really haven't found one. We did Thursdays one for a while. I just talked to Ami. It's- it's a coincidence that he can't do - couldn't do it today here. @@ Usually  he can. Usually he has no real constraints. So - And the N_T_L meeting moved to Wednesday  cuz of - of  uh Yeah  it was just an exception. Yeah  you weren't here  but - but - but - s- uh  - And so  if that's O_K with you  you would - It's i- Is it staying basically at the Wednesday noon? Yeah  it was th- Yeah. I always thought it was staying. O_K. It was th- off this week  yeah. Yeah  I thought it was just this week that we were changing it. Mmm. Yeah. Right. O_K. And  um. How do we feel about doing it Wednesdays? Because it seems to me that this is sort of a time where when we have things to discuss with other people  there - they seem to be s- tons of people around. The only disadvantage is that it may interfere with other Or - subgroup meetings s- you know  other - other - No  you - Uh  people in this group connecting with - with Those people who happen to be around. those people who - who might not be around so much. Uh  I don't care. I- I- uh you know I have no fixed - To tell you the truth  I'd rath- I'd  I'd - would like to avoid more than one I_C_S_I meeting per day  if possible. But - I mean. I don't know. Whatever. O_K. No  that's fine. I mean that - The - I'd like to have them all in one day  so package them up and then - Yeah  I can understand that. Well p- people - people differ in their tastes in this matter. I - I'm neutral. Yeah. Yeah. Yeah. I'm always here anyway  so - It's O_K  that - Yeah. @@ That's - Me too. I'm basically - I'm here. So. It doesn't matter. Well  if - one sort of thing is  this room is taken at - after three-thirty pr- pretty much every day by the data collection. So we have subjects anyway - Except for this week  we have subjects in here. That's why it was one. Oh. Oh. O_K. So we just knew i- So did you just say that Ami can't make one o'clock? No  he can. Oh  O_K. Oh. So let's say Thursday one. But for next week  this is a bit late. So I would suggest that we need to - to talk - Oh  oh  O_K. O_K. About the c- the - th- Could we do Thursday at one-thirty? Would that - No. that be horrible? Yes. Oh really? Because  uh  this room is again taken at two-thirty by Morgan. Oh  O_K. O_K. You didn't tell me that. O_K  that's fine. And the - s- meeting recorder meeting meeting meeting recording on meeting meetings - O_K  O_K  O_K. O_K. Yeah. So. Interesting. So you're proposing that we meet Tuesday. Ah  yeah. How about that? Well  we're meeting Tuesday. I mean we usually meet Tuesday - or l- like  linguists um  at two. Next week. I - I could Would it - That's right. So. And the s- Do you want to meet again here bef- I mean w- Is the Speech-Gen meeting still at - on Tuesdays? Well  actually we- w- we- we did scrap our Monday time just because Bhaskara couldn't Hhh. Maybe I do need a Palm Pilot. come Monday. So there's - Nothing's impeding Monday anymore either. That doesn't apply to a - Get a fresh start - Although I thought you wanted to go camping on Monday - er  take off Mondays a lot so you could go camping. Yeah  that's another s- thing. Yeah. But  um. I mean  there are also usually then holidays anyways. I mean like - Usually? Sometimes it works out that way. So. Hmm! Well  I mean  the linguists' meeting i- happens to be at two  but I think that's - I mean. That should be relatively flexible be- - pretty flexible  I think. So. Yeah. There's just sort of the The multiple meetings yeah. two to four of us. Right? Yeah. So. Right. And  you know  of course Nancy and I are just sort of always talking anyway and sometimes we do it in that room. So  you know  I mean. Yeah. O_K  so l- forget about the b- the camping thing. So let's - eh  any other problems w- w- w- ? But  I suggested Monday. If that's a problem for me then I shouldn't suggest it. Ha-ha-ha. So. O_K. Um  all of the proposed times sound fine with me. Same here. Monday? O_K  whate- I mean - What I think Robert's saying is that Earlier in the week earlier we - At least for next week  there's a lot of stuff we want to get done  so why don't we plan to meet Monday and we'll see if we want to meet any more than that. Mm-hmm. Yeah. Mmm. O_K. What time? O_K. At o- o- o- o- one  two  three - ? One  two  three? Three's too late. Two-thirty? Oh  I i- Yeah  I actually - Two is the earliest I can meet on Monday. O_K  two. Here I'm blissfully agreeing to things and realizing that I actually do have some stuff scheduled on Monday. Sure. Sounds great. Uh  so that's the eighteenth. You guys will still remind me  right? No way! Y- you'll come and take all the - the headph- the good headphones first and then remind me. W- why do you - ? Yeah  exactly. And Sorry  two P_M. Why do I have this unless I'm gonna write ? Fine. do I get to see th- uh  your formalism before that? Yes. Uh. Would you like to? Mm-hmm. O_K. I was actually gonna work on it for tomorrow - like this - this weekend. I wo- I would like - I would sort of get a - get a notion of what - what you guys have in store for me. Yeah. Well m- @@ you know  w- maybe Mond- Maybe we can put - This is part of what we can do Monday  if we want. Yeah. I- O_K. Alright. O_K. I mean  I - I - I - Is some - some version Yeah  so there was like  you know  m- m- in my head the goal to have like an intermediate version  like  everything I know. Mm-hmm. And then  w- I would talk to you and figure out everything you know  that - you know  see if they're consistent. Yeah. O_K. Why don't w- Maybe you and I should meet sort of more or less first thing Monday morning and then we can work on this. Yes. Yeah. That's f- fine with me. So. O_K. You- y- I might - I might - um  s- You said you're busy over th- until the weekend  right? Yeah  sort of through the weekend because Kate has a photography show. That's fine. So we might continue our email Yeah. thing and that might be fine  too. So  maybe I'll send you some - Um  if you have time after this I'll show you the noun phrase thing. O_K. That would be cool. So. O_K  and we'll - So the idea is on Monday at two we'll - You wanna m- we'll see an intermediate version of the formalism for the constructions  and do an on-line merging with my construal ideas. Yeah. So that's O_K for you - Sure  sure. O_K. Alright. So it won't be  like  a for- semi-formal presentation of my proposal. It'll be more like towards finalizing that proposal. O_K  that's fine. That's O_K. O_K. Cuz then you'll find out more of what we're making you do. Yep  and then - Hmm  hmm. Yeah. Yikes. We'll make a presentation of your propo- of your proposal. Oy  deadlines. Perfect. Can you also write it up? It's like  ""this is what we're doing. And the complement is Robert."" Abso- I'll - I'll send you - I'll - I'll send you a style file  right? You just - O_K. I already sent you my fi- my bib file. So. O_K. And  um. Sounds good. Someday we also have to - we should probably talk about the other side of the ""where is X_"" construction  which is the issue of  um  how do you simulate questions? What does the simspec look like for a question? Yeah. Mm-hmm. Because it's a little different. We had to - we had an idea for this which seemed like it would probably work. Yeah. Yeah  now  we- we w- Great. O_K. Yeah. Simspec may need - we may n- need to re-name that. I - Yeah. I - Yeah. O_K? So let's think of a name for - for whatever the - this intermediate structure is. Oh  we talked about semspec  for ""semantic spec- specification"" and that seems - Mmm. Um. It's more general @@ You know  so it's a m- minimal change. Only have to change one vowel. That's great. All the old like graphs  just change the - just  like  mark out the - Yeah. Just - Right. Right  a little substi- substi- You know  that's what text substitution Cool. Yeah. It's good for you. Yeah. uh macros are for. Anyway  uh  so let's - let's for the moment call it that until we think of something better. O_K. And  yeah  we absolutely need to find - Part of what was missing were markings of all sorts that weren't in there  incl- including the questions - We didn't - we never did figure out how we were gonna do emphasis Mm-hmm. Yeah. in - in uh  the semspec. Yeah  we've talked a little bit about that  too  which - uh  uh  it's hard for me to figure out with sort of our general linguistic issues  how they map onto this particular one  but - Yeah. O_K  yeah  understood. But that's part of the formalism - is got to be uh  how things like that get marked. Mm-hmm. W- do you have data  like the - the - You have preliminary data? Cuz I know  you know  we've been using this one easy sentence and I'm sure you guys have - uh  maybe you are the one who've been looking at the rest of it - it'd - it'd be useful for me  if we want to have it a little bit more Um  I- To tell you the truth  what I've been looking at has not been the data so far  I just sort of said ""alright let's see if I can get noun phrases and  uh  major verb co- uh  constructions out of the way first."" data oriented. Yeah. Mm-hmm mm-hmm. And I have not gotten them out of the way yet. Surprise. So  um. Mm-hmm. Yeah. So  I have not really approached a lot of the data  but I mean obviously like these - the - the question one  since we have this idea about the indefinite pronoun thing and all that  you know  I ca- can try and  um run with that  you know  try and do some of the sentence constructions now. It would make sense. O_K. Do you wanna run the indefinite pronoun idea past Jerry? O_K. Oh yeah  the basic idea is that um  uh you know - Uh  let's see if I can formulate this. So Mary fixed the car with a wrench. Yeah. So you perform the mental sum and then  you know  "" who fixed the car with a wrench?"" You basically are told  to - to do this In the - in - analogously to the way you would do "" someone fixed the car with a wrench"". And then you hand it back to your hippocampus and find out what that  you know  Means. means  and then come up with that - so who that someone was. The W_H question has this as sort of extra thing which says ""and when you're done  tell me who fills that slot"" or w- you know. So  um. Mm-hmm. And  you know  this is sort of a nice way to do it  the idea of sort of saying that you treat - from the simulation point of view or whatever - you treat  uh  W_H constructions similarly to uh  indefinite pronouns like ""someone fixed the car"" because lots of languages  um  have W_H questions with an indefinite pronoun in situ or whatever  and you just get intonation to tell you that it's a question. Use actually the same one. Alright  which is So it makes sense um Skolemization. Hmm? In - in logic  it's - it's - @@ it's actual- Huh? What? Mmm. Right. Let's put a Skolem - Skolem constant in  yeah. Yeah. shko- Sure. Yeah. Right. O_K. That- that's not - that's not saying it's bad  it's just that - Right. Right. No. Of course. that - that - the logicians have - have  uh - Mmm. That's right. come up with this It makes sense from that point of view  too  which is actually better. So yeah  um. Anyway  but just that kind of thing and we'll figure out exactly how to write that up and so on  but Good. Uh  no  all the focus stuff. We sort of just dropped that cuz it was too weird and we didn't even know  like  what we were talking about exactly  what the object of study was. So. Um-mmm. Yeah. Well  if - if - I mean  i- part of - of what the exercise is  t- by the end of next week  is to say what are the things that we just don't have answers for yet. Yeah. That's fine. I mean Yep. Mm-hmm. Well  if you - if you do wanna discuss focus background and then get me into that because - I mean  I wo- I w- scientifically worked on that for - for almost two years. Yeah. O_K  then certainly we will. Good. Yeah  you should definitely  um be on on that - maybe - maybe by - after Monday we'll - y- you can see what things we are and aren't - Yeah. w- We should figure out what our questions are  for example  to ask you. So. Yeah. Yeah. O_K. O_K. Wel- then- t- Hans. Has - I haven't seen Hans Boas? He's been around. Yeah. O_K. So has he been - been involved with this  or - ? Just maybe not today. Eh. with us? Yeah. I would say that tha- that those discussions have been primarily  um  Keith and - Keith and me  but um Yeah. Yeah. like in th- the meeting - I mean  he sort of - I thin- like the last meeting we had  I think we were all very much part of it but um Yeah. Sometimes Hans has been sort of coming in there as sort of like a devil's advocate type role or something  like ""This make - you know  I'm going to pretend I'm a linguist who has nothing to do with this. This makes no sense."" And he'll just go off on parts of it which definitely need fixing but aren't where we're at right now  so it's but different perspec- Yeah. Right. Like - like what you call certain things  which we decided long ago we don't care that much right now. Yeah. Right. O_K. But in a sense  it's good to know that he of all people - you know  like maybe a lot of people would have m- much stronger reactions  so  you know  he's like a relatively friendly linguist and yet a word like ""constraint"" causes a lot of problems. Yeah. Yeah. O_K. This is consistent with And  so. Right. So. um the role I had suggested that he - he play  Ah. Mm-hmm. O_K  which was that o- one of the things I would like to see happen is a paper that was tentatively called ""Towards a formal cognitive semantics"" which was addressed to these linguists uh who haven't been following this stuff at all. Yeah. So it could be that he's actually  at some level  thinking about how am I going to communicate this story - Yeah. Yeah. So  internally  we should just do whatever works  cuz it's hard enough. Yeah. Yeah. But if he g- if he turns - is - is really gonna turn around and help t- to write this version that Mm-hmm. Mm-hmm. does connect with as many as possible of the other linguists in the world Yeah. um then - then it becomes important to use terminology that Yeah. Mm-hmm. Sure. doesn't make it hard - I mean  it's gonna be plenty hard for - for people to understand it as it is  but y- y- you don't want to make it worse. Yeah. Yeah. So. No  right. I mean  tha- that role is - is  uh  indispensable but that's not where sort of our heads were at in these meetings. It was a little strange. Right. Yeah  yeah. - No  that's fine. I just wanted t- to I have to catch up with him  and I wanted t- to get a feeling for that. O_K. Yeah. Mm-hmm. So I don't know what his take will be on these meetings exactly  you know. O_K. Good. Cuz sometimes he sort of sounds like we're talking a bunch of goobledy-gook from his point of view. I think it's good when we're - when we're into data and looking at the - some specific linguistic phenomenon in - in English or in German  in particular  whatever  Yeah. Mm-hmm. that's great  and Ben and - and Hans are  if - if anything  more - you know  they have more to say than  let's say  I would about some of these things. Right. But when it's like  well  w- how do we capture these things  you know  I think it's definitely been Keith and I who have d- you know  who have worried more about the - Well  that's good. That's - I- I- I think that should be the - Mm-hmm. s- Which is fine. Yeah. the core group and um Mm-hmm. that's  you know  I think very close to the maximum number of people working together that can get something done. Yeah. Yes. Yeah . We actually have - I think we have been making progress  and its sort of surprising. You know  like - I - I - I - I definitely get that impression. Yeah. That's great. Yeah. Yep. Yeah. So anyone else would like uh ruin the balance of - Anyway. Well  but - Well. But th- th- then w- then we have to come back to the bigger group. Right. Yeah. Yeah. Great. And then we're gon- we're gonna - because of this other big thing we haven't talked about is actually implementing this stuff? So that I guess the three of us are gonna connect tomorrow about that. Yeah  we could talk tomorrow. I was just gonna say  though  that  for instance  there was - you know  out of a meeting with Johno came the suggestion that "" oh  could it be that the meaning constraints really aren't used for selection?"" which has sort of been implicit in the parsing strategy we talked about. In which case we w- we can just say that they're the effects or the bindings. Right. Which uh  so far  in terms of like putting up all the constraints as  you know  pushing them into type constraints  the - when I've  you know  propo- then proposed it to linguists who haven't yet given me - you know  we haven't yet thought of a reason that that wouldn't work. Right? As long as we allow our type constraints to be reasonably complex. Well  it - So - Anyway  to be - to talk about later. Yeah  it has to in the sense that you're gonna use them eventu- it's - you know  it's sort of a  um  generate and test Mm-hmm. Mm-hmm. kind of thing  and if you over-generate then you'll have to do more. I mean  if there are some constraints that you hold back and don't use uh  in your initial matching Mm-hmm. Mm-hmm. then you'll match some things - I mean  I - I d- I don't think there's any way that it could completely fail. It - it could be that uh  you wind up - I mean - The original bad idea of purely context-free grammars died because there were just vastly too many parses. You know  exponentially num- num- many parses. And so th- the concern might be that - not that it would totally fail  but that - Mm-hmm. Mm-hmm. That it would still generate too many. it would still genera- Right? So by just having semantic- even bringing semantics in for matching just in the form of j- semantic types  right? Like ""conceptually these have to be construed as this  this  and this"" Yeah. might still give us quite a few possibilities We don't know  but  yeah. that  you know - And - and it certainly helps a lot. I mean  le- let's put it that way. So. No question. Yeah. And I think it's a - it's a perfectly fine place to start. You know  and say  let- let's see how far we can go this way. Mm-hmm. Mm-hmm. And  uh - Well it definitely makes the problem easier. I'm - I'm in favor of that. Uh  cuz I think i- I think it's - As you know  I think it's real hard and if w- if we - Right. So Friday  Monday Yeah. Monday. So. O_K  that's - Tuesday. Like - th- that's the conclusion. O_K. Yeah. Yeah. Yeah. So  you- your dance card is completely filled now? Shoot. Mm-hmm. Why don't - Yeah  and I have nothing to do this weekend but work. No  that's not really true  but like - Bummer. What about - What about D_D_R? @@ It's almost true. Oh  I don't have it this weekend  so  tsk don't have to worry about that. Mmm. D_D_R  he asked? Speaking of dance  Dance Dance Revolution - I can't believe I'm - It's a - it's like a game  but it's for  like  dancing. Hard to - It's like karaoke  but for dancing  It's a - It's - It's a video game? You stand there and @@ Oh that's - Oh  oh  oh! and they tell you what - It's amazing. It's so much fun. Yeah  it's so good. My friend has a home version and he brought it over  and we are so into it. It's so amazing. Yeah. Yeah. Oh yeah  yeah  yeah. Oh  right! Oh  yeah! Now this - Well  y- you know of it? I- i- i- it's one of your hobbies? I do. @@ No  it's not one of my hobbies. Although I - God knows I - I - I could use something like that. @@ It's great exercise  I must say. But  uh No  I - I - I - I can't wait to hear this. I see - I see it - things about it on T_V. O_K? Uh-huh. ap- Apparently - Oh  it's the hottest thing in Tokyo? Oh  definitely. They have  like  places - instead of like - Yeah  instead of karaoke bars now that have  like  D_D_R  like - That's the sort of thing that would be hot in Tokyo. Yeah  I didn't know it by its initials. Yeah  yeah  I didn't until I started hanging out with this friend  who's like ""Oh  well  I can bring over the D_D_R if you want."" Oh  oh  Dance Dance Revolution - O_K. He actually brought a clone called Stepping Selection  but it's just as good. So. Anyw- ",In a brief meeting the Berkeley Meeting Recorder Group discussed what little progress they had made in the last week. They also talked about visits from members of research partner OGI  and a conference call held by the Aurora Project collaborators. One member brought up some work that backs up some points he was making in previous meetings. Nothing has so far happened about standardizing file sharing between groups  though the process has been started  and so this is a topic they will come back to after the meeting. There were still no final decisions made in the conference call held to discuss the Aurora project. OGI will probably use a VAD  and everyone will probably be given the same one  and the results weighting scheme will probably be the same. Since some members of OGI's team are absent  me013 feels ICSI's side should pick up the slack a little  which they are not doing by mn007 and fn002 taking time out to write a report. Speaker me018 has so far not mailed anyone details of changing iteration or running HTK on the Linux machine  so no one has been able to take advantage of his work of recent weeks. Speaker mn007 has run the tandem system with MSG  but there was no significant improvement. He and fn002 have also begun to write up the report on their recent work  so aren't making much progress on experimenting. Me006 is still reading  getting some background on phonetics. Speaker me026 is considering looking at using phase for reverberation removal  as the work he is basing his on did not appear to consider it. Speaker me018 has not made much progress  since he has been dealing with technical issues  but he has been reading a paper which backs up his belief that reducing the iteration in the HTK training does no harm. 
"O_K  this is one channel. Can you uh  say your name and talk into your mike one at a time? This is Eric on channel three  I believe. O_K. Uh  I don't think it's on there  Jane. Tasting one two three  tasting. O_K  this is Jane on channel five. Uh  I still don't see you Jane. Oh  darn  what am I doing wrong? Can you see me on channel four? Really? Uh  screen no  - it is  oh  maybe it just warmed up? My lucky day. Yeah  I s- No. Oh  darn  can- you can't see channel five yet? Uh  well  the mike isn't close enough to your mouth  so. Oh  this would be k- O_K  is that better? I like the high quality labeling. S- uh  try speaking loudly  so  O_K  good. Thank you. Hello  hello. Alright. David  can we borrow your labeling machine to improve the quality of the labeling a little bit here? Numbers from one to - zero to X_ or something? One t- @@ How - how many are there  one to five? One five  yeah. Would you like to join the meeting? I bet - Yeah  please. Well  we don't wanna renumber them  cuz we've already have like  forms filled out with the numbers on them. So  let's keep the same numbers on them. Yeah  O_K  that's a good idea. O_K  Dan  are you on? I'm on - I'm on two and I should be on. Good. Yeah. Want to join the meeting  Dave? Do we - do - do we have a spare  uh - And I'm getting lots of responses on different ones  so I assume the various and assorted P_Z_ Ms are on. I'm - I shouldn't. I've got something - We'r- we're - we'r- This is - this - this is a meeting meeting. This is abou- we're - we're mainly being taped but we're gonna talk about  uh  transcription for the m- future meeting meetings. Stuff. Yeah  this is not something you need to attend. So. No  I shouldn't really. I would have on another day  but it's been one of those days today. Yeah. e- O_K. You're always having one of those days  Dave. Y- you'd be welcome. You'd be welcome. Besides  I don't want anyone who has a weird accent. Right  Dan? So  I don't understand if it's neck mounted you don't get very good performance. It's not neck mounted. It's supposed to be h- head mounted. Yeah. It - it should be head mounted. Right? Right. Well  then put it on your head. What are you doing? I don't know. Cuz when you do this  you can - Rouww-Rouww. Why didn't I - you were saying that but I could hear you really well on the - on the transcription - on the  uh  tape. I - I don't know. Well  I m- I would prefer that people wore it on their head but they were complaining about it. i- Why? Because it's not - it doesn't go over the ears. It's badly designed. It's very badly designed so it's - It's very badly designed? Why? It's not s- It's not supposed to cover up your ears. I mean  it's only badly - What do you mean it doesn't go over the ears? Yeah but  there's nowhere to put the pad so it's comfortable. So that's what you're d- He's got it on his temples so it cuts off his circulation. Yeah  that's - that's what I have. Oh  that's strange. And it feels so good that way. It feels so good when I stop. Try it. Somebody wanna - So I - I again would like to do some digits. Um. Somebody wanna close the door? O_K. Sure. We could do it with noise. So let me - You're always doing digits. Well  you know  I'm just that sort of - digit-y g- sorta guy. When can we do the alphabet? O_K. So this is Adam. Uh  this is the same one I had before. I doubt it. It's still the same words. So this is Adam Janin on mike number one  transcript three seventy-one  three ninety  uh  wireless head mike. six eight one seven eight nine zero zero one two nine five one three eight three two seven seven three four five one five six O_ one O_ seven eight two four seven six four nine nine three five O_ six three four O_ s- zero seven one one two three zero five one O_ six five. I think we're session four by the way. Or m- it might be five. No- Psss! Oh  that's good. I didn't bring my previous thing. We didn't - Now  just to be sure  the numbers on the back  this is the channel? That's the microphone number. That's the microphone number. Uh-oh. Yeah  d- leave the channel blank. O_K  good. But number has to be - ? So we have to look up the number. O_K  good. Five - Right. Good. O_K. Well  uh  let me do one. This is Dan Ellis on mike two. Transcript four seventy-one to four ninety. O_. Zero zero five four six. Two three seven one. Three two two two seven. Four seven three two O_ one one. Five nine two six. Six. Seven O_ four O_ nine seven nine. Eight zero five eight. O_ two eight one three O_ one. Zero four. One seven eight eight zero four two. Two three seven O_ seven. Three. Four O_ one four. Five. Seven three six. Eight five one seven two. Nine six one. O_. Well  this is Jane  on mike number five. Um. I just start? Do I need to say anything more? Uh  transcript number. Transcript number - Transcript number four five one  four seven zero. nine O_ seven zero four two three one two nine seven eight seven zero two eight eight three four five six eight one three four nine one eight O_ two O_ zero six zero one two O_ three O_ five one six zero three four zero six three O_ O_ seven five eight nine nine nine seven. O_K  this is Eric on microphone number three  uh  reading transcript four ninety-one  five ten. Zero. One. Two O_ two nine. Four one three. Five two four. Six seven nine five. Seven nine seven. Eight nine four. Nine zero nine. O_ O_. One three nine O_. Two seven O_ six five seven one. Three. Four six two five. Five O_ two one seven. Six zero eight zero zero six four. Eight three six nine two. Nine four four three. O_ seven eight nine. Zero eight seven seven. This is Beck on mike four. Transcript five one one five three zero. Channel blank. one two O_ one five O_ three five three seven six eight six nine six two six O_ seven seven six four two one four eight nine O_ zero two one four nine three two O_ two four three zero eight three five nine three five six six seven O_ nine two seven eight O_ five nine zero seven nine six two one five zero zero two seven seven Thanks. Should I turn off the V_U meter Dan? Do you think that makes any difference? Oh  God. No  let me do it. Why? Are you gonna do something other than hit ""quit""? No  but I'm gonna look at the uh  logs as well. Oh. Uh  you said turn off the what? Should have done it before. The V_U meter which tells you what the levels on the various mikes are and there was one hypothesis that perhaps that - Oh. the act of recording the V_U meter was one of the things that contributed to the errors. Oh  I see. Yeah  but Eric  uh  you didn't think that was a reasonable hypothesis  right? I See. That was me  I thought that was - Oh  I'm sorry y- That was malarkey. Well  the only reason that could be is if the driver has a bug. Right? Because the machine just isn't very heavily loaded. No chance of that. No chance of that. Just because it's beta. Yeah  there - there - there was - there was a - there was a bug. There was a glitch last time we ran. Look O_K? Are- are yo- are you recording where the table mikes are by the way? Do you know which channels - No. Yeah  we usually do that. @@ No  we don't. Yeah. Why not? But we - we ought to st- we ought to standardize. I think  uh  I s- I spoke to somebody  Morgan  about that. I think - I think we should put mar- Well  no  w- we can do that. I know what they - they're - they're four  three  two  one. Why don't you just do this? I mean  that's what we've done before. In order now. Four. Three  Three. two  and one. But I think - I think we should put them in standard positions. I think we should make little marks on the table top. Oh  O_K. So that we can put them - Which means we need to move this thing  and sorta decide how we're actually going to do things. I guess that's the point. It'll be a lot easier if we have a - if we have them permanently in place or something like that. So. Right. I do wish there were big booms coming down from the ceiling. You do? Yeah. Would it make you feel more important? Yeah  yeah  yeah. You know. Mmm. I see. Wait till the projector gets installed. Oh  that'll be good. O_K. That'll work. That'll work. Oh  gosh. Cuz it's gonna hang down  make noise. When's it gonna be installed? O_K. Well  it depends on I see. Is this b- is this being recorded? That's right. Uh  I think Lila actually is almost getting r- pretty close to even getting ready to put out the purchase order. O_K. Cool. I handed it off to her about a month ago. I see. O_K  so  topic of this meeting is I wanna talk a little bit about transcription. Um  I've looked a little bit into commercial transcription services and Jane has been working on doing transcription. Uh  and so we wan- wanna decide what we're gonna do with that and then get an update on the electronics  and then  uh  maybe also talk a little bit about some infrastructure and tools  and so on. Um  you know  eventually we're probably gonna wanna distribute this thing and we should decide how we're gonna - how we're gonna handle some of these factors. So. Distribute what? The data? Hmm? Right. Right. I mean  so we're - we're collecting a corpus and I think it's gonna be generally useful. I mean  it seems like it's not a corpus which is - uh  has been done before. And so I think people will be interested in having - having it  and so we will - Oh. u- Using  like  audio D_V_Ds or something like that? Yes. Excuse me? Audio D_V_Ds? Well  or something. Yeah  audio D_V_Ds  C_Ds  Or t- you know. Yeah. tapes. And - and so how we do we distribute the transcripts  how do we distribute the audio files  how do we - how do we just do all that infrastructure? Well  I think - I mean  for that particular issue ther- there are known sources where people go to - to find these kind of things like the L_D_C for instance. Yeah  that's right. Right  but - but so should we do it in the same format as L_D_C and what does that mean to what we've done already? Right. The - It's not so much the actu- The logistics of distribution are secondary to preparing the data in a suitable form for distribution. Right. Right. So  uh  as it is  it's sort of a ad-hoc combination of stuff Dan set and stuff I set up  which we may wanna make a little more formal. And the other thing is that  um  So. University of Washington may want to start recording meetings as well  in which case Right. w- w- we'll have to decide what we've actually got so that we can give them a copy. A field trip. That's right. Yeah. I was actually thinking I wouldn't mind spending the summer up there. That would be kind of fun. Oh  really? Yeah. Different for you. Yes. Visit my friends and spend some time - Well  and then also I have a bunch of stuff for doing this digits. So I have a bunch of scripts with X_Waves  and some Perl scripts  and other things that make it really easy to extract out and align where the digits are. And if U_ d- U_W's going to do the same thing I think it's worth while for them to do these digits tasks as well. Mm-hmm. And what I've done is pretty ad-hoc  um  so we might wanna change it over to something a little more standard. Hmm. You know  S_T_M files  or X_M_L  or something. An- and there's interest up there? What's that? There's interest up there? Well they - they certainly wanna collect more data. And so they're applying  I think I_B_ M? Is that right? I don't know. Something like that. Um  for some more money to do more data. So we were planning to do like thirty or forty hours worth of meetings. They wanna do an additional hundred or so hours. So  they want a very large data set. Um  but of course we're not gonna do that if we don't get money. So. I see. And I would like that just to get a disjoint speaker set and a disjoint room. Mm-hmm. I mean  one of the things Morgan and I were talking about is we're gonna get to know this room really well  the - the acoustics of this room. Including the fan. Did you notice the fan difference? All about that. Including the fan. Oh  now you've touched the fan control  now all our data's gonna be - Hear the difference? Yeah  it's great. Oh  that's better. Oh  it's enormous. That's better. Do you wanna leave it off or not? All the others have been on. Yeah  the - You sure? That's - Oh  yeah. Absolutely. You - You think that - y- Absolut- things after the f- then @@ Yeah. This fan's wired backwards by the way. Uh  I think this is high speed here. Yeah  it's noticeable. Well  not clear. Maybe it - Maybe it isn't. Well it's - well like ""low"" is mid - mid-scale. So it could be that it's not actually wired backwards it's just that ambiguous. That's right. I was wondering also  Get ready. Uh-huh. whether the lights made any noise. There's definitely - Yep. High pitch hum. Wow. Yeah  a little bit. Oh  they do. Yeah. So  do our meetings in the dark with no air conditioning in the future. Yeah  just get a variety. I think candles would be nice if they don't make noise. They're very good. Oh  yeah. It would - you know  it would real- really mean that we should do short meetings when you turn off the - Carbon monoxide poisoning? Short meetings  that's right. Or - turn off the air conditioning  Yeah  sort of r- r- got to finish this meeting. Tear t- Tear your clothing off to stay cool. That's right. Actually  the a- th- air - the air conditioning's still working  that's just an auxiliary fan. Right  I see. So  um  in addition to this issue about the U_W stuff there was announced today  uh  via the L_D_C  um  a corpus from I believe Santa Barbara. So- Yeah  I saw it. I've been watching for that corpus. Um  of general spoken Yeah. English. Yep. And I don't know exactly how they recorded it but apparently there's a lot of different styles of speech and what not. And - Mm-hmm. They had people come in to a certain degree and they - and they have DAT recorders. I see. So it is sort of far field stuff. I - I assume so  actually  I hadn't thought about that. Unless they added Right? close field later on but  um  I've listened to some of those data and I  um  I've been - I - I was actually on the advisory board for when they set the project up. Mm-hmm. Oh  O_K. I'm glad to see that it got released. So it- it's a very nice thing. What's it sound like? Yeah  I - I wish - S- I wish we had someone here working on adaptation because it would nice to be able to take that stuff and adapt it to a meeting setting. How do you mean - do you mean mechanical adaptation or - But it may be - it may be useful in - You know - No  software  to adapt the speech recognition. O_K. Well  what I was thinking is it may be useful in transcribing  if it's far field stuff  right? Mm-hmm. In doing  um  some of our first automatic speech recognition models  it may be useful to have that kind of data because that's very different than any kind of data that we have so far. Great idea. That's true. And - and their recording conditions are really clean. I mean  I've - I've heard - I've listened to the data. It sounds - well but what I mean is that  um - Well that's not good  right? That's - that's not great. Tr- But far field means great distance? I mean - Just these. Not head mounted? Yeah. And so that's why they're getting away with just two channels or something  or are they using multiple DATs? Um  oh  good question and I can't ans- answer it. I don't know. Well we can look into it. No  and their web - their web page didn't answer it either. So I'm  I- uh  was thinking that we should contact them. So it's - that's sort of a beside-the-point point. But. O_K. So we can get that just with  uh  media costs  is that right? Still a point. Right. Uh  in fact we get it for free cuz they're distributing it through the L_D_C. Oh. Yep. Great. So that would be - yeah  that would be something to look into. So  I can - I can actually arrange for it to arrive in short order if we're - So. The other thing too is from - from a - Well  it's silly to do unless we're gonna have someone to work on it  so maybe we need to think about it a little bit. The other thing too is that their- their jus- their transcription format is really nice and simple in - in the discourse domain. Huh. But they also mentioned that they have it time aligned. I mean  I s- I - I saw that write-up. Yeah. Maybe we should - maybe we should get a copy of it just to see what they did so - so that we can - we can compare. Yeah  absolutely. It's very nice. Yeah. Absolutely. O_K  why don't you go ahead and do that then Eric? Alright  I'll do that. I can't remember the name of the corpus. It's Corps- C_S_A_E. Corpus of Spoken American English. S_ - Right  O_K. Yeah  sp- I've been - I was really pleased to see that. I knew that they - they had had some funding problems in completing it but  um  Yeah. Well they're - Uh-huh. this is clever. Apparently this was like phase one and the- there's still more that they're gonna do apparently or something like that unless of course they have funding issues and then Got it through the L_D_C. Great. Great. then it ma- they may not do phase two but from all the web documentation it looked like  ""oh  this is phase one""  whatever that means. Super. Super. Great. Yeah  that - I mean  they're really well respected in the linguistics d- side too and the discourse area  and - O_K. So this is a very good corpus. But  it- uh- it would also maybe help- be helpful for Liz  if she wanted to start working on some discourse issues  you know  looking at some of this data and then  you know - Right. So when she gets here maybe that might be a good thing for her. Actually  that's another thing I was thinking about is that maybe Jane should talk to Liz  to see if there are any transcription issues related to discourse that she needs to get marked. O_K. Maybe we should have a big meeting meeting. Sure  of course. That would be a meeting meeting meeting? A meeting meeting meeting. Yeah. Well this is the meeting about the meeting meeting meeting. So. Oh. Right. Um. But maybe we should  uh- find some day that Liz - uh  Liz and Andreas seem to be around more often. Mm-hmm. So maybe we should find a day when they're gonna be here and - and Morgan's gonna be here  and we can meet  at least this subgroup. I mean  not necessarily have the U_dub people down. Well  I was even thinking that maybe we need to at least ping the U_dub - We need - we need to talk to them some more. to see - you know  say ""this is what we're thinking about for our transcription ""  if nothing else. Mm-hmm. So  well w- shall we move on and talk a little bit about transcription then? O_K  so since that's what we're talking about. Yeah. Let's. What we're using right now is a tool  um  from this French group  called ""Transcriber"" that seems to work very well. Um  so it has a  uh  nice useful Tcl-T_K user interface and  uh  Thi- this is the process of converting audio to text? Right. And this requires humans just like the - Yes  yeah. the S_T_P stuff. Right  right. So we're - we're at this point only looking for word level. So all - all - so what you have to do is just identify a segment of speech in time  and then write down what was said within it  and identify the speaker. And so the things we - that we know - that I know I want are the text  the start and end  and the speaker. But other people are interested in for example stress marking. And so Jane is doing primary stress  um  stress marks as well. Um  and then things like repairs  and false starts  and  filled pauses  and all that other sort of stuff  we have to decide how much of that we wanna do. I did include a glo- uh  a certain first pass. My - my view on it was when you have a repair then  uh - it seems - I mean  we saw  there was this presentation in the - one of the speech group meetings about how - and I think Liz has done some stuff too on that  that it  uh - Mm-hmm. that you get it bracketed in terms of like - well  if it's parenthetical  which I know that Liz has worked on  then uh - y- y- you'll have different prosodic aspects. And then also if it's a r- if it's a repair where they're - like what I just did  then it's nice to have sort of a sense of Mm-hmm. Hmm. the continuity of the utterance  the start to be- to the finish. And  uh  it's a little bit deceptive if you include the repai- the pre-repair part - and sometimes or of it's in the middle. Anyway  so what I was doing was bracketing them to indicate that they were repairs which isn't uh  very time-consuming. Mm-hmm. I- is there already some sort of plan in place for how this gonna be staffed or done? Or is it real - is that what we're talking about here? Well  that's part of the thing we're talking about. So what we wanted to do was have Jane do basically one meeting's worth  you know  forty minutes to an hour  and - Mm-hmm. As a pilot study. Yeah  as a pilot study. Yourself? It - this is - this is like five times real time or ten times real time - Yeah. Ten times about  is - and so one of the things was to get an estimate of how long it would take  and then also what tools we would use. And so the next decision which has to be made actually pretty soon is how are we gonna do it? And so you make Jane do the first one so then she can decide  oh  we don't need all this stuff  just the words are fine. So. That's right  that's right. That's right. I wanna hear about these - uh  we have a g- you were s- continuing with the transcription conventions for s- R- right  so - so one - one option is to get linguistics grad students and undergrads to do it. And apparently that's happened in the past. And I think that's probably the right way to do it. Um  it will require a post pass  I mean people will have to look at it more than once to make sure that it's been done correctly  but I just can't imagine that we're gonna get anything that much better from a commercial one. And the commercial ones I'm sure will be much more expensive. Can't we get Joy to do it all? No  that's - Yeah right. We will just get Joy and Jane to do everything. Is tha- wasn't that what she was doing before? Yeah  that's right. But  you know  that's what we're talking about is getting some slaves who - who need money and  uh  Right. duh  again o- I object to that characterization! Oh  really. I meant Joy. And so again  I have to say ""are we recording "" and then say  uh  Morgan has - has consistently Oh  thank you. O_K. resisted telling me how much money we have. Right. Well  the answer is zero. There's a reason why he's resisted. But. So. Well  if it's zero then we can't do any transcription. I mean  cuz we're - we - Right. I have such a hard name. I mean  I - I can't imagine us doing it ourselves. Well  we already - we already - We already have a plan in place for the first meeting. Right? That's - Right? N- right. Well th- there is als- Yeah  really. There is also the o- other possibility which is if you can provide not money but instructional experience or some other perks  you can - you could get people to - to um  Well  i- b- but seriously  I - I mean  Morgan's obviously in a bind over this and thing to do is just the field of dreams theory  Right. to do it in exchange. which is we- we go ahead as though there will be money at the time that we need the money. Mm-hmm. And that's - that's the best we can do. i- b- To not do anything until we get money is - is ridiculous. Yeah. Right. Right. Right. We're not gonna do any - get anything done if we do that. Mm-hmm. Yeah. So at any rate  Jane was looking into the possibility of getting students  at - is that right? Talking to people about that? I'm afraid I haven't made any progress in that front yet. I should've sent email and I haven't yet. O_K. Yeah  right. So  uh - I- d- do - So until you actually have a little experience with what this - this French thing does we don't even have - And I do have - She's already done quite a bit. Yeah. I have - Oh  we have. I'm sorry. a bunch of hours  yeah. So that's where you came up with the f- the ten X_ number? Actually that's the - the one people usually use  ten X_. Or is that really just a guess? And I haven't really calculated - How fast am I? I haven't done a s- see  I've been at the same time doing kind of a boot strapping in deciding on the transcription conventions that - that are - How fast are you? Yeah i- Mmm. you know  and - and stuff like  you know  how much - Right. There's some interesting human factors problems like  yeah  what span of - of time is it useful to segment the thing into in order to uh  transcribe it the most quickly. Cuz then  you know  you get like - if you get a span of five words  that's easy. Yeah. But then you have to take the time to mark it. And then there's the issue of it's easier to - Yeah. hear it th- right the first time if you've marked it at a boundary instead of somewhere in the middle  cuz then the word's bisected or whatever and - Mm-hmm. And so I mean  I've been sort of playing with  uh  different ways of mar- cuz I'm thinking  you know  I mean  if you could get optimal instructions you could cut back on the number of hours it would take. D- does uh - this tool you're using is strictly - it doesn't do any speech recognition does it? Yeah. No. No  it doesn't but what a super tool. It's a great environment. But - but is there anyway to - to wire a speech recognizer up to it and actually run it through - That's an interesting idea. Hey! We've - we've thought about doing that but the recognition quality is gonna be horrendous. Well  a couple things. First of all the time marking you'd get - you could get Wow . by a tool. And so if the - if - if the issue really- uh  I'm think about the close caption that you see running by on - on live news casts. You know  yo- That's true. That's interesting. Most of those are done by a person. Yeah  I- I know - I know that. No  I understand. And - in a lot of them you see typos and things like that  but it - but it occurs to me that Mm-hmm. it may be a lot easier to correct things than it is to do things from scratch  no matter how wonderful the tool is. But if - if there was a way to merge the two - Yeah. Yeah  we - Well  I mean  but sometimes it's easier to type out something instead of going through and figuring out which is the right - I mean  it depends on the error rate  right? That'd be fun. I mean  we've talked about it but - Well s- but - but again the timing is for fr- should be for free. The timing should be - But we don't care about the timing of the words. Well I thought you just - that's - said that was a critical issue. No  uh the - the boundary - We don't care about the timing of the words  just of the utterances. We cut it s- s- We don't - we don't know  actually. We haven't decided which - which time we care about  and that's kind of one of the things that you're saying  is like - boundary. you have the option to put in more or less timing data - and  uh  be- in the absence of more specific instructions  we're trying to figure out what the most convenient thing to do is. Yeah  so - so what - what she's done so far  is sort of - more or less breath g- not breath groups  sort of phrases  Yeah. continuous phrases. And so  um  that's nice because you - you separate when you do an extract  you get a little silence on either end. So that seems to work really well. That's ideal. Although I was - I - you know  the alternative  which I was sort of experimenting with before I ran out of time  recently was  um - Um. Yeah. that  you know  ev- if it were like an arbitrary segment of time - i- t- pre-marked cuz it does take time to put those markings in. Yeah. It's really the i- the interface is wonderful because  you know  the time it takes is you listen to it  and then you press the return key. But then  you know  it's like  uh  you press the tab key to stop the flow and - and  uh  the return key to p- to put in a marking of the boundary. But  you know  obviously there's a lag between when you hear it and when you can press the return key so it's slightly delayed  so then you - Yeah. you listen to it a second time and move it over to here. So that takes time. Now if it could all be pre- marked at some  l- you know  good - a- i- a- ar- but- Are - are those d- delays adjustable? Hmm. Those delays adjustable? See a lot of people who actually build stuff with human computer interfaces understand that delay  and - and so when you - by the time you click it it'll be right on because it'll go back in time to put the - Yeah. Yeah. Yeah  uh  not in this case. It has other - It could do that couldn't it. We could program that pretty easily  couldn't we Dan? Yeah  mis- Mister T_C_L? Yeah. Oh  interesting point. Ah! Interesting point. O_K  that would make a difference. I would have thought so  yeah. Mmm. But  um - I mean  it's not bad but it does - take twice. But  if we tried to do automatic speaker I_D. I mean  cuz primarily the markings are at speaker change. Yeah  yeah  but - But we've got - we've got the most channel data. We'd have to do it from your signal. But that would be - Right. Oh  good point! Ah! We've got volume. I mean  we've - we've got - we've got a lot of data. Yeah  I guess the question is how much time will it really save us versus the time to write all the tools to do it. Right. but the chances are if we- if we're talking about collecting ten or a hundred hours  which is going to take a hundred or a thousand hours to transcribe - If - But - if we can go from ten X_ to five X_ we're doing a big - We're gonna need - we're gonna need ten to a hundred hours to train the tools  Right. So maybe - Wow. and validate the tools the do the d- to - to do all this anyway. But - but it op- If we're just doing silence detection - I knew you were gonna do that. Just saw it coming. I'm sorry. I wish you had told me - wish you'd told me. Put - put it on your sweater. At what part? O_K  I'm alright. Um  i- it seems like - Well  uh  I don't know. Yeah. I mean  it - it's - it's maybe like - a week's work to get to do something like this. So forty or fifty hours. Could you get it so that with - so it would - it would detect volume on a channel and insert a marker? And the - the format's really transparent. It's just a matter of Right. Sure. Yeah. a very c- clear - it's X_M_L  isn't it? Mm-hmm. It's very - I mean  I looked at the - the file format and it's just - it has a t- a time - a time indication and then something or other  and then an end time or something or other. So maybe - maybe we could try the following experiment. Take the data that you've already transcribed Mm-hmm. and - Is this already in the past or already in the future? Already in the past. She - she's done one - she's one - You've already - you've already done some? Yes I have. She's - she's done about half a meeting. Oh- Oh  I see. O_K  good. Right. Right. I'm go- Right? About half? S- I'm not sure if it's that's much but anyway  enough to work with. Right. Um  Several minutes. and - and throw out the words  but keep the time markings. And then go through - I mean  and go through and - and try and re-transcribe it  O_K. Good idea. given that we had perfect boundary detection. And see if it - see if it - see if it feels easier to you. O_K. And forgetting all the words because you've been thr- Yeah  that's what I was thinking. I'd - I'd be cheating a little bit g- with familiarity effect. Yeah  I mean uh  that's part of the problem is  is that what we really need is somebody else to come along. Well  no  you should do it - you should do it - Do it again from scratch and then do it again at the boundaries. So you do the whole thing three times and then we get - No. And then - then w- since we need some statistics do it three more. Yeah. Now  there's a plan. O_K. Oh  yeah. I'll do that tomorrow. And so you'll get - you'll get down to one point two X_ by the time you get done. I should have it finished by the end of the day. No  but the thing is the fact that she's - she's did it before just might give a lower bound. That's all. Uh  which is fine. It's - Exactly. Yeah. Right. Yeah. And if the lower bound is nine X_ then w- it's a waste of time. Right. Well  uh- but there's an extra problem which is that I didn't really keep accurate - uh  it wasn't a pure task the first time  so - Oh! uh  it's gonna be an upper bound in - in that case. And it's not really strictly comparable. So I think though it's a good proposal to be used on a new - Yeah. a new batch of text that I haven't yet done yet in the same meeting. Could use it on the next segment of the text. The point we - where do we get the - the - the oracle boundaries from? Right. Or the boundaries. Yeah  one person would have to assign the boundaries and the - and the other person would have to - I mean that's easy enough. I could do that. Well  but couldn't I do it for the next - We - we - we could get fake - Oh  I see what you mean. Well  but the oracle boundaries would come from volume on a partic- specific channel wouldn't they? That would be the automatic boundaries. No  no  no  no. You wanna know - given - Given a perfect human segmentation  I mean  you wanna know how well - I mean  the - the question is  is it worth giving you the segmentation? No  no. Yeah. Oh  I see what you mean. Yeah. I mean  that - that's easy enough. I could generate the segmentation and - and you could do the words  Right. and time yourself on it. So. A little double-blind-ear kind of thing. Yep. I see. O_K. So it - that might be worth doing. That's good. I like that. That would at least tell us whether it's worth spending a week or two trying to get a tool  that will compute the segmentations. And the thing to keep in mind too about this tool  guys is that sure  you can do the computation for what we're gonna do in the future but if - if Right. U_W's talking about doing two  or three  or five times as much stuff and they can use the same tool  then obviously there's a real multiplier there. Right. And the other thing too is with - with speaker identification  if - if that could handle speaker identification that's a big Well it w- I think it's pretty easy when we've got separate channels. deal. Well  use it. Yeah  that's why we s- bought the expensive microphones. O_K. Yeah  I mean  that's a nice feature. Right. Yep. Yeah  yeah. That's a major - that's like  one of the two things that - I mean  there's gonna - there's gonna be - in the meeting  like the reading group meeting that we had the other day  that's - it's gonna be a bit of a problem because  O_K. like  I wasn't wearing a microphone f- and there were other people that weren't wearing microphones. Yes. But you didn't say anything worth while anyway  right? That - Right. That'll s- Yeah. That's pretty much true but - it might save ninety percent of the work though. but  yes. So. So I - I need to - we need to look at what - what the final output is but it seems like we - it doesn't - it seems like it's not really not that hard to have an automatic tool to generate the phrase marks  and the speaker  and speaker identity without putting in the words. Yeah. I've already become pretty familiar with the format  so it would be easy. That'd be so great. Yeah. Yeah. Mm-hmm. We didn't finish the - If you'd tell me where it is  huh? the part of work already completed on this  did we? I mean  you - you talked a little bit about the transcription conventions  Mm-hmm. and  I guess you've mentioned in your progress report  or status report  that you had written a script to convert it into - So  I - when I - i- the - it's quickest for me in terms of the transcription part to say something like  you know  if - if Adam spoke to  um - to just say  ""A_ colon""  Like who could be  you know  I mean at the beginning of the line. Mmm. and E_ colon instead of entering the interface for speaker identification and clicking on the thing  uh  indicating the speaker I_D. So  and then he has a script that will convert it into the - the thing that  uh  would indicate speaker I_D. If that's clear. It's pretty cute. But at any rate. So  um  O_K. It's Perl script. Right. So - so I think the guess at ten X_ seems to be pretty standard. Everyone - more or less everyone you talk to says about ten times for hard technical transcription. Mm-hmm. Yeah. Using wh- using stone age- using stone age tools. That's right. Using - using stone age tools. Yeah  well that's true  but - I mean  I looked at Cyber Transcriber which is a service that you send an audio file  they do a first-pass speech recognition. And then they - they do a clean up. But it's gonna be horrible. They're never gonna be able to do a meeting like this. What - i- just approximately  what did you find out in terms of price or - or whatever? Right. No. Well  for Cyber Transcriber they don't quote a price. They want you to call and - and talk. So for other services  um  they were about thirty dollars an hour. Of - of tape? Or of action? O_K. Thirty - So  yeah. For thirty dollars an hour for - of their work. O_K. Oh  of their - Oh! So - so if it's ten times it's three hundred dollars an hour. So that's three - that's three hours. D- did you talk to anybody that does closed captioning for - for uh  T_V? O_K. Right. No. Cuz they a- usually at the end of the show they'll tell what the name of the company is  the captioning company that's doing it. Mm-hmm. Interesting. Yeah  so - so my - my search was pretty cursory. It was just a net search. And  uh  so it was only people who have web pages and are doing stuff Well  you know  the - the thing - the thing about this is thinking through that. kind of  maybe a little more globally than I should here but that really this could be a big contribution we could make. Uh  I mean  we've been through the S_T_P thing  we know what it - what it's like to - to manage the - manage the process  and admittedly they might have been looking for more detail than what we're looking for here but it was a - it was a big hassle  right? I mean  uh  you know  they - they constantly could've reminding people and going over it. Yeah. And clearly some new stuff needs to be done here. And it's - it's only our time  where ""our"" of course includes Dan  Dan and you guys. It doesn't include me at all. Uh. j- Just seems like - Yeah  I mean I don't know if we'd be able to do any thing f- to help S_T_P type problems. But certainly for this problem we can do a lot better than - Bec- Why? Because they wanted a lot more detail? No. Because they had - because they only had two speakers  right? I mean  the - the segmentation problem is - Right. Only had two. Trivial. They had two speakers over the telephone. Oh  I see. So what took them so long? Um  mostly because they were doing much lower level time. So they were doing phone and syllable Yeah. Right. transcription  as well as  uh  word transcription. Right. Right. Mm-hmm. And so we're - w- we decided early on that we were not gonna do that. I see. But there's still the same issue of managing the process  of - of reviewing and keeping the files straight  and all this stuff  that - which is clearly a hassle. Yep. Yeah. Right. And so - so what I'm saying is that if we hire an external service I think we can expect three hundred dollars an hour. Yeah. I think that's the ball park. There were several different companies that - and the - the range was very tight for technical documents. Twenty-eight to thirty-two dollars an hour. And who- who knows if they're gonna be able to m- manage multal- multiple channel data? Yeah  they won't. They w- they'll refuse to do it. Yeah. They won't. We'll have to mix them. Mm-hmm. And then there's the problem also that - Right. No  but I mean  they - they - they won't - they won't - they will refuse to transcribe this kind of material. That's not what they're d- quoting for  right? Yes  it is. Well  they might - they might quote it - For quoting meetings? Sev- several of them say that they'll do meetings  and conferences  and s- and so on. None of them specifically said that they would do speaker I_D  Wow. Yeah. or speaker change mark. They all just said transcription. Th- th- the- th- there may be just multiplier for five people costs twice as much and for ten people co- Something like that. Yeah  yeah  yeah. Well  the - the way it worked is it - it was scaled. So what they had is  if it's an easy task it costs twenty-four dollars an hour and it will take maybe five or six times real time. And what they said is for the hardest tasks  bad acoustics  meeting settings  it's thirty-two dollars an hour and it takes about ten times real time. I see. So I think that we can count on that being about what they would do. It would probably be a little more because we're gonna want them to do speaker marking. Yeah. Yeah. Right. A lot of companies I've worked for y- the  uh - So. the person leading the meeting  the executive or whatever  would sort of go around the room and - and mentally calculate h- how many dollars per hour this meeting was costing  right? In university atmosphere you get a little different thing. But you know  it's a lot like  ""he's worth fifty an hour  he's worth -"" And so he- so here we're thinking  ""well let's see  if the meeting goes another hour it's going to be another thousand dollars."" You know? It's - So ch- So every- everybody ta- Talk really fast. Let's get it over with. Yep  we have to have a short meeting. Stop talking! That's very interesting. Yeah. Talk slowly but with few words. And clearly. That's right. And only talk when you're pointed to. There you go. Content words only. We could have some telegraphic meetings. That might be interesting. Yeah  it'd be cheap. Cheap to transcribe. @@ So. But at any rate  so we - we have a ballpark on how much it would cost if we send it out. And we're talking about do- doing how many hours worth of meetings? Thirty or forty. So thirty or forty thousand dollars. Yeah. Well  for ten thousand dollars. So  meanwhile - Oh. What - Well  it was thirty times - Three hundred. Three hundred dollars an hour. Right. Oh  I'm sorry  three hundred. Right  I w- got an extra factor of three there. Yeah. So it's thirty dollars an hour  essentially  right? But we can pay a graduate student seven dollars an hour. And the question is what's the difference - or ei- eight dollars. What - do you know what the going rate is? It's - it's on the order of eight to ten. How - how much lower are they? I think uh- that would give us a - a good - good estimate. I think. But I'm not sure. I'd - I'd say - yeah  I was gonna say eight - you'd say ten? Ten. Let's say ten. Yeah  give them a break. The- these are not for engineering graduate students  right? Cuz it's easier. Right  these are linguistics grad students. That's right. Yeah  I - I - I don't - I don't know what the - I don't know what the standard - but there is a standard pay scale I just don't know what it is. Six. Yeah  that's right. That's right. Mm-hmm. Um  so that means that even if it takes them thirty times real time it's cheaper to - to do graduate students. And there's another aspect too. I mean  that's why I said originally  that I couldn't imagine sending it out's gonna be cheaper. No  it isn't. So. The other thing too is that  uh  if they were linguistics they'd be - you know  in terms of like the post editing  i- uh - tu- uh content wise they might be easier to handle cuz they might get it more right the first time. And also we would have control of - I mean  we could give them feedback. Whereas if we do a service it's gonna be limited amount. Mmm. Good point. Yep  yep. I mean  we can't tell them  you know  ""for this meeting we really wanna mark stress and for this meeting we want -"" Yep. No . Good point. And - and they're not gonna provide - they're not gonna provide stress  they're not gonna re- provide repairs  they're not gonna provide - they - they may or may not provide speaker I_D. So that we would have to do our own tools to do that. So - Mm-hmm. Yeah. Just hypoth- hypothetically assuming that - that we go ahead and ended up using graduate students. I- who - who's the person in charge? Who's gonna be the Steve here? You? I just - I hope it's Jane. Is that alright? Oh  interesting. Um  now would this involve some manner of uh  monetary compensation or would I be the voluntary  uh  coordinator of multiple transcribers for checking ? Um  I would imagine there would be some monetary involved but we'd have to talk to Morgan about it. Yeah  out of - out of Adam's pocket. Yeah. O_K. You know   it just means you have to stop working for Dave. Oh  I don't wanna stop working for Dave. See? That's why Dave should have been here. To pr- protect his people. Oh  cool. Yeah. Well  I would like you to do it because you have a lot more experience than I do  but if - if that's not feasible  I will do it with you as an advisor. Uh-huh. W- we'd like you to do it and we'd like to pay you. Not being Morgan though  it's - We'll see. O_K. Yeah. Right. Oh  I see. Well - We'd like to. Unfortunately - Yeah. Yeah  six dollars an hour. Yeah. And - and then - Yeah  I see. O_K. Boy  if I wanted to increase my income I could start doing the transcribing again. That's a - Yeah  that's right. Yeah. an- an- an- and be- and be sure and say  would you like fries with that when you're thinking about your pay scale. I see. Good. Yeah  no  that - I - I would be interested in that - in becoming involved in the project in some aspect like that - more. Yeah. Uh-huh. O_K. More. Yeah. Um  any more on transcript we wanna talk about? What s- so what are you - so you've done some portion of the first meeting. And what's your plan? To carry on doing it? Yes. Mm-hmm. What - Well  you know what I thought was right now we have p- So I gave him the proposal for the transcription conventions. He made his  uh  suggestion of improvement. The - the - It's a good suggestion. O_K. So as far as I'm concerned those transcription conventions are fixed right now. And so my next plan would be - What - what do they - what do they cover? They're very minimal. So  it would be good to - just to summarize that. So  um  one of them is the idea of how to indicate speaker change  Yeah. and this is a way which meshes well with - with  uh  making it Yeah. so that  uh  you know  on the - At the - Boy  it's such a nice interface. When you - when you get the  um - you - you get the speech signal you also get down beneath it  an indication of  uh  if you have two speakers overlapping in a s- in a single segment  you see them one - displayed one above each other. And then at the same time the top s- part of the screen is the actual verbatim thing. You can clip - click on individual utterances and it'll take you immediately to that part of the speech signal  and play it for you. And you can  eh- you can work pretty well between those two - these two things. Is there a limit to the number of speakers? Um  the user interface only allows two. Hmm. And so if - if you're using their interface to specify overlapping speakers you can only do two. But my script can handle any. And their save format can handle any. And so  um  using this - the convention that Jane and I have discussed  you can have as many overlapping speakers as you want. Do y- is this a  uh  university project? Th- this is the French software  right? Yeah. Yeah  yeah  their academic. Yeah  French. Yeah. And they're - they've been quite responsive. I've been exchanging emails on various issues. eh- Oh. Oh  really? Uh   did you ask them to change the interface for more speakers? Yes  and they said that's on - in - in the works for the next version. Good. Good. Oh  so multi- multichannels. Multichannels was also - Well  they said they wanted to do it but that the code is really very organized around single channels. So I think that's n- unlikely to ha- happen. I see. O_K. Do- do you know what they're using it for? Why'd they develop it? Are they linguists? For this exact task? For transcription. It's - But I mean  are they - are they linguists or are they speech recognition people? Ho- I think they're linguists. Hmm. Linguists. They're - they have some connection to the L_D_C cuz the L_D_C has been advising them on this process  the Linguistic Data Consortium. Yeah. Mm-hmm. Um  so - but a- apart from that. Yeah . It's also - All the source is available. So. Right. Great. If you - if you speak T_C_L_T_K. Mm-hmm. And they have - they've actually asked if we are willing to do any development and I said  well  maybe. Good. Right. Mm-hmm. So if we want - if we did - if we did something like programmed in a delay  which actually I think is a great idea  um  I'm sure they would want that incorporated back in. Yeah  I do too. Mm-hmm. Their pre- pre-lay. Pre-lay. Pre-lay. Well  and they've thought about things. You know  I mean  they - they do have - Way. So you have - when you - when you play it back  um  it's - it is useful to have  uh  a - a break mark to - se- segment it. But it wouldn't be strictly necessary cuz you can use the - uh  the tabbed key to toggle the sound on and off. I mean  it'll stop the s- speech you know if you- if you press a tab. And  um. And so  uh  that's a nice feature. And then also once you've put a break in then you have the option of cycling through the unit. You could do it like multiply until you get crazy and decide to stop cycling through that unit. Or - or - or - Loop it? Yo- you n- you know  there's al- also the - the user interface that's missing. It's missing from all of our offices  and that is some sort of analog input for something like this. It's what audio people actually use of course. It's something that wh- when you move your hand further  the sound goes faster past it  like fast forward. You know  like a joy stick or a - uh  you could wire a mouse or trackball to do something like that. Why  that's - That's not something I wanted to have happen. No  but I'm saying if this is what professionals who actually do this kind of thing for - for - for - m- for video or for audio - I see. where you - you need to do this  and so you get very good at sort of jostling back and forth  rather than hitting tab  and backspace  and carriage return  and enter  and things like that. Uh-huh. Mmm. Mmm. Uh-huh. Yeah. Yeah  we talked about things like foot pedals and other analog - So I mean  tho- those are things we could do but Mm-hmm. I - I just don't know how much it's worth doing. I mean we're just gonna have - Ye- Yeah. Right. Yeah  I - I agree. They - they have several options. So  uh  you know  I mentioned the looping option. Another option is it'll pause when it reaches the end of the boundary. Yeah. And then to get to the next boundary you just press tab and it goes on to the next unit. I mean  it's very nicely thought out. They thought about - and also it'll go around the c- the  uh  Hmm. Cool. Hmm. I wanna say cursor but I'm not sure if that's the right thing. Point  whatever. Anyway  you can - so they thought about different ways of having windows that you c- uh work within  and - But so in terms of the con- the conventions  then  uh  basically  uh  it's strictly orthographic which means with some w- Mm-hmm. provisions for  uh  w- uh  colloquial forms. So if a person said  ""cuz"" instead of ""because"" then I put a - an apostrophe at the beginning of the word and then in - in double ang- angle brackets Mm-hmm. what the full lexical item would be. And this could be something that was handled by a table or something but I think to have a convention marking it as a non-standard or wha- I don't mean standard - but a - a - a non- uh  ortho- orthographic  uh  whatever. Mm-hmm. Mm-hmm. Non-canonical. ""Gonna"" or ""wanna""  you know  the same thing. And - and there would be limits to how much refinement you want in indicating something as non-standard pres- pronunciation. How are you handling backchannels? Backchannels? Um  Comments. you know - oh  yes  there was some - in my view  when i- when you've got it densely overlapping  um  I didn't worry about - What do you mean by du- I didn't worry about s- specific start times. I sort of thought that this is not gonna be easily processed anyway and maybe I shouldn't spend too much time getting exactly when the person said ""no""  or  you know  uh  i- ""immediate"". And instead just sort of rendered ""within this time slot  Yeah. Yeah. there were two people speaking during part of it and if you want more detail  figure it out for yourself""  Mm-hmm. was sort of the way I felt @@ - I see. Well  I think what - w- what Eric was talking about was channels other than the direct speech  right? Well  yeah  what I mean is wh- I mean  when somebody says ""uh-huh"" in the middle of  uh  a @@ - Yep. Uh-huh. That happened very seldom. Oh  cuz I was - I was listening to - Dan was agreeing a lot to things that you were saying as you were talking. Uh-huh. Uh-huh. Oh  well  thank you Dan. Appreciate it. Well  if it - if there was a word like ""right""  So. Yeah  there's an overlapping mark. And - you know  then I wou- I would indicate that it happened within the same tem- time frame but wouldn't say exactly when it happened. Yeah. I'll be right back. I see. I transcribed a minute of this stuff and there was a lot of overlapping. It was - A lot of overlapping  yeah. Well there- there's a lot of overlapping at the beginning and end. Yeah. Yeah. Huge amounts. Um  when - when no one i- when we're not actually in the meeting  and we're all sort of separated  and - and doing things. It was at the beginning. But even during the meeting there's a lot of overlap but it - it's marked pretty clearly. Um  some of the backchannel stuff Jane had some comments - and - but I think a lot of them were because you were at the meeting. And so I think that - that often - Yeah  well that's true. That's another issue. often you can't tell. I mean  Jane had - had comments like Yeah. uh  to - who - who the person was speaking to. Yeah. Only when it was otherwise gonna be puzzling because he was in the other room talking. Mm-hmm. Yeah. Yeah  but someone who  uh  was just the transcriber wouldn't have known that. Yeah. That's true. Right. Or when Dan said  ""I wa- I wasn't talking to you "". I know. So you take a bathroom break in the middle and - and keep your head mount- Oh  you do? You have to turn off your mike. Yeah. Well he was so - so he was checking the meter levels and - and we were handling things while he was labeling the - the whatever it was  the P_D_A? You don't have to. Mm-hmm. Uh-huh. @@ And - and so he was - in - sort of - you were sort of talking - you know  so I was saying  like - ""and I could label this one left. Right?"" And he - and he said  ""I don't see anything"". And he said - he said  ""I wasn't talking to you"". Or - it wasn't - it didn't sound quite that rude. But really  no  uh - w- you know in the context if you know he can't hear what he's saying - But - but when you w- when you listen to it - he- he- It was a lot funnier if you were there though. Uh  yeah  I know. Well  you'll see. You can listen to it. Well what - what it - what happens is if you're a transcriber listening to it it sounds like Dan is just being a total - Oh  I thought it was you who was. totally impolite. No  well  but you were - you were asking off the wall questions. Um - But - but if you knew that - that I wasn't actually in the room  and that Dan wasn't talking to me  it - it became O_K. So. I see. So th- And that's w- that's where I added comments. The rest of the time I didn't bother with who was talking to who but - but this was unusual circum- circumstance. Hmm. So this is - this is gonna go on the meeting meeting transcriber bloopers tape  right? Yes. Right. Well and part of it was funny  uh - reason was because it was a mixed signal so you couldn't get any clues from Stereo. Yeah. volume that  you know  he was really far away from this conversation. You couldn't do that symmetrically in any case. No. Oh. I should rewrite the mix tool to put half the people in one channel and half in the other. That's a good idea. I have a auto-gain-mixer tool that mixes all the head mounted microphones into one signal and that seems to work really well for the uh transcribers. Mm-hmm. Great. But I thought it would be - you know  I - I didn't wanna add more contextual comments than were needed but that  it seemed to me  clarified that the con- what was going on. And  uh - O_K  so normalization - So  s- I was just gonna ask  uh  so I just wanted to c- sort of finish off the question I had about backchannels  if that's O_K  which - which was  so say somebody's talking for a while and somebody goes ""mm-hmm"" in the middle of it  and - and - and what not  Yeah. O_K. Yeah. Mmm. does the conversation come out from the - or the person who's speaking for the long time as one segment and then there's this little tiny segment of this other speaker or does it - does the fact that there's a backchannel split the - the - the - it in two. O_K  my - my focus was to try and maintain conten- con- content continuity and  uh  to keep it within what he was saying. Like I wouldn't say breath groups but prosodic or intonational groups as much as possible. So if someone said ""mm-hmm"" in the middle of a - of someone's  uh  uh  intonational contour  I - I indicated it as  O_K. like what you just did. then I indicated it as a segment which contained @@ this utterance plus an overlap. O_K. But that's - but there's only one - there's only one time boundary for both speakers  right? Yeah  that's right. And you know  it could be made more precise than that but I just thought - I see  I see  O_K. Yeah. Right. I think whenever we use these speech words we should always do the thing like you're talking about  accent  @@ @@ . Oh  I see what you mean. And then ""hesitation"". Yeah. O_K  and so then  uh  in terms of like words like ""uh"" and ""um"" I just wrote them because I figured there's a limited number  and I keep them to a - uh  limited set because it didn't matter if it was ""mmm"" or ""um""  you know  versus ""um"". So I just always wrote it as U_ M_. And ""uh-huh""  O_K. you know  ""U_H_U_H."" I mean  like a s- set of like five. But in any case - I didn't mark those. Mm-hmm. No. "" Uh-huh "" is ""U_H_H_U_H_."" ""Uh-uh"" is ""U_H_U_H."" I'd be happy with that. That'd be fine. It'd be good to have that in the - in the conventions  what's to be used. Huh-uh. I - I did notice that there were some segments that had pauses on the beginning and end. We should probably mark areas that have no speakers as no speaker. Yeah  that's a fine idea. Then  so question mark colon is fine for that. That's a fine idea. Yeah  O_K. Yeah. Well  what's that mean? You mean re- Just say silence. No one's talking. ye- s- Oh. Silence all around. Yep. Yep. So I had - We have to mark those? Don't they - d- can't we just leave them unmarked? I d- Well  you see  that's possible too. Well  I wanna leave the marked - I don't want them to be part of another utterance. O_K. So you just - you need to have the boundary at the start and the end. Sure. Mm-hmm. Now that's refinement that  uh  maybe it could be handled by part of the - part of the script or something more - Uh  yeah  it seems like - it seems like the  uh  tran- the transcription problem would be very different if we had these automatic speaker detection turn placing things. Because suddenly - I mean  I don't know  actually it sounds like there might be a problem putting it into the software if the software only handles two parallel channels. But assuming we can get around that somehow. Mm-hmm. Well you were saying  I think it can read - It can read and write as many as you want  it's just that it- Uh-huh. But what if you wanna edit it? Right? I mean  the point is we're gonna generate this transcript with five - five tracks in it  but with no words. Someone's gonna have to go in and type in the words. Um  and if there are five - five people speaking at once  Right  i- it's - I didn't explain it well. If we use the - the little - the conventions that Jane has established  I have a script that will convert from that convention to their saved convention. Oh  yeah. Yes. Which allows five. Right. And it can be m- edited after the fact  can't it also? But their - but their format  if you wanted to in- indicate the speakers right there instead of doing it through this indirect route  Yes. Yeah. then i- they - a c- window comes up and it only allows you to enter two speakers. Right. But you're saying that by the time you call it back in to - from their saved format it opens up a window with- window with five speakers? So. But. Right. Oh! That is sort of f- They didn't quite go the whole - It's just user interface. So i- it's - Yeah  they didn't go the whole route  did they? They just - the - the - the whole saved form- the saved format and the internal format  all that stuff  handles multiple speakers. It's just there's no user interface for specifying Right. multiple - any more than two. And that - So your - your script solves - Doesn't it solve all our problems  cuz we're always gonna wanna go through this Yep. Yep. preprocessing - uh  assuming it works. Yep. And that works nicely cuz this so quick to enter. So I wouldn't wanna do it through the interface anyway adding which - worry who the speaker was. Yep. I see. Right. Good. And then  uh  let's see what else. Oh  yes  I - I wanted to have - So sometimes a pers- I - uh- in terms of like the continuity of thought for transcriptions  it's - i- it isn't just words coming out  it's like there's some purpose for an utterance. And sometimes someone will do a backchannel in the middle of it but you wanna show that it's continued at a later point. So I have - I have a convention of putting like a dash arrow just to indicate that this person's utterance continues. And then when it uh  catches back up again then there's an arrow dash  and then you have the opposite direction to indicate continuation of ones own utterance versus  um  sometimes we had the situation which is - you know  which you - which you get in conversations  of someone continuing someone else's utterance  and in that case I did a tilde arrow versus a arrow tilde  Mmm. to indicate that it was continuation but it wasn't - Oh  I guess I did equal arrow for the - for the own - for yourself things cuz it's - the speakers the same. And then tilde arrow if it was a different - Mm-hmm. Mmm. if a different speaker  uh  Oh. con- continuation. But just  you know  the arrows showing continuation of a thought. Mm-hmm. And then you could track whether it was the same speaker or not by knowing - you know  at the end of this unit you'd know what happened later. And that was like this person continued and you'd be able to look for the continuation. Mm-hmm. But the only time So- that becomes ambiguous is if you have two speakers. Like  if you - If you only have one person  if you only have one thought that's continuing across a particular time boundary  you just need one arrow at each end  and if it's picked up by a different speaker  it's picked up by a different speaker. The time it becomes ambiguous if you have more than one speaker and that - and they sort of swap. I guess if you have more than one thread going  then you - then you need to know whether they were swapped or not. Mm-hmm. Mm-hmm. Mm-hmm. How often does that happen do you think? Hopefully not very much. Yeah  I didn't use it very often. Especially for meetings. It l- ou- I mean  if i- if you were just recording someone's day  it would be impossible. You know  if you were trying to do a remembrance agent. But I think for meetings it's probably alright. Hmm. But  a lot of these issues  I think that for - uh  from my point of view  where I just wanna do speech recognition and information retrieval  it doesn't really matter. But other people have other interests. So. Sure. I know. But it - it does feel - it does feel like it's really in there . I - you know I did this - I did this transcription and I marked that  I marked it with ellipsis because it seemed like there was a difference. It's something you wanted to indicate that it - that I - this was the end of the phrase  this was the end of that particular transcript  but it was continued later. And I picked up with an ellipsis. Right. Excellent. Yeah. Yeah. I didn't have the equal  not equal thing. Well that's - you know  I mean - I - that's why I didn't I didn't do it n- I mean  that's why I thought about it  and - and re-ev- and it didn't do - I didn't do it in ten times the - the time. Yeah  yeah. Yeah. Well  so anyway  are we interested then in writing tools to try to generate any of this stuff automatically? Is that something you want to do  Dan? No. But it's something @@ that I feel No. we definitely ought to do. I also wanted to ask you if you have a time estimate on the part that you transcribed. Do you have a sense of how long - Yeah  it took me half an hour to transcribe a minute  but I didn't have any - I didn't even have a - O_K. I was trying to get Transcriber to run but I couldn't. So I was doing it by typ- typing into a text file and trying to fit - It was horrible. O_K. O_K. So thirty to one's what you got? So that's a- new upper limit? Mm-hmm. Yeah. Well  I mean  that's - that's because you didn't have the segmentation help and all the other - Is it - But I think for a first try that's about right. So - so if we hired a who- if we hired a whole bunch of Dan's - That's right. a- Yeah. It was actually - it was quite - it was a t- If we hire an infinite number of Dan's - And there's always a warm up thing of - It'd b- a- a- it w- Are we gonna run out of disk space by the way? Yeah. No. O_K  good. O_K. d- Doesn't it beep in the other room when you're out of disk space? So - Is there - Maybe we should s- consider also  um  starting to build up a web site around No. Web site! That's great! all of these things. I know. Dan's sort of already started. We could have like business-to-business E_commerce as well! That's right. No  but I'm- it would be interesting - it would be interesting to see - Can we sell banner ads? Get - get paid for click-throughs? Yeah. What a good idea  that's how we could pay for the transcription. I want to introduce - I - I want to introduce the word ""snot-head"" into the conversation at this point. We can have - You wanna word that won't be recognized? You see  cuz - uh  cuz - Exactly. Oh  I don't think so. Um. No. The r- Hey  what about me? w- What - O_K. You're the one who raised the issue. No. Alright  see here's - here's - here's my thought behind it which is that  Alright. uh  the - the stuff that you've been describing  Jane  I gu- one has to  of course indicate  um  i- is very interesting  and I- I'd like to be able to - to pore through  you know  the - the types of tr- conventions that you've come up with and stuff like that. So I would like to see that kind of stuff on the web. Yeah  yeah  yeah. O_K  now  w- the alternative to a web site would be to put it in Doctor speech. Yes. Cuz - cuz what I have is a soft link to my transcription Yes. Either's fine. that I have on my account but it doesn't matter. We c- We can do it all. we can do it all! O_K. We can write - Web site's nice. Oh. Yeah. Then you have to t- you have to do an H_T access. Web site's what? We could actually - maybe we could use the T_C_L plug-in. Oh  man. Ooo! He's committed himself to something. Ow. See he said the word T_C_L and - and that's - But he does such a good job of it. He should be allowed to - to  you know  I know  I know. w- do it. I know  but that - but  I - Right. But I should be allowed to but - If you just did a crappy job  no- nobody would want you to do it. I sh- I shouldn't be allowed to by m- by my own - by my - according to my own priorities. Alright. Let's look at it anyway. So definitely we should - we should have some kind of access to And we have - we have quite a disparate number of Yeah. the data. web and other sorts of documents on this project sort of spread around. I have several and Dan has a few  and - Yes. Ah! Right  so we can add in links and stuff like that to other things. The - Nice. Yep. Well  yeah. Well so then th- Try - try to s- consolidate. I mean  who wants to do that though? No one wants to do that. So. the other side is  yeah. Uh  right. Yeah. Right  that's the problem. Why? What - what's - what's the issue? Well  we could put - we could put sort of a disorganized sort of group gestalt - No one owns the project. No one what? No one owns the project. Yeah  I own the project but I don't wanna do it. No one wants to own the project. Right. W- well Do - But - It's mine! All mine! Well then you have to do the web site. You know  it's like  it's that simple. But - ""Wah-hah-hah-hah-hah-hah."" b- but - but - but what are you - what are you talking about for web site hacking? You're talking about writing H_T_M_L  right? No - Yeah. Yeah  I - I'm talking about putting together all the data in a form that - that is legible  and pleasant to read  and up to date  and et cetera  et cetera  et cetera. But  is it against the law to actually use a tool to help your job go easier? Absolutely. It's - it's absolutely against the law to use a tool. I haven't found any tools that I like. It's just as easy to use - to edit the raw H_T_M_L as anything else. You y- No kidding? That's obviously not true  but you have - It's obviously not true. No  it- it- it's obviously true that he hasn't found any he likes. The question is what is - what's he looked at. Right. That's true. Which one do you use Jim? I use something called Trellix. Oh  that's right. I remember. Yeah. And it - Which produces also site maps. it's- it- it's very powerful. Now  I guess if I were - if I were doing more powerful - excuse me - more complex web sites I might want to. But most of the web sites I do aren't that complex. Well  would this be to document it also for outside people or mainly for in house use? But. No  I think in- I think both. I think mostly internal. That's right. Mostly in house. O_K. Well  yeah  but what does internal mean? I mean  you're leaving. No  both. People at U_W wanna look at it. I mean  it's - it's internal until - Right. Internal to the project. I see. We could do an H_T access which would accommodate those things. I - I - I - I - O_K  well  send me links and I wi- send me pointers  rather  and I'll put it together. Wonderful. I'm not - o- O_K. I'm not sure how - how important that distinction is. I don't think we should say  ""oh  it's internal therefore we don't have to make it very good"". I mean  you can say No. No. ""oh - oh  it's internal therefore we can put data in it that we don't - we don't have to worry about releasing"". But I think the point is to try and be coherent and make it a nice presentation. Right. I agree. Yeah  it is true  that is - it benefits to - Cuz you're gonna have to wor- do the work sooner or later. That's right. I mean  it's the early on. Yeah. Even if it's just writing things up. You know? Yep. It's a great idea. O_K  um  let's move on to electronics. Ah. Great. @@ d- we - we out of tape - out of disk? No  we're doing - we're doing great. I - I was looking for the actual box I plan to use  uh  but I - c- all I could - I couldn't find it at the local store. But this is the - the technology. It's actually a little bit thinner than this. And it's two by two  by one  and it would fit right under the - right under th- the- Yeah  does everyone know about the lip on the table? It's great. the - the - the lip  yeah. There's a lip in these tables. Nice. And  it oc- I p- especially brought the bottom along to try and generate some frequencies that you may not already have recorded. Clink! Clink! Let's see - see what it does to the - But this was the uh - just - just to review  and I also brought this along rather than the projector so we can put these on the table  and sort of And - and crinkle them and - w- push them around. And th- ""that"" being a diagram. What? What? That - that's the six tables that we're looking at. These six tables here  with - with little boxes sort of  uh  in the middle here. Which es- would - O_K. I see. I mean  the - the boxes are pretty much out of the way anyway. I'll- I'll show you the - the cro- this is the table cross section. I don't know if people realize what they're looking at . You trying to screw up the m- the microphones? I mean th- Yes. He is. Absolutely. Well why not? I mean  cuz this is what's gonna happen. You got plenty of data. I won't come to your next meeting. And - and - and you- So this is the box's - Get your paper off my P_D_A! Yeah. Yeah  let - let the record show that this is exhibit two B_. That's right. ""Or not to be"". Yeah  yeah. Yeah. Uh  the box  uh - there's a half inch lip here. The box is an inch thick so it hangs down a half an inch. And so the - the two head set jacks would be in the front and then the little L_E_D to indicate that that box is live. The - the important issue about the L_E_D is the fact that we're talking about eight of these total  which would be sixteen channels. And  uh  even though we have sixteen channels back at the capture  they're not all gonna be used for this. So there'd be a subset of them used for - obviously j- just use the ones at this end for - Hmm. for this many. So - Excuse me. you'd like a - a way to tell whether your box is live  so the L_E_D wouldn't be on. So if you're plugged in it doesn't work and the L_E_D is off that's - Right. All the lights. that's a tip off. And then the  uh - That's good. would wire the - all of the cables in a - in a bundle come through here and o- obviously collect these cables at the same time. Uh  so this - this notion of putting down the P_Z_Ms and taking them away would somehow have to be turned into leaving them on the table or - or - Right. Well  we wanna do that definitely. So. Right. Right. And so the - you - we just epoxy them down or something. Big screw into the table. Velcro. Uh  and even though there's eight cables they're not really very big around so my model is to get a - a - Sleeve. a- p- piece of - yeah  that - that stuff that people put with the little - you slip the wires into that's sort of shaped like that cross section. Yeah. I'm - I'm r- a- I'm going up and then I'm going down. Oh. O_K  not just sleeve them all? No. And leave them loose? That looks like a semi-circle. Yeah. It's like a - it's a sleeping policeman. Sleeping pol- Whoo! Speed- Speed bump! Speed bump. Speed bump. That's good. There we go s- Yeah  it's like a speed bum- An- A ""sleeping policeman""! Cool. And they're ac- they're actually ext- extruded from plastic. They sorta look like this. What is - Oh. What does that mean? Is it a speed bump? That's the s- that's British for speed bump  yeah. So that the wires go through here. Oh  is that right? I never heard that. Ah! Wow. Yeah. So. That's really cruel. O_K  so that - s- So it would c- basically go on the diagonal here. It could go either way. So why do we have sixteen channels instead of like some fewer number? Yeah. I guess. Uh  because the - How else are you gonna distribute them around the tables? Because they're there. Well  O_K  let me rephrase that. Why two each? Oh  because then you don't have to just have one each. So that if t- if you have two people sitting next to each other they can actually go into the same box. Yeah. And to - See  thi- this is really the way people sit on this table. O_K. Th- Mm-hmm. O_K. Uh. Dot  dot  dot. Which means two at each station. Well that - that's the way people sit. That's how many chairs are in the room. Yeah. Yeah  I'm just saying that for the recording. Alright. Yeah. Right. Right. O_K. And certainly you could do a thing where all sixteen were plugged in. Uh if - if you ha- if you had nothing else. Yeah. But then none of these. Right. N- none of these and no P_Z_Ms then. Right. Right. I agree. Only if you had - Well it depends on this box  right? Oh  true enough. And actually  at the m- my plan is to only bring eight wires out of this box. This - this box - Exactly. Oh  I didn't understand - Oh  I see  I see. That being the wiring box. Thi- thi- thi- this box is a one off deal. Uh. And  uh  it's function is to s- to  uh  essentially a wire converter to go from these little blue wires to these black wires  plus supply power to the microphones cuz the - the - he- the  uh  cheap head mounteds all require low voltage. So - so you'd imagine some sort of - in some sort of patch panel on top to figure out what the mapping was between each d- of these two and each of those one or what? Well I w- I- I- the simplest thing I could imagine  i- which is really  really simple is to - quite literally that these things plug in. Hmm! What - And there's a - there's a plug on the end of each of these - these  uh  ei- O_K. Yeah. Each of the blue wires? Mm-hmm. eight cables. An- and there's only - there's only four slots that are - But there are only four. Yeah. you know  in - in the first version or the version we're planning to - to build. So that - that was the whole issue with the L_E_D  that you plug it in  the L_E_D comes on  and - and - and you're live. Mm-hmm. Yeah. Oh  then it comes on. I see  I see. O_K  good. Now the - the - the subtle issue here is that - tha- I - I haven't really figured out a solution for this. So  we- it'll have to be convention. What happens if somebody unplugs this because they plug in more of something else? Well the - there's no clever way to let the up stream guys know that you're really not being powered. So - Mm-hmm. th- there will be a certain amount of looking at cables has to be done if people  uh  rewire things. But. Right. Yeah  I mean  we - I had that last time. But uh there are actually - that you know  there's an extra - there's a mix out on the radio receiver? Mm-hmm. So there are actually six X_L_R outs on the back of the radio receiver and only five cables going in  I had the wrong five  so I ended up not recording one of the channels and recording the mix. How interesting. D- did you do any recognition on the mix - mix out? Hmm. No. Wonder whether it works any - But I subtracted the four that I did have from the mix and got a pretty good approximation of the @@ . Yeah. Got the fifth? Cool. You g- Oh  how great. And did it work? Did it sound good? Is it - is - It's not bad. It's not bad  yeah. Ain't science wonderful? Wow. That's amazing. Yeah. So - So what's the schedule on these things? Wow. But  you always - Uh  well I was wrestling with th- with literally the w- number of connectors in the cable and the - the  uh  powering system. And I - I was gonna do this very clever phantom power and I decided a couple days ago not to do it. So I'm ready to build it. Hmm! Which is to say  uh  the neighborhood of a week to get the circuit board done. Mm-hmm. So I think the other thing I'd like to do is  do something about the set up so that it's a little more presentable and organized. See- I agree. And I'm - I'm just not sure what that is. I mean  some sort of cabinet. Well I can build a cabinet. The - the difficulty for this kind of project is the intellectual capital to design the cabinet. Mm-hmm. In other words  to figure out ex- exactly what the right thing is. That cabinet can - can go away. We can use that for - for uh kindling or something. But if you can imagine what the right form factor is. Dan- Dan and I have sort of gone around on this  and we were thinking about something that opened up in the top to allow access to the mixer for example. Mm-hmm. But there's these things sticking out of the mixer which are kind of a pain  so you end up with this thing that - if- if you stuck the mixer up here and the top opened  it'd be - it'd be fine. You wouldn't necessarily - Well  you s- understand what I'm - the - the - you can - you can start - start s- sketching it out  and I can certainly build it out of oak no problem  would it - you know  arb- you know  arbitrarily amount of - Yeah. Yeah  I understand. So. I need a desk at home too  alright? Is that gonna be a better solution than just going out and buy one? Well  the - as we found out with the - the thing that  uh  Jeff bought a long time ago to hold our stereo system the stuff you buy is total crap. And I mean this is something you buy. Mm-hmm. And - and - It's total crap. Well  it's useless for this function. Works fine for holding a Kleenex  but it - And it's total crap. Right  Kleenex and telephones. Right. Um  so yeah  I g- I guess it's just a question  is that something you wanna spend your time on? Oh  I - I'm paid for. O_K  great. I have no problem. No  but w- certainly one of the issues is - is the  uh - Hmm? is security. I mean  we've been - been - been lax and lucky. Mm-hmm. Yeah. Lax. Yep. Really lucky with these things. But they're not ours  so - the  uh - the flat panels. Yeah. Oh  yeah! I'm telling you  I'm just gonna cart one of them away if they stay there much longer. Wow. Tempting. Tempting. Yeah. Uh  let the record show at - uh at f- four thirty-five Adam Janin says - Well w- yeah  exactly. We'll know - we'll know to come after. So  um  j- uh  then the other question is do we wanna try to do a user interface that's available out here? Sorry? Use- user interface - Slipped - almost slipped it by Dan. A user interface. I mean  do we wanna try to get a monitor? Oh! Sure. Well of course we do. Or just something. Oh. And how do we want to do that? You mean like see - see meter readings  from - while sitting here. J- just so we see something. Wow. How about Yeah. use the thing that um ACIRI's doing. Which is to say just laptop with a wireless. Yeah  yeah  yeah. Sure. Oh. Which we'll borrow from them  when we need it. What's wrong with yours? If we bought you a - a - Oh  a- Applecard . Sure. Right. Yeah  you could use my machine. Well - What? I - or the - I have an I_RAM machine I've borrowed and we can use it. N- no  I'm - I'm - I'm serious. Does - does the wireless thing work on your - Wait  isn't that an ethernet connection or is that a phone? Uh  that's an ethernet connection. Well - Yeah - no - no I'm a- It's going next door. I - I - I ain't joking here. I'm serious  that - that - it - it - We jus- Yeah. No  no  absolutely  that's the right way to do it. T- to have it uh  just - It's very convenient especially if Dan happens to be sitting at that end of the table to not have to run down here and - and look in the thing every so often  but just have the - Yeah. And given - given that we've got a wireless - that we've got a - we got the field. Right. It's right there. Right? The antenna's right there  right outside the - Yeah. Yeah. Right. I don't know. Y- I mean  we need - obviously need to clear this with ACIRI but  uh  how tough can that be? There - it you'd - all you need's web access  isn't it? W- we don't need X_ access but I mean that's fine. That's - that's what it does  yeah. In - in theory. O_K  great  great. So - Um  right  so it's just a question of getting a laptop and a wireless modem. With a - with a - with a - w- 1x No  and he - he had  reque- @@ - my - my proposal is you have a laptop. You don't? No. Yeah. I do! Yeah. Yeah  yeah. If - if we bought you the thing would you mind using it with i- the - the - No  I would love to but I'm not sure if my laptop is compatible with the wave LAN thing they're using. Really? To Mac. He's - Your new one? Well Apple has their own thing  right? Airport. I'm sorry? Apple has their own thing. And - I thought it just came through a serial p- or an Ethernet port. Yeah  I think what - I think you - I think it just plug- plugs in a P_C card  so you could probably make it run with that  but. The question is  is there an Apple driver? I - e- Yeah  I'm sure. I imagine there is. But - uh- anyway But the two t- there are - there are abs- there are a bunch of machines at ICSI that have those cards and so I think if w- if it doesn't - we should be able to find a machine that does that. I - I mean I know that doesn't - don't - don't the important people have those little blue VAIOs that - Well  uh  b- that - to me that's a whole nother. That's a whole nother issue. Hmm. Hmm. Yeah. The - the idea of con- convincing them that we should use their network Yeah. i- is fairly straight forward. The idea of being able to walk into their office and say  ""oh  can I borrow your machine for a while""  is - is - is a non-starter. Yeah. Yeah. I see. That - I - I don't think that's gonna work. So  I mean  either - either we figure out how to use a machine somebody already - in the group already owns  Yeah. Yeah  yeah. a- a- and the idea is that if it's it perk  you know  it's an advantage not - not a disadvan- Absolutely. or else we - we literally buy a machine e- exactly for that purpose. Certainly it solves a lot of the problems with leaving a monitor out here all the time. Yeah. I - I - I - I'm - I'm not a big fan of doing things to the room that make the room Yeah. less attractive for other people  right? Which is part of the reason for getting all this stuff out of the way and - Right. Yeah. and  so- a monitor sitting here all the time you know people are gonna walk up to it and go  ""how come I can't get  you know  Pong on this"" or  whatev- Mm-hmm. Well - Right. I've - I've borrowed the I_RAM VAIO Sony thingy  and I don't think they're ever gonna want it back. Right. Yeah. You're kidding! Well  the next conference they will. So. Sure. Yeah. But that does mean - so we can use that as well. Well  uh  the - certainly  u- you should give it a shot first See whether Mm-hmm. you- you can get compatible stuff. Uh  ask them what it costs. Ask them if they have an extra one. Who knows  they might have an extra hardware s- I'd trade them a flat panel display for it. Yeah. Good. What is the  um  projector supposed to be hooked up to? Uh  the  uh - Tsk. It's gonna be hooked up to all sorts of junk. There's gonna be actually a - a plug at the front that'll connect to people's laptops so you can walk in and plug it in. And it's gonna be con- connected to the machine at the back. So we certainly could use that as - as a constant reminder of what the V_U meters are doing. So people sitting here are going Huge V_U meters. ""testing  one  two  three""! It - a- But I mean  that's another - that's another possibility that  you know  solves - Yeah. Yeah. That's an end- But - but - but I think the idea of having a control panel it's - that's there in front of you is really cool. Yeah. Yeah  yeah. Mm-hmm. I think and uh  having - having it on wireless is - is the neatest way - neatest way to do it. R- As long as you d- as l- as long as you're not tempted to sit there and f- keep fiddling with the volume controls going  ""can you talk a bit louder?"" I had - Yeah. I had actually earlier asked if I could borrow one of the cards to do wireless stuff and they said  ""sure  whenever you want"". So I think it won't be a problem. Yeah. Oh  cool. And - and it's a - a P_C_M_C_I_A card  right? P_C card  so you can have a slot  right? In your new machine? O_K. Yep. P_C card. Yeah  yeah. Is it with s- It's - it really come down to the driver. I mean - Yeah. Right  i- it'll - it'll work - Right  I mean  and if - and if his doesn't work  as I said  we can use the P_C. It'll work the first time. I - I trust Steve Jobs. Good. Um  well  that sounds like a d- good solution one way or the other. So - So Jim is gonna be doing wiring and you're gonna give some thought to cabinets? Uh  y- yeah. We - we need to Great. figure out what we want. Uh - We'd - I think - Hey  what are those green lights doing? They're flashing! Uh-oh! Uh-oh! Does that - it means - it means it's gonna explode. No. Cut the red wire  the red wire! Um - When people talk  it - they go on and off. This - So again  Washington wants to equip a system. Our system  we spent ten thousand dollars on equipment not including the P_C. However  seven and a half thousand of that was the wireless mikes. Uh  using - using these - Mm-hmm. And it - and the f- the five thousand for the wires  so if I'm gonna do - No. It's a joke. I have to do - Yeah  that's true but we haven't spent that  right? But once we - once we've done the intellectual part of these  uh  we can just knock them out  right? We can start - we - you can make a hundred of them or something. Cheap. Oh  of the - of the boards? Yeah  yeah  sure  right. And then we could - Washington could have a system that didn't have any wireless but would had - what's based on these and it would cost - Mm-hmm. Peanuts. A P_C and a peanuts. P_C and two thousand dollars for the A_-to-D_ stuff. Yeah. And that's about - cuz you wouldn't even need the mixer if you didn't have the - Oh th- Right. the P_Z_Ms. P_Z_Ms cost a lot. But anyway you'd save  on the seven - seven or eight thousand for the - for the wireless system. So actually that might be attractive. Right. O_K  I can move my thumb now. Good. That's a great idea. What? It's nice - it's nice to be thinking toward that. Oh  I thought like if we talked softer the disk lasts longer. Well  actually shorten - Yeah. There's a speech compression program that works great on things like this  cuz if the dynamic range is low it encodes it with fewer bits. And so most of the time no one's talking so it shortens it dramatically. But if you talk quieter  the dynamic range is lower and it will compress better. So. Yeah. Oh. Hmm. It also helps if you talk in a monotone. Probably. Constant volume all the time. Oh  interesting. And shorter words. Shorter words. Now  shorter words wouldn't - would induce more dynamics  right? You want to have - Yeah  but if the words are more predictable. How about if you just go  "" uh""? Huh. That's a long word! Uh. How do you spell that? I don't know. O_K  can you do one more round of digits? Are we done talking? Well it's a choice - if we get a choice  let's keep talking. Sure. Do we have more to talk about? No  I'm done. I'm done. Are you done? I'm done  yeah. I'm done. Dan isn't but he's not gonna say anything. But - you - you - you - there's a problem - a structural problem with this though. You really need an incentive at the end if you're gonna do digits again. Like  you know  candy bars or something  or - I'll - I'll remember to bring M_ and M_'s next time. or - or - or- or a little  uh - you know  toothbrushes like they give you at the d- dentist. Mmm! Or both. This is Adam Janin on the wireless lapel  mike number one  transcript five thirty-one  five fifty. Or both. two three O_ seven nine four five O_ three eight five two three seven one zero eight three two seven eight six - Excuse me. eight three two seven eight two six nine four two nine one O_ six two nine three zero one two s- zero six six four one six seven four seven eight five five one six eight two six five seven seven eight eight nine six five three nine O_ one two nine two three seven eight six. This is Dan Ellis  transcript five five one five seventy. Three five nine three. Four six. five Six. Seven zero six nine six u- one six. nine two zero O_ four five six zero four two zero two. Sorry. Zero four zero two. One eight O_ seven O_. Two. Three O_. Four. Six one two zero seven eight one. Seven one O_ one four. Eight four O_ one six three six. Nine seven three seven zero. O_ zero. One zero zero one three. Three six nine seven nine. This is Jane Edwards on microphone number five  transcript six one one  six three zero. five O_ six four four seven one six O_ four eight one eight four three six zero nine seven eight three O_ zero O_ one O_ - Darn it. Uh. one O_ one one two nine six two zero zero four five four one one four nine five three four six four nine three zero seven nine one eight eight seven six zero nine O_ O_ one two eight two three four one one three six five three four nine nine four five five zero five. This is Eric Fosler-Lussier on  microphone number three  uh  reading transcript five ninety-one to six ten. Five three three seven zero seven seven. Six four two. Seven six one nine one. Eight nine five O_ nine. Nine. O_ O_ seven. Zero. Two two. Three five. Four four three O_. Five eight zero. Six. Seven. Eight. O_ one eight. Zero three five seven. One six nine six two eight nine. Two four zero two. Three. Four O_ seven six. Jim Beck  transcript five seven one five nine zero channel four  wireless. four one five four five six six seven eight nine zero one eight zero two six five one four six three nine two five two three nine four three zero zero eight four five six zero five zero eight eight four nine four two three O_ seven three four zero nine three two one nine three two three. Eric  you and I win. We didn't make any mistakes. It's harder at the end than at the beginning. We don't know that for sure  do we? I should have mentioned that s- uh  to pause between lines but - No  I know. I'm just giving you a hard time. It's - it's only a hard time for the transcriber not for the speech recognizer. Me. Tha- tha- But - I also think you said channel four and I think you meant microphone four. And I think that's a mistake. Very good. So Eric  you win. But the other thing is that there's a - there's a colon for transcripts. And there shouldn't be a colon. Because see  everything else is stuff you fill in. Yeah  that's been filled in for you. Right? Automatically. But real- But they're in order! They start  six  seven  eight  nine  zero  one  two  three  four  five  six  eight  nine. Where'd they come from? Oh. And they're in order because they're sorted lexically by the file names  which are - have the numbers in digits. And so they're actually - this is like all the - all utterances that were generated by speaker M_P_J or something. And then within M_P_J they're sorted by what he actually said. Oh. Ugh! I didn't know that. I should have randomized it. Wow. It doesn't matter! It's like - Cuz you said ""six  seven  eight"". Well  we think it doesn't matter. But the real question I have is that  why bother with these? Why don't you just ask people to repeat numbers they already know? We think it doesn't matter. If I - if not I - Oh  interesting. Like phone numbers  you know  social security numbers. I know. I kn- Cuz we have these writt- written down  right? That's why - Because - Right. If we have it  uh - Social security numbers. Bank account numbers. Passport numbers. You can - you can generate - we don't have to transcribe. Credit card numbers  yeah. We don't have to tran- Yeah  please . Yeah. That's a great idea. Yeah  so you just say - say your credit card numbers  say your phone numbers  say your mother's maiden name. Bet we could do it. You know pe- Password to your account. Go on. people off the street. This - Alright. Actually  this - I got this directly from another training set  from Aurora. So. We can compare directly. Looks good. Looks like there were no errors. I was - I - the reason I made my mistake was - Wa- was this - ? What? There were no - there were no direct driver errors  by the look of it  which is good. Great. O_K  the mike's off. Thank you all. Good news. So I'm gonna stop it. Yeah  O_K. O_K. Mony on the mike. Uh-oh. ",The Berkeley Meeting Recorder group discussed efforts to train and test the Aurora group's HTK-based recognition system on ICSI's digits corpus. Members also discussed efforts to produce forced alignments from a selection of Meeting Recorder data. Performance in both tasks was adversely affected by the manner of recording conditions implemented and difficulties attributing utterances to the appropriate speakers. While debugging efforts resulted in improved forced alignments  dealing with mixed channel speech and speaker overlap remains a key objective for future work. The group is additionally focused on a continued ability to feed different features into the recognizer and then train the system accordingly. For comparing Meeting Recorder digits results  it was decided that the Aurora HTK-based system should be tested on data from the TI digits corpus. The script for extracting speaker ID information will require modifications to obtain a more accurate estimation of the amount of data recorded per speaker. Subsequent recognition experiments will look at large vocabulary speech from a far-field microphone (as performed in Switchboard evaluations). Hand-marked  word-level alignments are needed to reveal speaker boundaries and tune the parameters of the model. Modifications to the Transcriber tool are required for allowing transcribers to simultaneously view the signal in XWaves and see where words are located in time. Digits training needs to be performed on a larger data set. A significant loss in recognition resulted from not having included the type of phone-loop adaptation found in the SRI system. Recognition performance was worse for digits recorded in closed microphone conditions versus those recorded in a studio (e.g. TI-digits). A mismatch between the manner in which data were collected and the models used for doing recognition---e.g. bandwidth parameterization and the use of near- versus far-field microphones---was identified. Too little data per speaker can have a negative effect on VTL estimation. The PZM channel selected for obtaining digits data was too far away from most of the speakers. Current speech alignment techniques assume that foreground speech must be continuous and  barring some isolated words and backchannels  can not cope with overlapping background speech. Performing adaptations on both the foreground and background speaker produced a new variety of misalignments  a problem resulting  in part  from the fact that background speakers often match better to foreground conditionss. Transcribers occasionally misidentified speakers and omitted backchannels that were more hidden in the mixed signal. Good recognition performance was achieved with the lapel microphones. The recognizer performed well on time-aligned segments labelled as 'non-overlap' (i.e. one person talking)  while segments labelled as 'overlap' (i.e. multiple speakers talking at the same time) yielded poor results. Future recognition efforts will include looking at reverberation. Forced alignment improvements were gained by examining the types of errors generated and making the necessary adjustments. More accurate alignments were achieved by significantly increasing the pruning value. Future alignment efforts will include cloning the reject model  and adapting it to both the foreground and background speaker. Members of the group will also compare Meeting Recorder data with other corpora (e.g. Switchboard) to determine whether speaker overlap is a feature that is more specific to meetings versus other modes of spoken interaction. A cursory analysis of background speech revealed that backchannels frequently occurred after a question was asked. Backchannels also featured a high proportion of 'yeahs' and a substantially fewer 'uh-huhs'. Several group members are preparing Eurospeech submissions. Speakers fe016 and mn017 are preparing a paper about the 'spurt' format  wherein spurts from individual channels---i.e. continuous speech regions delineated by pauses---will be extracted  merged with alignments from different channels  and time-aligned. 
"- headphones that aren't so uncomfortable. I think - Well  this should be off the record  but I think - Hmm. Uh   O_K. We're not recording yet  are we? Well  I don't think - No. No  uh  that - that wasn't recorded. Um  I don't think they're designed to be over your ears. Yeah  I know. It just - it really hurts. It gives you a headache  like if you - Temple squeezers. On your temple - Yeah. Yep. Yeah. Mm-hmm. But I definitely haven't figured it out. Um  Meeting Recorder meeting. I guess I have to d- stop doing this sigh of contentment  you know  after sipping cappuccino or something. I was just noticing a big s- Yeah  with the - We kno- I know. We know exactly how much you have left in your cup. ""Sip  sigh."" So are we recording now? Is this - Yeah. Oh! We're - we're - we're live. O_K. Yeah. So  uh  what were we gonna talk about again? So we said - we said data collection  which we're doing. Were we gonna do digits? O_K. Do we do th- do you go around the room and do names or anything? I think that - u- It's a good idea. usually we've done that and also we've s- done digits as well  but I forgot to print any out. So. Besides with this big a group  it would take too much time. No. I- it'd be even better with this big - You can write them on the board  if you want. Which way is - Yeah  but it takes too much time. Mari? Y- I think your - your - your thing may be pointing in a funny direction. What - What? It's not that long. Sort of it's - it helps if it points sort of upwards. Whoops. Sort of it - you know. Yeah. Would it - m- So that thing - the little - th- that part should be pointing upwards. That's it. Yeah. w- u- So - Oh  this thing. Otherwise you just get a heartbeats. Yeah. It's kind of - Oh  yeah  the element  yeah  n- should be as close to you - your Yeah. O_K. That's good. That kind of thing is good. It's a - mouth as possible. This w- Alright. How's that working? Yeah. Oh  yeah. It's a - Yeah. It's working. O_K. Alright. So what we had was that we were gonna talk about data collection  and  um  uh  you - you put up there data format  Um. and other tasks during data collection  So  I think the goal - the goal was and - what can we do - how can you do the data collection differently to get - what can you add to it to get  um  some information that would be helpful for the user-interface design? Like - Uh  especially for querying. Especially for querying. So  getting people to do queries afterwards  getting people to do summaries afterwards. Well  one thing that came up in the morning - in the morning was the  um  Um. i- uh  if he - I  um - if he has - s- I - I don't remember  Mister Lan- Doctor Landry? La- Landay? Landay. James. So he has  um  these  uh  um  tsk note-taking things  then that would sort of be a summary which you wouldn't have to solicit. Mm-hmm. y- if - if we were able to - Well  if - if you to do that. actually take notes as a summary as opposed to n- take notes in the sense of taking advantage of the time-stamps. So action item or uh  reminder to send this to so-and-so  blah-blah-blah. Mm-hmm. So that wouldn't be a summary. That would just be - that would b- relate to the query side. But if we had the CrossPads  we could ask people  you know  if - if something comes up Mm-hmm. write it down and mark it somehow  you know. Right. I mean  we - because you'd have several people with these pads  you could collect different things. I mean  cuz I tend to take notes which are summaries. And so  you know - Right. I mean  the down-side to that is that he sort of indicated that the  uh  quality of the handwriting recognition was quite poor. Well - But that's alright. I don't think there'd be so many that you couldn't So - have someone clean it up pretty easily. Yeah. We also could come up with some code for things that people want to do so that - for frequent things. Yeah. And the other things  people can write whatever they want. I mean  it's to some extent  uh  for his benefit. So  if that - you know  if - if we just keep it simple then maybe it's still useful. Right. Yeah. I just realized we skipped the part that we were saying we were gonna do at the front where we each said who we were. The roll call. Right. I thought you did that Roll call. on purpose. But anyway  shall we do the roll call? No  not a- No  I just - My mind went elsewhere. So  uh  yeah  I'm Morgan  and where am I? I'm on channel three. And I'm Adam Janin on channel A_. I'm Jane Edwards  I think on channel B_. I'm Dan Ellis. Eric on channel nine. Liz  on channel one. Mari on channel zero. Katrin on channel two. Should we have used pseudo-names? Should we do it a second time with pseudo- No. No. I'm Rocky Raccoon on channel - And  uh  do you want to do the P_D_As and the P_Z_Ms? Let me  uh  turn that off. Oh. P_Z_M nearest  nearest  next nearest. Next one. Next nearest. Far. Furthest. P_D_M-right  P_Z_A-right - P_D_A-right  P_D_A-left. O_K. Thanks. Yeah  and eventually once this room gets a little more organized  the Jimlets will be mounted under the table  and these guys will be permanently mounted somehow. You know  probably with double-sided tape  but - So. You - So we won't have to go through that. Hmm. I have a question on protocol in these meetings  which is when you say ""Jimlet"" and the person listening won't know what that is  sh- shou- How - how do we get - Is that important information? You know  the Jimlet - I mean  the box that contains the - Well  I mean  suppose we broaden out and go to a range of meetings besides just these internal ones. There's gonna be lots of things that any group of people who know each other have in column - common that we will not know. Mm-hmm. Right. O_K. Right. So the- there will be jargon that we he- There'll be transcription errors. Good. Yeah. O_K. I mean  we - we were originally gonna do this with V_L_S_I design  and - and - and the reason we didn't go straight to that was because immediately ninety percent of what we heard would be jargon to - to us. So. Well  that was just one of the reasons. But  yeah  definitely. O_K. Good. Yeah. That - that's right. There were others of course. Yeah. O_K  so we were on the data collection and the summary issue. Right. We can go back. So  uh  u- u- So  actually there's kind of three issues. There's the CrossPad issue. Should we do it and  if so  what'll we have them do? Um  do we have s- people write summaries? Everybody or one person? And then  do we ask people for how they would query things? There's - there're sub-problems in that  in that where - or when do you actually ask them about that? I mean  that was - Is that - Right. One thing I was thinking about was is that Dan said earlier that  you know  maybe two weeks later  which is when you would want to query these things  you might ask them then. Right. But there's a problem with that in that if you're not - If you don't have an interactive system  it's gonna be hard to go beyond sort of the first level of question. Right. Right. And furth- id- explore the data further. Right. And - So. There's - there's another problem which is  um  we certainly do want to branch out beyond  uh  uh  recording meetings about Meeting Recorder. And  uh  once we get out beyond our little group  the people's motivation factor  uh  reduces enormously. And if we start giving them a bunch of other things to do  how - you know  we - we did n- you know another meeting here for another group and - and  uh  they were fine with it. But if we'd said  ""O_K  now all eight of you have to - have to come up with  uh  the summar-"" Well  I asked them to and none of them did. t- See? There we go. Mm-hmm. So  I - I asked them to send me They - ideas for queries after the meeting and no one ever did. I didn't follow up either. Mm-hmm . So I didn't track them down and say ""please do th- do it now"". But  uh  Yeah. no one spontaneously provided anything. I- I'm worried that if you did - even if you did push them into it  it - it - it might be semi-random  Right. uh  as opposed to what you'd really want to know if you were gonna use this thing. Right. I just don't know how else to generate the queries other than getting an expert O_K. to actually listen to the meeting and say ""that's important  that might be a query"". Tsk. Well  there is this other thing which y- which you were alluding to earlier  which is  um  there are certain key words like  you know  ""action item"" and things like that  which could be used in  uh  Although - t- to some degree finding the structure. And - and I also  um  was thinking  with reference to Yeah. W- the n- uh  note-taking  the advantage there is that you get structure without the person having to do something artificial later. And the fir- third thing I wanted to say is the summaries afterwards  um  I think they should be recorded instead of written because I think that  um  it would take so long for people to write that I think you wouldn't get as good a summary. How about this idea? That normally at most meetings somebody is delegated to be a note-taker. Yeah  good. Good point. Yeah. And - So why don't we just use the notes that somebody takes? I mean  that gives you a summary but it doesn't really - How do you generate queries from that? Well. But  I mean  maybe a summary is one of the things we'd want from the output of the system. Right? Yeah. Right. I mean  they're something. It's a - a kind of output you'd like. Actually - Uh  James and I were talking about this during one of the breaks. And the problem with that is  I'm definitely going to do something with information retrieval And so - even if it's sort of not full- full-bore Right. what I'm gonna do for my thesis. I'm gonna do something. I'm not gonna do anything with summarization. And so if someone wants to do that  that's fine  but it's not gonna be me. Well  I think that we - I mean  the - the f- the core thing is that you know once we get some of these Well - issues nailed down  we need to do a bunch of recordings and send them off to I_B_M and get a bunch of transcriptions even if they're slightly flawed or need some other - Yep. And then we'll have some data there. Yeah. And then  i- i- we can start l- looking and thinking  what do we want to know about these things and - at the very least. Mm-hmm. Yeah - I actually want to say something about the note pad. So  Hmm. if you could sense just when people are writing  and you tell them not to doodle  or try not to be using that for other purposes  and each person has a note pad. They just get it when they come in the room. Then you c- you can just have a fff plot of wh- you know  who's writing when. Activity. Yeah. That's all you - And  you can also have notes of the meeting. But I bet that's - that will allow you to go into the - Mm-hmm. sort of the hot places where people are writing things down. I mean  you can tell when you're in a meeting when everybody stops to write something down that something was just said. It may not be Oh  I see. Mm-hmm. kept in the later summary  but at that point in time is was something that was important. Mm-hmm. And that wouldn't take any extra - That's a nice idea. Or someone could just pu- you could just put your hand on the pad and go like that if you want to. It - Mm-hmm. Mm-hmm. That's a good idea but that doesn't - It's - Maybe I'm missing something  but that doesn't get to the question of how we come up with queries  right? Well  what it does - Well  then you can go to the points where the - you could actually go to those points in time and find out what they were talking about. And you r- Yeah. Well  what it does is provide a different - I - I think it's an interesting thing. I don't think it gets at the - And - Uh  y- the queries per- se  but it does give us an information fusion sort of thing that  you know  you wanna i- say Yeah. ""what were the hot-points of the meeting? "" That - that's what I mean  is that I think it gets at something interesting but if we were asking the question  which I thought we were  of - of - of  um  ""how do we figure out what's the nature of the queries that people are gonna want to ask of such a system?""  knowing what's important doesn't tell you what people are going to be asking. But I bet it's a good superset of it. Does it? Well  see  there are th- Well  yeah. I think you could say they're gonna ask about  uh  when - uh  when did so-and-so s- talk about blah. And at least that gives you the word that they might run a query on. At least you can find the locations where there are maybe keywords and - I mean  i- this would tell you what the hit is  not what the query is. What - Maybe. Right  right. But I think - I think thinking about queries is a little bit dangerous right now. We don't even know what - Right. It'll tell you the hit but not the query. And so you could - you can generate a query from the hits  but - Right. I mean  if you want to find out what any user will use  that might be true for one domain and one user  but I mean a different domain and a different user - Mm-hmm. Mm-hmm. Yeah  but we're just looking for a place to start with that because  you know  th- what - what - what James Um. is gonna be doing is looking at the user-interface and he's looking at the query in - in - i- We - we have five hours of pilot data of the other stuff but we have zero hours of - of - of queries. So he's just sort of going ""where - where do I - w- where do I start?"" Well  th- you could do - I think the summaries actually may help get us there  O_K. for a couple reasons. One  if you have a summary - if you have a bunch of summaries  you can do a word frequency count and see what words come up in different types of meetings. Mm-hmm. So ""action item"" is gonna come up whether it's a V_L_S_I meeting  or speech meeting  or whatever. So words that come up in different types of meetings may be something that you would want to query about. Mm-hmm. Um  the second thing you could possibly do with it is just run a little pilot experiment with somebody saying ""here's a summary of a meeting  what questions might you want to ask about it to go back? "" Yeah  I think that's difficult because then they're not gonna ask the questions that are in the summary. Well - But  I think it would give - That's one possi- one possible scenario  though  is you have the summary  Mm-hmm. and you want to ask questions to get more detail. th- Yeah  I think it has to be a participant. Well  it doesn't have to be. O_K. So that - that is another use of Meeting Recorder that we haven't really talked about  which is for someone else  as opposed to as a remembrance agent  which is what had been my primary thought in the information retrieval part of it would be. But  uh  I guess if you had a meeting participant  they could use the summary to refresh themselves about the meeting and then make up queries. But it's not - Mm-hmm. I don't know how to do it if - until you have a system. The summary is actually gonna drive the queries then. I mean  your research is going to be very circular. Yeah. Mmm. Yeah  that - that's what I was saying. Yeah. But th- there is this  um - There is this class of queries  which are the things that you didn't realize were important at the time but some- in retrospect you think ""oh  hang on  didn't we talk about that?"" And it's something that didn't appear in the summary but you - And that's kind of what this kind of  uh  complete data capture is kind of nicest for. Mm-hmm. Right. Right. Cuz it's the things that you wouldn't have bothered to make an effort to record but they get recorded. Right. So  I mean - And th- there's no way of generating those  u- u- until we just - until they actually occur. You know  it's like - But you could always post-hoc label them. Right  right. Exactly. But I mean  it's difficult to sort of say ""and if I was gonna ask four questions about this  what would they be?"" Yeah. Yeah. Those aren't the kind of things that come up. But at least it would get us started. Oh  yeah. Yeah  sure. I also think that w- if - if you can use the summaries as an indication of the important points of the - of the meeting  then you might get something like - y- So if th- if the obscure item you want to know more about was some form of data collection  you know  maybe the summary would say  you know  ""we discussed types of na- data collection"". And  you know - And - and maybe you could get to it by that. If you - if you had the - the larger structure of the - of the discourse  then if you can categorize what it is that you're looking for with reference to those l- those larger headings  then you can find it even if you Mmm. don't have a direct route to that. Although it seems like that's  um  I think that - a high burden on the note-taker. That's a pretty fine grain that the note-taker will have to take. Maybe Landay can put a student in to be a note-taker. I th- No. I think you got to have somebody No? who knows the pro- knows the topic or - you know  whose job it is delegated to be the note-taker. Somebody who's part of the meeting. Mm-hmm. No  I mean  but someone who can come sit in on the meetings and then takes the notes with them that the real note-taker - But they - And that way that one student has  you know  a rough idea of what was going on  and they can use it for their research. I mean  this isn't really necessarily what you would do in a real system  Mm-hmm. because that- that's a lot of trouble and maybe it's not the best way to do it. Mm-hmm. Mm-hmm. But if he has some students that want to study that then they should sort of get to know the people and attend those meetings  and get the notes from the note-taker or something. Right. Well  I think that's a little bit of a problem. Their sort of note-taking application stuff Hmm. they've been doing for the last couple of years  and I don't think anyone is still working on it. I think they're done. Yeah. Um  so I'm not sure that they have anyone currently working on notes. So what we'd have to interest someone in is the combination of note and speech. Mm-hmm. And so the question is ""is there such a person?"" And I think right now  the answer is ""no"". I've b- been thinking - I've been thinking about it a little bit here - about the - uh  th- this  e- um - Well- We'll just have to see. I think that the - now I'm thinking that the summary - a summary  uh  is actually a reasonable  uh  bootstrap into this - into what we'd like to get at. It's - it's not ideal  but we - you know  we - we have to get started someplace. So I was - I was just thinking about  um  suppose we wanted to get - w- We have this collection of meeting. We have five hours of stuff. Uh  we get that transcribed. So now we have five hours of meetings and  uh  you ask me  uh  uh  ""Morgan  what d- you know  what kind of questions do you want to ask?"" Uh  I wouldn't have any idea what kind of questions I want to ask. I'd have to get started someplace. So in fact if I looked at summary of it  I'd go ""oh  yeah  I was in that meeting  I remember that  um  what was the part that -"" And - and th- I think that might then help me to think of things - even things that aren't listed in the summary  Mm-hmm. but just as a - as a - as a refresh of what the general thing was going on in the meeting. I think it serves two purpo- purposes. One  as sort of a refresh to help bootstrap Mm-hmm. Yeah. queries  but also  I mean  maybe we do want to generate summaries. And then it's - Well  yeah. That's true too. Hmm. Yeah  absolutely. Then you want to have it. you know  it's kind of a key. So how does the summary get generated? I'm not against the idea of a summary  but Uh - Well  i- i- - ? By hand. I wanted to think carefully about who's generating it and how - because the summary will drive Or  d- o- the queries. So - What I - I think  you know  in most meetings  this one being different  but in most meetings that I attend  there's somebody t- explicitly taking notes  frequently on a laptop - Um  you can just make it be on a laptop  so then yo- you're dealing with ASCII and not somebody - you don't have to go through handwriting recognition. Mm-hmm. Um  and then they post-edit it into  uh  a summary and they email it out for minutes. I mean  that happens in most meetings. I- I - I think that  um  there's - we're using ""summary"" in two different ways. So what you just described I would describe as ""minutes"". Minutes. Yeah. Right. And what I originally thought was  um  if you asked someone ""what was the meeting about?"" Yeah. O_K. And then they would say ""well  we talked about this and then we talked about that  and so-and-so talked about -"" Hmm. And then you'd have  like - I - e- My thought was to have multiple people summarize it  on recording rather than writing because writing takes time and you get irrelevant other things that u- take time  that - Mm-hmm. Whereas if you just say it immediately after the meeting  you know  a two-minute summary of what the meeting was about  I think you would get  uh  with mult- See  I - I also worry about having a single note-taker because that's just one person's perception. And  um  you know  it - it's releva- it's relative to what you're focus was on that meeting  and - and people have different Mm-hmm. major topics that they're interested in. A- So  my proposal would be that it may be worth considering both of those types  you know  the note-taking and O_K. Yeah. a spontaneous oral summary afterwards  no longer than two minutes  Adam  you can - from multiple people. Yeah. you can correct me on this  but - but  uh  my impression was that  uh  pretty much  uh  true that the meetings here  nobody sits with a w- uh  with a laptop and - Never. Never. I've never seen it at ICSI. Does anyone - ? I mean  Dan is the one who - who I- Dan? most frequently would take notes  and - Yeah. I've d- When we - when we have other meetings. When I have meetings on the European projects  we have someone taking notes. Oh  really? In fact  I often do it. Yeah. Yeah  but those are bigger deal things. Right? Where you've got fifteen peo- I mean  most - th- this is one of the larger meetings. Most of the meetings we have are That's true - are four or five people. four or five people and you're not - you don't have somebody sitting and taking minutes for it. You just get together and talk about where you are. Yeah. Right. So  I think it depends on whether it's a business meeting Culture. or a technical discussion. And I agree  technical discussions you don't usually have somebody taking notes. Yeah. Yeah. Yeah. The I_RAM meeting  they - they take notes every - Yeah. Do they? There's uh a person with a laptop at each meeting. How many people are those meetings? There are more. I mean  there are ten-ish. Yeah. Yeah. Y- you should also have a record of what's on the board. I mean  I find it very hard to reconstruct what's going on. I - I don't know how - They're very sparse. Yeah. This is something early in the project we talked a lot about. I don't know how  but for instance  I mean  the outline is sort of up here and that's what people are seeing. And if you have a - Or you shou- could tell people not to - to use the boards. But there's sort of this missing information otherwise. I agree. I agree  but - but We sh- we should - you - you just - you g- end up with video  Well  I don't know. and - and instrumented rooms. And that's a different project  I think. f- u- I think for this data capture  it would be nice to have a digital camera Yeah  different - Uh  y- just to take pictures of who's there  where the microphones are  and then we could also put in what's on the board. You know  like three or four snaps for every - Right. I agree. That's wonderful. Yeah. People who were never at the meeting will have a very hard time understanding it otherwise. for every meeting. Mm-hmm. But don't you think that's - I agree. Don't you think that - But - Mm-hmm. Well  no. I mean  I - I just think - I mean  I think that right now we don't make a record of where people are sitting on the tables. Even people who were at the meeting. Huh . Right. Right. And that - the - at some point that might be awfully useful. But I think adding Mm-hmm. Mmm. photographs adds a whole nother level of problems. Yeah. We- It's just a digital record. n- uh  Not - not as part of the - not as a part of the data that you have to recover. Just - just in terms of - I don't mean that you model it. We should just - Like archiving it or storing it. Yeah. Yes  I agree. I agree. It's i- because discourse is about things  and then you have the things that are about  and it's recoverable. Mm-hmm. Because someone - someone later might be able to take these and say ""O_K  they  you know - at least these are the people who were there and here's sort of what they started talking about  and -"" So - and just - Yes. And it's so simple. Like you said  three snapshots and - Li- uh  L- L- L- Liz  you - Just to archive. u- uh  Liz  you sa- you sat in on the  uh  Actually - subcommittee meeting or whatever - I'll go and get the camera. uh  on - you - on the subcommittee meeting for - for - at the  uh - that workshop we were at that  uh  uh  Mark Liberman was - was having. So I - I wasn't there. They - they - they - they h- must have had some discussion about video and the visual aspect  and all that. Big  big interest. Yeah. Huge. I mean  it - personally  I don't - I would never want to deal with it. But I'm just saying Yeah. first of all there's a whole bunch of fusion issues that DARPA's interested in. You know  fusing gesture and face recognition  even lip movement and things like that  Yeah. Yeah. for this kind of task. And there's also I think a personal interest on the part of Mark Liberman Mm-hmm. in this kind of - in storing these images in any data we collect so that later we can do other things with it. Yeah. Mmm. Mm-hmm. So - so to address what - what Adam's saying  I mean  I think you - uh  that And - the key thing there is that this is a description of database collection effort that they're talking about doing. Mm-hmm. And if the database exists and includes some visual information that doesn't mean that an individual researcher is going to make any use of it. Right? So  uh - Mm-hmm. But that - it's gonna be a lot of effort on our part to create it  and store it  and Right. So we're gonna - get all the standards  and to do anything with it. So we're gonna do what we're gonna do  whatever's reasonable for us. But having - Yeah. I think even doing something very crude - Like I know with ATIS  we just had a tape recorder running all the time. Mm-hmm. And later on it turned out it was really good that you had a tape recorder of what was happening  even though you w- you just got the speech from the machine. So if you can find some really  you know  low  uh  perplexity  Low fidelity. Yeah. yeah  way of - of doing that  I think it would be worthwhile. I agree. And if it's simple as - I mean  as simple as just the digital - Otherwise you'd - you lose it. Yeah. Well  minimally  I mean  what - what Dan is referring to at least having some representation of the p- the spatial position of the people  cuz we are interested in some spatial processing. And so - Mm-hmm. Right. Mm-hmm. so  um - Well  once the room is a little more fixed that's a little easier cuz you'll - Well  the wireless. Yeah. Yeah. Yeah. But - Also C_M_U has been doing this and they were the most vocal at this meeting  Alex Waibel's group. And they have said  I talked to the student who had done this  that with two fairly inexpensive cameras they - they just recorded all the time and were able to get all the information from - or maybe it was three - from all the parts of the room. Mm-hmm. So I think we would be - we might lose the chance to use this data for somebody later who wants to do some kind of processing on it if we don't collect it Yeah. I - I - I don't disagree. I think that if you have that  then people who are interested in vision can use this database. at all. The problem with it is you'll have more people who don't want to be filmed than who don't want to be recorded. Well  she's not making - So that there's going to be another group of people who are gonna say ""I won't participate"". Mmm. That's true. Um - Mm-hmm. Or you could put a paper bag over everybody's head and not look at each other and not look at boards  and just all be sitting talking. That would be an interes- Bu- Well - Uh-huh. Great idea. Well  there's - that'd be the - the parallel  yeah. But I think y- she's - we're just proposing a minimal preservation of things on boards  Yeah. I definitely won't participate if there's a camera. sp- spatial organization - And you could anonymize the faces for that matter. You know  I mean  this is - We can talk about the - But  you know  that's a lot of infrastructure and work. To set it up and then anonymize it? It's just one snapshot. We're not talking about a movie. We're talking about a snapshot. No  it wa- n- not  um - No  no  no  no. Not for - not for C_M_U. They have a pretty crude set-up. And they had - So - Mm-hmm. Yeah. Mm-hmm. they just turn on these cameras. They were - they were not moving or anything. Couldn't find it? It's on its way. And stored it on analog media. Hmm. Hmm? It's coming. And they - they didn't actually align it or anything. They just - they have it  though. Yeah. Well  it's worth considering. Maybe we don't want to - spend that much more time discussing it  but - Did they store it digitally  or - ? Hmm-mm. I think they just - or just put it on videotape? I think they just had the videotapes with a c- you know  a counter or something. Um  Mm-hmm. Well  I think for - I mean  for our purposes we probably will d- I'm not sure. we - we might try that some and - and we certainly already have some recordings that don't have that  Yeah. uh  which  you know  we- we'll - we'll get other value out of  I think. Yeah. Th- The thing is  if it's easy to collect it - it th- then I think it's a wise thing to do because once it's gone it's gone. And - I'm just - The community - If L_D_C collects this data - u- I mean  and L_- if Mark Liberman is a strong proponent of how they collect it and what they collect  there will probably be some video data in there. There you go. And so that could argue for us not doing it or it could argue for us doing it. The only place where it sort of overlaps is when some of the summarization issues are - actually could be  um  easier - made easier if you had the video. I think at the moment we should be determining this on the basis of our own  Mm-hmm. uh  interests and needs rather than hypothetical ones from a community thing. As you say  if they - if they decide it's really critical then they will collect a lot more data than we can afford to  uh  and - and will include all that. Mmm. Um  I - I - I'm not worried about the cost of setting it up. I'm worried about the cost of people looking at it. e- In other words  it's - it - it'd be kind of silly to collect it all and not look at it at all. And so I - I - I think that we do have to do some picking and choosing of the stuff that we're doing. But I - I am int- I do think that we m- minimally want - something - we might want to look at - at some - some  uh  subsets of that. Like for a meeting like this  at least  uh  take a Polaroid of the - of the - of the boards  and - Of the board. Yeah. Exactly. Or at least make sure that the note-taker takes a sh- you know  a snapshot of the board. a- and know the position of the people - That'll make it a lot easier for meetings that are structured. I mean  otherwise later on if nobody wrote this stuff on the board down we'd Exactly. Mm-hmm. have a harder time summarizing it or agreeing on a summary. We - And it - Especially since this is common knowledge. I mean  this is shared knowledge among all the participants  and it's a shame to keep it off the recording. s- Uh  except in - er  if we weren't recording this  this - this would get lost. Right? Well  I don't understand that point. I mean  I just think that the - Yeah. The point is that we're not saving it anyway. Right? In - in our real-life setting. Well - What do you mean we're not saving it anyway? I've written all of this down and it's getting emailed to you. And you're gonna send it out by email  too. Well  uh  in that case we don't need to take pictures of it. Right. That would be the other alternative  to make sure that anything that was on the board  Yeah. Yeah. Well - Well  that's why - that's why I'm saying that I think the note-taking um  is in the record. would be - I think in many - for many meetings there will be some sort of note-taking  in which case  that's a useful thing to have - Uh  I mean  we - uh  we don't need to require it. Just like the - Mm-hmm. I mean  I think it would be great if we try to get a picture with every meeting. I agree. Um  so - so we won't worry about requiring these things  but the more things that we can get it for  the more useful it will be for various applications. So - So. So  I mean  departing for the moment from the data collection question but actually talking about  you know  this group and what we actually want to do  uh  so I guess that's th- the way - what you were figuring on doing was - was - was  uh  putting together some notes and sending them to - to everybody from - from today? O_K. So. Um - That's great. So - so the question that - that we started with was whether there was anything else we should do during - during th- during the collection. And I guess the CrossPads was certainly one idea  Ow. uh  and we'll get them from him and we'll just do that. Right? And then the next thing we talked about was the - was the summaries and are we gonna do anything about that. Well  before we leave the CrossPads and - and call it done. Oh  O_K. So  if I'm collecting data Yeah. then there is this question of do I use CrossPads? So  I think that if we really seriously have me collect data and I can't use CrossPads  it's probably less useful for you guys to go to the trouble of using it  um  unless you think that the CrossPads are gonna - n- I'm not - I'm not sure what they're gonna do. But - but having a small percentage of the data with it  I'm not sure whether that's useful or not. Maybe - maybe it's no big deal. Maybe we just do it and see what happens. What - I guess the point was to try - again  to try to collect more information that could be useful later for - for the U_I stuff. Mm-hmm. So it's sort of Landay supplying it so that Landay's stuff can be easier to do. So it - it - Right now he's g- operating from Right. Nothing. zero  and so even if we didn't get it done from U_W  it seems like that would - could still - O_K. You shou- I mean  at least try it. I think it'd be useful to have a small amount of it just as a proof of concept. It will - Yeah. Right. O_K. And - and they seem to not be able to give enough of them away  so we could probably get more as well. You know  what you can do with things. Yeah. That's true. So if it - if it seems to be really useful to you guys  we could probably get a donation to me. But not - not to rely on them for basic modeling. Yeah  I'm not sure. I think it- it - it will again depend on Landay  and if he has a student who's interested  and how much infrastructure we'll need. I mean  if it's easy  we can just do it. Yeah. Um  but if it requires a lot of our time  we probably won't do it. I guess a lot of the stuff we're doing now really is pilot in one sense or another. And so we try it out and see how it works. Right. Yeah. Yeah  we have to sort of figure out what we're gonna do. Right. Yeah. I just wouldn't base any of the modeling on having those. Right. I ag- I think I agree with that. Right. Yeah. It's just - Right. O_K. I think  though  the importance marking is a good idea  though. That if - if people have something I'd be sort of cool. I mean  it would - in front of them - Yeah. That w- shouldn't be hard for - Yeah. Do it on pilots or laptops or something. O_K  if something's important everyone clap. O_K. So CrossPads  we're just gonna try it and see what happens. O_K. Yeah. Um  I think that's right. O_K. O_K. The note-taking - So  I - I think that this is gonna be useful. So if we record data I will definitely ask for it. So  I j- I think we should just say this is not - we don't want to put any extra burden on people  but if they happen to generate minutes  Yeah. Oh  O_K. That's fine. could - could they send it to us? Absolutely. Mm-hmm. Mm-hmm. And then - Yeah. What I was gonna say is that I don't want to ask people to do something they wouldn't normally do in a meeting. It's ver- I just want to keep away from the artificiality. But Mm-hmm. I think it definitely if they exist. And then Jane's idea of summarization afterward I think is not a bad one. Um  picking out - basically to let you pick out keywords  um  and  uh  construct queries. So who - who does this summarization? Yeah  I'm thinking that - Yeah. People in the meeting. You know  just at - at the end of the meeting  before you go  Uh-huh. Without hearing each other though  probably. go around the table. Yeah. Yeah . Yeah. Or even just have one or two people stay behind. Ugh. Yeah. Yeah. People with radio mikes can go into separate rooms and continue recording without hearing each other. That's the nice thing. Well  then you should try them a few weeks later and - How fascinating. And see - score them? They have all these memory experiments about how little you actually retain and wasn't - That's right. Well  that's the interesting thing  though. If we do - if we collect four different summaries  you know  we're gonna get all this weird data about how people perceive things differently. It's like Oh . Hmm. this is not what we meant to research. Right  right. Oh. Yeah. That could be very interesting. Mm-hmm. Hmm. Yeah. Ru- But - but again  like the CrossPads  I don't think I would base a lot of stuff on it  because I think - I d- yeah  I don't know how you would do it  though. Mm-hmm. I know when I see the - the clock coming near the end of the meeting  I'm like inching towards the door. So  Running to - Yeah  fff! Hmm. you're probably not gonna get a lot of people wanting to do this. Well  I think if - Maybe e- Is email easier? I mean  I - when you first said do - do it  um  spoken  what I was thinking is  oh then people have to come up and you have to hook them up to the recorder. Mm-hmm. So  if they're already here I think that's good  but if they're not already here for - I'd rather do email. Right. Yeah  I'd just try - Well  however the least intrusive and - and quickest way is  and th- and closest to the meeting time too  cuz people will start to forget it as soon as they l- leave. I'm much faster typing than anything else. Yeah. Yeah. I think that - I think doing it orally I don't know. At - at the end of the meeting is the best time. I just don't - because Mm-hmm. they're kind of a captive audience. Once they leave  Mm-hmm. you know  forget it. But - but i- Yeah  read the digits  do the summary. Right. But  uh  I don't think that they'll necessarily - Hmm. you'll - you'll get many people willing to stay. But  you know  if you get even one - w- I would s- Yeah. Well  I think it's like the note-taking thing  that - that y- that you can't - certainly can't require it or people aren't gonna want to do this. But - but if there's some cases where they will  then it would be helpful. And I'm also wondering  couldn't that be included in the data sample so that you could increase the num- you know  the words that are  Hmm. uh  recognized by a particular individual? If you could include the person's meeting stuff and also the person's summary stuff  maybe that would be Yeah. uh  an ad- addition to their database. It's kind of nice. Yeah. Hmm. Under the same acoustic circumstance  cuz if they just walk next door with their set-up  Right. nothing's changed  just - So I have a question about queries  God  that's bugging me. which is  um  You turn - Can we turn that light off? uh - If - can we turn that just - that - that let - Uh  let the record show the light is flickering. The fl- the fluorescent light is flickering. Yeah. I don't know. Yeah. Yeah  there's a - Oh  it is - it is like - O_K. Very annoying. Oh  much better. Yeah. There you go. O_K. Good. Oh  yeah. That's better. Yeah. For a little while I thought it was just that I was really tired. That and y- Too much caffeine. Too much caffeine and really tired  but then I thought ""no  maybe that's real"". O_K. I thought it was the projector for a moment. It was like  ""what's going on?"" So  Yeah. the question I had about queries was  um  so what we're planning to do is have people look at the summaries and then generate queries? We - we've just been talking  how do we generate queries? And so that was one suggestion. Are - are we gonna try and o- Yeah. Well  I mean  so  the question I had is is have we given any thought to how we would generate queries automatically given a summary? I mean  I think that's a whole research topic un- unto itself  so that it may not be a feasible thing. But - Mmm. Hello. Dan here. n- Yeah. Shouldn't Landay and his group be in charge of figuring out how to do this? I mean  this is an issue that goes a little bit beyond where we are right now. They're the expert- O_K. Mari? Yeah? Someone wants to know when you're getting picked up. Is someone picking you up? Um  what's our schedule? Well  you still wanted to talk with Liz. Let's see  you and I need dis- Uh  no  we did the Liz talk. And you and I need to- Oh  oh. You already did the Liz talk. O_K. Yeah. So - so that was the prosody thing. We- I don't remember it. Um  we need to finish the - It's already four-fifteen. Oh  O_K. I have like no recall memory. Yeah. Uh  after. We need to finish this discussion  and you and I need a little time for wrap-up and quad chart. So  And what? I'm at your disposal. So  up to you. um - Um  what - what's the plan for this discussion? We should - Um  I think we should be able to wind up in another half-hour or something  you think? At least. Yeah. m- i- Even if that much? Less. Uh  less. Less? Yeah. So  I think - It's interesting that he's got  like  this discussion free yet it's separate. Well  I mean  we still haven't talked about the action items from here and so on. And - Action - Yeah. So  e- e- why don't you say five-thirty? O_K  five-thirty. I don't - Is that O_K? We'll probably hit horrible traffic. Sounds - O_K. h- Thanks  bye. That's not That's that. a lot of time  but - Yeah. Well  in answer to ""is it Landay's problem?""  um  he doesn't have a student who's interested right now in doing anything. So he has very little manpower. Um  there's very little allocated for him and also he's pretty focused on user interface. So I don't think he wants to do information retrieval  query generation  that sort of stuff. Yeah  well there's gonna be these student projects that can do some things but it can't be  yeah  very deep. u- I - I actually think that - that  uh  again  just as a bootstrap  if we do have something like summaries  then having the people who are involved in the meetings themselves  who are cooperative and willing to do yet more  come up with - with - with queries  uh  could at least give - give Landay an idea of the kind of things that people might want to know. I mean  ye- Right? If he doesn't know anything about the area  and - the people are talking about and - and  uh - But the people will just look at the summaries or the minutes and re- and sort of back-generate the queries. That's what I'm worried about. So you might as well just give him the summaries. Mm-hmm. And - Maybe. Well  I'm not sure - I'm not sure that's a solved problem. y- Well  but I think - Right? Of how to - how to generate queries from a - Oh  O_K. How to do this I  uh - Yeah. from the summary. That was sort of what my question was aimed towards. So what you want to h- to do is  people who were there  Hmm. who later see  uh  minutes and s- put in summary form  which is not gonna be at the same time as the meeting. There's no way that can happen. Right. Right. Are we gonna later go over it and  like  make up some stuff to which these notes would be an answer  or - or a deeper - Or - or just a memory refresher. Yeah. I mean - But that's done off - they have to do that off-line. Yep. I agree. I'm also wondering if we could ask the - the people a - a question which would be You- ""what was the most interesting thing you got out of this meeting?"" Becau- in terms of like informativeness  it might be  you know  that the summary would - That's a good one. would not in- even include what the person thought was the most interesting I would think that would be the most likely thing. fact. Dan doesn't know what sex he is. Yeah  really. But actually I would say that's a better thing to ask than have them summarize the meeting. I think you get two different types of information. You get two - Yeah  that's true. Yeah. Because you get  like  the general structure of important points and what the - what the meeting was about. Hey. Oh  you're still here. Ah- We're still here. Yeah. So you get the general structure  the important points of what the meeting was about with the summary. But with the ""what's the most interesting thing you learned? "" - Uh  so the fact that  uh  I know that Transcriber uses Snack Going to see the kids. You - you can keep it on. is something that I thought was interesting and that - and that Dan worked on - on that. So I thought that was really - you know. So  I mean  you could ge- pick up some of the micro items that wouldn't even occur as major headings but could be very informative. Mm-hmm. Yeah  that's actually a really good idea. I think it wouldn't be too  uh  uh  cost-intensive either. You know  I mean  it's like something someone can do pretty easily on the spur of the moment. Are you thinking about just asking one participant or all of them? As many are willing to do it. Make it a voluntary thing  and then - Yeah. Cuz you'll get - cuz you'll get very different answers from everybody  right? So - Yeah. That's why I was wondering. Well  maybe one thing we could do is for the meetings we've already done - I mean  I - we didn't take minutes and we don't have summaries. But  uh  people could  like  listen to them a little bit and generate some queries. Yeah. Of course Jane doesn't need to. I'm sure you have that meeting memorized by now. Yeah. But actually it would be an easy thing to just go around the room and say what was the most interesting thing you learned  Mmm. Yeah. Yeah. for those pe- people willing to stay. And that - I think it would pick up the micro-structure  the - some - some of the little things that would be hidden. And - and that might be something people are willing to stay for. That would be interesting. Boy  I - I don't know how we get at this - Or want to get up and leave. Yeah  but when you go around the room you might just get the effect that somebody says something and then you go around the room and they say ""yeah  me too  I agree."" Me too  me too  me too. Yeah. On the other hand people might try and come up with different ones  right? They might say ""oh  I was gonna say that one but now I have to think of something else"". That's fine. So - Well - Well  you have the other thing  that - that they know why we're doing it. We'll - I mean  we'll - we'll be telling them that the reason we're trying to do this is - is to d- generate queries in the future  so try to pick things that other people didn't say. It's gonna take some thought. I mean  It seemed - The kind of  uh  interest that I had in this thing initially was  uh  that i- basically the form that you're doing something else later  Mm-hmm. and you want to pick up something from this meeting related to the something else. So it's really the imp- the - the list of what's important's in the something else Mmm. Right. rather than the - And it might be something minor - of minor importance to the meeting. Right. Mm-hmm. Uh  in fact if - if it was really major  if it's the thing that really stuck in your head  then you might not need to go back and - and - and check on it even. So it's - it's that you're trying to find - You're - you've now - You weren't interested - Say I - I said ""well  I wasn't that much interested in dialogue  I'm more of an acoustics person"". Right. But - but thr- three months from now if for some reason I get really interested in dialogue  and I'm ""well what is - what was that part that - that - that  uh  Mari was saying?"" Yeah  like Jim Bass says ""add a few lines on dialogue in your next perf-"" Uh - Yeah. Yeah. And then I'm trying to fi- I mean  that's - that's when I look - in general when I look things up most  is when it's something that Yeah. didn't really stick in my head the first time around and - but for some new reason I'm - I'm - I'm interested in - in - in the old stuff. But that - that's gonna be very hard to generate. So  I don't - I don't know. Well  I - That's hard to generate and - and I think that's half of what Do we - Mm-hmm. i- I would use it for. But I also a lot of times Mm-hmm. um  make - you know  think to myself ""this is interesting  I've gotta come back and follow up on it"". Mm-hmm. Mm-hmm. So  things that I think are interesting  um  I would be  uh  wanting to do a query about. And also  I like the idea of going around the room  because if somebody else thought something was interesting  I'd kind of want to know about it and then I'd want to follow up on it. Hmm. Yeah. That - that might get at some of what I was - I was concerned about  uh  being interested in something later that w- uh  I didn't consider to be important the first time  which for me is actually the dominant thing  because if I thought it was really important it tends to stick more than if I didn't  but some new task comes along that makes me want to look up. But - But what's interesting to me may not b- have been interesting to you. By - so by going around - Yeah. Yeah. So having multiple people might get at some of that. Yeah. Yeah  I - I think you can't get at all of it  right? Yeah. W- we just need to start somewhere. Yeah  and this is a starting point. Uh-huh. The question - the question then is h- h- how much bias do we introduce by - you know  introduce by saying  you know  this was important now and  you know  maybe tha- something else is important later? Mm-hmm. I mean  does it - does the bias matter? I - I don't know. I mean  uh  that's  I guess  a question for you guys. But - Well  and - and one thing  we - we're saying ""important"" and we're saying ""interesting"". And - and those - those can be two different things. Uh  yeah  yeah. Sure  sure. But I - I - I guess that's the question  really  is that - I mean  Mm-hmm. Mm-hmm. W- does building queries based on what's important now introduce an irreversible bias on being able to do what Morgan wants to do later? Well  irreversible. I - I - I mean  I guess what I- what I - I keep coming back to in my own mind is that  um  O_K  good. That's - that's - Yeah. Right. the soonest we can do it  we need to get up some kind of system so that people who've been involved in the meeting can go back later  even if it's a poor system in some ways  and  uh - and ask the questions that they actually want to know. If - you know  if - uh  as soon as we can get that going at any kind of level  then I think we'll have a much better handle on what kind of questions people want to ask than in any - anything we do before that. But obviously we have to bootstrap somehow  and - Sure. Right. Mm-hmm. I agree. Right. I will say that - that I - I chose ""interesting"" because I think it includes also ""important"" in some cases. But  um  I - I - I feel like the summary gets at a different type of information. I think ""important"" can often be uninteresting. Mmm. Mmm. Well  and - and also - Hmm. And ""interesting"" is more interesting than ""important"". i- it puts a lot of burden on the person to - to evaluate. You know  I think inter- ""interesting"" is - is non-threatening in - O_K- O_K. Yeah. In the interest of  um  Yeah. Importance? generati- generating an interesting summary  um - Mm-hmm. Yeah. No  i- in the interest of generating some minutes here  uh  and also moving on to action items and other things  let me just go through the things that I wrote down as being important  um  that we at least decided on. CrossPads we were going to try  um  if Landay can get the  uh - get them to - to you guys  um  and see if they're interesting. And if they are  then we'll try to get m- do it more. Um  getting electronic summary from a note-taking person if they happen to do it anyway. Mm-hmm. Um  getting just  uh  digital pictures - a couple digital pictures of the - the table and boards to set the context of the meeting. Uh  and then going around the room at the end to just say - qu- ask people to mention something interesting that they learned. So rather than say the most interesting thing  k- something interesting  and that way you'll get more variety. Sure. Mm-hmm. That's good. I like that. I like that. I wouldn't even say that ""that they learned"". O_K. Yeah. Uh  you might want to mention something that - that you brought up. ""Thing that was discussed."" And then the last thing c- would be for those people who are willing to stay afterwards and give an oral summary. Mm-hmm. O_K? Does that pretty much cover everything we talked about? Mm-hmm. A- That - well  that we want to do? And one - and one qualification on - on the oral summaries. They'd be s- they'd be separate. They wouldn't be hearing each other's summaries. O_K. Yeah  that's like - n- I think that's gonna predominantly end up being whoever takes down the equipment then. And - and that would also be that the data would be included in the database. Yeah  that would be  let's see  me. Mm-hmm. Mm-hmm. O_K. I mean  there is still this hope that people might actually think of real queries they really want to ask at some point. And that if - if that ever should happen  then we should try and write them down. Mm-hmm. Right. Give them a reward  a dollar a query? Mm-hmm. Yeah  really. If they're real queries. Yeah. Well  and again  if we can figure out a way to jimmy a - a - a - a very rough system  say in a year  Yeah. O_K. So - Yeah. then - uh  so that in the second and third years we - we actually have something to - Play with and generate real queries from. ask queries. Uh - Yeah. Right. O_K. So. Yeah. Yeah. I think - I just wanted to say one thing about queries. I mean  the level of the query could be  you know  very low-level or very high-level. And it gets fuzzier and fuzzier as you go up  right? Well  we're gonna - So you need to have some sort of - if you start working with queries  some way of identifying what the - you know  if this is something that requires a - a one-word answer or it's one place in the recording versus was there general agreement Hmm. on this issue of all the people who ha- You know  you can gen- you can ask queries that are meaningful for people. In fact  they're very meaningful cuz they're very high-level. Yep. But they won't exist anywhere in the a- you know - Absolutely. So I think we're gonna have to start Mm-hmm. with keywords and - and if someone becomes more interested we could work our way up. But - I- I'm - I- I'm not so sure I agree with that. Because - uh  b- because it depends on  uh  what our goal is. If our goal is Wizard of Oz-ish  It - But it may well - Really? Oh  that's true. we might want to know what is it that people would really like to know about this data. And if it's - if - if it's something that we don't know how to do yet  th- great  that's  you know  research project for year four or something. You know? Yeah. Yeah. Right. Research  yeah. Mm-hmm. Mm-hmm. Yeah  I was thinking about Wizard of Oz  but it requires the wizard to know all about the meetings. We'd have to listen to all the data. Um  well  not - maybe not true Wizard of Oz because people are too So. Oh  yeah. I - I understand. Well just imagine if - Yeah. uh  aware of what's going on. But - but just - Get people to ask questions that they def- the machine definitely can't answer at the moment  but - Yeah. Yep. Yeah. w- Just ""what would you like to know?"" But that - neither could anyone else  though  is what  uh  my point is. Yes. I- I was wondering if - if there might be one s- more source of queries which is indicator phrases like ""action item""  O_K. which could be obtained from the text - from the transcript. Right. Since we have the transcript. Yeah. Dates maybe. I don't know. That's something I always forget. Yeah  that's something to be determined  something to be specified  but text-oriented. Well  probably if you have to sit there at the end of a meeting and say one thing you remember  it's probably whatever action item was assigned to you. I mean  in gen- that's all I remember from most meetings. I think you'd remember that  yeah. That - that's all I wrote down. So  in general  I mean  that could be something you could say  right? I'm supposed to do this. Yeah  that's true. Well  but then you could - you could prompt them to say  It - it doesn't - you know  ""other than your action item""  you know  whatever. But - but the action item would be a way to get  uh  maybe an additional query. So. Well - I mean  that's realistically what people might well be remembering. So. Hmm. Yeah. Well  but - you know  but you could get again @@ - Well  we're piloting. We'll just do it and see what happens. Yeah. Yeah. Yeah. Yeah. I usually don't remember my action items. But - I'd - I - O_K- O_K. Speaking of action items  can we move on to action items? Yeah. Mm-hmm. Yeah. yeah. Sure. Can you hand me my note pad? Um  or maybe we should wait until the summary of this - until this meeting is transcribed and then we will hav- Yeah. Then we'll know. We - we had - I mean  Thanks. somewhere up there we had milestones  but I guess - Did y- did you get enough milestone  uh  from the description things? I got - Yeah. In fact  why don't you hand me those transparencies so that I remember to take them. eee  O_K. O_K. And  you know  there's obviously detail behind each of those  as much as is needed. So  you just have to let us know. O_K. What I have down for action items is we're supposed to find out about our human subject  um  requirements. Good. Yep. Uh  people are supposed to send me U_R_Ls for their - for web pages  to c- and I'll put together an overall cover. And you're s- Right. We - Hmm? we need to look at our web page and make one that's - And - and you also need to look at your web page and clean it up by mid-July. that's p- Right. P_D_A-free. Yeah. Um  Right. let's see. Choo-choo-choo. Mailing lists. We - Mailing list? Uh  you need to put together a mailing list. Three of them. Well - I mean  Uh  I think w- Yeah. uh  Um  mostly together. uh  I need to email Adam or Jane  um  about getting the data. Who should I email? Uh  how quickly do you want it? Um. My July is really very crowded. And so  uh - How about if I just c- Uh  Right now all I want - I personally only want text data. I think the only thing Jeff would do anything with right now - But I'm just speaking fr- based on a conversation with him two weeks ago I had in Turkey. But I think all he would want is the digits. Um  but I'll just speak for myself. I'm interested in getting the language model data. Eh  so I'm just interested in getting transcriptions. Mm-hmm. O_K. So y- So then just email you? Sure  sure  sure. O_K. Wh- You could email to both of us  uh  just - I mean  if you wanted to. I mean  I don't think Mm-hmm. O_K. either of us would mind recei- but - but in any case I'd be happy to send you the - i- That's right. And your email is? i- Edwards at ICSI. w- O_K. Dot Berkeley dot E_D_U  of course. In - in our phone call  uh  before  we - we  uh - It turns out the way we're gonna send the data is by  uh  C_D-ROMs. And then - And  uh - and then what they're gonna do is take the C_D-ROM and transfer it to analog tape and give it to a transcription service  uh  that will - Yeah. Oh  is this I_B_M? Yeah  using foot pedals and - Yeah  foot - foot pedals and - Uh  so do they - How are they gonna do the multi-channel? See  that's a good question. Yeah. They - they don't have a way. I thought so. No  I mean  it'll be But they have a verification. Mix? probably about like you did  and then there will be some things - you know  many things that don't work out well. And that'll go back to I_B_M and they'll - they'll  uh - they run their aligner on it and it kicks out things that don't work well  which - you know  the overlaps will certainly be examples of that. And  uh - I mean  what w- we will give them all of it. Right? We'll give them all the - the multi-channel stuff and - O_K. That's  uh  my question. So we'll give them all sixteen channels and they'll do whatever they want with it. Yeah. But you also should probably give them the mixed - Yeah. You know  equal sound-level - Good idea. Mm-hmm. I mean  they're not gonna easily be able to do that  probably. It's not hard. Well - Ah  yeah. So. It's also won't be adding much to the data to give them the mixed. But w- Mm-hmm. Yep. Absolutely. It's not - Right. It doesn't - it isn't difficult for us to do  so we might as well just do it. I- Right. i- You should - Absolutely. So  sure. Yeah. You should - that may be all that they want to send off to their transcribers. O_K. Related to - to the conversation with Picheny  I need to email him  uh  my shipping address and you need to email them something which you already did. I did. I - I m- emailed them the Transcriber U_R_L  um  the on-line  uh  data that Adam set up  The U_R_L so they can click on an utterance and hear it. and I emailed them the str- streamlined conventions which you got a copy of today. Right. And I was gonna m- email them the - which I haven't yet  a pointer to - to the web pages that we - that we currently have  cuz in particular they want to see the one with the - the way the recording room is set up Good. and so on  your - your page on that. Oh  excellent. Good. And then p- possibly - I C_- I C_C'ed Morgan. I should have sent - I should have C_C'ed you as well. O_K. Not an immediate action item but something we do have to worry about is data formats for - for higher-level information. O_K. Oh  yeah. We were gonna - Well  or d- or not even higher level  different level  prosody and all that sort of stuff. We're gonna have to figure out how we're gonna annotate that. Yeah  we w- Yeah. We never had our data format discussion. Oh  I thought we did. We discussed  uh  musi- Right. musical score notation and - But that's not - That's display. That's different than format. Oh  O_K. and its X_M_L - That's - Well  um - W- My - my u- feeling right now on format is you guys have been doing all the work and whatever you want  we're happy to live with. Um  Well - uh  yeah. Yeah. O_K  excellent. O_K. So  what n- other people may not agree with that  but - Cuz I'm not actually touching the data  so I shouldn't be the one to talk. But - important thing - Well  it c- Right. No  I think that's fine. So a key thing will be that you - we tell you Great. what it is. Yeah. ""Here's a mysterious file and -"" Uh  we also had - Yeah. We also had the  uh  uh - that we were s- uh  that you were gonna get us the eight-hundred number and we're all gonna - Oh  yeah. we're gonna call up your Communicator thing and - and we're gonna be good slash bad  depending on how you define it  uh  users. Now  something that I mentioned earlier to Mari and Liz is that it's probably important to get as many non-technical and non-speech people as possible in order to get some realistic users. So if you could ask other people to call and use our system  that'd be good. Cuz we don't want people who already know how to deal with dialogue systems  who know that Yeah. Or  like if you have a - you shouldn't hyper-articulate  for instance  and things like that. So. Or  like if you have somebody who makes your - your plane reservations for you  Yeah. um  which is Yeah  we can do that. the n- Get my parents to do it. Yeah  for instance. Yeah. Yeah. Seriously. Yeah. Your grandmother. Hmm. Yeah. e- You know  it could result in some good bloopers  which is always good for presentations. So - Yeah. @@ Um  anyway - I think my father would last through the second prompt before he hang - hung up. My mother would have a very interesting conversation with it but it wouldn't have anything to do with the travel. Mmm. He would never use it. O_K. Um  other - O_K. Let's see  other action items. So I have the - We talked about that we're getting the recording equipment running at U_W. And so it depends  w- e- e- e- they're - you know  they're p- m- If that comes together within the next month  there at least will be  uh  uh  major communications between Dan and U_W folks as to - Yeah. I mean  I'm - I'm shooting to try to get it done - we should talk about it  but - get it put together by the beginning of August. So  um  Mmm. But we have - it - it's - it's pretty - We don't know. I mean  he - he s- uh  he said that it was sitting in some room collecting dust and - and so we don't know  you know  if- We don't know. i- e- i- It's probably unlikely that we'll pull this off  but a- at least it's worth trying. Mm-hmm. What is it? We don't know. Oh  O_K. ""Recording equipment."" Yeah. It's a tape recorder. W- We know it's eight channels. Uh  we know it's digital. It's eight tape recorders. We don't even know if there're microphones. So  we'll find out. O_K. Um  and I will email these notes - Um  I'm not sure what to do about action items for the data stuff  although  then somebody - I guess somebody needs to tell Landay that you want the pads. Yeah  O_K. I'll do that. O_K. Um  and he also said something about outside - there - that came up about the outside text sources  that he - he may have Mm-hmm. Oh! some text sources that are close enough to the sort of thing that we can Hmm. play with them for a language model. Yeah  that was - uh  that was - What he was saying was this - he - this thing that  uh  Jason had been working on finds web pages that are thematically related to what you're talking about. Well  that's the idea. So that that - that would be a source of text which is - supposedly got the right vocabulary. Mm-hmm. Right. But it's obviously very different material. It's not spoken material  for instance  so - Yeah. But it's p- it might be - But - but that's actually what I wanna do. That's - that's what I wanna work with  is - is O_K. Yeah. things that s- the wrong material but the right Yeah. Yeah. da- the right source. Un- unfortunately Landay told me that Jason is Yeah. not gonna be working on that anymore. He's switching to Yeah. He seemed - when I asked him if he could actually supply data  he seemed a little bit more reluctant. other stuff again. So  I'll - I'll send him email. I'll put it in an action item that I send him email about it. And if I get something  great. If I don't get something - Who? Landay or Jason? Landay. O_K. O_K. And  uh  um  you know  otherwise  if you guys have any papers or - I could - I could use  uh - I could use your web pages. That's what we could do. You've got all the web pages on the Meeting Recor- Yeah  why search for them? They're - we know where they are. Yeah  that's true. True. Yeah! Absolutely. Sure. Oh  forget this! Well  but that's not very much. I - One less action item. I can use what web pages there are out there on meeting recorders. Yep. I mean  that - that's - Yeah. Basically what his software does is h- it picks out keywords and does a Google-like Right. Yeah. Yeah. Yeah. search. So you could - Yeah. So we can - we can - we can do better than that. Yeah. We can do that. Yeah. Yeah. Mm-hmm. There's - there's some  uh  Carnegie Mellon stuff  right? On - on meeting recording  and - Yep. And Xerox. So  there's - there's ICSI  Xerox  And Xerox. Yeah. And there's - You should l- look under  like  intelligent environments  smart rooms  Um  the ""Georgia Tech Classroom Two Thousand"" is a good one. um - C_M_U  Right. And then - Right. J- There's - th- That's where I thought you would want to eventually be able to have a board or a camera  because of all these classroom - Well  Georgia Tech did a very elaborate instrumented room. And I want to try to stay away from that. So - Mm-hmm. Yeah. Mm-hmm. O_K. Great. That solves that problem. One less action item. Um - O_K. I think that's good enou- that's - that's pretty much all I can think of. Can I ask  uh  one thing? It relates to data - data collection and I - and I'd - and we mentioned earlier today  this question of - um  so  um  I s- I know that from - with the near-field mikes some of the problems that come with overlapping speech  uh  are lessened. But I wonder if - Uh  is that sufficient or should we consider maybe getting some data gathered in such a way that  um  u- w- we would c- uh  p- have a meeting with less overlap than would otherwise be the case? So either by rules of participation  or whatever. Oh  yeah. Now  I mean  you know  it's true  I mean  we were discussing this earlier  that depending on the task - so if you've got someone giving a report you're not gonna have as much overlap. But  um  Adam! i- i- uh  so we're gonna have s- you know  non-overlapping samples anyway. But  um  in a meeting which would otherwise be highly overlapping  is the near-field mike enough or should we have some rules of participation for some of our samples to lessen the overlap? Hmm. @@ turn off I don't think we should have rules of participation  but I think we should try to get a variety of meetings. That's something that if we get the - the meeting stuff going at U_W  O_K. that I probably can do more than you guys  cuz you guys are probably mostly going to get ICSI people here. Mm-hmm. But we can get anybody in E_E  uh  over - and - and possibly also some C_S people  uh  over at U_W. So  I think that - O_K. that there's a good chance we could get more variety. Just want to be sure there's enough data to - O_K  good. They're still gonna overlap  but - Um  Mark and others have said that there's quite a lot of found data from the discourse community that has this characteristic and also the political - Y- you know  anything that was televised for a third party Mm-hmm. has the characteristic of not very much overlap. Wasn- but w- I think we were saying before also that the natural language group here had less overlap. So it also depends on the style of the group of people. Mm-hmm. Mm-hmm. So. Like the  um  dominance relations of the people in the meeting. Right. Mm-hmm. On the task  and the task. It's just - Mm-hmm. I just wanted to - uh  because you know  it is true people can modify the amount of overlap that they do if - if they're asked to. Yeah. Yeah. Not - not entirely modify it  but lessen it if - if it's desired. But if - if that's sufficient data - I just wanted to be sure that we will not be having a lot of data which can't be processed. O_K. So I'm just writing here  we're not gonna try to specify rules of interaction but we're gonna try to get more variety by i- using different groups of people Time. and different sizes. Fine. And I - you know  I - I know that the near f- near-field mikes will take care of also the problems to s- to a certain degree. I just wanted to be sure. e- e- Yeah. And then the other thing might be  um  uh  technical versus administrative. Cuz if I recorded some administrative meetings then that may have less overlap  because you might have more overlap when you're doing something technical and disagreeing or whatever. Mm-hmm. Mm-hmm. Well  I - just as - as - as a contributary - eh  so I - I know that in l- in legal depositions people are pr- are prevented from overlapping. They'll just say  you know - you know  ""wait till each person is finished before you say something"". So it is possible to lessen if we wanted to. But - but these other factors are fine. I just wanted to raise the issue. Well  the reason why I didn't want to is be- why I personally didn't want to is because I wanted it to be as  Mm-hmm. uh  unintrusive as possi- as you could be with these things hanging on you. Mm-hmm. Oh  yeah. Yeah  I think that's always desired. I just want to be sure we don't - that we're able to process  i- u- uh  you know  as much data as we can. Yeah. Yeah. Did they discuss any of that in the - the meeting they had with L- Liberman? What - Mm-hmm. What - what do they - And there was a big division  so Yeah. Liberman and others were interested in a lot of found data. So there's lots of recordings that - Yeah. They're not close-talk mike  but - And - and there's lots of television  you know  stuff on  um  political debates and things like that  congre- congressional hearings. Boring stuff like that. Um  and then the C_M_U folks and I were sort of on the other side in - cuz they had collected a lot of meetings that were sort of like this and said that those are nothing like these meetings. Um  so there're really two different kinds of data. And  I guess we just left it as - @@ that if there's found data that can be transformed for use in speech recognition easily  then of course we would do it  Mm-hmm. but newly collected data would - would be natural meetings. Actually  th- @@ the C_M_U folk have collected a lot of data. Is that - is that going to be publicly available  or - ? So. As far as I know  they h- have not. Um  but e- O_K. It's also - it's not - it's not near-far  right? I'm not sure. Um  if people were interested they could talk to them  but I - I got the feeling there was some politics involved. I think @@ gonna add that to one of my action items. No. Just to check. Yeah. W- we should know what's out there certainly. Yeah. I - I don't know. Yeah. I mean  the - Cuz I had thought they'd only done far-field  I think you need to talk to Waibel and - intelligent-room sorts of things. I hadn't known that then - they'd done any more than that. Oh  really? It's those guys. Oh  they only did the far-field? I see. Yeah. But they had multiple mikes and they did do recognition  and they did do real conversations. But as far as I know they didn't offer that data to the community at this meeting. Mm-hmm. But that could change cuz Mark - you know  Mark's really into this. We should keep in touch with him. Yeah. Yeah  I think - Well  once we send out - I mean  we still haven't sent out the first note saying ""hey  this list exists"". But - but  uh  once we do that - Is that an action item? Yeah. It's on - I already added that one on my board to do that. So  uh - uh  hopefully everybody here is on that list. We should at least check that everybody here - ? I think everyone here is on the list. Yeah. I'm not. u- e- e- I think you are. We haven't sent anything to the list yet. Oh! O_K. Yeah. We're just compiling the list. I - I added a few people who didn't - who I knew had to be on it even though they didn't tell me. I see. Mm-hmm. Who specifically ask not to be. Like Jane  for example. Yeah. You are on it  aren't you? Yeah  I am. Yeah. So  I w- uh  just - just for clarification. So ""found data""  they mean like established corpora of linguistics and - and other fields  right? It sounds like such a t- What they mean is stuff they don't have to fund to collect  and especially good - Yeah  O_K. Well  I mean  ""found"" has  uh  also the meaning that's it very natural. It's things occur without any - You know  the pe- these people weren't wearing close-talking mikes  but they were recorded anyway  like the congressional hearings and  you know  for legal purposes or whatever. O_K. But it includes like standard corpora that have been used for years in linguistics and other fields. ""Hey  look what we found!"" Mark's aware of those  too. That would be found data because they found it and it exists. They didn't have to collect it. O_K. Exactly. Hmm. ""I found this great corpora."" Yeah. ""Psst. Want to buy a corpora?"" Of course it's not ""found"" in the sense that at the time it was collected for the purpose. But what he means is that - Yeah. O_K  O_K. That's interesting. You know  Mark was really a fan of getting as much data as possible from - you know  reams and reams of stuff  of broadcast stuff  web stuff  T_V stuff  radio stuff. Mm-hmm. But he well understands that that's very different than these - this type of meeting. But  It's not the same. so what? It's still - it's interesting for other reasons. O_K. Yeah. Just wanted to know. So  seems like we're winding down. Right? Mm-hmm. Many ways. So we should go - go around and s- You can tell by the prosody. Yeah. We should go around and say something interesting that happened at the meeting? Oh. Yes  we should do that. Rrrh! Now  I was already thinking about it  so - Oh! Good man. This is painful task. I - Hmm. So  um  I really liked the idea of - what I thought was interesting was the combination of the CrossPad and the speech. Especially  um  the interaction of them rather than just note-taking. So  can you determine the interesting points by who's writing? Can you do special gestures and so on that - that have  uh  special meaning to the corpora? I really liked that. Well  I - I just realized there's another category of interesting things which is that  um  I - I found this discussion very  uh  i- this - this question of how you get at queries really interesting. And - and the - and I - and the fact that it's sort of  uh  nebulous  what - what that - what kind of query it would be because it depends on what your purpose is. So I actually found that whole process of - of trying to think of what that would involve to be interesting. But that's not really a specific fact. I just sort of thought we - we went around a nice discussion of the factors involved there  which I thought was worthwhile. I had a real revelation about taking pictures. I don't know why I didn't do this before and I regret it. So that was very interesting for me. Mm-hmm. Did you take pictures of the boards? Not that I - Yeah. The boards aren't really related to this meeting. I mean  I will take pictures of them  but - That's a good point. Yeah. They're related to this morning's meeting. But - To the pre- previous meeting. That's right. O_K. Well  that's why I'll take pictures of them  then. I'm gonna pass because I can't - I mean  of the - Jane took my answer. Ah! Oh. So. Um  so I'm gonna pass for the moment but y- come - come back to me. For the moment. Pass. I think - I think ""pass"" is socially acceptable. But I will say - uh  I will actually - uh  a spin on different - slightly different spin on what you said  this issue of  uh  realizing that we could take minutes  and that actually may be a goal. So that - that may be kind of the test - in a sense  test data  uh  the - the template of what we want to test against  generating a summary. So that's an interesting new twist on what we can do with this data. I agree with Jane and Eric. I think the question of how to generate queries automatically was the most interesting question that came up  and it's something that  as you said  is a whole research topic in itself  so I don't think we'll be able to do anything on it because we don't have funding on it  uh  in this project. But  um  it's definitely something I would want to do something on. I wonder if work's already been done on it. Like e- expert systems and stuff  or - ? Hmm. Uh-huh. I especially liked the part where we were asked what questions were interesting. That was the part I was at. Well  being more management lately than - than research  I think the thing that impressed me most was the people dynamics and not any of the facts. That is  I - I really enjoyed hanging out with this group of people today. So that's what really impressed me. How are we gonna find that in the data? Well  if we had people wearing the wireless mikes all the time - Oh  yeah. Yeah  I think - Well  I mean  one thing you could search for is were people laughing a lot. Right? So. Right. How happy were they? Yeah. I'd probably search for something like that. That actually has come up a couple times in queries. I was talking to Landay and that was one of his examples. When - when did people laugh? Yeah. Yeah. That's great. Find me a funny thing that Jeff said. Yeah. So we need a laugh detector. Yeah. Mm-hmm. Yeah. Perfect. Cuz that seems to be pretty common. Not in the congressional hearings. Not in the congressional hearings. No. Quiet sobbing. So I think we're done. O_K. O_K. Great. I think we're done. O_K. Great. @@ Great. h- Do we need - do I need to turn something off here  or I do unplug this  or - ? No. Just - just leave it. Now these we turn off. Right? Now they even have working L_E_Ds. Oh! Hey  that's cool. Do they? Yeah. Absolutely. Very nice. That'd be nice. O_K. Huh. @@ knows . You don't think John - uh  Jim would make a box that didn't have a They're using my - flashing light on it  do you? ",The meeting was dominated by a discussion of the first results coming in. There have been four types of test  in which the training data varies  and a variety of input features have been tried. The process and results were explained to the group  the implications of the results discussed  and plans for moving forward were made. There was also discussion of some of the work being conducted by research partners OGI  including how the two groups should best work together. The group also briefly touched upon resource issues. Speaker mn007 would like to investigate increasing the context of the phonemes. speaker mn013 does agree with mn007s assessment of the outcome  and points out the lack of data  but acknowledges that if mn007 is interested he will go ahead with it. Must be careful in choosing which experiments to perform as an important visitor is coming soon. Also need to come up with a stronger plan for collaboration with OGI. Must decide what from both can be brought together  and how then can the work be divided. Someone (implied with gestures in the meeting) must speak to a person outside the group with regards to using a multiprocessor Linux machine that is available. Debugging the process while there are just two processors bodes well for when they have 8 to multi-thread. Speaker mn026 volunteers to get some training running under Linux. It is agreed that he should start with the neural net training  then work on HTK. Incorrect assumptions were made when considering the on-line normalization for the main task.  members used different values to a previous study  and whilst it was believed not to make a difference  it does  so networks are being retrained. Currently working with noise conditions being the same in training and test data  but there is nothing which matches the noise on the Italian test data. In fact no other language matches the noise from Aurora data. Spanish was being used to train for Italian as it was assumed they were the most similar  but that may not be as close a match as thought. OGI have an interesting approach to Voice Activation Detection for removing blocks of silence  that shows good results  but currently the word model being used is too poor to make good use of this and no one is working on improving it. Speakers mn007 and fn002 have been running experiments. Looking at different features  under different training conditions. Moving from training with task data to broad data increases error rate by 10%  and moving to multiple languages increases a further 20-30%. PLP with JRASTA better than just PLP on mismatched conditions  but slightly worse on well matched. Speaker fn002 is also looking at the HTK training  but does not yet have results. Speaker mn007 is going to start work on creating broad phonetic categories based on various features  and combine this with original features like PLP. As yet unsure how to combine the data however. 
"O_K. So uh  he's not here  so you get to - So. Yeah  I will try to explain the thing that I did this - this week - during this week. Yeah. Well eh you know that I work - I begin to work with a new feature to detect voice-unvoice. Mm-hmm. What I trying two M_L_P to - to the - with this new feature and the fifteen feature uh from the eh bus- base system The - the mel cepstrum? No  satly the mes- the Mel Cepstrum  the new base system - the new base system. Oh the - O_K  the Aurora system. Yeah  we - yeah the Aurora system with the new filter  O_K. V_A_D or something like that. And I'm trying two M_L_P  one one that only have t- three output  voice  unvoice  and silence  and Mm-hmm. other one that have fifty-six output. The probabilities of the allophone . And I tried to do some experiment of recognition with that and only have result with - with the M_L_P with the three output. And I put together the fifteen features and the three M_L_P output. And  well  the result are li- a little bit better  but more or less similar. Uh  I - I'm - I'm slightly confused. What - what feeds the uh - the three-output net? Hmm. Voice  unvoice  and si- No no  what feeds it? What features does it see? The feature - the input? The inputs are the fifteen - the fifteen uh bases feature. the - with the new code. Uh-huh. And the other three features are R_  the variance of the difference between the two spectrum  Uh-huh. the variance of the auto-correlation function  except the - the first point  because half the height value is R_zero and also R_zero  Mm-hmm. Mm-hmm. Mm-hmm. Mm-hmm. the first coefficient of the auto-correlation function. That is like the energy Right. with these three feature  also these three feature. You wouldn't do like R_one over R_zero or something like that? I mean usually for voiced-unvoiced you'd do - yeah  you'd do something - you'd do energy but then you have something like spectral slope  which is you get like R_one ov- over R_zero or something like that. Yeah. Uh yeah. What are the R_'s? I'm sorry I missed it. R_ correlations. No  R_ c- No. Oh. Auto-correlation? Yes  yes  the variance of the auto-correlation function that uses that @@ Ye- Well that's the variance  but if you just say ""what is -"" I mean  to first order  um yeah one of the differences between voiced  unvoiced and silence is energy. Another one is - but the other one is the spectral shape. Yeah  I- I'll - The spectral shape  yeah. Yeah  and so R_one over R_zero is what you typically use for that. No  I don't use that - I can't use - No  I'm saying that's what people us- typically use. Mmm. See  because it - because this is - this is just like a single number to tell you um ""does the spectrum look like that or does it look like that "". Mm-hmm. Oh. R_ - R_zero. Right? Mm-hmm. So if it's - if it's um - if it's low energy uh but the - but the spectrum looks like that or like that  Mm-hmm. it's probably silence. Uh but if it's low energy and the spectrum looks like that  it's probably unvoiced. Yeah. So if you just - if you just had to pick two features to determine voiced-unvoiced  you'd pick something about the spectrum like uh R_one over R_zero  Mm-hmm  O_K. um and R_zero or i- i- you know you'd have some other energy measure and like in the old days people did like uh zero crossing counts. Yeah  yeah. Right. S- Yeah. Um  Well  I can also th- use this. Bec- because the result are a little bit better but we have in a point that Yeah. everything is more or less the similar - more or less similar. But um It's not quite better. Right  but it seemed to me that what you were what you were getting at before was that there is something about the difference between the original signal or the original F_F_T and with the filter which is what - and the variance was one take uh on it. Yeah  I used this too. Right. But it - it could be something else. Suppose you didn't have anything like that. Then in that case  if you have two nets  Alright  and this one has three outputs  Mm-hmm. and this one has f- whatever  fifty-six  or something  if you were to sum up the probabilities for the voiced and for the unvoiced and for the silence here  we've found in the past you'll do better at voiced-unvoiced-silence than you do with this one. Mm-hmm. So just having the three output thing doesn't - doesn't really buy you anything. Yeah. The issue is what you feed it. Yeah  I have - yeah. So uh So you're saying take the features that go into the voiced-unvoiced- silence net and feed those into the other one  No - w- as additional inputs  rather than having a separate - W- well that's another way. That wasn't what I was saying but yeah that's certainly another thing to do. No I was just trying to say if you b- if you Yeah. bring this into the picture over this  what more does it buy you? Mmm. And what I was saying is that the only thing I think that it buys you is um based on whether you feed it something different. And something different in some fundamental way. And so the kind of thing that - that she was talking about before  was looking at something uh ab- um - something uh about the difference between the - the uh um log F_F_T uh log power uh and the log magnitude uh F_F_- spectrum uh and the um uh filter bank. Yeah. And so the filter bank is chosen in fact to sort of integrate out the effects of pitch and she's saying you know trying - So the particular measure that she chose was the variance of this m- of this difference  Mm-hmm. but that might not be the right number. Maybe. Right? I mean maybe there's something about the variance that's - that's not enough or maybe there's something else that - that one could use  but I think that  for me  the thing that - that struck me was that uh you wanna get something back here  so here's - here's an idea. uh What about it you skip all the - all the really clever things  and just fed the log magnitude spectrum into this? Ah - I'm sorry. This is f- You have the log magnitude spectrum  Yeah. Mm-hmm. and you were looking at that and the difference between the filter bank and - and c- c- computing the variance. Mm-hmm. That's a clever thing to do. What if you stopped being clever? Mm-hmm. And you just took this thing in here because it's a neural net and neural nets are wonderful and figure out what they can - what they most need from things  and Yeah. I mean that's what they're good at. So I mean you're - you're - you're trying to be clever and say what's the statistic that should - we should get about this difference but uh in fact  you know maybe just feeding this in or - Hmm. or feeding both of them in you know  another way  saying let it figure out what's the - what is the interaction  especially if you do this over multiple frames? Mm-hmm. Then you have this over time  and - and both kinds of measures and uh you might get uh something better. Mm-hmm. Um. So - so don't uh - don't do the division  but let the net have everything. That's another thing you could do yeah. Yeah. Yeah. Um. I mean  it seems to me  if you have exactly the right thing then it's better to do it without the net because otherwise you're asking the net to learn this - you know  say if you wanted to learn how to do multiplication. Mm-hmm. I mean you could feed it a bunch of s- you could feed two numbers that you wanted to multiply into a net and have a bunch of nonlinearities in the middle and train it to get the product of the output and it would work. But  it's kind of crazy  cuz we know how to multiply and you - you'd be you know much lower error usually if you just multiplied it out. But suppose you don't really know what the right thing is. And that's what these sort of dumb machine learning methods are good at. So. Um. Anyway. It's just a thought. How long does it take  Carmen  to train up one of these nets? Oh  not too much. Yeah. Mmm  one day or less. Hmm. Yeah  it's probably worth it. What are - what are your f- uh frame error rates for - for this? Eh fifty-f- six uh no  the frame error rate? Fifty-six I think. O- Is that - maybe that's accuracy? Percent. The accuracy. Fif- fifty-six percent accurate for v- voice-unvoice Mm-hmm. No for  yes f- I don't remember for voice-unvoice  maybe for the other one. Oh  O_K. Yeah  voiced-unvoiced hopefully would be a lot better. O_K. for voiced. I don't reme- Should be in nineties somewhere. Better. Maybe for voice-unvoice. This is for the other one. I should - Right. I can't show that. O_K. But I think that fifty-five was for the - when the output are the fifty-six phone. Mm-hmm. That I look in the - with the other - nnn the other M_L_P that we have are more or less the same number. Silence will be better but more or less the same. I think at the frame level for fifty-six that was the kind of number we were getting for - for uh um reduced band width uh I think that - I - I - I think that for the other one  for the three output  is sixty- sixty-two  sixty- stuff. Mm-hmm. three more or less. It's - That's all? Yeah. That's pretty bad. Yeah  because it's noise also. Aha! Oh yeah. And we have- Aha! Yeah. Yeah. O_K. I know. But even i- in - Oh yeah  in training. Still  Uh. Well actually  so this is a test that you should do then. Um  if you're getting fifty-six percent over here  uh that's in noise also  right? Yeah  yeah  yeah. Oh O_K. If you're getting fifty-six here  try adding together the probabilities of all of the voiced phones here and all of the unvoiced phones will be - and see what you get then. Yeah. I bet you get better than sixty- three. Well I don't know  but - I th- I - I think that we - I have the result more or less. Maybe. I don't know. I don't - I'm not sure but I remember @@ that I can't show that. O_K  but that's a - That is a - a good check point  you should do that anyway  O_K? Yeah. Given this - this uh regular old net that's just for choosing for other purposes  uh add up the probabilities of the different subclasses and see - see how well you do. Uh and that - you know anything that you do over here should be at least as good as that. Mm-hmm. O_K. I will do that. The targets for the neural net  But - uh  they come from forced alignments? Uh  no. TIMIT canonical ma- mappings. TIMIT. Ah! Oh. So  this is trained on TIMIT. O_K. Yeah. Yeah  noisy TIMIT. O_K. Yeah this for TIMIT. But noisy TIMIT? Right. Noisy TIMIT. We have noisy TIMIT with the noise of the - the T_I-digits. And now we have another noisy TIMIT also with the noise of uh Italian database. I see. Yeah. Well there's gonna be - it looks like there's gonna be a noisy uh - some large vocabulary noisy stuff too. Somebody's preparing. Really? Yeah. I forget what it'll be  resource management  Wall Street Journal  something. Some - some read task actually  that they're - Hmm! preparing. For what - For Aurora? Yeah. Oh! Yeah  so the uh - Uh  the issue is whether people make a decision now based on what they've already seen  or they make it later. And one of the arguments for making it later is let's make sure that whatever techniques that we're using work for something more than - than connected digits. Hmm. So. When are they planning - When would they do that? Mmm  I think late - uh I think in the summer sometime. Hmm. So. O_K  thanks. This is the work that I did during this date and also mmm Uh-huh. I - H- Hynek last week say that if I have time I can to begin to - to study well seriously the France Telecom proposal to look at the code and something like that Mm-hmm. to know exactly what they are doing because maybe that we can have some ideas Mm-hmm. but not only to read the proposal. Look insi- look i- carefully what they are doing with the program @@ and I begin to - to work also in that. But the first thing that I don't understand is that they are using R_- the uh log energy that this quite - I don't know why they have some constant in the expression of the lower energy. I don't know what that means. They have a constant in there  you said? Yeah. Oh  at the front it says uh ""log energy is equal to the rounded version of sixteen over the log of two"" This - Yeah. Uh. uh times the - Well  this is natural log  and maybe it has something to do with the fact that this is - Then maybe I can understand. Is that some kind of base conversion  or - ? I - I have no idea. Yeah  that's what I was thinking  but - but um  then there's the sixty-four  Uh  I don't know. Because maybe they're - the threshold that they are using on the basis of this value - I don't know exactly  because well th- I thought maybe they have a meaning. But I don't know what is the meaning of take exactly this value. Experimental results. Mc- McDonald's constant. Yeah  it's pretty funny looking. So they're taking the number inside the log and raising it to I don't know. sixteen over log base two. Yeah  I - um Right. Sixteen over Does it have to do with those sixty-fours  or - ? two. Um. If we ignore the sixteen  the natural log of t- one over the natural log of two times the natu- I don't know. Hmm. Well   maybe somebody'll think of something  but this is uh - It may just be that they - they want to have - for very small energies  they want to have some Yeah  the e- kind of a - The effect I don't - @@ I can understand the effect of this  no? because it's to - to do something like that. No? Well  it says  since you're taking a natural log  it says that when - when you get down to essentially zero energy  Mm-hmm. this is gonna be the natural log of one  which is zero. So it'll go down to uh to the natural log being - So the lowest value for this would be zero. So y- you're restricted to being positive. And this sort of smooths it for very small energies. Uh  why they chose sixty- four and something else  that was probably just experimental. Yeah. And the - the - the constant in front of it  I have no idea. um Well. I - I will look to try if I move this parameter in their code what happens  maybe everything is - uh - Maybe they tres hole are on basis of this. I mean - I don't know. it - they - they probably have some fi- particular s- fixed point arithmetic that they're using  and then it just - Yeah  I was just gonna say maybe it has something to do with hardware  something they were doing. Yeah. Yeah  I mean that - they're s- probably working with fixed point or integer or something. I think you're supposed to on this stuff anyway  and - and so maybe that puts it in the right realm somewhere. Well it just  yeah  puts it in the right range  or - Yeah. I think  given at the level you're doing things in floating point on the computer  I don't think it matters  would be my guess  but. Mm-hmm. I - this more or less anything Yeah. O_K  and wh- when did Stephane take off? He took off - I think that Stephane will arrive today or tomorrow. Oh  he was gone these first few days  and then he's here for a couple days before he goes to Salt Lake City. O_K. Mm-hmm. He's - I think that he is in Las Vegas or something like that. Yeah. Yeah. So he's - he's going to I_CASSP which is good. I - I don't know if there are many people who are going to I_CASSP Yeah. so - so I thought  make sure somebody go. Yeah. Do - have - Have people sort of stopped going to I_CASSP in recent years? Um  people are less consistent about going to I_CASSP and I think it's still - it's still a reasonable forum for students to - to present things. Uh  it's - I think for engineering students of any kind  I think it's - it's if you haven't been there much  it's good to go to  uh to get a feel for things  a range of things  not just speech. Uh. Hmm. But I think for - for sort of dyed-in-the-wool speech people  um I think that I_C_S_L_P and Eurospeech are much more targeted. Mm-hmm. Uh. And then there's these other meetings  like H_L_T and - and uh Mmm. A_S_R_U - so there's - there's actually plenty of meetings that are really relevant to - to uh computational uh speech processing of one sort or another. Mm-hmm. Um. So. I mean  I mostly just ignored it because I was too busy and didn't get to it. So uh Wanna talk a little bit about what we were talking about this morning? Just briefly  or Oh! um uh Or anything else? Yeah. So. I - I guess some of the progress  I - I've been getting a - getting my committee members for the quals. And um so far I have Morgan and Hynek  Mike Jordan  and I asked John Ohala and he agreed. Cool. Yeah. Yeah. So I'm - I - I just need to ask um Malek. One more. Um. Tsk. Then uh I talked a little bit about um continuing with these dynamic ev- um acoustic events  and um we're - we're - we're thinking about a way to test the completeness of a - a set of um dynamic uh events. Uh  completeness in the - in the sense that um if we - if we pick these X_ number of acoustic events  do they provide sufficient coverage for the phones that we're trying to recognize or - or the f- the words that we're gonna try to recognize later on. And so Morgan and I were uh discussing um s- uh s- a form of a cheating experiment where we get - um we have uh um a chosen set of features  or acoustic events  and we train up a hybrid um system to do phone recognition on TIMIT. So i- i- the idea is if we get good phone recognition results  using um these set of acoustic events  then um that - that says that these acoustic events are g- sufficient to cover a set of phones  at least found in TIMIT. Um so i- it would be a - a measure of ""are we on the right track with - with the - the choices of our acoustic events"". Um  So that's going on. And also  just uh working on my uh final project for Jordan's class  uh which is - Actually  let me - Hold that thought. Let me back up while we're still on it. The - the other thing I was suggesting  though  is that given that you're talking about binary features  Yeah. O_K  sure. uh  maybe the first thing to do is just to count and uh count co-occurrences and get probabilities for a discrete H_M_M cuz that'd be pretty simple because it's just - Say  if you had ten - ten events  uh that you were counting  uh each frame would only have a thousand possible values for these ten bits  and uh so you could make a table that would - say  if you had thirty-nine phone categories  that would be a thousand by thirty-nine  and just count the co-occurrences and divide them by the - the uh - uh uh occ- uh count the co-occurrences between the event and the phone and divide them by the number of occurrences of the phone  and that would give you the likelihood of the - of the event given the phone. And um then just use that in a very simple H_M_M and uh you could uh do phone recognition then and uh wouldn't have any of the issues of the uh training of the net or - I mean  it'd be on the simple side  but Mm-hmm. uh um you know  if - uh uh the example I was giving was that if - if you had um onset of voicing and - and end of voicing as being two kinds of events  then if you had those a- all marked correctly  and you counted co-occurrences  you should get it completely right. Mm-hmm. So. um - But you'd get all the other distinctions  you know  randomly wrong. I mean there'd be nothing to tell you that. So um uh If you just do this by counting  then you should be able to find out in a pretty straightforward way whether you have a sufficient uh set of events to - to do the kind of level of - of uh classification of phones that you'd like. So that was - that was the idea. And then the other thing that we were discussing was - was um O_K  how do you get the - your training data. Mm-hmm. Cuz uh the Switchboard transcription project uh uh you know was half a dozen people  or so working off and on over a couple years  and uh similar - similar amount of data to what you're talking about with TIMIT training. So  it seems to me that the only reasonable starting point is uh to automatically translate the uh current TIMIT markings into the markings you want. And uh it won't have the kind of characteristic that you'd like  of catching funny kind of things that maybe aren't Mm-hmm. there from these automatic markings  but - but uh it's uh - It's probably a good place to start. Yeah. Yeah. Yeah and a short - short amount of time  just to - again  just to see if that information is sufficient Mm-hmm. to uh determine the phones. Hmm. So. Yeah  you could even then - to - to get an idea about how different it is  you could maybe take some subset and you know  go through a few sentences  mark them by hand and then see how different it is from you know  the canonical ones  just to get an idea - a rough idea of Right. h- if it really even makes a difference. You can get a little feeling for it that way  yeah that is probably right. Yeah. I mean uh my - my guess would be that this is - since TIMIT's read speech that this would be less of a big deal  if you went and looked at spontaneous speech it'd be more - more of one. Mm-hmm. Right. Right. And the other thing would be  say  if you had these ten events  you'd wanna see  well what if you took two events or four events or ten events or t- and you know  and - and hopefully there should be some point at which having more information doesn't tell you really all that much more about what the phones are. Mm-hmm. You could define other events as being sequences of these events too. Uh  you could  but the thing is  what he's talking about here is a uh - a translation to a per-frame feature vector  so there's no sequence in that  I think. I think it's just a - Unless you did like a second pass over it or something after you've got your - Yeah  but we're just talking about something simple here  Yeah. Yeah  yeah. yeah  to see if - Yeah. I'm adding complexity. Yeah. Just - You know. The idea is with a - with a very simple statistical structure  could you - could you uh at least verify that you've chosen features that Yeah. are sufficient. O_K  and you were saying something - starting to say something else about your - your class project  or - ? Oh. Yeah th- Um. Yeah. So for my class project I'm um I'm tinkering with uh support vector machines? something that we learned in class  and uh um basically just another method for doing classification. And so I'm gonna apply that to um compare it with the results by um King and Taylor who did um these um using recurrent neural nets  they recognized um a set of phonological features um and made a mapping from the M_F_C_C's to these phonological features  so I'm gonna do a similar thing with - with support vector machines and see if - So what's the advantage of support vector machines? What - Um. So  support vector machines are - are good with dealing with a less amount of data Hmm. and um so if you - if you give it less data it still does a reasonable job in learning the - the patterns. Hmm. Um and um I guess it - yeah  they're sort of succinct  and - and they Yeah. uh Does there some kind of a distance metric that they use or how do they - for cla- what do they do for classification? Um. Right. So  the - the simple idea behind a support vector machine is um  you have - you have this feature space  right? and then it finds the optimal separating plane  Mm-hmm. Mm-hmm. um between these two different um classes  Mm-hmm. and um and so um  what it - i- at the end of the day  what it actually does is it picks those examples of the features that are closest to the separating boundary  and remembers those Mm-hmm. and - and uses them to recreate the boundary for the test set. So  given these um these features  or - or these - these examples  um  critical examples  which they call support f- support vectors  Oh. then um given a new example  if the new example falls um away from the boundary in one direction then it's classified as being a part of this particular class and otherwise it's the other class. So why save the examples? Why not just save what the Mm-hmm. boundary itself is? Um. Hmm. Let's see. Uh. Yeah  that's a good question. I - yeah. That's another way of doing it. Mmm. Right? So - so it - I mean I - I guess it's - Sort of an equivalent. You know  it - it goes back to nearest-neighbor sort of thing  right? Um  Mm-hmm. i- i- if - is it eh w- When is nearest-neighbor good? Well  nearest-neighbor good - is good if you have lots and lots of examples. Um but of course if you have lots and lots of examples  then it can take a while to - to use nearest-neighbor. There's lots of look ups. So a long time ago people talked about things where you would have uh a condensed nearest-neighbor  where you would - you would - you would pick out uh some representative examples which would uh be sufficient to represent - to - to correctly classify everything that came in. Oh. Mm-hmm. I - I think s- I think support vector stuff sort of goes back to - to that kind of thing. I see. Um. So rather than doing nearest neighbor where you compare to every single one  you just pick a few critical ones  and - Yeah. Hmm. And th- the You know  um neural net approach uh or Gaussian mixtures for that matter are sort of - fairly brute force kinds of things  where you sort of - you predefine that there is this big bunch of parameters and then you - you place them as you best can to define the boundaries  and in fact  as you know  these things do take a lot of parameters and - and uh if you have uh only a modest amount of data  you have trouble uh learning them. Um  Mm-hmm. so I - I guess the idea to this is that it - it is reputed to uh be somewhat better in that regard. Right. I- it can be a - a reduced um parameterization of - of the - the model by just keeping certain selected examples. Hmm. Yeah. So. But I don't know if people have done sort of careful comparisons of this on large tasks or anything. Maybe - maybe they have. I don't know. Yeah  I don't know either. Yeah. S- do you get some kind of number between zero and one at the output? Actually you don't get a - you don't get a nice number between zero and one. You get - you get either a zero or a one. Um  uh there are - there are pap- Well  basically  it's - it's um you - you get a distance measure at the end of the day  and then that distance measure is - is um - is translated to a zero or one. Um. But that's looking at it for - for classification - for binary classification  right? That's for classification  right. And you get that for each class  you get a zero or a one. Right. But you have the distances to work with. You have the distances to work with  yeah. Cuz actually Mississippi State people did use support vector machines for uh uh speech recognition and they were using it to estimate probabilities. Yeah. Yeah  they - they had a - had a way to translate the distances into - into probabilities with the - with the simple um uh sigmoidal function. Yeah  and d- did they use sigmoid or a softmax type thing? And didn't they like exponentiate or something and then divide by the sum of them  or - ? Um Yeah  there's some - there's like one over one plus the exponential or something like that. Oh it - i- Oh  so it is a sigmoidal. Yeah. O_K. Alright. Did the - did they get good results with that? I mean   they're O_K  I - I don't - I don't think they were earth - earth shattering  but I think that Hmm. uh this was a couple years ago  I remember them doing it at some meeting  and - and um I don't think people were very critical because it was interesting just to - to try this and you know  it was the first time they tried it  so - Hmm. so the - you know  the numbers were not incredibly good but there's you know  it was th- reasonable. Mm-hmm. I - I don't remember anymore. I don't even remember what the task was  it was Broadcast News  or something. I don't know. Hmm. Uh s- So Barry  if you just have zero and ones  how are you doing the speech recognition? Right. Oh I'm not do- I'm not planning on doing speech recognition with it. I'm just doing Oh. O_K . detection of phonological features. So uh for example  this - this uh feature set called the uh sound patterns of English um is just a bunch of um binary valued features. Let's say  is this voicing  or is this not voicing  is this sonorants  not sonorants  and O_K. stuff like that. So. Did you find any more mistakes in their tables? Oh! Uh I haven't gone through the entire table  yet. Yeah  yesterday I brought Chuck the table and I was like  ""wait  this - is - Is the mapping from N_ to - to this phonological feature called um ""coronal""   is - is - should it be - shouldn't it be a one? or should it - should it be you know coronal instead of not coronal as it was labeled in the paper?"" So I ha- haven't hunted down all the - all the mistakes yet  but - Uh-huh. But a- as I was saying  people do get probabilities from these things  and - and uh O_K. we were just trying to remember how they do  but people have used it for speech recognition  and they have gotten probabilities. So they have some conversion from these distances to O_K. O_K. O_K. probabilities. Right  yeah. There's - you have - you have the paper  right? The Mississippi State paper? Mm-hmm. Mm-hmm. Yeah  if you're interested y- you could look  yeah. And - O_K. O_K. Yeah  I can - I can show you - I - yeah  our - So in your - in - in the thing that you're doing  uh Mm-hmm. you have a vector of ones and zeros for each phone? Uh  is this the class project  or - ? Yeah. O_K. um Is that what you're - Right  Right  right f- so for every phone there is - there is a um - a vector of ones and zeros f- uh corresponding to whether it exhibits a particular phonological feature or not. Mm-hmm. Mm-hmm. And so when you do your wh- I'm - what is the task for the class project? To Um come up with the phones? or to come up with these vectors to see how closely they match the phones  or - ? Oh. Right  um to come up with a mapping from um M_F_C_C's or s- some feature set  Mm-hmm. um to uh w- to whether there's existence of a particular phonological feature. And um yeah  basically it's to learn a mapping from - from the M_F_C_C's to uh phonological features. Is it - did that answer your question? I think so. O_K. C- I guess - I mean  uh - I'm not sure what you - what you're - what you get out of your system. Do you get out a Mm-hmm. uh - a vector of these ones and zeros and then try to find the closest matching phoneme to that vector  or - ? Oh. No  no. I'm not - I'm not planning to do any - any phoneme mapping yet. Just - Uh-huh. it's - it's basically - it's - it's really simple  basically a detection of phonological features. I see. Yeah  and um cuz the uh - So King and - and Taylor Yeah. um did this with uh recurrent neural nets  and this i- their - their idea was to first find Mm-hmm. a mapping from M_F_C_C's to uh phonological features and then later on  once you have these phonological features  Mm-hmm. then uh map that to phones. So I'm - I'm sort of reproducing phase one of their stuff. Mmm. So they had one recurrent net for each particular feature? Right. Right. I see. Right. Right. I wo- did they compare that - I mean  what if you just did phone recognition and did the reverse lookup. Uh. So you recognize a phone and which ever phone was recognized  you spit out it's vector of ones and zeros. Mm-hmm. I mean uh - I expect you could do that. That's probably not what he's going to do on his class project. Uh. Yeah. No. Yeah. So um have you had a chance to do Yeah. this um thing we talked about yet with the uh - um Insertion penalty? Uh. No actually I was going a different - That's a good question  too  but I was gonna ask about the - the um changes to the data in comparing P_L_P and mel cepstrum for the S_R_I system. Uh. Well what I've been - ""Changes to the data""  I'm not sure I - Right. So we talked on the phone about this  that - Yeah. that there was still a difference of a - of a few percent Right. and you told me that there was a difference in how the normalization was done. And I was asking if you were going to do - redo it Mm-hmm. uh for P_L_P with the normalization done as it had been done for the mel cepstrum. Uh right  no I haven't had a chance to do that. What I've been doing is O_K. uh trying to figure out - it just seems to me like there's a um - well it seems like there's a bug  because the difference in performance is - it's not gigantic but it's big enough that it - it seems wrong. Yeah  I agree  but I thought that the normalization difference was one of the and - Yeah  but I don't - I'm not - possibilities  right? Yeah  I guess I don't think that the normalization difference is gonna account for everything. So what I was working on is um O_K. just going through and checking the headers of the wavefiles  to see if maybe there was a um - a certain type of compression or something that was done that my script wasn't catching. So that for some subset of the training data  uh the - the - the features I was computing were junk. O_K. Which would you know cause it to perform O_K  but uh  you know  the - the models would be all messed up. So I was going through and just double-checking that kind of think first  Mm-hmm. to see if I see. there was just some kind of obvious bug in the way that I was computing the features. O_K. Looking at all the sampling rates to make sure all the sampling rates were what - Yeah. eight K_  what I was assuming they were  um - Yeah  that makes sense  to check all that. Yeah. So I was doing that first  before I did these other things  just to make sure there wasn't something - Although really  uh uh  a couple three percent uh difference in word error rate uh could easily come from some difference in normalization  I would think. But Yeah  and I think  hhh - I'm trying to remember but I think I recall that Andreas was saying that he was gonna run sort of the reverse experiment. Uh which is to try to emulate the normalization that we did but with the mel cepstral features. Sort of  you know  back up from the system that he had. I thought he said he was gonna - I have to look back through my - my email from him. Yeah  he's probably off at - at uh his Yeah  he's gone meeting now  now. Um. yeah. Yeah. But yeah the - I sh- think they should be But - roughly equivalent  um I mean again the Cambridge folk found the P_L_P actually to be a little better. Right. Uh So it's - um I mean the other thing I wonder about was whether there was something just in the - the bootstrapping of their system which was based on - but maybe not  since they - Yeah see one thing that's a little bit um - I was looking - I've been studying and going through the logs for the system that um Andreas created. And um his uh - the way that the - S_R_ I system looks like it works is that it reads the wavefiles directly  uh Right. and does all of the cepstral computation stuff on the fly. Right. And  so there's no place where these - where the cepstral files are stored  anywhere that I can go look at and compare to the P_L_P ones  so whereas with our features  he's actually storing the cepstrum on disk  and he reads those in. But it looked like he had to give it - Right. uh even though the cepstrum is already computed  he has to give it uh a front-end parameter file. Which talks about the kind of uh com- computation that his mel cepstrum thing does  so Uh-huh. i- I - I don't know if that - it probably doesn't mess it up  it probably just ignores it if it determines that it's already in the right format or something but - the - the - the two processes that happen are a little different. So. Yeah. So anyway  there's stuff there to sort out. Yeah. Yeah. So  O_K. Let's go back to what you thought I was asking you. Yeah no and I didn't have a chance to do that. Ha! Oh! You had the sa- same answer anyway. Yeah. Yeah. I've been um  - I've been working with um Jeremy on his project and then I've been trying to track down this bug in uh the ICSI front-end features. Uh-huh. So one thing that I did notice  yesterday I was studying the um - the uh RASTA code Uh-huh. and it looks like we don't have any way to um control the frequency range that we use in our analysis. We basically - it looks to me like we do the F_F_T  um and then we just take all the bins and we use everything. We don't have any set of parameters where we can say you know  ""only process from you know a hundred and ten hertz to thirty-seven-fifty"". Um - At least I couldn't see any kind of control for that. Yeah  I don't think it's in there  I think it's in the uh uh uh the filters. So  the F_F_ T is on everything  but the filters um  for instance  ignore the - the lowest bins and the highest bins. And what it does is it - it copies The - the filters? Which filters? um The filter bank which is created by integrating over F_F_ T bins. Mm-hmm. um When you get the mel - When you go to the mel scale. Right. Yeah  it's bark scale  and it's - it - it um - it actually copies the uh um - the second filters over to the first. So the first filters are always - and you can s- you can specify a different number of uh features - different number of filters  I think  as I recall. So you can specify a different number of filters  and whatever um uh you specify  the last ones are gonna be ignored. So that - that's a way that you sort of change what the - what the bandwidth is. Y- you can't do it without I think changing the number of filters  but - I saw something about uh - that looked like it was doing something like that  but I didn't quite understand it. So maybe - Yeah  so the idea is that the very lowest frequencies and - and typically the veriest highest frequencies are kind of junk. Uh-huh. And so um you just - for continuity you just approximate them by - by the second to highest and second to lowest. Mm-hmm. It's just a simple thing we put in. And - and so if you h- But - so the - but that's a fixed uh thing? There's nothing that lets you - Yeah  I think that's a fixed thing. But see - see my point? If you had - If you had ten filters  then you would be throwing away a lot at the two ends. Mm-hmm. And if you had - if you had fifty filters  you'd be throwing away hardly anything. Mm-hmm. Um  I don't remember there being an independent way of saying ""we're just gonna make them from here to here"". Use this analysis bandwidth or something. But I - I - I don't know  it's actually been awhile since I've looked at it. Yeah  I went through the Feacalc code and then looked at you know just calling the RASTA libs and thing like that. And I didn't - I couldn't see any wh- place where that kind of thing was done. But um I didn't quite understand everything that I saw  so - Yeah  see I don't know Feacalc at all. Mm-hmm. But it calls RASTA with some options  and Right. um But I - I think in - I don't know. I guess for some particular database you might find that you could tune that and tweak that to get that a little better  but I think that in general it's not that critical. I mean there's - Yeah. You can - You can throw away stuff below a hundred hertz or so and it's just not going to affect phonetic classification at all. Another thing I was thinking about was um is there a - I was wondering if there's maybe um certain settings of the parameters when you compute P_L_P which would basically cause it to output mel cepstrum. So that  in effect  what I could do is use our code but produce mel cepstrum and compare that directly to - Well  it's not precisely. Yeah. I mean  um  Hmm. um what you can do is um you can definitely change the - the filter bank from being uh a uh trapezoidal integration to a - a - a triangular one  Mm-hmm. which is what the typical mel - mel cepstral uh filter bank does. Mm-hmm. And some people have claimed that they got some better performance doing that  so you certainly could do that easily. But the fundamental difference  I mean  there's other small differences - There's a cubic root that happens  right? Yeah  but  you know  as opposed to the log in the other case. I mean the fundamental d- d- difference that we've seen any kind of difference from before  which is actually an advantage for the P_L_ P i- uh  I think  is that the - the smoothing at the end is auto-regressive instead of being cepstral - uh  from cepstral truncation. So um it's a little more noise robust. Hmm. Um  and that's - that's why when people started getting databases that had a little more noise in it  like - like uh um Broadcast News and so on  that's why c- Cambridge switched to P_L_P I think. Mm-hmm. So um That's a difference that I don't think we put any way to get around  since it was an advantage. Mm-hmm. um uh but we did - eh we did hear this comment from people at some point  that um it uh they got some better results with the triangular filters rather than the trapezoidal. So that is an option in RASTA. Hmm. Uh and you can certainly play with that. But I think you're probably doing the right thing to look for bugs first. Yeah just - it just seems like this kind of behavior could be caused by I don't know. you know s- some of the training data being messed up. Could be. You know  you're sort of getting most of the way there  but there's a - So I started going through and looking - One of the things that I did notice was that the um log likelihoods coming out of the log recognizer from the P_L_P data were much lower  much smaller  than for the mel cepstral stuff  and that the average amount of pruning that was happening was therefore a little bit higher for the P_L_P features. Oh-huh! So  since he used the same exact pruning thresholds for both  I was wondering if it could be that we're getting more pruning. Oh! He - he - He used the identical pruning thresholds even though the s- the range of p- of the likeli- Yeah. Oh well that's - Right. Right. That's a pretty good point right there. Yeah. Yeah  so - I would think that you might wanna do something like uh you know  look at a few points to see where you are starting to get significant search errors. That's - Right. Well  what I was gonna do is I was gonna take um a couple of the utterances that he had run through  then run them through again but modify the pruning threshold and see if it you know  affects the score. Yeah. Yeah. So. But I mean you could - uh if - if - if that looks promising you could  you know  r- uh run the overall test set with a - with a few different uh pruning thresholds for both  Mm-hmm. Right. and presumably he's running at some pruning threshold that's - that's uh  you know - gets very few search errors but is - is relatively fast and - Mm-hmm. Right. I mean  yeah  generally in these things you - you turn back pruning really far  so I - I didn't think it would be that big a deal because I was figuring well you have it turned back so far that you know it - But you may be in the wrong range for the P_L_ P features for some reason. Yeah. Yeah. Yeah. And the uh the - the run time of the recognizer on the P_L_P features is longer which sort of implies that the networks are bushier  you know  there's more things it's considering which goes along with the fact that the matches aren't as good. So uh  you know  it could be that we're just pruning too much. So. Yeah. Yeah  maybe just be different kind of distributions and - and yeah so that's another Mm-hmm. possible thing. Mm-hmm. They - they should - really shouldn't - There's no particular reason why they would be exactly - behave exactly the same. Mm-hmm. Right. Right. So. So. There's lots of little differences. So. Yeah. Uh. Yeah. Trying to track it down. Yeah. I guess this was a little bit off topic  I guess  because I was - I was thinking in terms of th- this as being a - a - a - a core Yeah item that once we - once we had it going we would use for a number of the front-end things also. Mm-hmm. So. um Wanna - That's - as far as my stuff goes  What's - what's on - yeah  well I Yeah. tried this mean subtraction method. Um. Due to Avendano  I'm taking s- um six seconds of speech  um I'm using two second F_F_T analysis frames  stepped by a half second so it's a quarter length step and I - I take that frame and four f- the four - I take - Sorry  I take the current frame and the four past frames and the four future frames and that adds up to six seconds of speech. And I calculate um the spectral mean  of the log magnitude spectrum over that N_. I use that to normalize the s- the current center frame by mean subtraction. And I then - then I move to the next frame and I - I do it again. Well  actually I calculate all the means first and then I do the subtraction. And um the - I tried that with H_D_K  the Aurora setup of H_D_K training on clean T_I-digits  and um it - it helped um in a phony reverberation case um where I just used the simulated impulse response um the error rate went from something like eighty it was from something like eighteen percent to um four percent. And on meeting rec- recorder far mike digits  mike - on channel F_  it went from um forty-one percent error to eight percent error. On - on the real data  not with artificial reverb? Right. Uh-huh. And that - that was um trained on clean speech only  which I'm guessing is the reason why the baseline was so bad. Uh-huh . And - That's ac- actually a little side point is I think that's the first results that we have uh uh uh of any sort on the far field uh - on - on the far field data uh for - recorded in - in meetings. Oh um actually um Adam ran the S_R_I recognizer. Did he ? On the near field  on the ne- On the far field also. He did one P_Z_M channel and one P_D_A channel. Oh did he? Oh! I didn't recall that. What kind of numbers was he getting with that? I - I'm not sure  I think it was about five percent error for the P_Z_M channel. Five. f- I think. Yeah. So why were you getting forty- one here? Is this - Um. I - I'm g- I'm guessing it was the - the training data. Uh  clean T_I-digits is  like  pretty pristine training data  and if they trained the S_R_I system on this T_V broadcast type stuff  I think it's a much wider range of channels and it - No  but wait a minute. I - I - I th - I think he - What am I saying here? Yeah  so that was the S_R_I system. Maybe you're right. Yeah. Cuz it was getting like one percent - So it's still this kind of ratio. It was - it was getting one percent or something on the near field. Wasn't it? Mm-hmm  or Yeah. Yeah. I think it was getting around one percent for the near - for the n- it wa a- it was around one. Yeah. for the close mike. Huh? O_K. So it was like one to five - So it's still this kind of ratio. It's just - yeah  it's a lot more training data. So So probably it should be something we should try then is to - is to see if - is at some point just to take - i- to transform the data and then - and then uh use th- use it for the S_R_I system. b- You me- you mean um ta- So you're - so you have a system which for one reason or another is relatively poor  Yeah. and - and uh you have something like forty-one percent error uh and then you transform it to eight by doing - doing this - this work. Um. So here's this other system  which is a lot better  but there's still this kind of ratio. It's something like five percent error with the - the distant mike  and one percent with the close mike. O_K. So the question is how close to that one can you get if you transform the data using that system. r- Right  so - so I guess this S_R_I system is trained on a lot of s- Broadcast News or Switchboard data. Yeah. Is that right? Do you know which one it is? It's trained on a lot of different things. Um. It's trained on uh a lot of Switchboard  Call Home  Uh-huh. um a bunch of different sources  some digits  there's some digits training in there. O_K . O- one thing I'm wondering about is what this mean subtraction method Hmm. um will do if it's faced with additive noise. Cuz I - I - it's cuz I don't know what log magnitude spectral subtraction is gonna do to additive noise. Yeah  well  it's - it's not exactly the right thing but That's - that's the - Uh-huh. uh but you've already seen that cuz there is added noise here. That's - that's - Yeah  that's true. Yeah. That's a good point. So um - O_K  so it's then - then it's - it's - it's reasonable to expect it would be helpful if we used it with the S_R_I system and Yeah  I mean  as helpful - I mean  so that's the question. Yeah  w- we're often asked this when we work with a system that - that isn't - isn't sort of industry - industry standard great  Uh-huh. uh and we see some reduction in error using some clever method  then  you know  will it work on a - on a - on a good system. So uh you know  this other one's - it was a pretty good system. I think  you know  one - one percent word error rate on digits is - uh digit strings is not uh you know stellar  but - but given that this is real Mm-hmm. digits  as opposed to uh sort of laboratory - Well. And it wasn't trained on this task either. And it wasn't trained on this task. Actually one percent is sort of - you know  sort of in a reasonable range. People would say ""yeah  I could - I can imagine getting that"". Mm-hmm. And uh so the - the four or five percent or something is - is - is quite poor. Mm-hmm. Uh  you know  if you're doing a uh - a sixteen digit uh credit card number you'll basically get it wrong almost all the time. Hmm. So. So. Uh  um a significant reduction in the error for that would be great. Huh  O_K. And - and then  uh Yeah. So. Yeah. Cool. Sounds good. Yeah. Alright  um  I actually have to run. So I don't think I can do the digits  but um  I guess I'll leave my microphone on? Uh  yeah. Yeah. Thank you . Yep. Actually  I could just go first  come to think of it. Then I can be out of here quickly. Yeah. That'll work. That's alright. I just have to run for another appointment. O_K  did I t- Yeah. I left it on. O_K. O_K  this is transcript L_ dash one one zero. nine zero two five seven three two six six one six six six seven four two zero eight five one six nine four seven seven nine five zero five six zero two two five seven seven six zero eight nine zero seven two six four five two one nine two eight nine five five eight eight three two five zero two nine three four four six one two zero eight eight five Um transcript L_ one one one. three six two eight three three two two O_ seven one six six five four two six eight nine zero zero six three nine three three four seven five four seven nine six zero nine nine zero three seven four three two seven five four two four nine three five one nine seven eight one eight five O_ four nine four zero seven five three seven five eight four four one seven zero one eight five five three two five O_ three eight Transcript L_ dash one one two. zero seven eight seven six two one six four six five three one two five eight four seven seven two seven zero six three six three four seven nine seven six five two nine nine three two seven three two nine nine nine three eight nine one four one seven six O_ four four zero zero eight six seven four five eight zero six six three five nine seven seven seven seven three five five three two four five zero eight R- I'm reading transcript L_ dash one one three. six five six nine seven two eight four four O_ eight seven five one eight seven four zero five seven zero two four two five O_ three seven six eight one seven seven nine two one seven nine zero nine zero six zero five six eight eight five nine five two two two five five six six two nine one one seven two zero six seven one six two zero two three zero one nine zero two three Transcript L_ dash w- one one five. zero six four seven three two two seven one zero eight seven five eight six seven three seven five seven five five one one nine one four nine eight four one nine three one zero zero two four one zero three six eight two one nine six two eight nine six nine one eight six five six five zero three one one six six three nine eight seven eight three nine eight nine ",The ICSI Meeting Recorder Group at Berkeley are approaching an important milestone on their project. They discussed most recent results  finalized plans to continue and discussed the work required and timing needed for completion of this stage of the project. Mn007 and fn002 Need to find a way of combining result figures into one easily comparable way of judging performance. They were also asked to mail round numbers detailing the size of their test-sets  so that group members can assess the seriousness of figures such as word error rates. There are now just 4 architectures that will be carried forward for testing. Though the final system is not due to be fixed  until Tuesday  writing it up must begin sooner. Anything written must go through me013 for editing. Discussion on future work once this stage is out of the way will be held at the next meeting. While using a second silence detection system it was found that while providing some improvement  it added too great a delay on the server side of the system. The experiment team have been narrowing down their experiments  coming close to fixing their system. Partner OGI have been using a weighting scheme  but the ICSI group do not yet have all the parts to run a similar system. They have been able to come close to the OGI results  but since OGI have changed something they do differ  though for the better. So far  testing on Italian is the worst performing condition. With regards to the digits the group have been recording during meetings  the collection of data is growing  and will soon be a workable size. 
"@@ O_K  so we should be on. So And I'm gonna go ahead and start by reading the digit strings  just to give you an example of how to do it. And so the first thing to do is read the transcript number which is on the right hand side there. So this is transcript one five five one dash one five seven zero two seven three four five zero zero seven one six eight four four nine five two four seven O_ six O_ zero one two O_ O_ two two O_ four four one seven five five three six six eight seven eight nine O_ O_ nine O_ one one five nine two four six six two O_ five And so if you could just go around the table and read them out. O_K  should I next? Yep O_K  transcript is one seven three one dash one seven five O_ O_ one nine nine O_ one three zero seven six seven five one seven four two O_ two three four six one eight seven seven four zero seven eight six four eight nine five O_ zero one zero three two three four three seven O_ five three zero two six nine seven nine seven eight nine O_ nine O_K  this is transcript one five seven one  one five nine zero. three nine five four seven seven O_ five nine six seven O_ O_ six one nine one six four two O_ seven one zero three one eight zero zero nine eight five two three four zero zero one seven six three five seven seven eight nine nine three nine six O_ zero one three one eight five seven four three If you could sign that? O_K  Transcript script one four one one dash one four thre- uh three zero. seven six four six three seven four eight six six four seven nine nine O_ O_ zero zero six two two eight zero one two five three four four six O_ two five six four four six six eight one five seven O_ eight O_ three zer- zero three five three two one five three two six two two three eight four O_ one six one five seven two three five one two eight Um  transcript number one six five one  one six seven O_- uh O_. six O_ O_ seven zero nine one O_ three w- O_ O_ nine zero six two zero two eight five one eight two three four O_ eight five seven six two four seven four three nine five five zero eight seven two seven nine eight seven one one O_ zero one three two O_ seven three four two five five six six five Thanks. Transcript is one four nine one dash one five one O_ O_ O_ five six zero zero six four two one one seven O_ three three three eight four two five five three five six seven eight O_ two five three seven zero four seven four five six three one six eight eight two three O_ three three nine five one four six three seven four six six two eight five three nine O_ Transcript one seven one one dash one seven three O_ nine four O_ zero one two zero zero three one nine nine four one five two one six nine eight eight six eight five seven nine zero eight eight O_ six nine nine O_ three nine O_ O_ seven one one six eight five two two two six zero eight three six five nine four seven four five eight three nine six seven nine four two O_ nine Thanks So  I wanted to just era- it- reiterate cuz we did have have one person come in a little later that  um there will be an opportunity to remove anything from the transcript that you don't want made public after it's transcribed  so that'll be some months off  but you will have that opportunity O_K? Thanks. O_K. O_K. Thanks Adam. We could start with our meeting? Yep! Yep! Thanks very much. O_K. You have to stay all the time here? I don't have to Um. But ch- you wanna do? Well - It's just - At the end of the meeting  don't turn off the mikes just leave them on. O_K. And uh Uh. Actually could y- actually  could you call me  At the end of the meeting? I'll leave my number on the board there Yeah. O_K. We can do that. Thanks. Hey  Adam  you know what? It must be funny to do an a- research on how people behave while reading these numbers. Numbers are strange! That must be - It is! Yeah! It must be much more interesting to - to see their behavior. the - the meetings are strange. Oh y- yeah! People interrupt each other much more than I thought they did. Two nine seven seven? Right . two nine seven seven. O_K. Yeah. I will call you then. O_K. Thanks. Uh  O_K. So  then let's start with our weekly meeting. The last time's uh we learned too much  I believe  so uh  today  I have two topics. And the first topic  um  we have a new member in our group  Miguel Sanchez. Hi. I believe everybody knows him very well in the meantime because he stayed here quite a little time and I think he is talking to everybody in the meantime. So  but  anyway  I would like that Miguel Sanchez will introduce himself a little bit  concerning his background  what he intend to do. And - Because I discussing with him uh  uh many things concerning  uh  his skills and Mm-hmm. what the N_S_A group intends to do. I hope there will be a match  uh  then  for his future work. And the second topic is then to discuss a proposal in much more detail. I believe we still haven't left our starting position. And  um  yesterday I also discussed with Wilbert some things and - I would like to focus on the question ""What problem we are going to solve with such a proposal."" Not so will they have a common activity within the group itself and maybe with other partners outside  but  the technical problem. But that's under the s- second topic  so first I would like that Miguel will told us a little bit about his skills and - and uh O_K. his background. O_K  thank you Joe. So  please. Well  I  uh  as all of you know I came from - from Spain  from the uh Polytechnic University of Valencia and i- uh I um  just finished my P_H_D thesis about uh um- power-saving techniques for wireless networking. uh  In fact  I have developed an algorithm to control the ra- radio frequency power uh of a wireless transceiver. And  uh I've been working - also in routing issues  especially in the so-called ad-hoc netw- wireless networks. Where uh  you have a set of  uh  mobile nodes and  no other infrastructure  no base stations. So all the routing functions are done by the node - by the same nodes. O_K? So  nodes act as both end points and routers. eh I've in the study for some time these kind of networks. And  uh  in fact  there are a lot of - of - research uh articles about this - this kind of networks because You know  uh  mmm infrastructure-based network - network has been a long topic of research. So  for example  routing on - on wired networks is a - a topic still to be researched but it has an important background. But mobile network- this kind of mobile networks is a little bit - uh newer  especially because uh  until not so long uh  the - the things were too heavy to have a computer  a wireless transceiver  on a more or less portable thing. So  with the advent of this new technology  low power uh  high  uh  processing capability and wireless - eh - networking capability  in a really small package  new kind of devices are - are being built and are appearing  in the market. And  eh  well this is more or less what I have done. And the kind of  uh  things  I'm interested is more or less all  around this - these kind of networks. But I could say  uh in general  mobile service is not only about  uh these specific kind of networks. But  in general  about  uh services uh over mobile or services that can benefit from the - uh  capability of nodes to move around. And  uh  well  I - I've been teaching now for twelve years  comp- eh At a computer sciences school in my university  I've been teaching computer networks. And  well  I am - more or less uh  knowledgeable about  well  T_C_P I_P networking  I_S_O uh  networking  and  uh well I'm - I'm a computer guy  so I'm really - um Well  I can say uh  proudly uh  more or less  computer skilled. I mean  uh  operating systems uh  programming languages uh  networking @@ . So this kind of things I think I'm more or less well-trained. So this is  I think  um  my first presentation. Next - maybe next meeting I will uh  do a small sketch about uh  my past work  and - well  presenting some det- a little bit more detailed approach with some schemes and - and some things from previous talks  so you can have a better idea of w- the kind of work I've been doing. O_K? Mm-hmm. So  thanks  Miguel. Thank you. Yeah  we m- discussed it yesterday that you'll maybe - Y- will have a talk next Tuesday Yeah. But  Give me a sign and send me an abstract Yeah. so that I can announce it  then  for the next meeting. That's O_K. Um. One short other topic is Uh. Claudia did a lot of work concerning the web pages in the meantime. Oh  O_K. And  despite busy stuff with waivers and uh and visas and whatever  uh  she found the time to um write  with what we discussed before and in the form of H_T_M_L stuff. And we wanna put it on the web pages in the next days. So  I will send an email  then  to everybody that they should check their web pages whether they're aligned with  um  their own views and opinions of what we should - what should be presented on the web pages. And any feedback is then  well appreciated. No? O_K. So. Let's switch to the project proposal. Um  I mentioned that it is still f- Ah. Sorry. Can I? Yeah? Maybe I can add something. Claudia - Yeah  if there's anything else which we - what we could add on the web site. So  p- For example  if you have a small abstract or some pictures or whatever  about the work you did before  what you are planning to do here  O_K. that would be fine  cuz now we can add some more stuff there. Or if you have something - or if somebody has some slides  o- or articles  whatever he wants to be published there. Yeah. Then everybody sees the structure  - how it is uh  structured - Yeah and really structured. Then maybe you wi- can easy offer some additional input which should be presented  then  on the web pages. Yeah  it depends what - what everybody wants to do. Yeah. Yeah. So about the projects will be something  then  about publications will be another point  and - Yeah  additional information about how is life in Berkeley and i- Yep. I don't know  some - some hints  some web sites or links which are useful Yeah  it is. when somebody arrives or Yeah. Half collection of necessary information. Hmm-hmm-hmm. stuff like that. But there's no personal homepage for everybody? Or  is it planned or not  or - ? Yeah su- You can have - Yeah  you can have it. Because  as uh - Uh  uh  le- I'm not interested in this. That makes work. No  but this is - oh  uh - Everybody's obliged to do it for his own  you know. Yeah. O_K. So there is not uh - Claudia is not the webmaster here to  uh  get the collection for - Oh  oh  no  I'm not doing it for everybody but what I can - for - for everything. So. This is responsible for - uh  everybody's responsible for his own  yeah. Yeah  but - but what is planned is that I do something like a- Sure. previous  you know  layout  and give everybody the  uh  opin- uh  the possibility to - the chance to put his content inside so O_K  his name and so on. Well  like have a blueprint. that the layout is for everybody the same  so about the projects and stuff. Yeah. O_K Mm-hmm. So  there will be something. Yeah. Yeah. O_K  thanks. So  let's switch to the proposal. I ask everybody whether read it in the meantime  and understand everything. Yeah. Everybody say yes? Yeah? Yes. Yes  O_K. Um. From my point of view  As I mentioned before  we still in the starting position. I think the four building blocks on the network level will have really a major impact on - on f- current um  available wireless networks and  uh  also for future generations. And  what the missing thing is really what kind of problem we are - wan- wanna s- solve with such a project proposal. And Wilbert yesterday mentioned  and he is right  that  what kind of  maybe  service we are really going to offer. The linkage between such kind of application  also interactive multimedia application and  uh  this kind of networking stuff is  um  maybe not very well aligned  because there will be of course  a portion of mobile access to such kind of application. But  I think that's not the majority and that is not the major focus of um this kind of - um this kind of application anyway. So I would like to - to - uh split  in principle  the discussions of what kind of application tsk um f- far away and I would like to - more to focus if we have certain kind of vision t- concerning these building blocks and the network  Um  What kind of problem we are going to solve. What kind of  uh  um  um  glue between these four building blocks exists and what kind of synergy effects  in principle  as if they work and fit very well together exists for networking stuff. And what kind of service could we provide in that area. We will have these kind of  uh  really closely working together of these four building blocks. So maybe uh  Wilbert  you can start to discu- oder to - to tell the group about what you mentioned yesterday concerning certain kind of ideas. Maybe it's a brainstorming um  of - of these things and maybe we can comment later on th- these - uh these ideas. And I saw you sk- still discuss with - with - uh - Mark  right? some stuff. Was it also related with - with these things. No? O_K  It was @@ . No. This afternoon you mean? No  this was unrelated to - to the things of yesterday and last weeks Yeah. O_K Yeah  yeah. Then  maybe then  one - one w- uh sentence before. wha- I believe everybody fits very well in these kind of things. Because from the mathematical description with Q_S-in-Advance  me  as an expert for quality of service and  uh  Michael as an expert for uh  M_P_L_S and Wilbert an expert for - for Multicast  and  uh  I discussed with Miguel. It's uh maybe a little bit the shift to active routing  but it is still in the area of routing. So. My idea is  in principle  to have  independently of any funding  independently of any outer contact  such - such a - core activity  in principle. And we can see how much from the outer world can fit in. First of all  the first stage is the institution which is behind everybody. That means uh K_P_N in your case and Siemens in our case. And maybe  then  in the broader world here other partners registered in the proposal migh- Tomorrow  I will go to Cisco  for instance  and I will discuss uh  all this kind of things. Maybe they have certain additional activities and they're interested in the results before - The benefit here is  in principle  that the results are available. They are not restricted any way. If you have your own common activity. That makes a little bit more easier. And  uh  if there is a technical linkage between such  uh um  between the outer world and us O_K  the better it is. But  anyway  if we come to a certain kind of uh  common work within the core of the N_S_A group  I think that's really beneficial to everybody. And uh  especially  Wilbert  if you have in mind to go in the mmm  uh  really from the project proposal view  I think we can build it on this - this core. You know  and go to the outer world with  and - and - and align it in a certain way. For instance  that it also fit  maybe  to K_P_N  and Siemens  and in other companies. Yeah. Yeah  and - This probably has to be a brainstorm right now. Yep. Um What I told Joachim yesterday was that  uh uh for my company - the company I work for  they are more interested in - in an explicit service  than in core technology. u- Me  on the other hand  I'm more interested in uh  core service  or c- core technology  myself. So I'm u- I was thinking about uh  how I could combine the both of them. K_P_N was  um - they're interested in having  uh like  six people or something  working on the project for one year or maybe  uh  three people fo- working on the project for two years  and have a kind of a demonstration uh  for a specifically in- U_M_T_S service. Mm-hmm. I was thinking that um as a - well  not really a U_M_T_S service  but - yesterday  what I discussed with Joachim was uh  a w- little bit based on your ad-hoc networking also. That it would be interesting to have a kind of a device where you can switch from w- a WaveLAN Technology to  um  U_M_T_S. I think that's  uh  it's in - in the future that will be a main competitor of  uh  of U_M_T_S  Hmm-hmm-hmm. too  like the - the - the Lucent  uh  equipment has a l- huge scope already  like five hundred feet  or - or even more sometimes. so that's - it's might be interesting for  a service to walk into a building or whatever is available on that network. You can switch from U_M_T_S or uh uh  to WaveLAN Technologies. Hmm. Uh  maybe I will write a few keywords on the - on the whiteboard O_K. Hmm-hmm-hmm. Sure. Of course  it sounds strange for a company that does U_M_T_S to switch - to - to be able to switch or provide the service to switch to WaveLAN but at the end it makes it easier for your - and cheaper for your customers. So  if they don't do it  somebody else will probably do it. And you can build a - a - an - a - a double technology adapter  too. So you can market these double-technology adapters. And  this can be a - a potential Yeah. uh um you know  uh  income  too. Because you just have to license  uh uh WaveLAN technology. You can build  uh  the whole thing. This U_- U_M_T_S WaveLAN  uh- Yeah  and - mixed together. Yeah  something like that could be possible  maybe even a U_M_T_S-to-WaveLAN-gateway  or - I'm not sure if that's possible like we- U_- if U_M_T_S re- contains redirects or whatever  like switches. I'm not sure about it. It could be. Mmm. I think  they are using different  uh  radio technologies. So  uh  at the physical level  they're - they're - they are not compatible. It's not a problem. No  but s- You could have a U_M_T_S gateway that translates everything to that uh w- local @@ . Yeah. Yeah. If you will That - that's right. Yet the basic idea is  in principle  to switch seamlessly between wireless LAN technology  whatever and U_M_T_S  right? Yes. And you see the benefit that  in principle  the customer will save a lot of money. Because if in-house communication like WaveLAN or maybe I see it also in the American market  some WaveLAN communication in the outer world will be available  the - m- and that is your assumption  that the more cost uh extensive um U_M_T_ S connection could be principle used  I would like to say  over WaveLAN  and then I have a normal WaveLAN connection  right? Yes. And the router and the WaveLAN connection goes into the core internet elsewhere and Mm-hmm. the connection is not dropped between this seamless hand off I would like to say. Right? or what - or - Well  If connection is lost  I am - I'm not sure if sometimes you'd What - simply have to drop a connection. If you have outstanding connections. Yeah. But that's uh one thing of the internet  right? It's should be able to unplug a host  bring it somewhere else and everything continues like it should be. I'm not sure i- if it's uh if you really have to con- uh save a connection for all applications  I - I don't think so. Yeah  but if this is the final goal  it should be as much as possible available that you do not have to break the connection if you switch between the subnets' technology Right? Yeah but I - I have a point uh regarding the - this question  is eh the cost function. I mean. uh What about the user who is switching from one - let's say - zero-cost area network to another network where he has to pay. It is O_K for the service to continue when he uh switches from one network to another. But probably  the user wants to be aware that he's switching from one place - from one network to another. Because maybe  while he's uh having  let's say  a vide- video conference uh over the - the WaveLAN oh he's not paying any extra cost. But when he continues with this service on the U_M_T_S  he's having to cover uh an important - let's say  an important cost - or some cost  non-null cost. Yeah  non-null @@ So uh this can be something that probably the user wants to be aware of. Of course  this doesn't mean that the user wants the connection to be dropped when he switches. to the w- uh - r- uh from the zero-cost area network to - to the - uh um non- null cost area network. But maybe some users want to do this. Because they say ""O_K - I'm not willing to pay any back for this service  because I'm just watching uh you know the sports news. So  if I switch - If - if  because of my motion  I'm switching  or I'm going out of the coverage of the - of the uh wave - uh wireless uh local area network  well  I - I want the service to be uh stopped."" And this can be some pattern then that some users will follow. Yeah. Yeah  but if your s- uh I believe everybody is aware about the U_S_A_I_A architecture. And uh this one is  here  - this stuff is  here  also related to this kind what is mentioned there concerning the end system. And in my conf- uh considered end system  there is a certain kind of policy  concerning how to use subnets technologies and - and all these kind of things. How to use quality of service  you know   in advance  because you have to pay for certain technologies and maybe for certain services. And that should not be done automatically. But it could be done automatically. But nevertheless there must be a certain kind of um user interaction always possible  that you could set up a certain kind of mini-database how to deal with all these kind of things. I think that's not a problem. But uh what I - I would like to ask Wilbert here  and we discuss it yesterday  ""Is it only the usage of that P_C_M_C_I_A card where you have - instead of G_S_M today  U_M_T_S   and instead of wireless LAN today also wireless LAN. Remember your four and one card uh concerning um  what is the name  from Dataco- ? No. Dacom Oh  the one I have Sitcom ? Yeah. Where you can enable four technologies  and you plug in  in your laptop and then you have every four uh subnet layer technologies available in your laptop. Is that the problem? Yeah  that's not the problem. Right. Um. Well  the switching itself could be a problem. Becau- well it's a Why? depends like y- you're not going to probe on four different networks if one of the other networks is available  maybe you are  I don't know. So that could be a t- topic for research  so maybe you know more about it. Yeah  but this i- Yeah  but this is th- only the probing to figure out that you get at a certain location the information that a s- certain coverage of a certain subnet techn- technology is available. And if so  Yeah. then what Miguel mentions  then you add a certain kind of policy  to switch or not to switch  to this kind of technology  right? Y- Yes. But this is end-system related. That's not network related. I see no network related stuff within these - Um. in this uh idea. It's only to set up i- u- For me it's like a laptop t- to plug one P_C_M_C_I_A - and currently will have two Hmm-hmm-hmm. P_C_M_C_I_A cards  and I compare G_S_M with U_M_T_S. And today I would like to have G_S_M  and then I have my G_S_M connectivity. And I cannot use it other technology  only G_S_M  because the interfaces  the driver interface  does not permit me to switch between the - between different technologies. So if I want to go to wireless LAN I have to stop my connection and put in my wireless LAN P_C_M_C_I_A card  and then I can go on and set up the - the same connection one more time. And what you have in mind  to do it a bit more seamlessly  and that makes an only sense for me if both technologies  in principle  were close together in that sense for application. For instance  the application would be that I have my laptop here  and I work here with - wi- with my device  any kind of device  and I work here in wireless LAN area. And then I go outside  and want to go still on  leaving the coverage of - uh of the - uh wireless LAN. And at a certain state  the device uh receives certain signals that the coverage of U_M_T_S or G_S_M is available. And at that state  you automatically  or based on the certain kind of user profile  switched to the um U_M_T_S or G_S_M st- uh network. That's the only senseful application. From my understanding  that means only to have a certain kind of control layer for the different drivers  which you also mentioned as a U_S_A_I_A architecture. and uh then to do the switching for an existing stream or application between these - uh  transparently to this application  between those uh these both technologies. Maybe there's more potential. I don't s- see it  but maybe you have a additional idea. And I don't think that this end system device uh architecture  which will come automatically. And I believe there's a lot of activity throughout the world  because this seamlessly communication is always mentioned elsewhere where you cou- where you listen to. So uh uh- If - if you can leave it out the network that will be really cool. That's what you w- what I would like to do  so Yes. But when quality of service comes to the uh scene  well  I think we have a - a bigger problem because maybe this - this uh uh seamless migration cannot be seamless at all  because maybe the - the quality of service we are getting when we are connected to a certain network  let's say high-speed networking  cannot be sustained when we are switching to a different technology. So  this is another thing that is uh putting a little bit in more trouble our - our scheme or - Now  I would like not to hear only Hm- three opinions. But maybe there are also other opinions. And O_K. Yeah. Oh  sorry. maybe everybody should uh give a short - Let us understand  I think you - Yeah. built for everybody and I think we should - their com- whether their comments. In this thing of seamless handoff - or handovers - should think more maybe in different layers? So one layer is having the seamless handover. It's clear if a technology doesn't provide that quality of service  you can't do magic and have it. There are certain constraints on that. It's true. That's still in mind. But for - with the cases there - There  it is possible. So that should be made possible then? Mm-hmm. Yeah. Yeah. So It can be done. It is just - Yeah. Mmm uh There are still enough problems i- Trying to - to - to - to poi- to pinpoint in doing it. Yeah. The - the possible problems. Yep. But it might be the wrong thing to promise  uh  ten megabits to everywhere. There you are. But saying ""O_K if it's a - if the technology is O_K  you can do it."" Yeah. In fact  if you take the slower technology of the ones you are planning to - Yep. to support  you can offer this warran- this - uh this throughput as uh the minimum warranty you - you can get. Yep. Yeah. So  if everybody is - is asking for less than this minimum amount  no problem at all. But in this case  maybe not too much effort should be put on providing this quality of service because you just have a - usually a huge bandwidth compared Yeah. on a - a - w- with what the user is - is using. Yep. Mm-hmm. I mean  if you have a ten megabits bandwidth and the user is asking two kilobits per second  of course  you can uh build uh certain reserve mechanisms to warranty the user these two kilobits per second bandwidth. But probably in - it isn't - it is not worth because you have so many uh available bandwidths that maybe uh it is not a problem. Oh  please come in. Hi. Hi. Hmm? Oh! O_K. @@ ist fertig. Uh. O_K Good luck. Oh  O_K. The - the thing is that we are looking for a research topic on - on those Yeah. uh four areas. Yeah. And the building blocks  yeah. Multicast  not a topic. So - So we - Routing? So - so we went through uh uh to - through multicast. So this architecture with m- well  at least no wireless environment or a mobile I_P environment  comparable to uh U_M_T_S environments. We really did not find any - well  difficulties there. On the blackboard  that is. Of course  in real life there will b- probably be some problems. Have you - have you read about this - uh Barwan project held at Berkeley University? Well  what you - the pointer - the pointer you sent yesterday. Iceberg? Yeah. Yeah  I c- I didn't find the time  O_K. yeah  to do so  but  uh You mean the Iceberg project? Barwan. Barwan of uh E_S Barwan. How's it start? I know there's one that's of uh Randy Katz. Uh da- it was another related project called Daedalus. Yes. And uh well it seems they what they were doing was some kind of uh things si- somehow similar to this. They worked together wi- little bit with Nokia? u- Uh Don't know. Palo Alto? Mountain View? I - I - I - I just uh take a - a quick uh look at the - at the thing because I - I was - uh read about this uh a long time ago. And it - the - the latest report I - I found was in nineteen ninety eight. And in this report  well  they - they pres- they have uh some slides  some uh papers  some - well  a lot of things. Hmm. And well the main thing and the quick uh thing I - I was uh looking at was a - a video available in M_P-thr- in M_P_G M_peg format. And uh well what this video was presenting was a kind of test of - of one guy with a portable computer  moving across different networks. So starting at the C_S department at Berkeley and then going out the street switching to the - a campus-wide network  uh um I think it's a Metricom uh network  an- which is uh a test - A Ricochet here  right? Ricochet  yeah  that's right. Yeah. Yeah. Yeah. And then  finally switching to - I think it was city P_D or something like this. O_K. I - as lo- uh as the - as the user is moving out of the - of the coverage area of this second network. And  well  what they have built is a a - kind of uh proxy structure. So they are - they are putting most of the work of this adaptational layer on - on the side of the proxies. So the client or the server is not changed. It is the proxy  the one that does the work. Yeah. O_K  so you can even  for example  use - It's the same? Ah we just discussed a lot about proxies. Oh  O_K. That's a fun @@ I like them so O_K  I - It is just that - that - well  maybe we can - we can take a look at - oh maybe a more in-depth look at this project just to see if we are doing the same  or if we are doing something similar. It would be important for us - to - to highlight the differences. Yeah. Because if not  well  they can say  ""Oh  you are doing the same thing. "" ""This was done now."" Or maybe talk to them about s- Yeah  well we are close. operations they thought of but didn't touch yet. Mm-hmm. Uh maybe a few comments. First of all  Um. Do you mind Mi- Miguel  to write the name of the project on the whiteboard? Yeah. No problem. So that everybody - Uh I - I will send also the pointer which Miguel sent to me  to everybody so that everybody can take a look to this uh  project. I was - And uh I don't know what avail- information is available because I didn't find the time currently to check it out. It's a project of the U_C. But first of - Yeah  U_C Ber- uh Berkeley  yeah. Oh  yeah. Yeah. Wh- when did you go through this? I'll - I - you - uh I seen uh uh reports from nineteen ninety- uh ninety-six  ninety-seven and ninety-eight. Cuz yesterday I was uh Well coincidentally  I b- bumped into this project here. Maybe the project - n- uh It's funny. Uh  sorry? Yeah  yesterday I was just surfing a little bit. Uh do - Mm-hmm. Ah! And then from Nokia I came to this - that project  so. Ah! O_K  I see. Nokia research. And then - Yeah? @@ O_K. I think that - that the leader was uh Professor Katz. That's right. And he's also board in Nokia O_K. research  so Mm-hmm. Then the second thing is that uh Daimler-Chrysler research here in Palo Alto as well as in Germany  Ulm  is doing  in principle  the same with a car. Mm-hmm. They have a specific antenna for uh G_S_M Oh  G_P_S and - and fu- for future extended G_- uh G_S_M networks and wireless LAN and and other thi- and other network technology. In the roof of the car. They have a s- lot of servers in the trunk of the car. Then they collect data how it is - with handoff and all these kind of things and they are driving around the different areas. And the same car exists here in Palo Alto uh based on wireless LAN  G_S_M and 2x Ricochet networks. Mm-hmm. They collect the dat- collect - collecting a lot of data. concerning the seamless handoffs and whether it works very well with speed and um uh  Thomas is going to build mobile I_P stuff in their servers. uh because they want to use current technology with a c- a combination of mobile I_P. So I think there is a lot of activities in the area. Uh. And my third uh comment is uh if we are going something in that direction that is quite different from this thing here. But  anyway  maybe as a core technology with a real product in mind. I see some difficulty because nobody currently has here certain kind uh of U_M_T_S know how. Mm-hmm. And you need it definitely because then - it j- must be your uh Bible I would like to say that you are very familiar with U_M_T_S if you wanna go in - in that way. And As far as I know the U_M_T_S standardization is not finished yet. Ninety percent  maybe  or eighty percent. I kno- I'm not sure. But uh Yeah b- le- let me - let me ask you that - Yeah. a question about eh U_M_T_S or - or third or possible fourth- generation communic- wireless communication systems. Uh  from the point of view of the user  uh is there uh something more that - nnn  that - than more speed? Is there any other advantage  from the point of view of the user or of - of the terminals? Users of - of U_M_T_S? Yeah. You can request certain kind of Q_S. You can even request it in G_P_S but it's not end-to- end quality of service. Hmm-hmm-hmm. It is only um what the gateways O_K. Yeah. get. And within the core network there are certain kind of tunnels set up in the core network itself of the access provider  not the internet backbone core. And you use certain kind of tunnel mechanism to combine  in principle  the base station cluster control. But this is - uh is controlled by a certain kind of node . Hmm-hmm-hmm. And you have the gateway to the P_D_N  or the Public Data Network. And between both there's a certain kind of tunnel set up. But in fact - But in - fact now  at the G_S_M level  what you have is that every voice channel you use provides a - a p- a pretty fine fixed uh data speed. So  we can say that every voice channel has an i- an incr- uh - implied um quality of service  which is nnn ninety six hundred bits per second. Full duplex. You mean now for G_S_M  @@   or - ? Doesn't it? Yeah. Yeah. Yeah. So. Yeah  what is - what means quality of service. A certain degree. Right? Yeah. Uh. Sometimes we have some - No  but - um but not only because of - of data speed  but also about delay  you know  because G_S_M networks were developed to y- uh with a voice service in mind. Ah  delay is also considered in these networks. So  uh  in fact for every voice channel you have uh a pretty fine maximum uh delay and jitter  and a - a - a specified um data speed. So you can scale up - scale up this uh this um thing by using several channels. So  in fact  you have  using just G_S_M technology  you have more or less the same building blocks as you can get uh with the U_M_T_S. I mean that one thing is that the technology is a little bit different  that uh the base station thing is different and maybe uh the - the internal routing uh inside the network is different but from the point of view of the mobile things uh I'm not sure if - if - It's packaged data oriented. It's not really circuit-switched. In U_M_T_S you have always G_P_S as a - a involvement in the core network. You have  in principle  the path which still is available for - for the G_S_M network. Also they use   in principle  the same Mm-hmm. uh the same mechanism. But you also have a - dat- uh packet oriented communication that's on the G_P_S stuff. And that's also related to U_M_T_S. So  Yeah  but - The - that is - You know  it is also being d- uh developed  this G_ uh P_- D_P_R_S thing  G_P_R_S thing. which is uh pack- uh packet data over G_S_M. So  I mean. Uh  I don't see that a - a lot of difference. We can expect a lot of differences f- uh from the uh mobile terminal point of view. Uh  when uh U_M_T_S be deployed. What I'm trying to say is that - is that - well  uh maybe uh it is not a big problem that any of us uh be uh um were um a U_M_T_S expert because uh I - I don't see - it - I think that the U_M_T_S thing is more a technology issue  than a research issue. Maybe I'm wrong. Just es- um No  I think there's a lot - Yeah. - I think there's a lot of research. But the question is ""is this uh explaining my - m- a research wh- in which direction we are going?"" I mean - to be done here  not - not at the - of course  you know  I'm - I - I agree with you that there are a lot o- of things to be developed but - And because - For inf- But the point is that - uh this is probably not here. Because uh well  you know the - the - the orientation of our group which is uh ""Networks Services and Applications""  not uh wireless technology or the underlying wireless technology. You think more about hardware technology. So  there is a lot of hardware to develop  but not - Yeah. You know  radio frequency technology  wide-band w- uh C_D_M_A. These kind of things all have a lot of things to be - to be - uh researched. But uh I don't think w- this is not something we are doing here. Right  but bear in mind the picture is a - is a little bit different. The picture I have in mind concerning U_M_T_S is  first of all  if you use U_M_T_S only for voice  Like you did it or are doing now for G_S_M  it's not worth to have this kind of network. And  currently in Europe  where it will be deployed first  I believe  it will be very hard  in principle  to get some money back if you have only for - using it for voice. Because they paid a lot of money. So. Voice  yeah. What is the point? The point is that you use the U_M_T_S to connect to the internet and have certain kind of services which are still not available up to now. But the problem is if you have the internet content  and you have maybe your small uh um device of maybe only evolution based for - for these uh new technologies  for - for - certain mobile phones  Hmm-hmm-hmm. then you have certain kind of problems  you know  because you cannot use these things . And WAP is not an answer for - for these kind of things  you know. So. The - I believe there is a lot of research work and also if you see the end-to-end scope of the internet  from the mobile node up to the certain kind of server  ""correspondent node""  or whatever you call this destination  then you have certain kind of models  for instance  for the quality of service and all these kind of things. And then is U_M_T_S only one link where you have certain kind of subnet layer technology. And you have to match these things anyway. It's like a normal ethernet. Hmm-hmm-hmm. Whatever you see  it is only an additional subnet layer technology. Yeah. And you have all the things mapped to the - uh to - to the - to the - the internet stuff  in principle  to this technology. And furthermore  you see that the trend is to put more I_P technology  really  really I_P technology  and not certain kind of modifications and adaptations  to their access network itself. And th- there's a lot of potential  as I believe  for - for uh certain research work. But the question is  first of all  ""Do we have the competence to do something like that?"" First of all. Second  we are not hardware builders  in that sense to set up certain kind of end systems. We can design them  yeah  from the protocol layering and - and maybe have some simulations  or whatever. But isn't the final outcome really such a device? Prototype  Linux based. And we go around here and we have maybe a G_P_S-base station sponsored by Ericsson  since they are living not so far away and - and maybe WaveLAN connection  here now a testbed and then - then figure out that it works? So  that's my point. Well  just to make the list a little bit longer. Yep. May- Um  you could also think about uh session management. I do along with uh the network applications and services  uh as you might know. It's a - a if you can do it in the end systems or at sp- specific servers  then  I'm in favor of that. But se- session management  I mean uh the authentication  if you go from one network to the other  it's not solved. uh  Nobody really wants a - a guest on your network. Unless you get some money through some way - uh through some way. Then  there is also the session management in the sense that a lot of these things - at the Barwan project  as you said  go through proxies. There might be kind of handoffs at - uh at proxies  like the zip uh - s- zip proxy that we also discussed before a little bit. That - that's als- I think also in the line of uh session management. Then there is this uh perfect thing uh the Berkeley sockets. It's almost perfect. They s- kind of skipped uh layer five  uh session management or session layer  I think. That's um normally with - if you do a socket programming  it's directly the prot- application protocol on layer four  uh T_C_P socket  or layer five for U_D_P socket. There might also be some research - Well  could you introduce such a session management  or - Norm- what they do right now is they make it part of the application-level protocol. That's what they do most often. Like with session management? for mobile the - uh applications such as ist- instant messaging or uh talks  and so on  uh like zip . It's just doing a - a reconnect. If they think that the network is different or different capabilities come into play  they simply do a reconnect so it's part of the protocol. That's all what I think about session management  like part of - in proxies or is it en- embedded in application level protocols? But it's really higher level. Right? It's no more related to a network. We are leaving the network area. I- it's - That's dangerous. Cuz if you go to the app- If you - For me that's starting with middleware aspects  you know? So how do you want to focus on network services. And - Well the- So ""network applications""  if you look at the name of the group  one thing is that um the- they used to call a - a n- a ""network"" that was uh - if it was like distributed between two systems connected by a network. If you look at ""network "" from the O_Z layer  then it's uh  level two or something or maybe level three  but everything below three. So there are  I think  two - two different interpretations of uh ""network"". No   I believe ""network"" is  if you see it really from - from the I_P protocol suite  is then layer one  in principle  up t- including layer four  despite the fact that layer four is only residing  normally  in the end system and in the gateways. But are there ""network applications ""  then? Yeah. So  the - the name is ""Network Services and Applications""  s- so what could be an en- example of a network application if you look at the O_Z stick? Yeah  we have  I think  some. Yeah. Do you see this word ""application"" related to a network? Or do you see this word independently? That was never discussed in detail. ""Network Services and Applications"". So  everything O_K. in the world. Or is the application related to the network? For my understanding and as far as we discussed  so - such a long time uh  these applications should - For instance  e-commerce. O_K  O_K  well  they will - I didn't want to get a discussion now. And it's really  really yeah You know? That - Yeah. No no  uh that is a typical application  I think  which is  in principle  also a little bit related to - uh to the network. But not any application. And we also have the aspects for security. Miguel  you mentioned here authentication and - and triple-A_  but we do not have any expert anymore. Because Hannes was an expert for that. So the problem for me  it still exists that we have  in principle  here basic building blocks in the proposal from the networking side. We could have a certain kind of application and there's a lot of activities in the outside world and I believe there's a lot of contacts. Or there could be a lot of contacts uh with - with these people. Especially at uh Georgia Tech. I know on - next week on Tuesday. Tuesday  right? We will go to U_C_B. There are three guys. Then Morgan mentioned one. I do not know exactly um uh what he is doing  but  he's also interesting in some kind of collaboration. He's also a professor in the U_C_B. And uh many universities maybe Claudia mentioned Duisburg. And uh - in principle  most of the university have some activities in that area. We will not take care about these one  but we can use them as um for field ex- uh field trial access and - and - and maybe some support and - and whatever kind of thing. And we focus really on the networking stuff and going in more detail in this area. And the alternative is maybe what uh currently we have only uh this uh proposal from Wilbert to go more in the smaller scope. Figure out some potential from maybe this uh WaveLAN U_M_T_S stuff and - and starting from that. Or  are there other suggestions? I think we uh have already so much work put into this proposal  why to switch now? So I don't see the reason for that. Maybe the application we suggested is not the best one. So we have to think about an application which - which is um um  for mobile users  more relevant. I don't know. Maybe video games. If you are traveling uh  it's boring to travel and then you decide to play a g- video game with somebody else who's traveling. So. Yeah  you know  it's still mentioned in the proposal. And th- um I - I'm sure  O_K  that it is the wrong place to get the foundation why we are use this kind of application  but uh - And - In - in the end of the text - in the end of the pr- text of the proposal. But it is mentioned that it is only an example. And that we use this application  in principle  while it is then very very easy to get a certain kind of access to these kind of things. When you have a video game server and - and you want to have mobile access you must at least have one in your consortium  or as a partner whatever  who provides this service. And then we - we are playing videos  in principle  if you have a field trial. And that is  I think  much more harder than to have a certain kind of - certain kind of access to the University  and uh to this server  which is run on the campus of the University. O_K  that's right. And that is the point. You can have this f- to figure out all the things and the - the um and the - the um um  now  how to say  the requirements of this application using this classroom scenario. But nevertheless  it's not the best for 3x and - and - and - and uh s- networking stuff. You are right. But it - Does anybody see certain kind of potential here to go on  and to start the work maybe in this little bit smaller oh uh scope based on these building blocks? Because Multicast you still mentio- uh you mentioned that there is no potential. I think it's described here that the potential  if you use it for the mapping to the next generation networks in the wireless access area they are still some potential to map Multicast because they do not use it. And to figure out whether they're useful inse- instead of using certain kind of tunneling mechanism and to set up these tunnels and have the mapping between a P_S_D_N uh telephone number or whatever kind of number they are using to I_P addresses and then there's the tunnel I_Ds  and - and all this kind of things  I think  is very beneficial as well for - for telecommunication providers. as well as for I_S_P's. I - I think the potential is still there. You - you neglected that  but I don't think that you are right in that sense. Well  think it works  but uh yes  the mapping of course between multicasting and the U_M_T_S or - or whatever In next generation? Yeah. That's - or C_D_M_A or whatever - that's uh  Yeah. yeah  probably a thing that has to be find out. I don't know what statuses. Maybe they already did it par- as part of U_M_T_S  I'm not sure. I don't have a clue. I - I only know how it works on copper and like ethernet and on P_S P_P_P over something and how it works like that  but on the wireless I'm not sure. Now what do you see about the um  um  K_P_N interests? Because  I believe s- if the argument changes as follows that you are here and that you are very familiar with uh the Multicast stuff  and K_P_N is very interesting in U_M_T_S stuff  and this is one potential target network environment we are focusing on  but only for the uh I_P -layer technology  then  why do - the mapping of multica- also potential mapping of Multicast  I think  must be of major interest for them. Right? Yes  but if - if it's available One question. that means so - so if uh if it's already part of the U_M_T_S standardization then uh it's immediately finished  like  O_K  you can tell  K_P_N well go to Ericsson or Nokia and buy it? And if it's not in there yet well  it could be a suggestion and then the hardware people will make it part u- like the ETSI or whatever - where U_ - wherever U_M_T_S is standardized they will d- well  go for it. Mm-hmm. Of course  yeah  this - this group could do that  too. Could be  but I have a - an important doubt about the I_P availability over U_M_T_S. I mean - you know that the carriers and some manufacturers have done an important effort in developing this web thing and uh I'm not sure what they plan to do with next generation. I mean  are they maintaining - are they holding the same web technology? Or  are ju- just they - discarding this technology and forgetting about the thing? I - I don't know what will happen. uh  I - I don't have a clue. Probably  if after searching a little bit on the - on the net I can get the answer  but initially this is something to - to - to have in mind. I mean. Because maybe this U_M_T_S thing will appear on the market without I_P as a native protocol. And if this is the case well  thus seamless integration could be we- strongly affected. At least  uh  if you - we want to use U_M_T_ S. I'm - I'm - just uh throwing the question. uh I mean  looking for an answer because I don't know if uh some of you have a clue about this. I don't know what - what carriers are planning to do about this. I'm not really sure  but I thought I - I had the impression that we can throw away our Nokia stuff when - our web gateways  when we g- even go to G_P_R_S   so let alone U_M_T_S . Now  WAP is ja not only related now to available bandwidth. That's not the only thing. It's only the adaptation towards the That's the impression I get. capabilities of the end device. Right? Yeah  definitely. So. uh - I never was a friend - But but still a le- d- it's a - it's a fundamental Huh. Yeah. uh it's a - part of a - of design like the - the - the terminal has a WAP stick. And there's a box that has also a - a WAP stick. So yeah. And they do sometime - most cases  they do share an I_P address. But doesn't uh have to deal with I_P at all. It's l- just a thirty-bit number Mm-hmm. that a- actually is an assigned number. But it's all only used for identification. That's the only thing. It's stored in a WAP cookie  much like H_T_P in a cookie and from the WAP gateway then it will be H_T_P - real H_T_P - or something. So it's - it's really a different uh type of connection. It looks a little bit like uh the - the well  W_M_L looks a little bit like H_T_M_L  but that's all what's uh - It's not I_P at all. Right. Because there is a gateway  you can access with almost the same microbrowser  the same i- uh web servers. If they provide the right content - uh  spit out the right uh W_M_L language. So it's - it's really a different network - technology - I think. Yeah. It's transparent  in principle  what you can carry  but - Anyway  you have the gateway. It must be uh Yeah. The ca- uh c- uh t- conversion of the media But @@ @@ of the contents  in principle. But  I don't see that uh - It's too late  I believe. If WAP was available in the beginning of G_S_M  Mm-hmm. then I think it was there would be a good chance that it is well-deployed. But now  the first generation of I- i- web-browsers are in the ha- and - and uh I would say ""handies""  so that's wrong - in the mobile phones  but the - the next generation of networks are still available. and the ba- uh the bandwidth constraints are no more Well - applicable? Ri- right now there is still a lot of WAP  actually. And - Yeah  but - The hype but So that the - Mm-hmm. pffft uh Well  users - usage. I never saw a user using WAP. Please raise your hand  who is using WAP. My colleagues are. Yeah. Yeah  they are using WAP. Yeah? Yeah? O_K  one of six. So. I don't have a mobile phone. Yeah. Oh. i- Who is using mobile phones? Bu- well  you don't have a mobile phone in - in the U_S_A here. No. No. So that's maybe the reason. And I'm not - I doubt that it's not available in Germany. It's - would be unbelievable to me. Not me. Now  do you - yeah  do you see the chance for WAP? I don't see it. It i- it is. In - in - in Sweden it is also there. And in the Netherlands  it is also available. Mmm. every provider provides - In Spain it's available but uh you know  uh my main complaint has been all the time that it is difficult to me to believe that I can do interesting work with a four lines display. Ah Maybe  check the lottery result or something like this  but - Oh. Yeah  but - Oh! Bu- I'm the first one to admit that's a h- that it is a hype  but - and - but I'm - I just wanted to say it is being used. But if it makes sense to me and to - especially to have WAP in - in between  no. I'm not a supporter of it. Huh-huh-huh. So they have this optimized protocol to - to transmit a few ASCII bytes that can rescr- refresh the screen with a different animation every  well  second  a few times. Yeah. So  that's - that's true. I immediately agree. But the thing was  y- you just asked well  ""will - will it be the same  will there be WAP for U_M_T_S"" and so on  Q_P_R_S. And - well - I don't have the answer  but I thought it was not. Well I - I don't know what will - I'm just - It's weird - it's weird to stop doing it with G_P_R_S. Yeah. Hmm-hmm-hmm. But usually WAP shouldn't - is so general that it can be used with U_M_T_S. So if you still want to use WAP with U_M_T_S you might be able to do it. But Yes. that shouldn't be your only way of - There's a gate- You can put the gateway - if you want to do it in a - in a mobile phone - want to implement it  as a light stack or something. Yes? Yeah. Yeah. Yep. Sure. But usually - that's not the reason to have U_M_T_S. No  not - not if you have a big uh web pad or something. Then you m- There - there's an I_P stack in it and - Yep. Mm-hmm. Mm-hmm. So maybe I would like to ask Claudia  you read the proposal in the meantime? Hmm. Yeah  but the task changed really a lot  so  I'd - I rather would like to Yeah. Yes  that's right. And do you have some comments on it from the next - this is version zero dot five right  yeah. Yeah  but I'd rather like to follow the discussion before I gave any comments  cuz it's really The first point is it's much more than it was before I left  so it's like five weeks in between  so it's like that whew! that big now. That's right. Yeah. And it really changed a lot cuz the - I mean I found the classroom inside but  the thing where I left the discussion was that we are talking about uh a classroom where you know everybody could look on with any kind of device and follow lectures or do exercise or whatever. So. Now we are talking about something which is much more precise and the - the range of stuff is maybe more narrow than before so I'd rather like to - to follow it a bit  and then to get more into the ideas. So- O_K. c- Can I do another one  like U_M_T_S or wireless u- i- generally are - will be m- more expensive. And especially U_M_T_S. If they want to get back the money. So is it maybe a research topic to focus on the - not really billing  of course. I wouldn't - but at least the different application of protocol-based usage. So you have uh one connection  one application uses F_T_P whatever  bit voice or some other - That - those type of application you m- may - might want to make more out of a megabit per second or something. Could that be something  or - ? Also we want to go more in the triple-A_ area. Right? Because billing alone Or is n- No - Well  it's - Or or ac- um Billing is related to accounting  Accounting is related to Authentification and Authorization. e- Yeah  that's true. So. And then you are still at the triple-A_ service Yeah  that's right. uh activity and nobody is - has the skills to deal with that. And then we are leaving our scope anyway. O_K. My impression on this proposal is that every single block has some research done already. So  multicast. This means that there are many things done. Oh  what was it? Maybe I was routing mobility management But no one actually put them together. Yeah. That's - But - So that's the big problem. But That is a - the problem with this problem  might be that it's too big to put all these big pieces to- What do you mean by ""put together""? So that you really have a complete - As it - What I mean  the synergy effect  right? From - from - Yeah. a complete scenario. Yeah. You have  Ah. in one scenario  multicast  quality of service  routing and mobile um networking. Yeah. I- Mm-hmm. Not just the islands alone. So why I wasn't  well  yeah  uh hold myself back a bit with comments  is uh cuz when I left the discussion  the discussion was what to - to have something like a big umbrella about everybody who was at this point  here. And so the discussion was much broader than it is maybe right now. And there I had the impression the umbrella was this kind of classroom thing. Right? Or wrong? Yeah  in principle  it is related to the current state. But the networking stuff is more explained in much more detail Yeah  yeah  yeah. It is. Of course. and the work packages are Yeah. better ident- oder what c- could be derived for work packages is much better aligned . Yeah  yeah  yeah. That's - that's clear. But that has not changed  yeah? But- Yeah  that's - that's clear  but then I don't get - because when we were discussion - discussing this uh stuff weeks ago  uh I had the impression that  you know  everything of these work packages uh fit together under this one big umbrella. And now it sounds different. So that's what I don't understand. Ah but  it's also because I missed the last four weeks. So. Yeah  the problem is if you - It's a - a bit hard for me to follow the development what happened in the last four weeks  why it changed so. Yeah O_K maybe one sentence - one sentence. If you're focussing on the argument chain as follows. I have an application. This application has certain kind of requirements. These requirements are  maybe  Multicast and - and - and quality of service and mobility access and then mobility management and - That is it. I want to focus on it. Then you have in mind that - Or you must have in mind that all other applications and all other networking stuff is not affected by this. That you can use it with every application and if you introduce some new mechanism in the network  that is not related only to interactive multimedia applications like the classroom  it must be fit end to end. And if you assume something  like we discussed it  very hard with - with Multicast  that you assume that Multicast will be available at certain portions of the network or if you - if you want to have it end to end  this mechanism must be applied really for - all throughout of the internet. Otherwise it makes no sense  and we come to the conclusion  ""O_K  uh Multicast  we must deal with both approaches""  that you have eh application layer Multicast as well as maybe some other Multicast and maybe really Multicast @@ and then the point is how to select these mechanism if you want to have not a certain kind of access to the server elsewhere in the network you want to use any application. So if I understand it right  what has changed is before we were focussing on - Yeah  and - So - Cuz what's difficult for me is what is meant by ""application""? So. Cuz I have sometimes the impression that everybody is defining ""application"" in different way. So  what exactly is mean by ""application""? The application  for instance  here  that eh the interactive classroom. Yeah. But elsewhere any application in in in any kind of video audio or whatever kind of - Oh  O_K. So  what has changed - game server what -whatever kind of interactive multimedia especially do- uh downstreaming applications maybe eh video on demand then  eh whatever kind of application you can consider. So what has changed is that we're not only focussing on this intelligent classroom  but on everything. Not on everything. That's not true. But we s- That's why we say here  that we have a certain kind of eh possibility to change in the access networks. Mm-hmm. That means the wireless networks because that provides  in principle  the mobility. And these mechan- And you see that there's a trend to provide I_P functionality more and more in these kind of networks. But the problem is that you could not focus on this kind of um technology only for the access network. You have to have in mind always the end to end scope. And if you see  for instance - We have routing algorithm but different protocols and whatever kinds of things  u- especially also for Multicast  different things really for different providers  and uh and it's uh very easy to change it  maybe in the access network  for novel things. For instance  that's one provider domain and you can say well why not use active routing in that provider domain? So it means that packet are delivering information  and you do not have the separation of routing and - and - and forwarding process. It's impossible to deal with that in the whole internet  because security reasons and whatever kinds of things make it impossible. So how to combine these pos- uh potential um extensions in the um access networks  and - without losing the end to end scope in the internet. Then you can have certain n- novel features give the providers like K_P_N or whatever  the po- the potential  in principle  to - to uh see the building blocks for the future networks because there's a three g- third generation like U_M_T_S  a fourth generation more I_P related. There will be a fifth generation  I'm sure. And then you derive the certain kind of building blocks for this network  if you ever really full I - uh uh full I_ P  end to end. But nevertheless you can only - not only focus on ""O_K  I provide this mechanism"" because u- you are accessing a certain kind of server node in the internet  using any application. And then this mechanism must fit it there. But nevertheless eh internet backbone is in the meantime  based on its success  impossible to change. You see the difficulties with I_P version six  eh eh you see the difficulties with Multicast. They are discussed since  you know  ten years or whatever. The specification are rather stable and - and available but we have only islands in that. We cannot assume that everything is end to end available. And so you have to mi- to have to have in mind that certain kinds of mechanisms are maybe at a certain stage end to end available  maybe we have a lucky that the islands are connected to each other or - the same applies for M_P_L_S  for instance  maybe they are only portions available  how to deal with that when the islands are not available and maybe nothing is available. But you have to select it because I have a mobile phone  a mobile node or whatever kind of device  and I want to contact a certain kind of content in the internet. So is it possible to d- have certain kind of normal features in the access network without losing the end to end scope of - with these difficulties we are dealing with. I always say that the success of internet make it unflexible. It's impossible to change something very easy. s- It's easy maybe more on the access network as in the internet backbone. So that is eh overall picture that we have in mind with these things. And we can not u- We - we are six people or seven or whatever if Hannes come  We are not able to solve the problems uh in - in uh w- in the few years or whatever what the whole internet community has um done in - in - si- since - uh since a decade. But we can f- pick up some f- some um potential and - and - and - to start with something. And that's my missing point. That we have here  in principle  really the description of this problem I mentioned and focus really on the reasonable size of the project maybe in the first stage fo- for the N_S_A core itself  and maybe possible extension from  for instance  if K_P_N say ""yes  that's great "" ""that's the right direction. We will support that."" And if Siemens say ""O_K  great  we will support it""  and - and what - it - mmm University of Berlin or something and maybe University of Mannheim or Duisburg or in Spain oder whatever. So if we have all the similar project then you have only to take care about that's a little bit aligned  that there's some transparency concerning the results and and the activities are literally going in the - in the same um pace. So that's the basic idea. But really it's to - to figure out certain kind of smaller activities where we can start it . That's the point. And if we get some funding back  and we have certain potent- uh potential partners in the boat  O_K  we can go to a broader scope of this whole thing  yeah. u- Let me - let me propose a - a - a - a reasoning way. Uh let's assume that we are successful with this proposal  so we get the funding to do whatever what we want to do. So my question is  what we want to do is? Let's assume that we have the funding. What will be our first and or - or and second steps? Ah  what kind of things uh we - eh we will buy  what kind of things we will eh program or we will deploy? Because answering this - these questions we can know much better what the application we want. So let's assume - Yeah  but the problem is that f- f- f- funding is the second step. Mmm. N- Well  I'm not sure. I - I th- I think I agree with him. But uh right now Yeah. we're looking at b- a th- You want to do a big thing instead of a great thing. So if you d- assume that it should connect uh really good to Siemens and to uh K_P_N  and to the - then it starts to get big we should all go to the - in the same direction. If you already assume ""well  O_K  they paid money""  and now you think  O_K  what - what great thing you want to do  then it gives you more freedom to think about this great thing you want to m- do. Yeah  but we have no eh - current no public funding  No. But you - but you don't get one if you - if you try to - to align K_P_N with Siemens  for instance. O_K  everybody's paid here. Yeah  but - but - Yeah. But - Yeah. Yeah. Yeah. Yeah. But for theoretical - or for historical reasons a really good question  "" if we would have the money what would we do with that?"" Uh. So if we don't know what we would like to do with the money  Mm-hmm. we probably wouldn't get any money to do what we don't know. That's right. That's right. Yeah. Yeah. Absolutely. So that's why  I think  Yeah. yeah  it's a good question. Now   that's the point why I am focussing on - more on the - Yep. Yeah. I want to have really the activities you know  what is really the outcome what is really the problem we are going to solve. These are my questions. And if we have identified this block then it is possible to raise some money. Yeah. That's just the same question just other way around . Let - let me put an example. You know I - Some days ago  I bought  like some of you  this kind of things. This is a P_D_A eh three com uh Palm Pilot compatible  nothing - nothing that great  but it has some - some possibility of being expanded. Modernistic stuff! Completely useless! But the point is eh  the only - in here - The only thing - wireless thing I can put on - on this is just uh this Omnisky modem  which is uh  I think it is a subscription-based uh thing. You have to pay every - every month  and eh of course  I cannot imagine that uh tomorrow I'll be able to buy a different thing with four different networks  four different providers on it. Maybe I'm wrong but if I have the funding I say ""O_K  well  I can buy this or even I can buy a mobile computer  a notebook"". O_K? A lot more expensive  but that's O_K  I have the funding Then I can add a couple of cards at most  because mmm the majority of the - of the notebooks only have two P_C cards uh type two slots. So In - in the better case  maybe I'm only able to add two different networks to this thing. So ah the difficulty I see is that ah if we are - if you want to think in - in two three four different technologies um  maybe we are uh too advanced too  um  let's say  uh too in advance for the current technology. Which is not bad  but uh  I'm just saying that it will be able - It will - Sorry. It will be difficult for us to develop a - a reasonable test bed with the available technology. If somebody tell us  ""O_K  now here you have the thing  and just put these four networks you want on the thing."" How can I do it now? So mmm I'm still trying to - you know  to get used to the uh - all the ideas and to see that all the things are really connecting because this the eh most difficult thing I see to - to - to get this alignment to this connection point among the different things we are trying to align. So that's why I was proposing this - this question. Let's assume we have the funding. Now let's proceed. What - what kind of things we can buy  we can put together  we can program. Because oh at the end uh somebody will uh be asking us ""O_K  did you do your homework? Did you do what you - So. It's clear. Now from my point of view in the beginning  one more time  if I see the four building blocks  and eh I could speak only for my skills  I believe to derive a certain kind of Q_S Reservation-in- Advance scheme Mm-hmm. Much more uh general as it is done for - for U_S_A_I_A  but mapped then really to desired network subnet layer technologies. But for different networks? @@ Yes. For - The point is for No  for y- U_M_T_S. But first of all  a generic system  but because you can have several mechanism really using certain kind of - of um um quantitative description  you can have more of a qualitative description  you can have taken into account moving patterns  and - and - and whatever kind of things  but there are a lot of - lot of ways to have certain kind of Q_S Reservation-in-Advance  and I see it as definitely is necessary to have this one if I want be mobile and I want to have not a interruption of my - in my application qualities of service. So. Hmm-hmm-hmm. Um But then  to go  for instance  to U_M_T_S  and figure out what is available within U_M_T_S. And if I really have an I_P application  where I need this kind of - of quality of service and having in mind that certain kind of Q_S will be available in the rest of the internet. So how can I map these things uh to the U_M_T_S stuff? And what is the trend towards U_M_T_S to the first generation? And how it is possible to map this one to this technology? And if you have the building blocks identified  what is really necessary for Q_S - eh Q_S-in-Advance for - for mobile systems  then you have  in principle  the ideas for the following generation what you could improve. Hmm-hmm. Because then you are maybe no more related to this kind of thing. I believe the same applies for Multicast. And I believe the same applies for routing. So  in principle  in the beginning  there would be a certain kind of uh paperwork anyway  some theoretical examination of these problems  I think so and then some prototypes must be derived from that. And then maybe we will start here with a certain kind of uh testbed equipment we still have here. We have four uh P_Cs available. Maybe we have to extend it then  and figure out some real scenarios  and maybe then pfff! - Oh yeah. Because uh  despite the fa- uh uh Hopefully  then we have W- with the money also not eh only for the room   that would be great  but there are also some sponsors then then then we add a certain kind of field trial. But I think that's - pfff! that's years  you know. A long way  yeah. That's years. That's right. But in the beginning we must have a certain kind of theoretical framework with the relevant technology. Real technology. Maybe you can start with G_P_S or - or U_M_T_S  or whatever. because these technology will definitely available in Europe. There's some money spent on it  so that it will be in a certain kind of flavor. It will be available. Hmm-hmm-hmm. And it will be also available in the U_S_A . Because there is a big eh harmonization effort  despite the fact they are not aligned to each other anyway  but they will have s- also certain kind of thr- third generation and fourth generation networks. So. And after this theoretical framework  if this one is aligned  O_K you have some specification and you set up some Linux or Free B_S_D or whatever kind of prototype. So that is what I have in mind with that. And then - O_K  and then you will see. Hmm-hmm. The problem is - which I see is that it could be fruitful to standardization that it could be fruitful to some extent to telecommunication providers. But there's not a real product like you say. There's not the selling idea behind it. Hmm-hmm-hmm. Yeah  if you go with something what - what Wilbert mentioned "" O_K   let's have this one here"". We have a new P_C_M_C_I_A card and - and extend something and the end system  or what you mentioned here was the seamless four technologies inter-working in the end s-   then you have an - an - a device which you can sell. If you go for a networking pfff um uh improvements  in principle  and - and understanding next generation building blocks  uh there's no product. Hmm-hmm. But it is a long term - it's long term  I think  uh work you can do. And it's more related I believe to the skills to what is available and - and currently as I see what uh will be available within the N_S_A group. O_K. And uh  the point is if we focus on - on this networking - small - more networking stuff and em some application guys came into the boat and they focus on their application stuff  whatever this means  It might be we have then the active classroom  but it could be another one that is not representative  it's only - Hmm? Yeah but there's something needed to sell it. And I think - Yeah. Then - then is something available but I don't know you know  I'm not an application expert  I - We have some content. We will then discuss it on next Tuesday as I mentioned. Bless you. Bless you. Uh Anyway  but uh that were - is my answer to the question if some money is available. The good thing is that the money is available  because everybody is paid here. So that is money that the - the smallest portion of money which is available. Hmm-hmm-hmm. But first of all Wilbert need more  definitely  because maybe then K_P_N is uh no more interesting to go on with this work  or we can convince with these kind of things which we have in mind  uh companies like K_P_N. So And if we have other suggestions and eh have this as a long term N_S_A core group activity  and this theoretical framework  whether it is finished in half a year or one year doesn't matter  in principle  because the road map for the - the next generation networks is - is eh - is for many years. And b- before U_M_T_S will be deployed very well  I believe eh it will be common t- uh two thousand four or something like that. So. And - and have an intermediate project  which is related in this direction  something like the seamless handover for two technologies or whatever. This is a smaller portion in the gen- more generic framework  I would like to say  from this proposal. Why not go in this way? I - O_K I don't know. But uh that's the point eh we are sitting here together. Yeah. But - But for - for Dietmar I think eh eh he mentioned it to me  it's very clear that he will go on with a portion of time  with this U_S_A_I_A stuff  That's right. It depends on my - in the matical sense because that means for you in a certain kind - a certain extent  you are decoupled from networking stuff. Because your focusing on the mathematical thing of whether it's a network or other thing  it doesn't matter - Well  I'm foc- I'm focused on performance evaluation and um - especially of communication networks and - Yeah. Yeah. Yeah. I never eh e- um dealed with um mobile communications. So why not? Yeah. So. There is it still ongoing activity So that's eh  to open my mind. with uh maybe a portion of time at the University of Berlin - Berlin. And independently whether I will leave the uh uh I_C_S_I in - in the mid of December or not  I will still focus on - on - on these Q_S-in-Advance schemes eh also within Siemens and also with some project there. And I believe your routing stuff is also available  uh  Hmm-mmm. And when you - when you will go back to Spain  right? Yeah. Yeah. Sure. Yeah. And - and Yeah  so. and - and that's why  in principle  the skills will be always available I guess . I'm fortunate be- if there is a certain type of project in the home countries and then the Mm-hmm. eh institutions and - @@ N- Not only this. We can even use this u- platform to - to get some European projects in the future because yeah you know  sometimes eh it's a good thing - maybe Currently no money is available  you know. Yeah but uh  this will - I think w- This will improve over time  I'm sure. So uh - Probably. You know  this kind of joint work of different countries de- companies  universities. This is usually the best ah - the best place to - to put a project on. Because it is involving different partners from different points of - of Europe . Now  I will not prevent anybody from getting some cake and we are sitting here together it's uh very uh very slow  in principle  but my last question is for Wilbert. Wilbert  you I - I think you got the basic idea because it was mostly discussed with you here from this current available scrap project proposal. Don't you think that K_P_N could be convinced to go on this way what I mentioned before? There's no way? No. They needed definitely  in principle  some kind of service or device or product or whatever. Yeah. That's right now the s- the status. Yes. Can we both have together  maybe  I think it's a little bit more easier  a discussion on Thursday? I'm out for th- one - a week and a half. So on Wednesday  Thursday  Friday  I have a workshop at Stanford and next week I have a week on holiday. O_K. O_K. Maybe that's enough time for you to consider a little bit more in this direction? Could be. And then we could have eh discussion and maybe this one is an overall work  in principle  where everybody could hook in and - and get certain kind of small portions to solve it Yeah. So - so the only solution would - for - and for K_P_N would be that there is a p- a project in this area. So - that makes sense to do in this ar- This  I mean the Bay Area @@ Hm-hmm. with a group of uh a few people  like four or something  and that connects to this project. So  one somebody or two somebodies can be found uh founded or get money to join this project. But that the other four should build a service on top of it  mm-hmm. that has a clear business focus. It means eh a - a U_M_T_S service basically. Mm-hmm. Can you consider poten- also in the meantime  some potential U_M_T_S services besides this one? Yeah. And - Yeah  but you see the - the - they ask something pretty strange. They - they want something pretty strange from me that I think of a service  a network service. There are sixty million billion people in the - in the world that think of services all day and try to make a lot of money with it in the form of start-ups or whatever. And just think of a service like that  if I could do this. No  but  uh fff maybe based on this one and you see the networking stuff w- that it is a little bit more evol- evolved and - than in the - Mm-hmm. the core network technology in the access network  I would like to say. And this is aligned then as much as possible to the internet. That's the basic idea  that you can have a certain kind of service which make use of these novel things. Mm-hmm. Because normally I - I believe the - the - the people are I - you know - either on the application layer  that means higher than layer four  or they are on layer f- one to l- l- up to layer four. Bu- And the service people are considering ""O_K  U_M_T_S you have some location information  and maybe you are - have the infrastructure in the car  so  if I provide a certain kind of service  independently what the network is offering  you know. Because they do not consider it very well. They know there is a certain kind of location information and you have a G_P_S  and they know  O_K  it's based on satellite. And there's U_M_T_S  that is based on base station. There's all that they consider. And then assume  O_K  I can do that. And the networking guy said O_K it would be great to have Multicast and all these kind of things in the network  but do not consider really what a sense of benefit uh for applications  there were new app- old application make use of it and how can new application arise which we haven't considered up to now based on this new network function  L_A_T enrich - enriched network function  L_A_T. So  I think it's worth to - to consider something in that way Well  u- until now I have been trying to think about it  but eh f- to me u- what I have every - every time I encountered it  it is all - this is all pretty close by  like the industry is already behind this. So they - they want to make this available pretty soon. Yeah  what is our - So I'm just trying to think uh m- more ahead  like uh use the wireless device as a - only as a signalling thing or something. So - so whenever you come in with your uh G_S_M phone and there is a P_C available  like everything will be directed to the - the bigger device or something. So more services like that. I think the- they are working pretty hard to get I_P available very good and efficiently from uh two mobile devices or wireless devices. Not currently there is a techno- technology split. a th- thing Of course you have I_P in the mobile device but if you see the strange protocols text there's I_P over I_P and tunnels and - and whatever  so  but it's uh looks very strange to me because you have different numbering screens  concerning you have a phone number  you have to map it to I_P addresses and - and so on  but for me  you have not the end to end parts of for - for I_P functionality. We are far away from that. And if you take a really look to the G_P_R_S protocols texts and I believe also they apply for U_M_T_S but I'm not eh very familiar with that  and the next generations access network is - will come soon. You see it's every three years or four years they will have a new generation. And there's a - We m- But anyway you - yeah? yeah? Our group might fo- want to focus on the uh I_P over ether no nets or something like Why? What kind - what kind - I_P over what do you call it? or over ether - N- no nets  No net. Yeah. That would be great! Over - Like only over ether. carefully integrated for net but there is no net. If you want to do - make like also for wireless devices end to end I_P available  that could be a vision. Like. Can you repeat it? Well  so he - he says well I_P to I_P or end to end I_P  is far - well  it's far away. um So you - what you might want to do is well get U_M_T_ S out there and just do uh I_Ps playing over radio. Mm-hmm. Something like that is my final vision. Like - like S_D_H  I_P over fiber or W_D_M- W_D_M. Mm-hmm. This gets rid of the S_D_H. Or  well  start with get rid of the A_T_M first and then get rid of S_D_H. @@ Yeah. A_T_M stuff. Yeah. Yeah. @@ There's - the problem is - is  you see that the providers eh have always there broken view for - for the management. If you have different kind of protocols texts and network management system  and the billing system and all these kinds of things which is besides the networking stuff  in principle  but you still need it  Yeah. is eh not aligned. We have it for different islands and - and Hmm-hmm-hmm. @@ Well  that's my final vision you know. If you have really then I_P networks at a certain generation where you have really the physical media and where necessary uh additional maclear protocols  and then you have really always end to end. It makes you things much more simple much more easier  uh  but is far far away  you know. Yeah. Uh. Unless - unless we speak about quality of service  Yeah. Yeah. which is not eh embedded on - on I_P services. Yeah but you have the extension for the eh quality of service mechanism which you have to map then to the underlying technology. Yeah  but - And - Yeah but for example nobody's really using R_S_V_P. Yeah but maybe in the LAN interface it makes since to use it Eh you guys do that discussion later on please  uh? R_S_V_P. And - and - But yeah but I think we sh- we will miss the cake and I think we should stop now but please everybody  O_K  sorry  sorry. short eh s- go on with considerations and potential and - and uh um we didn't make s- much progress today in the meeting  but anyway this kind of activity lives from all  from the input of all. And so I really if somebody has an idea send an email and then I would like to enforce everybody not that I'm doing everything I could not - I f- didn't have the time in the past and I will not have it in the future and maybe I will stay here only s- four weeks or five weeks. So it would be really not the best thing if the activity is dead when I am leaving and if I stay longer of course we can go on but nevertheless I rely heavily uh also from the contributions of everybody  and it's in their own interest Hmm-hmm-hmm. That's right. of - of everybody  also from the institution behind. So If there is certain kind of idea or questions or whatever  we are s- only a small group of people we can easily join on a white board and - and start a discussion. It must not be on every Tuesday  you know  Well  if Klaus - O_K. at a certain time. And I try to motivate the people. That it's free. That's a starting. That's a ramp. And we can go in different directions. And if you have a real focus on the certain novel thing for the services or U_M_T_S or whatever  great. If not  the long term and then these things must be  nevertheless - must be defined in much more detail. Yeah. O_K? So  thanks O_K O_K please do not  um what Adam mentioned to uh switch off  Not to switch off. Don't switch it off. Do not - Don't switch it off. O_K  yeah  that's right. So maybe I will find him @@ . Oh  can you call Adam? cake area. At this number? and now let me see whether he will be here  and We did not forget. We are done. O_K O_K. Hopefully everything went fine. Yeah  just leave them on for now. Sometimes if you turn it off while it's recording it crashes ","The focus of the meeting was on a presentation of the work done already on the building of the Bayes-net. The input layer deriving information from things like the user and situation models  feeds into a set of decision nodes  such as the Enter/View/Approach (EVA) endpoint. In any particular situation  most of the outputs will not be relevant to the given context. Therefore  they will either have to be pruned a posteriori  or only a subset of the possible decision nodes will be computed in each occasion. The latter option could could follow a binary search-tree approach and it could also be better in computational terms. In any case  on what basis the ""winner"" output is chosen is not clear. One suggestion was discussed: the particular constructions used can determine the pertinent decision (output) nodes. The complete prototype of the Bayes-net will be presented in the next meeting. After that  it will be possible to define interfaces and a dummy construction parser  in order to test and link modules together. The suggestion that the most appropriate decision node of the belief-net in each situation could be chosen as a function of what construction was used  was deemed unsuitable at this stage. There are many interdependencies between the output nodes that this approach could not take into account. The rest of the values for the Bayes-net nodes will be built in within the week. The finished prototype will be presented during the next meeting. Any set of inputs will provide either the whole range of output values of the Bayes-net or an a priori selection of those outputs. In both cases  what is needed is a way to single out the appropriate outputs for any given context. For example  in the case of a ""where is"" question  whether the prevalent output should be ""Go-there"" or ""Info-on"" or even a third option has to be computed somehow. In any case  there are many input values that have not been entered in the Bayes-net at this stage. Furthermore  no inputs for the Ontology and Discourse can be built in yet  as they involve research that will be carried out in the future. Finally  there has been a problem with adding probabilities in a net created with the Hugin software  but this should be overcome very shortly. The presented Bayes-net takes inputs from the Situation  User  Discourse and Ontology models. There are several values (elements) defined in each of these models. The inputs are fed into the belief-net  which  in turn  outputs the posterior probabilities for the values of all the decision nodes. These comprise ""Go-there""  ""EVA""  ""Info-on""  ""Location""  ""Timing""  etc. At this stage  all the decision nodes are evenly weighted: regardless of the context  each output is trusted equally. Input and output node structure was presented in XML  as this is the format that will be used for the system. A large number of the value probabilities have already been set. "
"That's looks strange. O_K  now we're on and it seems to be working. Oh there we go. One two three four five six This looks good. That is weird. It's like when it's been sitting for a long time or something. So  I mean - I don't know what it is. But all - all I know is that it seems like every time I am up here after a meeting  and I start it  it works fine. And if I'm up here and I start it and we're all sitting here waiting to have a meeting  it gives me that error message and I have not yet sat down with - been able to get that error message in a point where I can sit down and find out where it's occurring in the code. Next time you get it maybe we should write it down. Yep  we will. Yeah. Was it a pause  or - ? One of these days. O_K. Was it on ""pause"" or something? No. O_K. Don't know. So uh - so the uh  the new procedural change that just got suggested  which I think is a good idea is that um  we do the digit recordings at the end. And that way  if we're recording somebody else's uh meeting  and a number of the participants have to run off to some other meeting and don't have the time  uh  then they can run off. It'll mean we'll get somewhat fewer uh  sets of digits  but um  I think that way we'll cut into people's time  um  if someone's on strict time uh  less. So  I th- I think - I think we should start doing that. Um  so  uh  let's see  we were having a discussion the other day  maybe we should bring that up  about uh  the nature of the data that we are collecting. uh @@ that uh  we should have a fair amount of data that is um  collected for the same meeting  so that we can  uh - I don't know. Wh- what - what were some of the points again about that? Is it - Uh  well  O_K  I'll back up. Um  at the previous - at last week's meeting  this meeting Yeah. I was griping about wanting to get more data and I - I talked about this with Jane and Adam  um  and was thinking of this mostly just so that we could do research on this data um  since we'll have a new - this new student di- does wanna work with us  th- the guy that was at the last meeting. Well  great. Great. And he's already funded part-time  so we'll only be paying him for sort of for half of the normal part-time  What a deal. uh - Yeah. And what's he interested in  specifically? So he's - comes from a signal-processing background  but I liked him a lot cuz he's very interested in higher level things  like language  and disfluencies and all kinds of eb- Mm-hmm. maybe prosody  so he's just getting his feet wet in that. Great. Anyway  I thought O_K  maybe we should have enough data so that if he starts - he'd be starting in January  next semester that we'd have  you know  enough data to work with. Right. But  um  Jane and Adam brought up a lot of good points that just posting a note to Berkeley people to have them come down here has some problems in that you m- you need to make sure that the speakers are who you want and that the meeting type is what you want  and so forth. So  I thought about that and I think it's still possible  um  but I'd rather try to get more regular meetings of types that we know about  and hear   then sort of a mish-mosh of a bunch of one - one-time - One offs? Yeah  just because it would be very hard to process the data in all senses  both to get the  um - to figure out what type of meeting it is and to do any kind of higher level work on it  like well  I was talking to Morgan about things like summarization  or what's this meeting about. I mean it's very different if you have a group that's just giving a report on what they did that week  versus coming to a decision and so forth. So. Then I was um  talking to Morgan about some new proposed work in this area  sort of a separate issue from what the student would be working on where I was thinking of doing some kind of summarization of meetings or trying to find cues in both the utterances and in the utterance patterns  like in numbers of overlaps and amount of speech  sort of raw cues from the interaction that can be measured from the signals and from the diff- different microphones that point to sort of hot spots in the meeting  or things where stuff is going on that might be important for someone who didn't attend to listen to. And in that uh  regard  I thought we definitely w- will need - it'd b- it'd be nice for us to have a bunch of data from a few different domains  or a few different kinds of meetings. So this - this meeting is one of them  although I'm not sure I can participate if I - You know  I would feel very strange being part of a meeting that you were then analyzing later for things like summarization. Mm-hmm. Um  and then there are some others that menti- that Morgan mentioned  like the front-end meeting and maybe a networking group meeting. Right. Yep. So - Yeah  we're - we're hoping that they'll let us start recording regularly. So. So if that were the case then I think we'd have enough. But basically  for anything where you're trying to get Mm-hmm. a summarization of some kind of meeting - meaning out of the meeting  um  it would be too hard to have fifty different kinds of meetings where we didn't really Yeah. have a good grasp on what does it mean to summarize  but - rather we should have different meetings by the same group but hopefully that have different summaries. And then we need a couple that - of - We don't wanna just have one group because that might be specific to that particular group  but @@ S- So - three or four different kinds. Yeah  we have a lot of overlap between this meeting and the morning meeting. See  I've never listened to the data for the front-end meeting. Yeah. Yeah. Yeah  we - we've only had three. So. O_K. But maybe that's enough. So  in general  I was thinking more data but also data where we hold some parameters constant or fairly similar  like a meeting about Mm-hmm. of people doing a certain kind of work where at least half the participants each time are the same. Um - Now  let - l- l- let me just give you the other side to that cuz I ca- because I - I don't disagree with that  but I think there is a complimentary piece to it too. Uh  for other kinds of research  particularly the acoustic oriented research  I actually feel the opposite need. Right. I'd like to have lots of different people. Right. As many people here a- a- and talking about the kind of thing that you were just talking about it would have uh too few people from my point of view. I'd like to have many different speakers. So  um I think I would also very much like us to have a fair amount of really random scattered meetings  of somebody coming down from campus  and - Mm-hmm. and uh  I mean  sure  if we can Mm-hmm. Right. get more from them  fine  but if we only get one or two from each group  that still could be useful acoustically just because we'd have Yeah  I definitely agree with that. Definitely. close and distant microphones with different people. Yeah. Mm-hmm. Yeah. Can I - can I say about that - that the - the issues that I think Adam and I raised were more a matter of advertising so that you get more native speakers. Because I think if you just say - an- And in particular  my suggestion was to advertise to linguistics grad students because there you'd have so- people who'd have proficiency enough in English that - Mm-hmm. that uh  it would be useful for - for purposes - You know. But you know  I think I've been - I've I - I've gathered data from undergrads at - on campus and if you just post randomly to undergrads I think you'd get such a mixed bag that it would be hard to know how much conversation you'd have at all. Well  you want to i- And - and the English you'd have - The language models would be really hard to build because it would not really be - it would be an interlanguage rather than than a - Well  O_K  uh  first place  I - I - I don't think we'd just want to have random people come down and talk to one another  I think O_K. there should be a meeting that has some goal and point cuz I - I think that's what we're investigating  so It has to be a - a pre-existing meeting  like a meeting that would otherwise happen anyway. That's I think what we - and I agree with. Yeah  Right. O_K. yeah. @@ So I was - Yep. I was thinking more in terms of talking to professors Oh  interesting! uh  and - and - and uh  senior uh  uh  d- and Yeah. uh  doctoral students who are leading projects and offering to them Oh  I see. that they have their - hold their meeting down here. Oh  interesting! Uh  that's the first point. The second point is um I think that for some time now  going back through BeRP I think that we have had speakers that we've worked with who had non-native accents Oh  oh. I'm not saying accents. and I th- I think that - u- The accent's not the problem. Oh  O_K. No  it's more a matter of uh  proficiency  e- e- just simply fluency. I mean  I deal with people Yeah. on - on campus who - I think sometimes people  undergraduates um in computer science uh  have language skills that make  you know - that their - their fluency and writing skills are not so strong. Oh! You're not talking about foreign language at all. You're just talking about - Yeah. Yeah  just talking about. Well  e- I just think  but you know  it's like when you get into the graduate level  We all had the same thought. uh  no problem. I mean  I'm not saying accents. I'm say- I'm saying fluency. Well  yeah. Uh-huh. Yeah  then we're completely gone. It's - Mm-hmm. The - the habits are already burnt in. But - I'm just saying fluency. Well  I think that  um - I think that the only thing we should say in the advertisement is that the meeting should be held in English. Yeah. And - and I think if it's a pre-existing meeting and it's held in English  I - I think it's probably O_K if a few of the people don't have uh  g- particularly good English skills. O_K  now can I - can I say the other aspect of this from my perspective which is that um  there's - there's this - this issue  you have a corpus out there  it should be used for - for multiple things cuz it's so expensive to put together. Right. Right. And if people want to approach - Um  i- so I know e- e- You know this - The idea of computational linguistics and probabilistic grammars and all may not be the focus of this group  Uh-huh. but the idea of language models  which are fund- you know generally speaking uh  you know  t- t- terms of like the amount of benefit per dollar spent Mm-hmm. or an hour invested in preparing the data  Mm-hmm. if you have a choice between people who are pr- more proficient in - um  i- more fluent  more - more close to being academic English  then it would seem to me to be a good thing. I guess - I- maybe - Hmm. Because otherwise y- you don't have the ability to have - I- Uh  so if - if you have a bunch of idiolects that's the worst possible case. If you have people who are using English as a - as an interlanguage because they - they don't - uh  they can't speak in their native languages and - but their interlanguage isn't Uh-huh. really a match to any existing  uh  language model  Yeah. Yeah. this is the worst case scenario. Well  that's pretty much what you're going to have in the networking group. And - Right. because - because they - most - the network group is almost entirely Germans and Spaniards. Well I- Oh. But the thing is  I think that these people are of high enough level in their - in their language proficiency that - And I'm not objecting to accents. I - I'm - I'm just thinking that we have to think at a - at a higher level view  could we have a language model  a - a grammar - I see. O_K. a grammar  basically  Uh-huh. that um  wo- would be a - a possibility. So y- so if you wanted to bring in a model like Dan Jurafsky's model  an- and do some top-down stuff  it - to help th- the bottom-up and merge the things or whatever  Mm-hmm. uh  it seems like um  I don't see that there's an argument - I'm - I - what I think is that why not have the corpus  since it's so expensive to put together  uh  useful for the widest Mm-hmm. range of - of central corp- things that people generally use corpora for and which are  you know  used in computational linguistics. That's - that's my point. O_K. Which - which includes both top-down and bottom-up. It's difficult. Yeah. O_K  well  i- i- let's - let's see what we can get. I mean  it - it - I think that if we're aiming at - at uh  groups of graduate students and professors and so forth who are talking about things together  Yes  that's fine. and it's from the Berkeley campus  probably most of it That's fine. Exactly. And my point in m- in my note to Liz was will be O_K  but - O_K. I think that undergrads are an iff- iffy population. I definitely agree with that  I mean  for this purpose. O_K. O_K. Well  not to mention the fact that I would be Grads and professors  fine. Yeah. Yeah. hesitant certainly to take anyone under eighteen  probably even an- anyone under twenty-one. So. Oh  you age-ist! What's that? Age-ist. Well  age-ist. The "" eighteen "" is because of the consent form. Yeah. ""Age-ist"". Right  We'd hafta get - find their parent to sign for them. Yeah. Yeah. Yes. Yeah  that's true. Yeah. So. I have a - uh  um  question. Well  Morgan  you were mentioning that Mari may not use the k- equipment from I_B_M if they found something else  cuz They're - they're - yeah  they're d- they're uh - assessing whether they should do that or y- do something else  there's a - hopefully over the next few weeks. Cuz I mean  one remote possibility is that if we st- if we inherited that equipment  if she weren't using it  could we set up a room in the linguistics department? And - and I mean  there - there may be a lot more - or - or in psych  or in comp- wherever  in another building where we could um  record people there. I think we'd have a better chance of - I think we'd need a real motivated partner to do that. We'd need to find someone on campus who was interested in this. Right  but - Right. But if there were such a - I mean it's a remote possibility  then So. um  you know  one of us could you know  go up there and record the meeting or something rather than bring all of them down here. So it's just a Yep. Well  the other thing - just a thought if they end up not using the - the hardware. Yeah  I mean the other thing that I was hoping to do in the first place was to turn it into some kind of portable thing so you could wheel it around. Uh. Right. But. Um  and - Well  I know that space is really scarce on - at least in C_S. Uh - You know  to - to actually find a room that we could use regularly might actually be very difficult. Yeah. But you may not need a separate room  you know  the idea is  if they have a meeting room That's true. True. Yeah. and they can guarantee that the equipment will be safe and so forth  and if one of us is up there once a week to record the meeting or something - Mm-hmm. Yep. Well  maybe John would let us put it into the phonology lab or something. You know. Huh. Yep. I - I think it's not out of the question. Um. Yeah. Yeah  I think it would be interesting because then we could regularly get another meeting. So. Yeah. another type of meeting. Right. But I - I - I think you need  uh  another portable thing Right. a- another portable equipment to - to do  eh  more e- easier the recording process  eh  out from ICSI. Hmm. Yeah. Yeah. Eh and probably. I don't know. Eh  if you - you want to - to record  eh  a seminar or a class  eh  in the university  you - you need - Right. Yeah. It- it would be eh eh very difficult to - to put  eh  a lot of  eh  head phones eh in different people when Yeah  but - you have to - to record only with  eh  this kind of  eh  d- device. I think if we - if we wanna just record with the tabletop microphones  that's easy. Oh- yeah. Ye- Yeah  yeah. Right? That's very easy  but that's not the corpus that we're collecting. Yeah. Actually  that's a int- that raises an interesting point that came up in our discussion that's maybe worth repeating. We realized that  um  when we were talking about this that  O_K  there's these different things that we want to do with it. So  um  it's true that we wanna be selective in some ways  uh  the way that you were speaking about with  uh  not having an interlingua and uh  these other issues. But on the other hand  it's not necessarily true that we need all of the corpus to satisfy all of it. So  a- a- as per the example that we wanna have a fair amount that's done with a small n- recorded with a small  uh  typ- number of types of meetings But we can also have another part that's  uh  just one or two meetings of each of a - of a range of them and that's O_K too. Uh  i- We realized in discussion that the other thing is  what about this business of distant and close microphones? I mean  we really wanna have a substantial amount recorded this way  that's why we did it. But what about - For th- for these issues of summarization  a lot of these higher level things you don't really need the distant microphone. Right  I mean  I c- I think there's - You actually don't. And you don't really need the close microphone  you mean. Yeah. Yea- yeah yeah  you actually don't really even need any fancy microphone. You can use found data. Which one did you mean? You d- You don't ne- it doesn't - you just need some microphone  somewhere. Ye- Yeah. Yep. Yeah. Tape recorder. Oh. Yeah. Yeah. You need some microphone  but I mean - @@ You - you can. You can use - Mm-hmm. Um  but I think that any data that we spend a lot of effort to collect  Yeah. you know  each person who's interested in - I mean  we have a cou- we have a bunch of different  um  slants and perspectives on what it's useful for  um  they need to be taking charge of making sure they're getting enough of the kind of data that they want. Right. And - So in my case  um  I think there w- there is enough data for some kinds of projects and not enough for others. Not enough for others  right. And so I'm looking and thinking  ""Well I'd be glad to walk over and record people and so forth if it's - to help th- in my interest."" Mm-hmm. And other people need to do that for themselves  uh  h- Right. So that - or at least discuss it so that we can find some optimal - Yeah. But I think that - I'm raising that cuz I think it's relevant exactly for this idea up there that if you think about  ""Well  gee  we have this really complicated setup to do "" well maybe you don't. Maybe if - if - If really all you want is to have a - a - a recording that's good enough to get a - Yeah. For some of it. Right. uh  a transcription from later  you just need to grab a tape recorder and go up and make a recording. I mean  we - we could have a fairly - We could just get a DAT machine and - Yep. Well  I agree with Jane  though  on the other hand that - Yeah. So that might be true  you may say for instance  summarization  or something that sounds very language oriented. You may say well  ""Oh yeah  you just do that from transcripts of a radio show."" I mean  you don't even need the speech signal. But Right. what you - what I was thinking is long term what would be neat is to be able to pick up on um - Suppose you just had a distant microphone there and you really wanted to be able to determine this. There's lots of cues you're not gonna have. Right. So Yeah. Yeah. I do think that long term you should always try to satisfy the greatest number of - of interests and have this parallel information  which is really what makes this corpus powerful. Special? I - I - Yep. Otherwise  you know  lots of other sites can propose - I - I - I agree. Uh individual studies  so - but I - I think that the uh i- We can't really underestimate the difficulty - shouldn't really u- underestimate the difficulty of getting a setup like this up. Yep. And so  - uh it took quite a while to get that together and to say  ""Oh  we'll just do it up there "" - O_K. If you're talking about something simple  where you throw away a lot of these dimensions  then you can do that right away. Talking about something that has all of these different facets that we have here  it won't happen quickly  it won't be easy  and there's all sorts of issues about th- you know keeping the equipment safe  or else hauling it around  and all sorts of o- So then maybe we should try to bring people here. I think the first priority should be to pry to get - try to get people to come here. Here. I mean  that's that's - O_K  so that's We're set up for it. Mm-hmm. O_K. The room is - is really  uh  underused. Right. Uh - I thought the free lunch idea was a great idea. Yeah  I thought so too. Yeah  I - And I think we can get people to come here  that - Free lunch is good. Yeah. But the issue is you definitely wanna make sure that the kind of group you're getting is the right group so that you don't waste a lot of your time and the overhead in bringing people down. Mm-hmm. No crunchy food. Yeah. So - Yeah. Well  it would be lunch afterwards. Right. Well  I was thinking  lunch after. And they'd have to do their digits or they don't get dessert. Yeah  they have to do their digits or they don't get - they don't get their food. Yep. Yeah. Yeah Um  I had a - I spoke with some people up at Haas Business School who volunteered. Should I pursue that? Oh  definitely  yeah. Yeah. Yeah. So. They - they originally - They've decided not to do - go into speech. So I'm not sure whether they'll still be so willing to volunteer  but I'll send an email and ask. I'll tell them about the free lunch. And they'll say there's no such thing. Tell them about the free lunch. Yeah. Yeah. So. I'd love to get people that are not linguists or engineers  Yeah. Right. cuz these are both weird - Yeah. The - the - well  I know  I shouldn't say that. The oth- the other h- That's alright. No  the- they - they're very weird. We need a wider sampling. ""Beep."" Uh  ""beep"" Yeah. Uh  the - the - The problem with engineers is ""beep."" They make funny sounds. The o- the o- the other - The other thing is  uh  that we - we talked about is give to them - uh  burn an extra C_D-ROM. Yep. and give them - So if they want a - basically and audio record of their - Let them have their meeting. Well  I thought that was - I thought he meant  ""Give them a music C_D "" like they g- Oh. Then he said a C_D of the - of their speech and I guess it depends of what kind of audience you're talking to  but - You know  I personally would not want a C_D of my meeting  but Mmm. Of the meeting? maybe - yeah  maybe you're right. If you're having some planning meeting of some sort and uh you'd like - Oh  that's a good idea. Right. It'd be fun. Right. Right. Yeah. I think it would just be fun  you know  if nothing else  you know. Yeah. But it als- It's a novelty item. Right. It - it - it also I think builds up towards the goal. We're saying  ""Look  you know  you're gonna get this. Is- is- isn't that neat. Then you're gonna go home with it. It's actually p- It's probably gonna be pretty useless to you  Yep. but you'll ge- appreciate  you know  where it's useful and where it's useless  Right. and then  we're gonna move this technology  so it'll become useful. "" So. No  I think that's a great idea  actually. But we might need a little more to incentivize them  that's all. Yeah. What if you could tell them that you'll give them the - the transcripts when they come back? Alth- Oh  yeah. Yeah. Oh yeah. I mean  anyone can have the transcripts. So. Well  that's interesting. I hav- I have to uh raise a little eensy-weensy concern about doing th- giving them the C_D immediately  I thought we could point that out. because of these issues of  you know  this kind of stuff  where maybe - You know? Good point. So. That's a very good point. So we can - so we can - We could burn it after it's been cleared with the transcript stage. r- Right. And then they - they get a C_D  but just not the same day. Oh  right. If - It should be the same C_D-ROM that we distribute publically  right? Yeah  that's right. That's a good point. Right  it can't be the internal one. Otherwise they're not allowed to Although it's - There we go. Oh  I like that. That's right. play it for anyone. Well put. Well put. So  after the transcript screening phase. Yeah  that's true. Things have been weeded out. Otherwise we'd need two lawyer stages. Yeah  that's right  say Yeah. ""Yeah  well  I got this C_D  and  Your Honor  I -"" That's a good point. Yeah so that's - so let's start with Haas  and Sorry to have to - Yeah. Sorry I have to leave. I will be here full-time next week. Oh  that's fine. O_K  see you. O_K. No. Bye. That's alright. See you. O_K. See you. So  uh - Let's see. So that was that topic  and then um  I guess another topic would be where are we in the whole disk resources question for - We are slowly slowly getting to the point where we have uh enough sp- room to record meetings. So I uh did a bunch of archiving  and still doing a bunch of archiving  I - I'm in the midst of doing the P_files from uh  Broadcast News. and it took eleven hours Eleven? to do - to uh copy it. And it'll take another eleven to do the clone. Where did you copy it to? Well  it's Abbott. It's Abbott  so it just - But it's - it's a lot of data. Sk- It's copying from one place on Abbott to another place on Abbott? Tape. Tape? I did an archive. Oh! I'm sorry. Oh  on the tape. Ah! Oh. So I'm archiving it  and then I'm gonna delete the files. So that will give us Eleven hours? Wow! ten gigabytes of free space. Oh. Yeah  the archiving m- program does take a long time. Yeah. And - and - Yep. And so one- Mm-hmm. That - that will be done  like  in about two hours. And so uh  at that point we'll be able to record five more meetings. So. Yeah. One thing - The good news about that - that is that once - once it's archived  it's pretty quick to get back. I mean  it - it - it - The other direction is fast  Yeah. Is it? Right. but this direction is really slow. Hmm. Yeah. Well  especially because I'm generating a clone  also. Yeah  O_K. So. Yeah. And that takes a while. Generating a clone? Yeah  that's a good point. Two copies. Yeah. One offsite  one onsite. Oh! Oh! Hunh! Now  what will uh - Is the plan to g- to - S- So stuff will be saved  it's just that you're relocating it? I mean  so we're gonna get more disk space? Or did I - ? No  the - the - these are the P_files from Broadcast News  which are regeneratable - regeneratable O_K. Oh  good. I see. um  if we really need to  but we had a lot of them. O_K. And - for the full  uh  hundred forty hour sets. And so they - they were two gigabytes per file and we had Wow. Yeah. six of them or something. Wow. W- w- we are getting more space. We are getting  uh  another disk rack and - and four thirty-six gigabyte disks. Uh so uh but Wonderful. that's not gonna happen instantaneously. Or maybe six. Or maybe six? The SUN  ha- uh  takes more disks than the Andatico one did. How many - The SUN rack takes - How much - Th- One took four and one took six  or maybe it was eight and twelve. Whatever it was  it was  you know  fifty percent more. Is there a difference in price or something? Well  what happened is that we - we bought all our racks and disks from Andatico for years  according to Dave  and Andatico got bought by another company and doubled their prices. Oh! Oh. And so  uh  we're looking into other vendors. ""We"" - Wow. By "" we "" of course I mean Dave. So. Mm-hmm. Hmm. I've been looking at the  uh  Aurora data and  um  first - first look at it  there were basically three directories on there that could be moved. One was called Aurora  one was Spanish  which was Carmen's Spanish stuff  and the other one was  um  SPINE. SPINE. And so  um  I wrote to Dan and he was very concerned that the SPINE stuff was moving to a non-backed-up disk. So  um  I realized that well  probably not all of that should be moved  just the C_D-ROM type data  the - the static data. So I moved that  and then um  I asked him to check out and see if it was O_K. before I actually deleted the old stuff  um  but I haven't heard back yet . I told him he could delete it if he wanted to  I haven't checked today to see if he's deleted it or not. And then Carmen's stuff  I realized that when I had copied all of her stuff to X_A  I had copied stuff there that was dynamic data. And so  I had to redo that one and just copy over the static data. And so I need to get with her now and delete the old stuff off the disk. And then I lo- haven't done any of the Aurora stuff. I have to meet with  uh  Stephane to do that. So. So  but  uh y- you're figuring you can record another five meetings or something with the space that you're clearing up from the Broadcast News  but  we have some other disks  some of which you're using for Aurora  but are we g- do we have some other - Yep. other space now? So  so  uh  we have space on the current disk right now  where Meeting Recorder is  Yeah. and that's probably enough for about four meetings. Is that the one that has - is that D_C? Yeah. So. Yep. No  no  well  it's wherever the Meeting Recorder currently is. I think it's D_I. O_K  I - but the stuff I'm moving from Aurora is on the D_C disk that we - I don't remember. Th- I think it's D_C- It's whatever that one is. O_K  D_C. Yeah. I just don't remember  it might be D_C. And that has enough for about four more meetings right now. Mm-hmm. Yeah  I mean we were at a hundred percent and then we dropped down to eighty-six for reasons I don't understand. Um  someone deleted something somewhere. And so we have some room again. And then with Broadcast News  that's five or six more meetings  so  you know  we have a couple weeks. Uh  so  yeah  I think - I think we're O_K  until we get the new disk. O_K . So should  um - One question I had for you was  um  we need - we sh- probably should move the Aurora an- and all that other stuff off of the Meeting Recorder disk. Is there another backed-up disk that you know of that would - ? We should put it onto the Broadcast News one. That's probably the best thing to do. And that way we consolidate Meeting Recorder onto one disk rather than spreading them out. O_K. Right. Right. Do you know what - happen to know what disk that is off - ? O_K. No. I mean  I can tell you  I just don't know off the top of my head. Yeah. O_K. Alright  I'll find out from you. But  so we could jus- just do that at the end of today  once the archive is complete  and I've verified it. O_K. Cuz that'll give us plenty of disk. Uh  O_K  @@ So  uh  then I guess th- the last thing I'd had on my - my agenda was just to hear - hear an update on what - what Jose has been doing  so that's - Uh-huh. O_K. I have  eh  The result of my work during the last days. O_K. Thank you for your information because I - I read. Eh  and the - the last  eh  days  eh  I work  eh  in my house  Yeah. eh  in a lot of ways and thinking  reading eh  different things about the - the Meeting Recording project. Uh-huh. And I have  eh  some ideas. Eh  this information is very - very useful. I'm glad to hear it. Glad to hear it. Because you have the - the - the distribution  now. But for me  eh is interesting because  eh  eh  here's i- is the demonstration of the overlap  eh  problem. It's a real problem  I've seen it already. Yeah. Yeah. a frequently problem uh  because you have overlapping zones eh  eh  eh  all the time. Yep. Yeah. Throughout the meeting. Eh  by a moment I have  eh  nnn  the  eh  n- I - I did a mark of all the overlapped zones in the meeting recording  with eh  a exact mark. Mm-hmm. Oh  you did that by hand? Heh? That's eh  yet b- b- Yeah  by - b- b- by hand - by hand because  Can I see that? Can I get a copy? Oh. eh  eh - ""Why."" My - my idea is to work - I - I - I do- I don- I don't @@ - Wow! I don't know  eh  if  eh  it will be possible because I - I - I haven't a lot - eh  enough time to - to - to work. uh  only just eh  six months  as you know  but  eh  my idea is  eh  is very interesting to - to work in - in the line of  eh  automatic segmenter. Mm-hmm. Eh but eh  eh  in my opinion  we need eh  eh  a reference eh session Yes  absolutely. to - t- to - to evaluate And so are you planning to do that or have you done that already? the - the - the tool. And - No  no  with i- Sorry? With - ? Have you done that or are you planning to do that? No  I - I - plan to do that. O_K. Darn! I plan - I plan  but eh  eh  the idea is the - is the following. Now  eh  I need ehm  to detect eh all the overlapping zones exactly. I - I will - I will eh  talk about eh  in the - in the blackboard Yeah. Duration. Mm-hmm. about the - my ideas. Eh  um  eh - This information eh  with eh  exactly time marks eh  for the overlapping zones eh - overlapping zone  and eh  a speaker - a - a pure speech eh  eh  speaker zone. I mean  eh zones eh of eh speech of eh  one speaker without any - any eh  noise eh  any - any acoustic event eh that eh  eh  w- eh  is not eh  speech  real speech. And  I need t- true eh  silence for that  because my - my idea is to - to study the nnn - the - the set of parameters eh  what  eh  are more m- more discriminant Right. to eh  classify. the overlapping zones in cooperation with the speech eh zones. The idea is to eh - to use - eh  I'm not sure to - eh yet  but eh my idea is to use a - a cluster eh algorithm or  nnn  a person strong in neural net algorithm to eh - to eh study what is the  eh  the property of the different feat- eh feature  eh  to classify eh speech and overlapping eh speech. Mmm. And my idea is eh  it would be interesting to - to have eh  a control set. And my control set eh  will be the eh  silence  silence without eh  any - any noise. Mm-hmm. Which means that we'd still - You'd hear the - Yeah. That's interesting. This is like a ground level  with - It's not- it's not total silence. Yeah  acoustic with this. Yeah  fans. With - with  yeah  the background. Eh  I - I mean eh  noise eh  eh claps eh  tape clips  eh  the difference eh  Mm-hmm. eh  eh  event eh  which  eh  eh  has  eh eh  a hard effect of distorti- spectral distortion in the - in the eh speech. Mm-hmm. Mm-hmm. So - so you intend to hand-mark those and exclude them? Yeah  I have mark- Mm-hmm. in - in - in - in that - Not in all - in all the - the file  only eh  eh  nnn  mmm  I have eh  ehm I don't remind what is the - the - the - the quantity  but eh  I - I have marked enough speech on over- and all the overlapping zones. I have  eh  two hundred and thirty  more or less  overlapping zones  and Whew! is similar to - to this information  because with the program  I cross Great. Mm-hmm. Great. the information of uh  of Jane with eh  my- my segmentation by hand. And is eh  mor- more similar. Excellent. Glad to hear it. Good. @@ But - @@ Go ahead. Sorry  sorry. And the - the idea is  eh  I - I will use  eh  - I want - My idea is  eh  to eh - I should've got the digital camera. Oh well. to classify. I - I need eh  the exact eh  mark of the different  eh  eh  zones because I - I want to put  eh  for eh  each frame a label indicating. It's a sup- supervised and  eh  hierarchical clustering process. I - I - I put  eh  eh  for each frame a label indicating what is th- the type  what is the class  eh  which it belong. Mm-hmm. Eh  I mean  the class you will overlapping speech ""overlapping"" is a class  eh  ""speech"" Nonspeech. @@ the class that's a- These will be assigned by hand? Based on the - I - I - I ha- I h- I - I put the mark by hand  Uh-huh. because  eh  my idea is  eh  in - in the first session  I need  eh  I - I need  eh  to be sure that the information eh  that  eh  I - I will cluster  Well  training  and validation. Sure. is - is right. Because  eh  eh  if not  eh  I will - I will  eh  return to the speech file to analyze eh  what is the problems  eh. Mm-hmm. And I - I'd prefer - I would prefer  the- to - to have  eh  this labeled automatically  but  eh  eh  fro- th- You need truth. I need truth . Yeah  but this is what you're starting with. Hmm. Yeah. Yeah. Yeah. Yeah. I've gotta ask you. So  uh  the difference between the top two  i- So - so - I start at the bottom  so ""silence"" is clear. By ""speech"" do you mean speech by one sp- by one person only? Speech - So this is un- Yeah. Yeah. Yeah. Is - O_K   and then and then the top includes people speaking at the same time  or - or a speaker and a breath overlapping  someone else's breath  or - or clicking  overlapping with speech - So  One or- two or more. One  two  three. but- No  by th- by the moment n- Yeah. Yeah. Yeah. that - that's all those possibilities in the top one. O_K. Yeah. Yeah. Eh  in the first moment  because  eh  eh  I - I have information  eh  of the overlapping zones  eh  information about if the  eh  overlapping zone is  eh  from a speech  clear speech  from a one to a two eh speaker  or three speaker  or is - is the zone where the breath of a speaker eh  overlaps eh  onto eh  a speech  another  So it's basi- it's basically speech wi- som- with - with something overlapping  which could be speech but doesn't need to be. especially speech. No  no  es- especially eh  overlapping speech from  eh  different eh  eh  speaker. No  but there's - but  I think she's saying ""Where do you - In these three categories  where do you put the instances Eh - in which there is one person speaking and other sounds which are not speech?"" Ah! Which category do you put that in? Yeah  that's right. That's my question. Yeah. Yeah  he- here I - I put eh speech from eh  from  eh  one speaker Oh! without  eh  eh  any - any - any events more. Right  so where do you put speech from one speaker that does have a nonspeech event at the same time? Which catege- which category? Where? Where - What is the class? Like a c- No. By the moment  no. Oh. Yeah  yeah  that's what he was saying before. Oh  so you - So you don't - i- i- it's not in that - For - for the - by the @@ not - not marked. no  @@ because I - I - I - I want to limit O_K. the - the - nnn  the - the study. Got it. Fine. So - so - Yeah  so that's what he was saying before  is that he excluded those. So you're not using all of the data. The - All - I - Yeah. Exactly. Yeah. Yeah  you mean - So you're ignoring overlapping events unless they're speech with speech. Yeah  be- Yeah  that's fine. Yeah. O_K. ""Why? Why? What's the reason?"" because @@ i- it's the first study. Oh  no - no  it's a perfectly sensible way to go. We just wondered - the first We're just- trying to understand what - what you were doing. O_K. Yeah. Yeah. Yeah cuz you've talked about other overlapping events in the past. So  Yeah. this is - this is - a subset. Yeah. In the - in the future  the - the idea is to - to extend the class  Is - is - to consider all the - all the information  you - you mentioned before Yeah. Yeah  I - I don't think we were asking for that. We were jus- just trying to understand - O_K. Yeah. but eh  the - the first idea - Because eh  I don't know Yeah  we just wanted to know what the category was here. what hap- what will happen with the study. Yeah. Right. Yeah. Sure. Is your silence category pure silence  or - ? What if there was a door-slam or something? i- it's pure - No  no  it's pure silence. Pure silence. It's the control set. O_K. O_K? It's the control set. What you - Well - w- It's pure si- pure silence with the - with the machine on the - on the roof. With the fan. I - I think what you m- I think what you mean is that it's nonspeech segments that don't have impulsive noises. Yeah. Right? Cuz you're calling - what you're calling ""event"" is somebody coughing or clicking  or rustling paper  or hitting something  which are impulsive noises. Yeah. But steady-state noises are part of the background. Yeah. Which  are being   included in that. Right? @@ h- here yet  Yeah. So it's like a signal-noise situation. yet I - I - I - I - I think - I - I think  eh  Yeah. Well - there are - that - some kind of noises that  eh  don't - don't wanted to - to be in that  eh  Yeah. in that control set. But I prefer  I prefer at - at the first  eh  the - the silence with eh  this eh this kind Well  steady state. of the - of eh - of noise. Right  it's - I mean  it's - ""Background"" might be - might be a better word than ""silence"". It's just sort of that - the - the background acoustic - Yeah. Yeah. Yeah. Yeah. Right. So - Is - is - is only - Fine. Go on. O_K. Yeah. Well  we needed to get the categories  yeah. And  um  with this information The idea is eh  eh  nnn  I have a label for - for each  eh  frame and  eh with a cluster eh - algorithm I - and - Sorry. And eh I am going to prepare a test bed  eh  well  eh  a - a set of feature structure eh  eh  models. Right. And my idea is ""Tone""  whatever. @@ so - so - on - Right. because I have a pitch extractor yet. Mm-hmm. I have to - to test  but eh You have your own? I - Yeah  yeah  yeah. Oh! I ha- I have prepare . Is a modified version of - of - of a pitch tracker  eh  from  eh  Standar- eh Stanford University - in Stanford? No. From  eh  em  Cambridge University. Oh! What's it written in? Eh  em  I - I - I don't remember what is the - the name of the - of the author  because I - I have several - I have eh  eh  em  eh  library tools  from eh  Festival Mm-hmm. Ah. Mm-hmm. and - of - from Edinburgh eh  from Cambridge  eh  and from our department. And - And I have to - because  in general the pitch tracker  doesn't work Bad. Right. very well and - But  you know  as a feature  it might be O_K . Yeah. So  we don't know. Yeah. This - this is - And th- the idea is to - to  eh  to obtain  eh  for example  eh  eh diff- eh  eh  different - well  no  a great number of eh F_E_C for example  eh  eh  twenty-five  eh  thirty - thirty parameters  eh  for - for each one. And in a first eh  nnn  step in the investi- in the research in- eh  my idea is try to  eh  to prove  Supervised clustering. what is the performance of the difference parameter  Mm-hmm. eh to classify the different  eh  what is the - the - the - the front-end approach to classify eh  the different  eh  frames of each class eh and what is the - the  nnn  nnn  nnn  eh  what is the  the error eh  of the data This is the - the eh  first idea Mm-hmm. and the second is try to - eh  to use some ideas eh  similar to the linear discriminant analysis. Mm-hmm. Eh? Eh  similar  because the- the idea is to - to study what is the contribution of eh  each parameter to the process of classify correctly the different - the different parameters. Mm-hmm. What sort of classifier ar- ? Eh  the - the - the classifier is - nnn by the moment is eh - is eh  similar  nnn  that the classifier used eh  in a quantifier - vectorial quantifier is eh  used to - to eh  some distance to - to put eh  a vector eh  in - in a class different. @@ Unimodal? So is it just one cluster per - Mm-hmm. Is - Yeah? W- with a model  is - is only to cluster using a eh  @@ or a similarity. A- another possibility it to use eh a netw- netw- a neural network. Right. But eh what's the p- What is my idea? What's the problem I - I - I - I see in - in - in - if you - you use the - the neural network? If - w- when this kind of eh  mmm  cluster  clustering algorithm to can test  to can eh observe what happened you - you can't - you can't eh  Right  you can't analyze it. eh put up with your hand in the different parameter  but eh - If you use a neural net is - is a good idea  but eh you don't know what happened in the interior of the neural net. Well  actually  you can do sensitivity analyses which show you what the importance of the different parce- pieces of the input are. Yeah. Mm-hmm. It's hard to - w- w- what you - It's hard to tell on a neural net is what's going on internally. Yeah. But it's actually not that hard to analyze it and figure out the effects of different inputs  especially if they're all normalized. Yeah. Yeah. Um  Well  using something simpler first I think is probably fine. but - Well  this isn't tru- if - if - if you really wonder what different if - if - Yeah. But - Decision tree. Yeah  then a decision tree is really good  but the thing is here he's - he's not - he's not like he has one you know  a bunch of very distinct variables  like pitch and this - he's talking about  like  a- all these cepstral coefficients  and so forth  in which case Right. Yeah. Yeah. Right. Yeah. And - a- a- any reasonable classifier is gonna be a mess  and it's gonna be hard to figure out what - what uh - Right. I - I - I will include too the - the - the differential de- derivates Yeah. Deltas  yeah. So. I - I mean  I think the other thing that one - I mean  this is  I think a good thing to do  to sort of look at these things at least - too. See what I'd - I'd - Let me tell you what I would do. I would take just a few features. Instead of taking all the M_F_C_C's  or all the P_L_P's or whatever  Yeah. I would just take a couple. O_K? Like - like C_one  C_two  something like that  so that you can visualize it. Yeah. and look at these different examples and look at scatter plots. Yeah. O_K  so before you do - build up any kind of fancy classifiers  just take a look in two dimensions  at how these things are split apart. Yeah. That I think will give you a lot of insight of what is likely to be a useful feature when you put it into a more complicated classifier. Yeah. And the second thing is  once you actually get to the point of building these classifiers  @@ what this lacks so far is the temporal properties. So if you're just looking at a frame and a time  you don't know anything about  you know  the structure of it over time  and so Yeah. you may wanna build @@ - build a Markov model of some sort Context window ? uh  or - Yeah. or else have features that really are based on Yeah. um on - on some bigger chunk of time. But I think this is a good place to start. But don't uh- anyway  this is my suggestion  is don't just  you know  Yeah. throw in twenty features at it  the deltas  and the delta del- and all that Yeah  yeah. into some classifier  even - even if it's K_nearest-neighbors  you still won't know what it's doing  even - You know it's not a neural net. Uh  I think to know what it's - to have a better feeling for what it's doing  you wanna look at it. So you wanna look at - at som- some picture that shows you  ""Here's - These things uh  uh are - offer some separation."" Yep. And  uh  in L_P_C  uh  the thing to particularly look at is  I think - is something like  uh  the residual - the energy in the residual. Yeah. Um Yeah. So. Can I ask? It strikes me that there's another piece of information S- um  that might be useful and that's simply the transition. So  w- if you go from a transition of silence to overlap versus a transition from silence to speech  Yeah  because - Yeah yeah. there's gonna be a b- a big informative area there  it seems to me. Yeah. Yeah. I - Yeah. But eh I - I - Is my- my - my own vision  of the - of the project. So  some sort of - That's - Mm-hmm. I - eh the - the Meeting Recorder project  for me  has eh  two eh  w- has eh several parts  several p- objective eh  because it's a - Mm-hmm. a great project. But eh  at the first  in the acoustic  eh  eh  parts of the project  eh I think you eh - we have eh two main eh objective. One - one of these is to - eh to detect the change  the acoustic change. And for that  if you don't use  eh  eh  a speech recognizer  eh broad class  or not broad class to - to try to - to to label the different frames  I think the Ike criterion or BIC criterion eh O_K. will be enough to detect the change. And - Probably. I - I - I - I would like to - to t- prove. Uh  probably. When you you have  eh  eh s- eh the transition of speech or - or silence eh to overlap zone  this criterion is enough with - probably with  eh  this kind of  eh  eh the - the - the more eh use eh - use eh - used eh em normal  regular eh parameter M_F- M_F_C_C. you - you have to - to - to find - you can find the - the mark. You can find the - nnn  the - the acoustic change. But eh eh I - I understand that you - your objective is to eh classify  to know that eh that zone not is only a new zone in the - in the file  that eh you have eh  but you have to - to - to know that this is overlap zone. because in the future you will eh Mm-hmm. try to - to process that zone with a non- regular eh eh speech recognizer model  I suppose. you - you will pretend Mm-hmm. to - to - to process the overlapping z- eh zone with another kind of algorithm because it's very difficult to - to - to obtain the transcription from eh using eh eh a regular  normal speech recognizer. That   you know  I - I - I think is the idea. And so eh the  nnn - the - Clustering. the system eh will have two models. A model to detect more acc- the mor- most accurately possible that is p- uh  will be possible the  eh - the mark  the change O_K. and another - another model will @@ or several models  to try s- but - eh several model- eh robust models  sample models to try to classify the difference class. I'm - I'm - I'm sorry  I didn't understand you - what you said. What - what model? @@ Eh  the - the classifiers of the- of the n- to detect the different class to the different zones Mm-hmm. before try to - to recognize  eh with eh - to transcribe  with eh a speech recognizer. So p- And my idea is to use eh  for example  a neural net with the information we obtain from this eh - this eh study Features. Yeah. of the parameter with the selected parameter to try to eh - to put the class of each frame. Eh for the difference zone you - you eh  eh have obtained in the first Mm-hmm. eh  step with the for example  BIC eh  eh criterion compare model And - You- I don't- u- O_K  but  I - I think - in any event we're agreed that the first step is - Yeah. i- Because what we had before for - for uh  speaker change detection did not include these overlaps. Yeah. So the first thing is for you to - to build up something that will detect the overlaps. Yeah. Right? So again  I think the first thing to do to detect the overlaps is to look at these uh  in - in - in - Yeah. Features? in - Well  I - again  the things you've written up there I think are way too - way too big. Yeah. O_K? If you're talking about  say  twelfth - twelfth-order uh M_F_C_C's or something like that it's just way too much. You won't be able to look at it. Yeah. All you'll be able to do is put it into a classifier and see how well it does. Yeah. Whereas I think if you have things - if you pick one or two dimensional things  or three of you have some very fancy display  uh  and look at how the - the different classes separate themselves out  you'll have much more insight about what's going on. It will be enough. Well  you'll - you'll get a feeling for what's happening  you know  so if you look at - Yeah. Suppose you look at first- and second-order cepstral coefficients for some one of these kinds of things and you find that the first-order is much more effective than the second  and then you look at the third and there's not - and not too much there  Yeah. you may just take first- and second-order cepstral coefficients  right? And with L_P_C  I think L_P_C per se isn't gonna tell you much more than - than - than the other  maybe. Yeah. Uh  and uh on the other hand  the L_P_C residual  the energy in the L_P_C residual  will say how well  uh the low-order L_P_C model's fitting it  which should be pretty poorly for two- two or more Yeah. people speaking at the same time  and it should be pretty well  for w- for - for one. Yeah. Yeah. And so I - i- again  if you take a few of these things that are - are prob- um promising features and look at them in pairs  uh  I think you'll have much more of a sense of ""O_K  I now have - uh  doing a bunch of these analyses  I now have ten likely candidates."" And then you can do decision trees or whatever to see how they combine. Yeah. Yeah. I've got a question. Yeah. This- Interesting. Hmm. Sorry. but eh  eh eh eh eh I don't know it is the first eh way to - to - do that and I would eh like to - to know what eh  your opinion. Eh all this study in the f- in the first moment  I - I w- I - I will pretend to do with eh eh equalizes speech. The - the equalizes speech  the speech eh  the mixes of speech. With - With what? With what? Right. Mixed. the - the mix  mixed speech. ""Mixed"". Thank you. Eh  why? Because eh the spectral distortion is - more eh - a lot eh clearer  very much clearer if we compare with the P_D_A. Right. P_D_A speech file is eh - it will be eh difficult. I - So it's messier. The - the P_D_A is messier. Yeah  fff! Because the n- the noise eh to sp- the signal-to-noise relation is eh - is - is low. O_K. And   I don't know - Yeah  I think that that's a good way to start. But. I don't know eh uh i- i- that eh the - the result of the - of the study eh with eh - with eh this eh - this speech  the mix speech eh will work exactly with the eh P_D_A files. It would be interesting in itself to see. Well  I think that would be an interesting result. N- u- eh What  I - I mean  what- what is the effect We- of the low signal to - to - to noise relation  you know  eh with - Well  I think - I think - I think it's not a - it's not at all unreasonable. It makes sense to start with the simpler signal because if you have features which don't - aren't even helpful in the high signal-to-noise ratio  then there's no point in putting them into the low signal ratio  one would think  anyway. Yeah. And so  if you can get - @@ Uh again  my prescription would be that you would  with a mixed signal  you would take a collection of possible uh  features look at them  look at how these different classes that you've marked  separate themselves  Yeah. and then collect  uh in pairs  and then collect ten of them or something  and then proceed Yeah. with a bigger classifier. And then if you can get that to work well  then you go to the other signal. And then  and you and you know  they won't work as well  but Yeah. Right. how m- you know  how much - Yeah. And then you can re-optimize  and so on. Yeah. Yeah. But it- I think it would be interesting to try a couple with both. Because it - I think it would be interesting to see if some features work well with close mixed  and - Hmm. Ah  yeah  yeah yeah yeah. @@ That's - well  the - And don't - It - it's - it's true that it also  it could be useful to do this exploratory analysis where you're looking at scatter plots and so on in both cases. Sure. But - Mm-hmm. I - I - I - I think that the - the eh parameter we found  eh  That's good. eh worked with both eh  speech file  but eh what is the - the - the relation of eh - of the performance when eh you use eh the  eh Hmm. eh speech file- the P_D_A speech files. Yeah  I don't know. But it - Right. I - I - I - I think it will be important. Because eh people eh eh  different groups eh has eh experience with this eh kind of problem. Is - eh is not easy eh to - to solve  because if you - I - I - I have seen the - the - the speech file from eh P_D_A  and s- some parts is very difficult because you - you don't see the spectrum - the spectrogram. Is very difficult to apply Right. Yeah  they're totally hidden. Yeah. Yeah. eh  eh a parameter to detect change when you don't see. But I suppose - Well  that - that - that's another reason why very simple features  things like energy  and things - Yeah  yeah yeah  I - I - I will put eh the energy here. things like harmonicity  and residual energy are Are probably better  yep. uh  yeah are - are better to use than very complex ones because they'll be more reliable. Yeah. Yeah. Ch- Chuck was gonna ask something I guess . Yeah. You have a question . Yeah. Yeah  I maybe this is a dumb question  but w- I thought it would be - I thought it would be easier if you used a P_D_A because can't you  couldn't you like use Nah. beam-forming or something to- detect speaker overlaps? I mean - Well  if you used the array  rather than the signal from just one. Uh-huh. Yeah  no  you- you're - you're right that - But that's - In fact  if we made use of the fact that there are two microphones  you do have some location information. which we don't have with the one and - and so that's - Is that not allowed with this project? Uh  well  no  I mean  we- we don't have any rules  r- really. I think - But I didn't mean - I w- Given - given the goal. I mean  is - is that violation of the - Oh. I - I think - I think it's - it's - it's a - it's an additional interesting question. No. Yeah. I mean  I think you wanna know whether you can do it with one  because you know it's not necessarily true that every device that you're trying to do this with will have two. Mm-hmm Yeah. Uh  if  on the other hand  we show that there's a huge advantage with two  well then that could be a real point. Yeah. But  we don't n- even know yet what the effect of detecting - having the ability to detect overlaps is. You know  maybe it doesn't matter too much. Right. Right. Yeah. So  this is all pretty early stages. But no  you're absolutely right. That's a good thing to consider. O_K. Yeah. I see. Yeah. Yeah  yeah  yeah. O_K. There - there is a complication though  and that is if a person turns their back to the - to the P_D_A  then some of the positional information goes away? Well  it - it - it does  i- it d- it does  but the - And then  Yeah. No  it's not - it's not that so much as - the - the issue is that - that - And if they're on the access - on the axis of it  that was the other thing I was thinking. Mm-hmm. He - You mentioned this last time  that - that if - if you're straight down the midline  Yeah  we hav- need to put it on a little turntable  and - I - I - I - then - then - the r- the left-right's gonna be different  and - and - and in his case  I - I th- Well  it's- Yeah. Yeah. I mean  he's closer to it anyway. It seems to me that - that it's not - a p- uh  you know  it's - this - the topograph- the topology of it is - is a little bit complicated. Yeah. But it's another source of information. I - I - Yeah. I don't - I don't know ho- I - I - I think - Sorry. I - I - I think because the- the- the distance between the two microph- eh  microphone  eh  in the P_D_A is very near. But it's uh - from my opinion  it's an interesting idea to - to try to study the binaural eh problem eh  with information  because I - I found I would guess - Yep. difference between the - the speech from - from each micro- eh  in the P_D_A. Yeah  it's timing difference. It- it's not amplitude  right? Oh yeah! Oh I agree! And we use it ourselves. S- I mean  I know - I n- I know that's a very important cue. Yep. Yeah. Right. But I'm just - I'm just saying that the way we're seated around a table  is not the same with respect to each - to each person with respect to the P_D_A  so we're gonna have No. No. No  no  no. a lot of differences with ref- respect to the speaker. That's - But th- I don't think that matters  though. That's fine. But - That's - So - so i- @@ I think the issue is  ""Is there a clean signal coming from only one direction?"" Right. If it's not coming from just one direction  if it - if th- if there's a broader pattern  it means that it's more likely there's multiple people speaking  Yeah. wherever they are. So it's sort of like how - how confused is it Is it a - is it - Yeah  is there a narrow - Yeah. about where the beam is. Is there a narrow beam pattern or is it a - a distributed beam pattern? So if there's a distributed beam pattern  then it looks more like it's - it's uh  multiple people. O_K. Yeah. Wherever you are  even if he moves around. Yeah . O_K  it just - it just seemed to me that - uh  that this isn't the ideal type of separation. I mean  I - I think it's - I can see the value o- Oh  ideal would be to have the wall filled with them  but I mean - Yeah  O_K. Yeah. Yeah. But the thing is just having two mikes - If you looked at that thing on - on Dan's page  Yeah. it was - When - when there were two people speaking  and it looked really really different. Oh yeah yeah. O_K. Yeah. Yep. Yeah. What looked different? Uh  well  basic- he was looking at correlation. Cross-co- cross-correlation. Just cross-correlation between two sides. Correlation  yeah. So cross-correlation is pretty sensitive. Did- Sorry  b- uh I'm not sure what Dan's page is that you mean. He was looking at the two - Uh  his a web page. You take the signal from the two microphones and you cros- and you cross-correlate them with different lags. Subtract them. O_K. Mm-hmm. Yeah. Uh-huh. O_K. And you find - So when one person is speaking  then wherever they happen to be at the point when they're speaking  then there's a pretty big maximum right around that point in the l- in - in the lag. They get peaks. So if - at whatever angle you are  O_K. O_K. So - so if there's two - at some lag corresponding to the time difference between the two there  you get this boost in the - in - in the cross-correlation value - function. And if there are multiple people talking  you'll see two peaks. It's spread out. Yeah. Well  let me ask you  if - if both people were over there  Yeah. Yeah. it would be less effective than if one was there and one was across  catty-corner? No? The- the - Oh  I'm sorry  if they're right next to one another? i- i- If I was - if I was here and Morgan was there and we were both talking  Next - next one over n- over on this side of the P_ - P_D_A. There we go. Right. Yeah. Yeah. it wouldn't work. Good example  the same one I'm asking. Yeah  e- I see. Versus you - versus - you know  and we're catty-corner across the table  and I'm farther away from this one and you're farther away from that one. Yeah. Yes. Yeah. Yeah. Or - or even if  like  It seems like that would be pretty strong. Yeah. Oh  yeah. Yeah. if people were sitting right across from each other  you couldn't tell the difference either. Across - the same axis  you don't have as much to differentiate. Yeah. Yeah. And so my point was just that it's - it's gonna be differentially - Well  we d- yeah  we don't have a third dimension there. Yeah  so it's - Right. differentially varia- valuable. I mean  it's not to say - I mean  I certainly think it's extremely val- And we - we humans n- n- depend on you know  these - these binaural cues. But. Yeah  yeah. But it's almost - but it's almost a - I think what you're talking about i- there's two things. There's a sensitivity issue  and then there's a pathological error uh issue. Must do. Yeah. So th- the one where someone is just right directly in line is sort of a pathological error. If someone just happens to be sitting right there then we won't get good information from it. Yes. Yeah. Yeah. O_K. and i- and if there - So it - And if it's the two of you guys on the same side - Uh  if they're - if they're close  it's just a question of the sensitivity. Yep. Yeah. So if the sensitivity is good enough - and we just - we just don't have enough  uh  experience with it to know how - O_K. Yeah yeah  O_K. But - Yeah. Yeah. Oh I'm not - I'm not trying to argue against using it  by any means. I just wanted to point out that - Yeah. Mm-hmm. that weakness  that it's topo- topologically impossible And I think Dan is still working on it. So. He actually - to get it perfect for everybody. he wrote me about it a little bit  so. Great. No  I don't mean to discourage that at all. I mean  the other thing you can do - uh  if - I mean  i- We're assuming that it would be a big deal just to get somebody - convince somebody to put two microphones in the P_D_A. But if you h- put a third in  you could put in the other axis. And then you know - then you're sort of - Once you got two - Interesting. Yeah  then - then you pretty much could cover - Yeah. Interesting. Well what about just doing it from these mikes? You know? Yeah. Yep. It will be more interesting to study the P_Z_M because the - the - the separation - I - I think - Uh @@ But- but that's - Then they're much broader. I mean  we can- we'll be - all of this is there for us to study. Yeah  we can do whatever we want. Yeah. But - but - but the thing is  uh  one of the - at least one of the things I was hoping to get at with this is what Whatever you're interested in. can we do with what we think would be the normal situation if some people get together and one of them has a P_D_A. Yeah. Yeah. Yeah. That's what I was asking about  what are the constraints? Right. Yeah. Yeah. Well  that's - that's the constraint of one question that I think both Adam and I were - were - were interested in. Yeah. Well - Mm-hmm. Yep. Uh  but - you know if you can instrument a room  Yeah. Mm-hmm. this is really minor league compared with what some people are doing  right? Some people at - at - uh  yeah  at Brown and - and - and - and - at uh Big micro @@ arrays. Yeah. Didn't they have something at Cape? Uh. um and at Cape  they both have these  you know  big arrays on the wall. And you know  if you could do that  you've got microphones all over the place Very finely. uh  you know p- tens of microphones  and - and uh - Oh! I saw a demo. Oh  right  oh  yeah. And if you do that then you can really get very nice uh kind of selectivity - Yeah. Oh  I saw one that was like a hundred microphones  a ten by ten array. And they had very precision. - Yeah. Hundred. And you could - In a noisy room  they could have all kinds of noises and you can zoom right in on somebody. Yeah. Yeah. Yeah. Right. Ye- Pretty much. Yeah. Very complex  uh - It was all in software and they - and you could pick out an individual beam and listen to it. Yeah. Yeah. That is cool. Yeah. It was - yeah  it was interesting. But  the reason why I haven't focused on that as the fir- my first concern is because um  I'm interested in what happens for people  random people out in some random place where they're p- having an impromptu discussion. And you can't just always go  ""well  let's go to this heavily instrumented room that we spent tens of thousands of dollars to se- to set up"". Yeah. Yeah. No  what you need to do is you'd have a little fabric thing that you unroll and hang on a wall. It has all these mikes and it has a plug-in jack to the P_D_A. Interesting. The other thing actually  that gets at this a little bit of something else I'd like to do  is what happens if you have two P_D_As and they communicate with each other? But I think - Yep. Yeah. And then - You know  they're in random positions  the likelihood that - I mean  basically there wouldn't be any - l- likely to be any kind of nulls  if you even had two. Ooo! That's on my web pages. Yeah. Network! Interesting. If you had three or four it's - Though - Interesting. All sorts of interesting things you can do with that  I mean  not only can you do microphone arrays  but you can do all sorts of um Yeah. Hmm. multi-band Yeah. Yeah. as well. Ah! So it's - it would be neat. I still like my rug on the wall idea  so if anybody patents that  then - But - I think - in terms of - Well  you could have strips that you stick to your clothing. Yeah. Yeah! Hats? In terms of the research th- research  it's really - it's whatever the person who is doing the research wants to do. Shirts. So if - if Jose is interested in that  that's great. But if - if Yeah. he's not  that's great too. Yeah. Yeah  yeah. Um  I - i- I - i- I would actually kind of like us to wind it down  see if we can still get to the end of the  uh  birthdays thing there. Catch some tea? So th- Um. Well  I had a couple things that I did wanna bring out. One is  do we need to sign new - these again? O_K. Well  it's slightly different. So I - I would say it would be a good idea. Cuz - it - it's slightly different. Are they new? Yep. Oh. Oh  this morning we didn't sign anything cuz we said that if anybody had signed it already  we didn't have to. Yeah  I - I should've checked with Jane first  but the ch- the form has changed. It's slightly different. Ah-oh. So we may wanna have everyone sign the new form. O_K. I had to make one - Um  I had some things I wanted to talk about with the thresholding stuff I'm doing. But  if we're in a hurry  we can put that off. Um and then also anonymity  how we want to anonymize the data. Well  should I - Uh. I mean I have some results to present  but I mean I guess we won't have time to do that this time. But it seems like um the anonymization is uh  is also something that we might wanna discuss in greater length. Um. I mean  wha- what - If - if we're about to wind down  I think - what I would prefer is that we uh  delay the anonymization thing till next week  and I would like to present the results that I have on the overlaps. We still have to do this  too  right? Right. Digits? Right. No- well  we don't have to do digits. Well  why don't we - Uh  so @@ O_K. @@ It sounds like u- uh  there were - there were a couple technical things people would like to talk about. Why don't we just take a couple minutes to - to briefly do them  and then - and then - and then - and then - and then we - O_K  go ahead  Jane. I'd - Oh  I'd prefer to have more time for my results. e- Could I do that next week maybe? O_K. Oh  yeah. Sure. O_K  that's what I'm asking. Oh yeah  yeah. And I think the anonymization  if y- if you want to proceed with that now  I just think that that's - that's a discussion which also n- really deserves a lo- a - you know  more that just a minute. I really do think that  because you raised a couple of possibilities yourself  you and I have discussed it previously  and there are different ways that people approach it  e- and I think we should - We could s- Mm-hmm. Alright. We're - we're just - We're getting enough data now that I'd sort of like to do it now  before I get overwhelmed with - once we decide how to do it Well  O_K. going and dealing with it. It's just - Yeah. O_K. I - I'll give you the short version  but I do think it's an issue that we can't resolve in five minutes. Mm-hmm. O_K  so - the - the short thing is um  we have uh  tape recording- uh  uh  sorry  digitized recor- recordings. Those we won't be able to change. If someone says ""Hey  Roger so-and-so"". Right. So that's gonna stay that person's name. Yep. Now  in terms of like the transcript  the question becomes what symbol are you gonna put in there for everybody's name  and whether you're gonna put it in the text where he says ""Hey Roger"" or are we gonna put that person's anonymized name in instead? No  because then that would give you a mapping  and you don't wanna have a mapping. O_K  so first decision is  we're gonna anonymize the same name for the speaker identifier and also in the text whenever the speaker's name is mentioned. No. I don't - Because that would give you a mapping between the speaker's real name and the tag we're using  I - I don't think you understood what I - what I said. So - and we don't want - O_K. uh  so in - within the context of an utterance  someone says ""So  Roger  what do you think?"" O_K. Then  uh  it seems to me that - Well  maybe I - uh it seems to me that if you change the name  the transcript's gonna disagree with the audio  and you won't be able to use that. We don't - we wanna - we ha- we want the transcript to be "" Roger "". Right  you don't wanna do that. Because if we made the - the transcript be the tag that we're using for Roger  Yeah. someone who had the transcript and the audio would then have a mapping between the anonymized name and the real name  and we wanna avoid that. O_K  well  but then there's this issue of if we're gonna use this for a discourse type of thing  then - and  you know  Liz was mentioning stuff in a previous meeting about gaze direction and who's - who's the addressee and all  Oh. then to have ""Roger"" be the thing in the utterance and then actually have the speaker identifier who was ""Roger"" be "" Frank ""  that's going to be really confusing and make it pretty much useless for discourse analysis. Ugh! That's a good point. Now  if you want to  you know  I mean  in some cases  I - I - I know that Susan Ervin-Tripp in some of hers  uh  actually did do uh  um  a filter of the s- signal where the person's name was mentioned  except Yeah And - and I - cer- and I - So  I mean  the question then becomes one level back. Um  how important is it for a person to be identified by first name versus full name? Well  on the one hand  uh  it's not a full identity  we're taking all these precautions  um and they'll be taking precautions  which are probably even the more important ones  to - they'll be reviewing the transcripts  to see if there's something they don't like - O_K. So  maybe  uh  maybe that's enough protection. On the other hand  this is a small - this is a small pool  and people who say things about topic X_ e- who are researchers and well-known in the field  they'll be identifiable and simply from the - from the first name. However  taking one step further back  they'd be identifiable anyway  even if we changed all the names. Right. Mmm. So  is it really  um - Ugh! You know? Now  in terms of like - so I - I did some results  which I'll report on n- next time  which do mention individual speakers by name. Mm-hmm. Now  there  the Human Subjects Committee is very precise. You don't wanna mention subjects by name in published reports. Now  it would be very possible for me to take those data put them in a - in a study  and just change everybody's name for the purpose of the publication. Yeah  once you get to the publication you can certainly do that. You can go  you know  uh  ""Z_"" uh  for instance. Uh. And someone who looked - Yeah  exactly. Doesn't matter if - Um  yeah  I mean  t- it doesn't - I mean  I'm not knowledgeable about this  but it certainly doesn't bother me to have someone's first name in - in the - in the transcript. That's the same thing you saw. O_K. @@ Uh  I think - you don't wanna have their full name to be uh  Yeah  and - and in the form that they sign  listed. And so - Yeah. it does say ""your first name may arise in the course of the meetings"". Well - Yeah. So again  th- the issue is if you're tracking discourse things  you know  if someone says  uh  uh  ""Frank said this"" and then you wanna connect it to something later  you've gotta have this part where that's ""Frank colon"". Right? Or ""your name"". Yeah  shoot! Yeah  and - and - you know  even more i- i- uh  immediate than that just being able to  uh - Well  it just seems like to track - track from one utterance to the next utterance who's speaking and who's speaking to whom  cuz that can be important. S- Mm-hmm. i- You know  ""You raised the point  So-and-so""  it's be kind of nice to Yeah. Shoot! be able to know who "" you "" was. And ac- I - I'm thinking too much. and actually you remember - furthermore  you remember last time we had this discussion of how you know  I was sort of avoiding mentioning people's names  and - and it was - and we made the decision that was kind of artificial. Well  I mean  if we're going to step in after the fact and change people's names in the transcript  we've basically done something one step worse. Yeah  I was too. Yeah. Yep. Yeah. Yeah. Well  I would sug- I - I - don't wanna change the names in the transcript  but that's because I'm focused so much on the acoustics instead of on the discourse  and so I think that's a really good point. Misleading. Yeah. You're right  this is going to require more thought. Yeah. L- let me just back up this to make a - a brief comment about the  uh  what we're covering in the meeting. Uh I realize when you're doing this that uh - I mean  I didn't realize that you had a bunch of things that you wanted to talk about. Uh  and so  uh - and so I was proceeding some- somewhat at random  frankly. So I think what would be helpful would be uh  i- and I'll - I'll mention this to - to Liz and Andreas too  that um  before the meeting if anybody could send me  any - any  uh  uh  agenda items that they were interested in O_K. Sure. and I'll - I'll take the role of organizing them uh  into - into the agenda  but I'd be very pleased to have everyone else completely make up the agenda. I've no desire to - to make it up  but if - if no one's told me things  then I'm just proceeding from my - my guesses  and - and uh  and i- ye- yeah  I - I'm sorry it ended up with your- out your time to - Mm-hmm. Oh  it's not a problem. Not a problem. Yeah. I mean  I'm just always asking Jose what he's doing  you know  and - and so it's - There's uh  there's obviously other things going on. I just - I just couldn't do it in two minutes. How will we - how would the person who's doing the transcript even know who they're talking about? Do you know what I'm saying? ""The person who's doing the transcript -"" The I_B_M people? Yeah. I mean  so- so - how is that information gonna get labeled anyway? How do you mean  who - what they're - who they're talking about? How do you mean? I mean  so if I'm saying in a meeting  ""oh and Bob  by the way  wanted - wanted to do so-and-so ""  They're just gonna write "" Bob "" on it or do @@ - if you're doing - Yeah  @@ they're just gonna write ""Bob"". And so. They won't be able to change it themselves. If you're - if you're doing discourse analysis  a- What ar- how are they gonna do any of this? Well  I - I'm betting we're gonna have huge chunks that are just totally un- Yeah  really. I mean  they're gonna say speaker-one  or speaker-two or speaker- I mean I - I - untranscribable by them. Well  the current one they don't do speaker identity. Yeah  I think - They can't do that. because in NaturallySpeaking  or  excuse me  in ViaVoice  it's only one person. and so in their current conventions there are no multiple speaker conventions. So it may just be one long transcript of a bunch of words. Oh. I think that - My understanding from Yen- Yep. Is it Yen-Ching? Is that how you pronounce her name? Oh  uh Yu-Ching? Yu-Ching? Uh Yu-Ching  Yu-Ching. Yeah. y- Yu-Ching. was that um  they will - that they will adopt the - part of the conventions that - that we discussed  where they put speaker identifier down. But  you know  h- they won't know these people  so I think it's - Well  they'll - they'll adopt some convention but we haven't specified to them - So they'll do something like speaker-one  speaker-two  is what I bet  but I'm betting there'll be huge variations in the accuracy of - of their labeling the speakers. We'll have to review the transcripts in any case. And it - and it may very well be - I mean  since they're not going to sit there and - and - and worry ab- about  uh  it being the same speaker  Yeah. they may very well go the - eh the - the first se- the first time it changes to another speaker  that'll be speaker- two. And the next time it'll be speaker- three even if it's actually speaker- one. You know - Uh-huh. You know  that would be a very practical solution on their part. Yeah. It's a good idea. And - and - but then we would need to label it. And that's O_K. Yeah. Yeah. Yeah  I think - Yeah we - we can probably regenerate it pretty easily from the close-talking mikes. Yes  I was thinking  the temp- the time values of when it changes. Yeah. Yeah. So. But I mean that doesn't - Yeah. But that - That'd be very efficient. This doesn't answer the - the question. The p- It's a good point  ""which - what do you do for discourse tracking?"" Because y- y- you don't know to know  eh - Hmm. you don't need to know what i- what is the iden- identification of the - of the speakers. You only eh want to know - For - for acoustics you don't but for discourse you do. Well  you do. Ah  for discourse  yeah. Yeah. If - if - if - if someone says  uh  ""what - what is Jose doing? "" and then Jose says something  you need to know that that was Jose responding. Yeah. Yeah. Yeah  yeah. Yeah. Yeah. Yeah  yeah  yeah. Yeah. Uh  so. Yeah  Mm-hmm. @@ Yeah. Ugh  that's a problem. Unless we adopt a different set of norms which is to not id- to make a point of not identifying people by name  which then leads you to be more contextually ex- explicit. That would be hard. Well  people are very flexible. You know? I mean  so when we did this las- last week  I felt that you know  now  Andreas may  uh  @@ uh  he - he - i- sometimes people think of something else at the same time and they miss a sentence or something  and - and because he missed something  then he missed the r- the initial introduction of who we were talking about  and was - was unable to do the tracking. But I felt like most of us were doing the tracking and knew who we were talking about and we just weren't mentioning the name. So  people are really flexible. Mm-hmm. Yeah. But  you know  like  at the beginning of this meeting - Or  you I think said  you know  or s- Liz  said something about um  uh  ""is Mari gonna use the equipment? "" Yeah? I mean  how would you say that? I mean  you have to really think  if you wanted to anonymize. you know  about what you're saying bef- Yeah. ""Is you know who up in you know where?"" Right? Use the - Yeah. Yeah. Mm-hmm. I think it would be really hard if we made a policy where we didn't say names  Yeah  darn! I mean  what I was gonna say is that the other option is that we could bleep out the names. Yeah  is - Well  it- plus we'd have to tell everybody else. Yeah. but then  again that kills your discourse analysis. Ugh! Right. Uh-huh. Yeah. Yeah. Yeah. Yeah. That's - that's the issue. I - I think the - I think - I don't know  my own two cents worth is that you don't do anything about what's in the recordings  you only anonymize to the extent you can  Well  but that- but that - as I said  that - that - that works great for the acoustics  but it - it hurts you a lot for trying to do discourse. the speakers have signed the forms and all. Well. Mm-hmm. Yeah. Th- Why? Yeah. Because you don't have a map of who's talking versus their name that they're being referred to. Bec- Yeah. I thought we were gonna get it labeled speaker-one  speaker-two - Sure but  h- then you have to know that Jose is speaker-one and - Why do you have to know his name? O_K  so suppose someone says  ""well I don't know if I really heard what - uh  what Jose said."" Yeah. Yeah. And then  Jose responds. And part of your learning about the dialogue is Jose responding to it. Yeah. But it doesn't say ""Jose""  it says ""speaker- five "". Yeah. O_K. So uh u- Yeah. Oh  I see  you wanna associated the word ""Jose"" in the dialogue with the fact that then he responded. Right. Someone who's doing discourse would wanna do that. And so  if we pass out the data to someone else  and it says ""speaker- five "" there  we also have to pass them this little guide that says that speaker-five is Jose  and if were gonna do that we might as well give them ""Jose"" - say it was ""Jose"". And that violates our privacy. Yeah. Yeah. And that violates our privacy issue. Mm-hmm. Yeah. Yeah. Yeah. Now  I - I think that we have these two phases in the - in the data  which is the one which is o- our use  University of Washington's use  I_B_M  S_R_I. Yeah. And within that  it may be that it's sufficient to not uh change the - to not incorporate anonymization yet  Mm-hmm. but always  always in the publications we have to. And I think also  when we take it that next step and distribute it to the world  we have to. But I - but I don- that's - that's a long way from now and - and it's a matter of - between now and then of d- of deciding how - Making some decisions? i- i- it - You know  it may be s- that we- we'll need to do something like actually X_ out that part of the um - the audio  and just put in brackets ""speaker-one"". Yeah. For the public one. the ?? You know  what we could do also is have more than one version of release. One that's public You know. Yeah. and one - one that requires licensing. And so the licensed one would - w- we could - it would be a sticky limitation. Uh-huh. I think that's risky. I think that the public should be the same. You know  like - Well  we can talk about that later. I think that when we do that world release  it should be the same. I - I agree. For a bunch of reasons  legal. I - I agree with Jane. I - I think that we - we have a - need to have a consistent licensing policy of some sort  But I also think a consistent licensing policy is important. and - Yeah. Well  one thing to- to take into consideration is w- are there any um - For example  the people who are funding this work  they want this work to get out and be useful for discourse. If we all of a sudden do this and then release it to the public and it's not longer useful for discourse  Well  depending on how much editing we do  you might be able to still have it useful. you know - because for discourse you don't need the audio. Right? So you could bleep out the names in the audio. Mm-hmm. and use the anonymized one through the transcript. Excuse me. We - we do need audio for discourse. Uh. But if you release both - But  n- excuse me  but you could bleep out just the names. She - No  but she's saying  from the argument before  she wants to be able to say if someone said ""Jose"" in their - in their thing  and then connect to so- to what he said later  then you need it. Right. But in the transcript  you could say  everywhere they said "" Jose "" that you could replace it with ""speaker- seven "". Oh I see. Yeah. But I - I also wanna say that people - I see. And then it wouldn't meet - match the audio anymore. But it would be still useful for the - Uh-huh. But if both of those are publically available - Yeah. That's good. But they - Right. Well  you see? So  it's complicated. And th- and the other thing is if - if - if Liz were here  what she might say is that she wants to look if things that cut across between the audio and the dialogue  and so  uh  yeah. Mm-hmm. Yeah. I think we have to think about w- Sorry. @@ how. I think that this can't be decided today. Yeah  O_K  good point. But it's g- but I think it was good to introduce the thing and we can Yeah. O_K. I didn't think - when I wrote you that email I wasn't thinking it was a big can of worms  but I guess it is. do it next time. O_K. Yeah  a lot of these things are. Well it - Discourse  you know - Also I wanted to make the point that - that discourse is gonna be more than just looking at a transcript. It's gonna be looking at a t- Discourse. Yeah  ab- absolutely. Oh  yeah  sure. You know  and prosod- prosodic stuff is involved  and that means you're going to be listening to the audio  and then you come directly into this - confronting this problem. Maybe we should just not allow anybody to do research on discourse  So. and then  we wouldn't have to worry about it. O_K. Yeah  we should just market it to non-English speaking countries. Uh  maybe we should only have meetings between people who don't know one another and who are also amnesiacs who don't know their own name. O_K. Did you read the paper on Eurospeech? We could have little labels. I - I - I wanna introduce my Reservoir Dogs solution again  which is everyone has like ""Mister White""  ""Mister Pink""  ""Mister Blue"". Mister White. Yeah. Did you read the paper a few years ago where they were reversing the syllables? They were di- they- they had the utterances. and they would extract out the syllables and they would play them backwards. But - so  the syllables were in the same order  with respect to each other  but the acous- Everything was in the same order  but they were - the individual syll- syllables were played backwards. And you could listen to it  and it would sound the same. What did it sound like? People had no difficulty in- interpreting it. So what we need is something that's the reverse  that a speech recognizer works exactly the same on it but people can't understand it. What? Oh  well that's - there's an easy way to do that. Jus- jus- just play it all backwards. Oh right. The speech recognizer's totally symmetric  isn't it. What  what does the speech recognizer care? Ah  anyway. Um  Oh  do we do digits? Or - ? What do we do? Let's do digits. Yeah  we - we - we already missed the party. So. Uh - O_K  we'll quickly do digits. Or do we just quit? O_K. Yeah. Uh  reading one four seven one one four nine zero nine zero one one five two four three O_ three nine four two five four five six O_ O_ eight four eight three three nine three six O_ three seven O_ two zero eight three uh  strike that zero eight six three one one eight two three zero five three nine O_ O_ six two eight three seven five one six eight seven two nine zero zero three nine nine eight nine two seven six Transcript one five three one dash one five five zero one two zero five zero eight three zero five three six six seven seven five eight nine eight nine six O_ zero zero one three one two one O_ three five four two nine one four five seven O_ two six seven eight O_ three O_ eight O_ O_ one zero one one four nine one zero zero three Transcript one six three one dash one six five zero. five O_ seven two eight five nine eight six eight seven O_ eight three O_ nine one one zero eight four one two O_ four three five two six five eight seven seven two seven eight nine O_ O_ eight O_ three one three one two six three three four seven one five Transcript one three three one dash one three five O_ three zero five three nine six eight zero four six two seven six eight eight nine O_ nine seven six nine nine four six O_ zero zero three two one O_ O_ five three four four two seven five zero four six seven three five eight nine five seven two nine eight two three six seven eight zero eight eight O_ two nine zero four seven one eight O_ nine two six five three nine O_ three Transcript one six seven one dash one six nine O_ seven eight nine zero one two two one two six one two three three seven eight four five six O_ eight one seven nine seven six three zero O_ seven O_ O_ four zero eight six eight seven one three one two three O_ three O_ five three three zero six six five eight seven eight two O_ seven O_ O_ I think it would be fun sometime to read them with different intonations. O_K  go off here. like as if you were talking like  ""nine eight six eight seven?"" Well  you know  in the - in the one I transcribed  I did find a couple instances - I found one instance of contrastive stress  where it was like the string had a - li- So it was like ""nine eight two four  nine nine two four"". Oh  really. So they were like looking ahead  huh? And - Well  they differed. I mean  at that - that session I did feel like they did it more as sentences. But  um  sometimes people do it as phone numbers. I mean  I've - I am sort of interested in - in - And sometimes  you know  I s- Yeah  yeah. Yep. And I - I never know. When I do it  I - I ask myself what I'm doing each time. and I - Well  I was thinking that it must get kind of boring for the people who are gonna have to transcribe this They may as well throw in some interesting intonations. Well  except  yeah. We have the transcript. We have the actual numbers they're reading  so we're not necessarily depending on that. O_K  I'm gonna go off. I like your question intonation. That's very funny. I haven't heard that one. ",The ICSI Meeting Recorder Group at Berkeley met once more to discuss group members' progress. The majority of the group are working on tasks related to the Aurora Project  including on-line normalization and Wiener filtering. Other progress was also reported. A large part of the meeting was spent discussing calculations and approaches using the white-board in the room. At me013's behest  the group need to look closer at the errors made in tests on the aurora project  because the error rate may not be telling the whole picture. Mn052 volunteers to run some experiments into how different numbers of MFCCs affect results. Some previously reported results from me026 were determined to be garbage due to a bug in the code  Speaker mn052 also feels that his strange results are down to a bug. This week  speaker mn007 has mostly been focusing on trying different approaches to on-line normalization  but making little impact on results. He has also been playing with thresholding  effectively adding white noise to the data  but again with minimal affect. Also making little improvement is fn002's work with Vectorial Taylor series  as a means of dealing with noise. Mn052 has been adding Wiener filtering to the aurora task  and is thinking about future work on subspace. Speaker me026 has been investigating phase normalization  and the possibility of adding spectral subtraction to an existing system. Speaker me006 is still planning some cheating experiments to investigate features for recognition  alongside preparing for his quals. also  the groups submission to the Eurospeech conference has been accepted. 
"Alright  so I'm- I should read all of these numbers? eight five  five one  one four  eight one  nine eight. eight zero seven nine  six  six eight five. seven six nine one  nine four nine three  four seven seven one. two four six  zero zero  seven five three three. three two two  six three  eight eight one zero. seven  four nine one  nine five  four eight nine  six. zero nine seven  three four  seven one six three. five six one  six one  nine seven three one. O_K. O_K. Transcript L_ eighty-seven. eight six four  six six four  one seven one eight O_ three  two two five  two O_ two three one one  six one one  seven four two four O_ five eight  eight four two  one eight three seven six nine one seven  five  two one four three five  nine zero  nine seven  six one  six nine six six zero  two eight  one two O_ nine eight one three  O_ four  five O_ seven eight. Transcript L_ eighty-five. five seven three  six seven one  three four eight four five four five eight  five  nine nine six five four seven  three two O_  one eight three seven eight two  two one  nine O_  nine two  O_ six. six seven seven  two six  four six one six. four four four  three nine seven  five four one one. O_ five three  three two six  four three eight four eight five  one six  seven five five seven. Transcript number L_ eighty-four. four O_ one  four five  two three two four. one six  one eight  one eight  nine seven  three five three three four  zero four four  four six two two zero  nine zero  four six  seven zero  four five nine eight four  one six  eight zero one one one four one  three one four  four seven three six four six seven  one two nine seven  eight three three three four zero  nine three  zero three  nine nine  nine seven Transcript L_ eighty-three. nine two four seven  six nine four five  five seven eight seven zero nine nine  eight eight two  nine four five three zero  nine seven one  five four  one four two  seven six six eight  six one eight  seven six eight zero seven two  seven four two  four nine nine four six  nine six seven  zero seven  four two nine  four two one four two  two  five five zero zero six  four six  eighty-two - Oh  excuse me. eight two  four five  four six. Piece of paper? I could borrow? Oh yeah. O_K  so uh i- um I don't know whether Ami's coming or not um Nancy but I think we oughta just get started. is uh- Nancy's still stick? currently in Berkeley but not here? Don't know. O_K. Anyway Oh  so there you go. Anyway  so my idea f- for today and we can uh decide that that isn't the right thing to do was to at - spend at least part of the time trying to eh build the influence links  you know which sets of things are uh relevant to which decisions and actually I had uh specific s- suggestion to start first with the path ones. The database ones being in some sense less interesting to us although probably have to be done and so to do that so there's - and the idea was we were gonna do two things Is your mike on? Oops. Ah. Oh right  well. That's funny. Yeah. We were gonna do two things one of which is just lay out the influence structure of what we think influences what and then as a uh separate but related task uh particularly Bhaskara and I were going to try to decide what kinds of belief nodes are needed in order to um do what we - what we need to do. Once so but du- we should sort of have all of the uh basic design of what influences what done before we decide exactly how to compute it. So I didn't - did you get a chance to look at all - yet? Yeah  I looked at some of that stuff. Great. O_K so let's start with the uh belief-nets  the general influence stuff and then we'll - then we'll also at some point break and talk about the techy stuff. Well I think one could go there's I think we can di- discuss everything. First of all this I added  I knew from sort of basically this has to be there right? Um Oh are you gonna go there or not? Yeah  so one i- Given - given uh uh not transverse the castle  the decision is does the person want to go there or is it just Right  true. Does have to be there. And And I'm sure we'll find more as we go that Hmm? So Go-there in the first place or not is definitely uh one of the basic ones. We can start with that. Interesting effect. Um Is this basically true or false or maybe we'll get Well - Which one? what? ""Go there"". m- right. Here we so there is this question about we actually get just probabilities  right for each down here. Yeah. When we're - yeah when we're done. Hmm. So - so @@ the - the reason it might not be true or false is that we did have this idea of when so it's  you know uh current @@ and so forth and so on or not at all  right? Mm-hmm. And so that a decision would be do we want that so you could - two different things you could do  you could have all those values for Go-there or you could have Go-there be binary and given that you're going there When. when. How. Yeah and so forth . Why  yeah. So I'll let we'll see. Hmm? I mean it seems that you could um uh it seems that those things would be logically independent like you would wanna have them separate or binary  Go-there and then the - the possibilities of how to go there because - O_K  that's - let's start that way. because  you know it might be easy to figure out that this person is going to need more film eventually from their utterance but it's much more complex to query Hmm. when would be the most appropriate time. Hmm. O_K. And so I've tried to come up with some initial things one could observe so who is the user? Everything that has user comes from the user model everything that has situation comes from the situation model-A_. We should be be clear. But when it comes to sort of writing down when you - when you do these things is it here? You sort of have to a- write the values this can take. Right. And here I was really uh in some s- sometimes I was really sort of standing in front of a wall feeling very stupid because um - this case it's pretty simple  but as we will see the other ones um for example if it's a running budget so what are the discrete values of a running budget? So maybe my understanding there is too impoverished. Hmm. No uh How can I write here that this is something  a number that cr- keeps on changing? But O_K. Thus is understandable? Yes. Think so. So here for example. You've s- have you seen this before at all Keith  these belief-net things? Uh  no  but I think I'm following it. So here is the - the - we had that the user's budget may influence So far. the outcome of decisions. Yeah. Hmm. There we wanted to keep sort of a running total of things. Is this like a number that represents how much money they have left to spend? O_K  h- well I mean how is it different from user finance? Um the finance is sort of here thought of as - as the financial policy a person carries out in his life  he - is he cheap  average  or spendy? Alright. And um I didn't come uh maybe a user I don't know  I didn't want to write greediness  but Yeah. Hmm. Welcome. Or cheapness. Welcome. User thrift. Yeah. Thrift  that's good. Great. There it is. Yeah. So Keith w- what's behind this is actually a program that will once you fill all this in actually s- solve your belief-nets for you and stuff. So this is not just a display  this is actually a GUI to Mm-hmm. a simulator that will if we tell it all the right things we'll O_K. wind up with a functioning belief-net at the other end. O_K. And it's so simple even I can use it. Wow  that is simple. O_K  so here was O_K  I can think of uh people being cheap  average  or spendy or we can even have a - a finer scale moderately cheap  doesn't matter. Doesn't matter. Agree there but here um I wasn't sure what to write in. Let's - go ahead. Well  I mean you've written in - you've written in what uh seems to be required like what else is - is do you want? If that's permissible then I'm happy. Well yeah. So here's - here's what's permissible is that you can arrange so that the um the value of that is gonna have to be updated and n- it's not a belief update  right? It's - you took some actions  you spent money and stuff  so the update of that is gonna have to be essentially external to the belief-net. Yeah. Right? And then what you're going to need is uh for the things that it influences. Well let's - first of all let's see if it does influence anything. And if it does influence anything then you're gonna need something that converts from the - the number here to something that's relevant to the decision there. So it could be ra- they create different ranges that are relevant for different decisions or whatever - but for the moment this is just a node that is conditioned externally and might influence various things. Hmm. Yeah - this is where um O_K anyways let's forget it. Well that's fine. Well anyway  go ahead. O_K  and so this  oh that The other thing is that um every time that's updated beliefs will have to be propagated but then the question is do you - do we wanna propagate beliefs every single time it's updated or only when we need to? Yeah  that's a good question. And uh does it have a lazy mode? I don't remember. Uh Well  I mean  in Srini's thing there was this thing - there was this um Oh right. option like proper inferences which suggests that uh Yeah. S- probably does. doesn't happen  automatically. Yeah someone has to track that down  but I - but uh I just accidentally And - and - and I think - actually uh Oops. one of the we- w- items for the uh user home base uh should be uh essentially non-local. I- they're only there for the day and they don't have a place that they're staying. Well Oh just uh accidentally erased this  I - I just had values here such as uh um is he s- we had in our list we had ""Is he staying in our hotel?""  ""Is he staying with friends?""  and so forth uh Yeah. so we're O_K. So it's clear where w- w- w- where we are right now. So my suggestion is we just pick uh Something down here? one  you know one uh particular one of the uh well let's do the first - first one let's do the one that we sort of already think we did so w- that was the - of the endpoint? Mm-hmm. And um Oops. Is hmm Ah  O_K. So it's true or false? No no no  E_V_A. No  that's - that's a @@ Missed that one. So What's the difference between mode I thought mode  yeah. and endpoint? Um mode was um although that Well  that's @@ Mode of transportation? Yeah. O_K. Also true or false. Mm-hmm. No  he has- he hasn't filled them in yet  is what's true. Yeah  O_K. Did I or didn't I? Ah. Probably nothing done yet  oh I just did it on the upper ones  O_K. Makes sense. O_K  so this was E_V_A. Maybe we can think of more things  cross Yeah. O_K. Climb  rob. climb  emerge No no no  these are ju- that's just a point  this is ju- Uh Well some of those are subsumed by approach. Would it be an endpoint if you were crossing over it? Yeah  would be a f- for a given segment. The Charles Bridge  you know. You know  you - y- you go - first go the town square Well I eh No  I mean  if you go to re- you know if you go to Prague or whatever one of your - your key points that you have to do is cross the Charles Bridge and doesn't really matter which way you cross which - where you end up at the end but the part - the good part is walking over it  so. That's subtle  but true. Anyway so let's just leave it three - with three for now Mm-hmm  mmm. Yeah. and let's see if we can get it linked up just to get ourselves started. O_K  we You'll see it - you'll see something comes up immediately  that the reason I wanna do this. w- well the uh user was uh Right. definitely more likely to enter if he's a local Right. more likely to view if he's a tourist um and then of course we had the fact that given the fact that he's thrifty and there will be admission then we get all these cross um We did  but the three things w- that - that it contributed to this in fact  the other two aren't up there. so one was the ontology We'll d- what type of building is it? Yeah. Yeah. And the - and the third thing we talked about was something from the discourse. What he has mentioned before. O_K  so this is w- Right  so what w- I - what we seem to need here  this is why it starts getting into the technical stuff mm-hmm the way we had been designing this  there were three intermediate nodes uh which were the endpoint decision as seen from the uh user model as seen from the ontology and as seen from the discourse. So each of those the way we had it designed  now we can change the design  but the design we had was there was a decision with the same three outcomes mm-hmm uh based on the th- those three separate considerations so if we wanted to do that Uh we can load it up it you know very simple. So- would have to put in uh three intermediate nodes and then what you and I have to talk about is  O_K if we're doing that and they get combined somehow uh how do they get combined? But the - they're - they're undoubtedly gonna be more things to worry about. So this was adjusted for this one mode thing. Oh yes. Yeah. So that's w- w- in our uh in - in Johno's sort of pictogram everything that could contribute to whether a person wants to enter  view  or approach something. Oh  it was called mode  so this - this is m- mode here means the same as Is now this endpoint. endpoint. Right. We can just rename that  yeah. O_K  why don't we ch- can we change that? Alright. You know  but that was actually  yeah unfortunately that was a um kind of an intermediate versio- that's I don't think what we would currently do. Can I ask about ""slurred"" and ""angry"" as inputs to this? That's a What - why? Like they're either true or false and The prosody? O_K. they uh oh I see. If the - if the person talking is angry or slurs their speech Mm-hmm. they might be tired or  you know O_K. Drunk. Therefore And  you know  possibly uh Less likely to enter. Hmm. some  yeah. uh I was thinking less likely to view Yeah. But that's- that seems to  yeah. So - so my advice to do is - is get this down to what we think is actually likely to - to be a - a strong influence. O_K. Right. But yeah  that was what he had in mind. So let's think about this - this question of how do we wanna handle - so there're two separate things. One is - uh at least two. One is how do we want to handle the notion of the ontology now what we talked about  and this is another technical thing Bhaskara  is uh can we arrange so that I think we can so that the belief-net itself has properties and the properties are filled in uh from on- ontology items. So the - let's take the case of the uh this endpoint thing  the notion was that if you had a few key properties like is this a tourist site  you know some kind of landmark is it a place of business uh is it something you physically could enter Mm-hmm. O_K  et cetera. So that there'd be certain properties that would fit into the decision node and then again as part of the ou- outer controlling conditioning of this thing those would be set  so that some- somehow someone would find this word  look it up in the ontology  pull out these properties  put it into the belief-net  and then the decision would flow. Mm-hmm. Now Seems to me that we've sort of e- em- embedded a lot  em- embedded a lot of these uh things we had in there previously in - in - in some of the other final decisions done here  for example if we would know that this thing is exhibiting something Right. Right. um if it's exhibiting itself it is a landmark  meaning more likely to be viewed Yeah. Yep. if it is exhibiting pictures or sculptures and stuff like this  then it's more likely to be entered. I uh that's - I think that's completely right and um I think that's good  right? So what - what that says is that we might be able to uh take and in particular so - so the ones we talked about were uh exhibiting and selling Accessibility. no  accessibility meant If it's closed one probably won't enter. Or if it's not accessible to a tourist ever O_K. the likelihood of that person actually wanting to enter it  given that he knows it  of course. Alright. So let me suggest this. Uh w- could you move those up about halfway. Uh The ones that you th- And selling I guess. Yeah  all - all of these if it's fixing things selling things  or servicing things Right. So here - here's what it looks like to me. is that you want an intermediate structure which i- uh is essentially the or of uh for this purpose of - of uh selling  f- fixing  or servicing. So that it uh that is  for certain purposes  it becomes important but for this kind of purpose uh one of these places is quite like the other. Does that seem right? So we di- if we Basic- you're basically just merging those for just the sake of endpoint decision? Yeah. Yes. So if - well it may be more than endpoint decisions  so the idea would be Mm-hmm. that you might wanna merge those three These three? Yeah. Eh ser- s- uh selling  fixing  and servicing. Yeah. What ex- um and so either those is true f- or false? So Uh Uh well it - it - i- here's where it gets a little tricky. Uh from the belief-net point of view it is from another point of view of course it's interest- it's - it's important to know what it's selling or servicing and so forth. Yeah. O_K. So for this decision Yeah. it's just uh true or false and in th- this is a case where the or seems just what you want. O_K. That - that if any of those things is true then it's the kind of place that you uh Um more likely to enter. are more likely to enter. So you just wanna have them all pointing to a You could  yeah. summary thing? Yeah  so let's do that. No no  no eh to - to an inter- no  an intermediate node. T- Oh  O_K. That's the p- part of the idea  is Um is - is that the object type node? I d- So are they the - is it the kind of object that sells  fixes  or services things? Well  o- open up object type and let's see what its values are. Oh I just created it  it has none so far. Oh  well O_K first of all it's not objects  we called them entities  right? Yeah. And then we have sort of the um Let's say I put commercial. Yeah  I w- I was just gonna commercial action inside where people p- Well couldn't I do - let's do commercial uh landmark and And where was the Well accessible  yeah. accessible I think is different Yeah. cuz that's tempor- that - that varies temporally  whereas Mm-hmm. this is a What would a hotel fall under? I would call that a service  but - but I don't know. Well I mean in terms of entity type? Say w- w- well it's co- I would s- a- a- again for this purpose I think it's commercial. Someplace you want to go in to do some kind of business. O_K. Um what does the underscore-T_ at the end of each of those things signify? Um things. So places that service things Uh-huh. sell things or fix things and pe- O_K. places that e- exhibit things. O_K. That also points to entity type I guess. So we're deriving um this - the - this feature of whether the - the main action at this place happens inside or outside or what we're deriving that from what kind of activity is done there? Couldn't you have it as just a primitive feature of the Well you could  that's a - that's a choice. entity? O_K. So uh I mean it seems like that's much more reliable cuz you could have outdoor places that sell things and you know indoor places that do something else and Yeah  the problem with it is that it sort of putting in a feature just for one decision  now w- we may wind up having to do that Hmm. this i- anyway  this i- at a mental level that's what we- we're gonna have to sort out. O_K. O_K. So  you know what does this look like  what are - what are uh intermediate things that are worth computing  what are the features we need Mm-hmm. in order to make all these decisions and what's the best way to organize this so that um it's clean and - and O_K. consistent and all that sort of stuff. I'm just thinking about how people  human beings who know about places and places to go and so on would store this and it would probably - you wouldn't just sort of remember that they sell stuff and then deduce from that that Well I think an entity maybe should be regard as a vector of several possible things  it can either it must be going on inside or something. em do s- do sell things  fix things  service things  exhibit things  it can be a landmark at the same time as doing these things  it's not either or mmm Mm-hmm. certainly a place can be a hotel and a famous site. Mm-hmm. Many come to mind. Things can be generally um a landmark and be accessible. I_E a - a castle or can be a landmark a- or not accessible  some statue you know can Mm-hmm. go inside. O_K. Anyway so let me suggest you do something else. Uh which is to get rid - get rid of that l- long link between who - the user and the endpoint. Could we just move it like this? No no  I don't want the link there at all. Oh  O_K. Because what we're gonna want is an intermediate thing which is uh the endpoint decisi- the endpoint decision based o- on the user models  so what we - we - what we talked about is three separate endpoint decisions  so let's make a new node Yeah. Yeah. Just as a suggestion maybe you could ""save as"" to keep your old one nice and clean and so you can mess with this one. Mmm. The old one was not that not that important  I think but O_K  well  not a big deal then. Let's do it then. Well the - Isn't there a ""save as"" inside of java base? But I can just take this O_K. copy it somewhere else. This was user something or Well this was uh let's p- put it this - let's do endpoint underbar-U_. end point? i- endpoint  e- end poi- this is sa- Ah. it's the endpoint Gotcha  yeah. let's say underbar-U_  so that's the endpoint decision uh as seen through the As related from the user model. Right. So let's - let's actually yeah so lin- you can link that up to the Should I rename this too? No. uh yeah  so that  I guess that's endpoint uh It's underscore-E_. underscore-E_ for entity  and we may change all this  but. Right. And O_K  shouldn't I be able to move them all? No. Or - ? Can I? Mmm-mm. Where? What? Oh I d- eh I don't know. Actually  I guess the easiest thing would move - mo- move the endpoint  well  go ahead. Just do whatever. Wasn't this possible? Well. Yeah. I think you have to be in move mode before Uh-huh. O_K. Good. Right. So now we're looking for user related things that um Yeah. And uh maybe th- maybe it's just one who is the user  I don't know  maybe - maybe there's more. Huh. Well if he's usi- if he's in a car right now what was that people with Harry drove the car into the cafe Never mind. Uh anyway  this is crude. Now but the - now so - so - but then the question is uh so - and - and we assume that some of these properties would come indirectly through an ontology  but then we had this third idea of input from the discourse. Well let's - should we finish this  I mean but surely the user interests Sure  O_K. The user thrift  the user budget. yeah  yeah Well  maybe  I- again  I d- well  O_K  put em in but what we're gonna wanna do is actually uh Well is - Here this was one of my problems we have the user interest is a - is a vector of five hundred values  so um Oh you mean level of interest? That's from the user model  mm-hmm  no not levels of interest but things you can be interested in. Well - somebody else has built this user model. Gothic Oh I see  right. churches versus Baroque townhouses versus So why is it oh it  so it's like a vector of five hundred ones or zeros? Yea- n- is that Like for each thing are we - are you interested in it or not? yeah uh I - I think I see. Hmm. O_K. So uh you cou- and so here let me give you two ways to handle that. Alright? One is um you could ignore it. But the other thing you could do is have an - and this will give you the flavor of the - of what @@ you could have a node that's - that was a measure of the match between the Mm-hmm. object's feature  you know  the match between the object the entity  I'm sorry Uh. and the user. So you could have a k- a ""fit"" node and again that would have to be computed by someone else Mm-hmm. but uh so that uh Just as a mental note uh Yeah  that's all. Mm-hmm. And - and should we say that this interests eh affects the likelihood of - of entering? Yeah. Yeah. I mean  we could. And also if it's an expensive place to enter  this may also O_K. Budget. User schedule. Schedule? ""Do I have time to go in and climb all the way to the top of the Koelner Dome or do I just have to -"" Right. ""time to take a picture of the outside?"" It seems like everything in a user model a- affects - Well that's what we don't wanna do  see that - se- cuz then we get into huge combinatorics and stuff like that Yeah. Mm-hmm. an- Cuz if the  I mean  and if the user is tired  the user state  right  it would affect Well stuff  but Well  but I can't see why e- anything w- everything in the model wouldn't be Right. Well  that - that's - we can't do that  so we- we're gonna have to Yeah. but this is a good discussion  we're gonna have to somehow figure out uh some way to encapsulate that uh so if there's some general notion of for example the uh relation to the time to do this to the amount of time the guy has or something like that is - is the uh compatibility with his current state  so that's what you'd have to do  you'd have to get it down to something which uh was itself relatively compact  so it could be compatibility with his current state which would include his money and his time and - and his energy Yeah  just seems like it'd push the problem back a level. Yeah  but Right. It does. Mm-hmm. No but  it's more than that  like the - the more sort of you break it up @@ like because if you have everything pointing to one node it's like exponential whereas if you like keep breaking it up more and more it's not exponential anymore. So it yeah  there are two advantages. That's tha- there's one technical one Sh- sh- yeah  - and the other is it - it gets used S- so we'd basically be doing subgrouping? Subgrouping  basically into mo- Yeah. so basically make it more tree like going backwards? Right. Right. Yeah. But it - there's two advantages  one is the technical one that you don't wind up with such big exponential @@ Bhaskara? uh C_B_T's  the other is it can be - it presumably can be used for multiple decisions. Mm-hmm. So that if you have this idea Right. of the compatibility with the requirements of an action to the state of the user one could well imagine that that was u- not only is it sim- is it cleaner to compute it separately but it could be that it's used in multiple places. Anyway th- so in general this is the design  this is really design problem. Yeah. O_K  you've got a signal  a d- set of decisions um how do we do this? What do I have under user state anyhow cuz I named that already something. Oh that's tired  fresh  yeah. Maybe should be renamed into physical state. Or fat- user fatigue even. Hmm. That's with a ""G_""? Mm-hmm. Whatever. Then we can make a user state. What's th- what we're talking about is compatibility. Uh or something  I don't know  but. I guess the - the question uh is It's hard for me to imagine how everything wouldn't just contribute to user state again. Or user compatibility. Oh but the thing is that we uh uh we had some things that uh That don't. that don't The user interests and the user who - who - who the user is are completely apart from the fact whether he is tired broke Sure  but other - I thought though the node we're creating right now is user compatibility to the current action  right? the right Seems like everything in the user model would contribute to whether or not the user was compatible with something. Uh maybe not. I mean the - that's the - the issue is um would Even if it was true in some abstract general sense it might not be true in terms of the information we actually had and can make use of. And anyway we're gonna have to find some way to cl- uh get this sufficiently simple to make it feasible. Maybe um if we look at the - if we split it up again into sort of um if we look at the uh the endpoint again we - we said that for each of these things there are certain preconditions so you can only enter a place if you are not too tired to do so and also eh have the money to do so if it costs something so if you can afford it and perform it is preconditions. Viewing usually is cheap or free. Is that always true? I don't know. Mm-hmm. Well  with the way we're defining it I think yeah. W- w- but that eh viewing it without ent- yeah view w- with our definition of view it's free cuz you And so is approaching. Yeah. Well what about the Grand Canyon  right? No  never mind. I mean are there - are there large things that you would have to pay to get up close to like  I mean never mind  not in the current - No we have to enter the park. O_K. Eh almost by definition Yeah. um paying involves entering  ge- going through some O_K. Right  sure. Right. Uh So let me suggest we switch to another one  I mean clearly there's more work to be done on this but I think it's gonna be more instructive to - to think about Mm-hmm. uh other decisions that we need to make in path land. And what they're gonna look like. So you can save this one as and open up the old one  right and - and then everything would be clean. You could do it again. Why  I think it's worth saving this one but I think I'd - I'd like to keep this one cuz I wanna see if - Yeah. if we're gonna reuse any of this stuff. Mm-hmm. Yeah. Um so this might be What next? Well you tell me  so in terms of the uh Well let's - th- this planner what's - what's a good one to do? go there or not I think is a good one. Uh Is a very basic one. So what makes things more likely that - So Well the fir- see the first thing is  getting back to thing we left out of the other is the actual discourse. So Keith this is gonna get into your world because Mm-hmm. uh we're gonna want to know you know  which constructions indicate various of these properties s- Mm-hmm. and so I - I don't yet know how to do this  I guess we're gonna wind up pulling out uh discourse properties like we have object properties and we don't know what they are yet. Mm-hmm. So that - that the Go-there decision will have a node from uh discourse  and I guess why don't we just stick a discourse thing up there to be as a placeholder for We - we also had discourse features of course for the endpoint. Of - of course. Identified that Yeah. and so again re- that's completely correct  we have the user model  the situation model here  we don't have the discourse model here yet. Much the same way as we didn't - we don't have the ontology here. Well the ontology we sort of Really. said we would pull these various kinds of properties from the ontology like exhibiting  selling  and so forth. So in some sense it's - it's there. Mm-hmm. But the discourse we don't have it represented at all yet. Yeah. Um This be specific for second year? Um And - and we probably will have uh something like a discourse for endpoint. But if we do it'll have the three values. Hmm? It'll have the E_V_A values if - if we have it. Yeah. Yeah. O_K just for starters and here discourse um For Go-there  probably is true and false  let's say. That's what we talked about. um well  I think um we're looking at the - the little data that we have  so people say how do I get to the castle and this usually means they wanna go there. Mm-hmm. So this should sort of push it Right. in one direction however people also sometimes say how do I get there in order to find out how to get there without wanting to go there. Mm-hmm. And sometimes um people say where is it Mm-hmm. because they wanna know where it is but in most cases they probably Yeah  but that doesn't change the fact that you're - you want these two values. Oh yeah  true. So this is sort of some external thing that takes all the discourse stuff and then says here it's either yep  yay  A_  or nay. Yeah. O_K? And they'll be a y- uh  a user Go-there and maybe that's all  I don't know. Situation Go-there  I mean  because it's - whether it's open or not. Mm-hmm. O_K  good. That definitely interes- @@ Yep. But that now that kind of um what's the word um Hmm. the - that interacts with the uh E_V_A thing if they just wanna view it then it's fine to go there when it's closed whereas if they want to um Right. so Right  so that's - that's where it starts getting to be uh uh essentially more interesting  so what uh Bhaskara says which is completely right is if you know that they're only going to view it then it doesn't matter whether it's closed or not in terms of Mm-hmm. The time of day  right I - well  right. uh uh Mm-hmm. you know  whether - whether you wanna go there. It does matter though if there's like a strike or riot or something. Absolutely there are other situational things that do matter. Right. So yeah  that's what I said just having one situational node may not be enough because this - that node by itself wouldn't distinguish Well i- i- it can have di- various values. Yeah  but we eh you - you're right it might not be enough. Yeah  I mean  see I'm - Well  what - Whoops. I'm thinking that any node that begins with ""Go-there"" is either gonna be true or false. Yeah. Right. Ah. I see that could be. Also  that node  I mean the Go-there s- S_ node would just be fed by separate ones for Mm-hmm. Could be. you know  there's different things  the strikes and the Yeah. N- Like situation traffic and so on. Yeah. Yeah. Yeah  the time of day. So - so now the other thing that Bhaskara eh pointed out is what this says is that uh there sh- should be a link  and this is where things are gonna get very messy from the endpoint uh I guess the final decision maybe the t- they're final re- and  I guess the very bottom endpoint decision uh to the Go-there node. And I - don't worry about layout  I mean then we'll go - we'll go nuts Yeah. Mm-hmm. but Mmm. Maybe we could um have intermediate node that just the Endpoint and the Go-there S_ node sort of fed into? Could be  yeah. Right. Because that's what we  I mean that's why this situation comes up. Yeah. Well the Go-there  actually the Endpoint node could feed - feed into the Go-there S_ Yeah  right. That's right  so the Endpoint node  make that up t- t- Mm-hmm. Yeah. to the Go-there then and again we'll have to do layout at some point  but something like that. Now it's gonna be important not to have loops by the way. I was just gonna Uh really important in - in the belief worl- net world not to have loops uh Yes. How long does it take you to - to compute uh No it's much worse than that. It - if i- loo- It - things don't converge  yeah. uh it - it - it - it - it's not def- i- it's not well defined if you're there are loops  you just R- recursive action? you have to there are all sorts of ways of breaking it up so that there isn't uh Uh but this isn't  this is - this line is just coming from over here. O_K. Yeah. Yeah. Yeah  no it's not a loop yet  I'm just saying we - we  in Mmm. Well  but the good thing is we - no  in we could have loopy belief propagation which we all love. Right. O_K  so anyway  so that's another decision. Uh what's - what's another decision you like? O_K  these have no parents yet  but I guess that sort of doesn't matter. Right? Well  the idea is that you go there  you go comes from something about the user I mean this is sort of This comes from traffic and so forth  yeah. from something about the situation and the uh the discourse is - is a mystery. Sh- Should we just make some Sure  if you want. um if there's parking maybe Mmm Oh who cares. O_K. And if he has seen it already or not Right. and so forth  O_K. Um and discourse is something that sort of should we make a Keith note here? That sort of comes from Keith. Sure. Mm-hmm. Just sort of so we don't forget. Oops. Have to get used to this. O_K  whoops. And then also the discourse endpoint  I - I guess endpoint sub-D_ is - if you wanna make it consistent. Um actually - Wh- ah. Mm-hmm. Um actually is this the - the right way to have it where um go there from the user and go there from the situation just sort of don't know about each other but they both feed the go there decision because I think so. S- isn't the  I mean uh  hmm O_K. Maybe not  a- But that still allows for the possibility of the - of the user model affecting our decision about whether a strike is the sort of thing which is going to keep this user away from - Right. That - all that - that kind of decision making happens at the Go-there node. Uh y- you - yeah you - i- you - if you needed to do that. Uh. If you needed it to do that. Yeah. But uh O_K I was just thinking Yeah. I guess maybe I'm conflating that user node with possible - possible asking of the user you know Ah. hey there's a strike on  uh does that affect whether or not you wanna go or something or Good point  I don't - I don't know how we're going to - t- uh Yeah  so that might not come out of a user model but  you know  directly out of interaction. Right. Uh I gu- yes my curr- you know  don't yeah yeah yeah that's enough. Yeah. Uh My current idea on that would be that each of these decision nodes has questions associated with it. Mm-hmm. And the question wouldn't itself be one of these conditional things O_K. you know  given that you know there's a strike do you still wanna go? Yeah. But uh if you told him a bunch of stuff  then you would ask him do you wanna go? But Mm-hmm. O_K. I think Right  right. trying to formulate the conditional question  that sounds too much. Yeah. Right  sure  O_K. To me. Mm-hmm. Alright  but let me - let - let's stay with this a minute because I want to do a little bit of organization. But Before we get more into details. The organization is going to be that uh the flavor of what's going on is going to be that uh as we s- e- sort of going to this detail indeed Keith is going to - to worry about the various constructions that people might use Mm-hmm. and Johno has committed himself to being the parser wizard  so Alright. what's going to happen is that eventually like by the time he graduates  O_K uh they'll be some sort of system which is able to take the discourse in context and have outputs that can feed the rest of belief-net. I j- wa- I - I assume everybody knows that  I just wanna you know  get closure that Mm-hmm. that'll be the game then  so the semantics that you'll get out of the discourse will be of values that go into the various discourse-based decision nodes. And now some of those will get fancier like mode of transportation and stuff so it isn't by any means uh necessarily a simple thing that you want out. So uh if there is an- and there is mode of transportation And it there's a sort of also a split if you loo- if you blow this up and look at it in more detail there's something that comes from the discourse in terms of what was actually just said what's the utterance go- giving us Yeah. and then what's the discourse history give us. Yeah  well that  well  we'll have to decide Mm-hmm. That's uh two things then . uh how much of th- where that goes. Mmm. Mm-hmm. an- and it's not clear yet. I mean it could be those are two separate things  it could be that the discourse gadget itself integrates em as - which would be my guess that you'd have to do see in order to do reference and stuff like that um you've gotta have both the current discourse and the context to say I wanna go back there  wow  what does that mean and Mm-hmm. Mm-hmm Now. uh Mm-hmm. Alright. So But is th- is this picture that's emerging here just my wish that you have noticed already for symmetry or is it that we get for each - each decision on the very bottom we sort of get the sub-E_  sub-D_  sub-U_ and maybe a sub-O_ - ""O_"" for ""ontology"" um meta node I don't know. but it might just It could be. could be so this This is - this is getting into the thing I wanna talk about next  which is s- if that's true uh how do we wanna combine those? O- or when it's true? but this eh w- wou- wou- would be nice though that  you know  we only have at most four at the moment Yeah. um arrows going f- to each of the uh bottom decisions. Yeah. And four you - we can handle. No. It's too much? Yeah. Well i- i- it see- i- if it's fou- if it's four things and each of them has four values it turns out to be a big C_P_T  it's not s- completely impossi- I mean it's - it's not beyond what the system could solve but it's probably beyond what we could actually Right  true. uh write down. or learn. Uh but  you know it's four to the fourth. It's pretty big. Uh. Two fifty-six  is that what that Yeah. Yeah  I mean it's and I don't think it's gonna g- e- I don't think it'll get worse than that by the way  so le- that's a - that's a good Mmm yeah. But - but four - didn't we decide that all of these had true or false? So is - it's four Uh for go there  but not f- but not for - the other one's three values for endpoint already. Yeah. Yeah  I mean you need actually three to the five because uh well I mean if - if it has four inputs and then it itself has three values so Right. I mean it can get big fast. Um for endpoint? No it's - it's sh- E_V- it's the E_V_A. yeah  down here  but this one only has two. No it still has three  E_V_A. No. Since ta- they will still have three. Each - so you're uh uh from each point of view you're making the same decision. Mm-hmm. So Mm-hmm. from the point of view of the ob- of the entity Want to view that  yeah yeah. C- sl- yeah Yeah. This - and also  I mean  the other places where  like for example consider endpoint view  it has inputs coming from user budget  user thrift Right. so even Those are not necessarily binary. S- so we're - we're gonna have to use some t- care in the knowledge engineering to not have this explode. And in fact I think it doesn't in the sense that um Read it  you know actually with the underlying semantics and stuff I think it isn't like you have two hundred and fifty-six different uh ways of - of thinking about whether this user wants to go to some place. Alright. So we - we just have to figure out what the regularities are and and code them. But um What I was gonna suggest next is maybe we wanna work on this a little longer but I do want to also talk about the thing that we started into now of uh well it's all fine to say all these arrows come into the si- same place what rule of combination is used there. So th- yes they - so these things all affect it  how do they affect it? Mm-hmm. Right. And belief-nets have their own beliefs about uh what are good ways to do that. So is it - it's - it's clearer n- clear enough what the issue is  right? Right. So do we wanna switch that now or we wanna do some more of this? R- basically w- we just need to sort of in order to get some closure on this figure out how we're gonna get this picture sort of uh completely messy. Well  here - he- here's one of the things that - that I th- you sh- you - no  I don't know how easy it is to do this in the interface but you - it would be great if you could actually just display at a given time uh all the things that you pick up  you click on ""endpoint""  O_K Mm-hmm. and everything else fades and you just see the links that are relevant to that. And I does anybody remember the GUI on this? Uh d- I would almost say the other way to do that would be to open u- or make you know N_many belief-nets and then open them Mm-hmm. every time you wanted to look at a different one vers- cuz uh It's probably pretty easy do it - to do it in H_T_M_L  just - Yeah  but Uh H_T_M_L? Yeah I have each of these thing- each of the end belief-nets be - be a page and then you click on the thing and then O_K. Yeah li- consider that it's respective  but the - well the b- anyway so uh it clear that even with this if we put in all the arrows nobody is gonna be able to read the diagram. Yeah. Alright  so e- we have to figure out some eh eh uh basically display hack or something to do this because anyway I - I - let me consi- suggest that's a s- not a first-order consideration  we have two first-order considerations which is what are the uh influences A_  and B_ how do they get combined mathematically  how do we display them is an issue  but um I don't  yeah I just don't think this has been designed to support something like that. Yeah. Yeah  I - I mean  it might soon  if this is gonna be used in a serious way like java base then it might soon be necessary to uh start modifying it for our purposes. Right. Yeah  and Um I - that seems like a perfectly feasible thing to get into  but um we have to know what we want first. O_K  so why don't you tell us a little bit about So decision nodes and what - what the choices might be for these? Ah  sorry. I guess that's You can technically wear that as you're talking. Yeah  it's right  I guess I can do that. Darn. Put it in your  yeah. I guess this board works fine. So um recall the basic problem which is that um you have a belief-net and you have like a lot of different nodes all contributing to one node. Right? So as we discussed specifying this kind of thing is a big pain and it's so- will take a long time to write down because for example if these S have three possibilities each and this has three possibilities then you know you have two hundred and forty-three possibilities which is already a lot of numbers to write down. So what um helps us in our situation is that these all have values in the same set  right? These are all like saying E_V or A_  right? So it's not just a generalized situation like I mean basically we wanna just take a combination of - we wanna view each of these as experts ea- who are each of them is making a decision based on some factors and we wanna sort of combine their decisions and create you know  um sorta weighted Hmm. combination. ROVER  the ROVER decision. The what decision? ROVER. All of their outputs combined to make a decision. Hmm. Yeah. Yeah. So the problem is to specify the uh so the conditional property of this given all those  right? That's the way belief-nets are defined  like each node given its parents  right? So um that's what we want  we want for example P_ of um let's call this guy Y_ and let's call these X_one  X_two X_N  right. So we want probability that Y_ equals  you know  for example um E_ given that these guys are I'll just refer to this as like X_ um hat or something  uh the co- like all of them? Given that for example the data says you know  A_  V_  A_  E_  or something right? Yep. So we would like to do this kind of combination. Alright  so um Is that uh I - yeah  I just wanna make sure everybody is with us I think so  yeah. before he goes on. It's - it's cl- e- is - is it clear what he wants to compute? Right. Mm-hmm. So  right. So Basically um what we don't wanna do is to for every single combination of E_ and V_ and A_ and every single letter E_  s- give a number because that's Mm-hmm. obviously not desirable. What we wanna do is find some principled way of um saying what each of these is and we want it to be a valid probability distribution  so we want it to um add up to one  right? Hmm. So those are the two things that we uh need. So what uh I guess  what Jerry suggested earlier was basically that we  you know view these guys as voting and we just take the uh we essentially take um averages  right? So for example here two people have voted for A_  one has voted for V_  and one has voted for E_  so we could say that the probabilities are  you know  probability of being E_ is one over four  because one person voted for E_ out of four and similarly  probability of so this is probability of E_ s- and then probability of A_ given all that is um two out of four and probability of V_ is one out of four. Right? So that's step - that's the uh yeah that's the - that's the basic uh thing. Now Um Yeah. Is that all O_K? And that one outcome  that's What? it's X_ - X_one voted for A_ X_two voted for V_ and so forth? Mm-hmm. Y- right. Yeah. Yep. Yeah. That's the outcome. S- so this assumes symmetry and equal weights and all this sort of things  which Mm-hmm. may or may not be a good assumption  so that Yeah. Right. Yeah. So step two is um right. So we've assumed equal weights whereas it might turn out that you know  some w- be that for example  what the um the actual the uh verbal content of what the person said  like what uh what might be uh somehow more uh important than the uh X_one matters more i- than X_two or Right. Sure  so we don't wanna like give them all equal weight so currently we've been giving them all weight one fourth so we could replace this by uh W_one  W_two  W_three  and W_four right? Hmm. And in order for this to be a valid probability distribution for each um X_hat  we just need that the W_'s sum to one. So they can be for example  you know you - you could have point one  point three  point two  and point four  say. That's one. And that'd be one. So that um also seems to work fine. And uh So I jus- just to make sure I understand this  so in this case um we would still compute the average? You'd compute the weighted average  so O_K  so - the probability of E_ would be uh so it'd be so in this case the probability that Y_ equals A_ would be uh Point three. W_ one times - or A_ or - let's see  one full quarter times point one e- Not one quarter  so @@ three. No. these numbers have been replaced with point one  point three  point two  and point four. So it would be  in this case So you can view these as gone. you just add up the - Yeah. So it would - it would - in this case  it would be - probability of A_ would be point three. O_K. Probability of B_ - yeah  B_ would be point three and the probability of E_ would be point four. O_K. Yeah. If that - if that was the voting plan. Yeah. O_K. So  alright. So this is uh step two. So the next possibility is that um we've given just a single weight to each expert  right  whereas it might be the case that um in certain situations one of the experts is more uh reliable and in certain situations the other expert is more reliable. So the way this is handled is by what's called a mixture of experts  so what you can have is you augment these diagrams like this so you have a new thing called ""H_""  O_K? This is a hidden variable. And what this is is it gets its input from X_one  X_two  X_three  and X_four  and what it does is it decides which of the experts is to be trusted in this particular situation. Right? And then these guys all come here. O_K. So this is sightly uh more complicated. So what's going on is that um this H_ node looks at these four values of those guys and it decides in given these values which of these isn't likely to be more reliable or most reliable. So H_ produces some you know  it produces a number  either one  two  three  or four  in our situation  right? Now this guy he looks at the value of H_ say it's two  and then he just selects the uh @@ - Yeah  so that's one thing you can do  but the other - So  are you going to go on and say @@ - thing. That's all there is to say  I guess about it. No it isn't. Because the other thing you can do is you - So this - in this case  a mixture of experts just picks one. Mm-hmm. Right  so you can have a mixture that But - A weighted one. So what you can do  if you - if you need it is to say that - I mean here there's a fourth parameter. There's a fifth parameter. O_K? which is the H_ parameter. So let's suppose the H_ says in situ- @@ different uh  alpha. O_K  in situation-alpha  that's the weighting factor  but in situation- beta  it's some other weighting factor. Right. So you still combine these with weight  but the weights aren't always the same. That this guy uh detects for you that this is a situation in which you should weight this more or less. So - so the function of the thing that comes out of H_ is very different from the function of the other inputs. Correct. It's actually s- it - it - It's driving how the other four are interpreted. In- indeed. O_K. Yeah. It's - it's - it's conditioning how the other four are @@ . And what people do  in another literature  is learn all this stuff. So you learn the weights  you can learn the expert  et cetera. And if you have enough data  then it makes perfectly good sense to do this. Yeah. So H_ passes a vector on to the next node? Well now  y- it - it - Well it could  or you co- could just pass other - It could. It could? A vector of the weights as the se- oh . No  no. No. Not normally. It would just pass a - a selection. So - so it could pass either - Yeah  it could Sorry? Well a vector with three zeros and one one  right? Well  it could. Yeah. If - if - @@ Yeah  it could do that. O_K. Uh  you know  uh  uh  a ma- a binary mask. Oh it's basically to tell the bottom node which one of the situations that it's in We- yeah. Or w- in this case  wh- which one of these guys to pay attention to. or which one of the weighting systems Right  so W- I was just  if you wanted to pay attention to more than one you could pass a w- a weighting s- system though too  couldn't you? I mean the way you desc- I mean  the way you - Well  that - yo- You could but I think what - that's not - that's harder than to just say they're a discrete number of situations and this says  you know  hey  you're in situation-alpha. O_K. Um Does H_ have to have another input to tell it alpha  beta  whatever  or is the - No. that's determined by what the experts are saying  like the type of situ- O_K. Yeah. Yeah. Or in @@ definitely could have - have other - it absolutely could have other inputs from something that has n- doesn't have anything directly to do with these experts. Hmm. O_K. O_K. And it's s- it actually often does. So it is - I mean It - it just seems that like without that - that outside input that you've got a situation where  you know  like if - if uh X_one says no  you know  a low value coming out of X_on- or i- if X_one says no then ignore X_one  you know  I mean that seems like Yeah. Yeah. Yeah  well could be things like if X_two and X_three say yes then i- ignore X_one also. that'd be weird  right? Oh  O_K. Yeah. O_K. Or if X_two is greater than X_- I could imagine i- all sorts of garbage. Alright  right. And so these - so this is - this is one collection of technology that we could bring to bear. Yeah. Oh The situations that H_ has  are they built into the net or Depends. So @@ you - O_K  so they - they could either be hand coded or learned or Yeah. O_K. Yeah. It depends on - on - on  you know  what situations you're in. Based on training data  O_K. Mm-hmm. And there're bits @@ actually Michael Jordan did a lot of nice work  uh  earlier on sit- situations which you could actually train these up and stuff. Um  I don't think we're gonna be in a situation where realistically we're gonna have enough data to actually train these things  uh  anytime soon. Yeah. But  I still think it's a nice idea if the architecture supports it. Yes. So that  you know  you say  ""here's the design and if enough data comes along uh  you co- you could use it - use the technology to - to do that."" I still have trouble understanding the very first step  maybe just because I'm approaching it from the very wrong end. I don't know. So  this is true for the case that X_one voted for A_  X_two voted for B_   Yes. and so forth. Now  X_one could potentially also vote E_ . That wo- that would be another sort of i- Absolutely. So you specify one of these things for every one of those possi- possible situations. Oh yeah. Yeah. Um Well  I mean to learn them we need data  where are we gonna get data? Well I mean we need data with people intentions  right? Which is slightly tricky. Right  right. Right. Uh-huh. Mm-hmm. But what's the data about like  are we able to get these nodes from the data? Like how thrifty the user is  or do we have access to that? Mm-hmm. Oh right. Oh good. Yeah. O_K. Mm-hmm. Mm-hmm. O_K. Yeah  but that's my question  like how do we - I mean  how do we have data about something like um um endpoint sub-E_  or endpoint sub uh you know s- S_? Well  basically you would say  based on - in this dialogue that we have Mmm. which one of the things that they said eh whether it was the entity relations or whatever Mmm. was the thing that determined what mode it was  right? So this is what we wanna learn. Yep. Right. Hmm. Yeah. I don't think  well you have a - can you bring up the function thing? Um w- where is the thing that allows you to sort of That's on the added variable  isn't it? Oh function properties  is that it? Hmm  I guess not. Yeah  that's No. Right. O_K. And um it so e- either it'll allow us to do everything which I think is unlikely  I think more likely it'll allow us to do very few of these things and in that case we'll have to um just write up little things that allow you to um create such C_P_U's on your own in the java base format. Yeah. Yeah. Yeah  I was assuming that's what we'd always do because yeah I was assuming that's what we'd always do  it's Right. Yeah. Ah. Well in terms of java base I think it's basically what you see is what you get in I don't yeah  I would be surprised if it supports anything more than what we have right here. So Yeah. Yeah. By the way um uh just talking about uh about that general end of things uh is there gonna be data soon from what people say when they're interacting with the system and so on? Like  I mean  what kind of questions are being given - being asked? Cuz - O_K. Yeah yeah. O_K. O_K. Fey  you mean. O_K. O_K. O- O_K. O_K. I'm just wondering  because in terms of  you know  I mean uh w- the figure - I was thinking about this figure that we talked about  fifty constructions or whatever that's uh that's a whole lot of constructions and um you know  I mean one might be f- fairly pleased with getting a really good analysis of five maybe ten in a summer so  I mean I know we're going for sort of a rough and ready. Mm-hmm. Mm-hmm. O_K. O_K. I mean  I - I - I - I was - uh I was talking about the  you know  if you wanted to do it really in detail and we don't really need all the detail for what we're doing right now but anyway in terms of just narrowing that task you know which fifty do I do  I wanna see what people are using  so Well  it will inspire me. Right  sure sure. Right. Yeah  sure. Sure. Yeah. O_K. Touche. Good enough. ",Another weekly meeting on ICSI's Meeting Recorder Group at Berkeley  though the members are joined by a visiting researcher. The groups regulars reported progress on their work on mean subtraction  noise estimation  voice activity detection and the Vector Taylor Series. While on these topics  related areas discussed included recognition window length  training versus test set sizes  artificial distortion and latency concerns. Speaker fn002 is soon to be leaving the group  and so she will choose her best setup  run a complete set of experiments  and write up her work  procedure and results for next week. New filters introduced to reduce latency by mn052   performed slightly worse than those they replaced. Whereas mn007 has added some latency to the process which he feels he can reduce. Speaker me026 has been working on mean subtraction  his most recent results are suspiciously poor  and he is attempting to integrate the method into the SmartKom system. Speaker mn052 has been looking at noise estimation  because he was getting better results with one data set than another. Speaker mn007 has been looking at VAD performance  and getting some good results  though nothing that hasn't been produced before. Speaker fn002 has been running VTS experiments  but her results aren't particularly impressive. Speaker me006 has been working on the proposal for his thesis and outlined his idea. 
